{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.CROP标签需要删除\n",
    "2.除了B、I标签，还有结束E标签、单个字符实体S标签需要添加上\n",
    "3./usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, msg_start, len(result))\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text_in_file(file_path, old_text, new_text):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    content = content.replace(old_text, new_text)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def process_files_in_directory(directory, old_text, new_text):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') or file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                replace_text_in_file(file_path, old_text, new_text)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "directory_path = 'G:/python_codes/Principles_of_Computer_Construction_KG/corpus'  # 替换为你的目录路径\n",
    "old_text = 'CROP'\n",
    "new_text = 'CORP'\n",
    "process_files_in_directory(directory_path, old_text, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unlabled_shots_from_corpus(corpus: dict, blank_seq: list):\n",
    "    for n in blank_seq:\n",
    "        for idx in range(n * 8, (n + 1) * 8):\n",
    "            try:\n",
    "                del corpus[str(idx)]\n",
    "            except:\n",
    "                pass\n",
    "    return corpus\n",
    "\n",
    "def remove_null_dict_from_labels(corpus_labels: dict, blank_seq: list):\n",
    "    for n in blank_seq:\n",
    "        del corpus_labels[n]\n",
    "    return corpus_labels\n",
    "\n",
    "def initial_dataset(corpus: dict, corpus_labels: dict):\n",
    "    dataset_dict = {\"corpus\": [], \"labels\": []}\n",
    "    for key, value in corpus.items():\n",
    "        dataset_dict['corpus'].append(value)\n",
    "    for key, value in corpus_labels.items():\n",
    "        for i in value:\n",
    "            dataset_dict['labels'].append(i)\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels(dataset_dict: dict):\n",
    "    dataset_dict['new_labels'] = []\n",
    "    for i, sentence in enumerate(dataset_dict['corpus']):\n",
    "        tmp = ['O'] * len(sentence)\n",
    "        labels = dataset_dict['labels'][i]['output']\n",
    "        for label in labels:\n",
    "            for idx in range(label['start'], label['end']):\n",
    "                tmp[idx] = f'I-{label['type']}'\n",
    "        dataset_dict['new_labels'].append(tmp)\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"G:\\python_codes\\Principles_of_Computer_Construction_KG\\corpus\\texts_sentence.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.loads(f.read())\n",
    "\n",
    "with open(r\"G:\\python_codes\\Principles_of_Computer_Construction_KG\\corpus\\corpus_labels.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_labels = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_seq = []\n",
    "for k, v in corpus_labels.items():\n",
    "    if not v:\n",
    "        blank_seq.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5653"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = remove_unlabled_shots_from_corpus(corpus, blank_seq)\n",
    "new_corpus_labels = remove_null_dict_from_labels(corpus_labels, blank_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = initial_dataset(new_corpus, new_corpus_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'计算机系统不同于一般的电子设备，它是一个由硬件、软件组成的复杂的自动化设备'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['corpus'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'PROD', 'start': 0, 'end': 5, 'prob': 0.12411527, 'span': '计算机系统'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['labels'][0]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_dict['corpus']) == len(dataset_dict['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练集：验证集：测试集 = 8：1：1\n",
    "- 先随机从数据集中取出1000条数据\n",
    "- 对这1000条数据进行校对（设立一个新的标签PCC。没有识别出来的要标记出来，改正识别错的）\n",
    "- 校对完之后，建立一个词典\n",
    "- 用这1000条数据进行预训练\n",
    "- 再拿出1000条数据用第一次训练后的模型进行标注，再进行一次人工标注后，更新词典。到此为止完成第一次迭代，下面重复这个过程，直到训练集的数据全部标注完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_slice = int(len(dataset_dict['corpus']) * 0.8)\n",
    "test_set_sclice = int(len(dataset_dict['corpus']) * 0.1)\n",
    "validset_slice = len(dataset_dict['corpus']) - trainset_slice - test_set_sclice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4147"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\first_epoch_trainset.json', 'r', encoding='utf-8') as f:\n",
    "    first_epoch_trainset = json.loads(f.read())\n",
    "all_idx = list(range(trainset_slice))\n",
    "first_epoch_idx = first_epoch_trainset['ids']\n",
    "second_epoch_idx = list(\n",
    "    random.sample([i for i in all_idx if i not in first_epoch_idx], 1000)\n",
    ")\n",
    "third_epoch_idx = list(\n",
    "    random.sample([i for i in all_idx if i not in (first_epoch_idx + second_epoch_idx)], 1000)\n",
    ")\n",
    "forth_epoch_idx = list(\n",
    "    random.sample([i for i in all_idx if i not in (first_epoch_idx + second_epoch_idx + third_epoch_idx)], 1000)\n",
    ")\n",
    "fifth_epoch_idx = list(set(all_idx) - set(first_epoch_idx) - set(second_epoch_idx) - set(third_epoch_idx) - set(forth_epoch_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_epoch_trainset(idx, dataset_dict, epoch='first'):\n",
    "    epoch_trainset = {\n",
    "        'ids': idx,\n",
    "        \"corpus\": [dataset_dict[\"corpus\"][i] for i in idx],\n",
    "        \"labels\": [dataset_dict[\"labels\"][i] for i in idx],\n",
    "    }\n",
    "    with open(f'G:/python_codes/Principles_of_Computer_Construction_KG/output/{epoch}_epoch_trainset.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(epoch_trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_epoch_trainset(second_epoch_idx, dataset_dict, epoch='second')\n",
    "generate_epoch_trainset(third_epoch_idx, dataset_dict, epoch='third')\n",
    "generate_epoch_trainset(forth_epoch_idx, dataset_dict, epoch='forth')\n",
    "generate_epoch_trainset(fifth_epoch_idx, dataset_dict, epoch='fifth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(fifth_epoch_idx) + len(forth_epoch_idx) + len(third_epoch_idx) + len(second_epoch_idx) + len(first_epoch_idx)) == len(all_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = {\n",
    "        'corpus': dataset_dict['corpus'][trainset_slice:trainset_slice + test_set_sclice],\n",
    "        'labels': dataset_dict['labels'][trainset_slice:trainset_slice + test_set_sclice],\n",
    "    }\n",
    "validset = {\n",
    "        'corpus': dataset_dict['corpus'][trainset_slice + test_set_sclice:],\n",
    "        'labels': dataset_dict['labels'][trainset_slice + test_set_sclice:],\n",
    "    }\n",
    "\n",
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\test_set.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(test_set))\n",
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\validset.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_epoch_trainset_idx = list(\n",
    "    random.sample(range(0, trainset_slice), 1000)\n",
    ")\n",
    "first_epoch_trainset = {\n",
    "    'ids': first_epoch_trainset_idx,\n",
    "    \"corpus\": [dataset_dict[\"corpus\"][i] for i in first_epoch_trainset_idx],\n",
    "    \"labels\": [dataset_dict[\"labels\"][i] for i in first_epoch_trainset_idx],\n",
    "}\n",
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\first_epoch_trainset.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(first_epoch_trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\first_epoch_trainset_v2.json', 'r', encoding='utf-8') as f:\n",
    "    first_epoch_trainset = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_template = {\n",
    "    \"data\": {\"text\": None},\n",
    "    \"predictions\": [\n",
    "        {\n",
    "            \"model_version\": \"nlp_raner_named-entity-recognition_chinese-large-generic\",\n",
    "            \"score\": 0.94,\n",
    "            \"result\": [],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "result_template = {\n",
    "        'id': None,\n",
    "        'from_name': 'label',\n",
    "        'to_name': 'text',\n",
    "        \"type\": \"labels\",\n",
    "        \"value\": {\n",
    "              \"start\": None,\n",
    "              \"end\": None,\n",
    "              \"score\": None,  # prob\n",
    "              \"text\": None,  # span\n",
    "              \"labels\": [\n",
    "                None  # type\n",
    "              ]\n",
    "            }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pre_labeled_dataset(dataset: dict):\n",
    "    pre_labeled_data = []\n",
    "    id_count = 1\n",
    "\n",
    "    for i, labels in enumerate(dataset[\"labels\"]):\n",
    "\n",
    "        json_template_tmp = {\n",
    "            \"data\": {\"text\": None},\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                    \"model_version\": \"nlp_raner_named-entity-recognition_chinese-large-generic\",\n",
    "                    \"score\": 0.86,\n",
    "                    \"result\": [],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        json_template_tmp[\"data\"][\"text\"] = dataset[\"corpus\"][i]\n",
    "\n",
    "        for label in labels[\"output\"]:\n",
    "\n",
    "            json_template_tmp[\"predictions\"][0][\"result\"].append(\n",
    "                {\n",
    "                    \"id\": str(id_count),\n",
    "                    \"from_name\": \"label\",\n",
    "                    \"to_name\": \"text\",\n",
    "                    \"type\": \"labels\",\n",
    "                    \"value\": {\n",
    "                        \"start\": label[\"start\"],\n",
    "                        \"end\": label[\"end\"],\n",
    "                        \"score\": label[\"prob\"],  # prob\n",
    "                        \"text\": label[\"span\"],  # span\n",
    "                        \"labels\": [label[\"type\"]],  # type\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "            id_count += 1\n",
    "\n",
    "        pre_labeled_data.append(json_template_tmp)\n",
    "\n",
    "    return pre_labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\first_epoch_v2_pre_labeled_data.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(generate_pre_labeled_dataset(first_epoch_trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\pre_labeled_test_set.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(generate_pre_labeled_dataset(test_set)))\n",
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\pre_labeled_validset.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(generate_pre_labeled_dataset(validset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\test_set_v2.json', 'r', encoding='utf-8') as f:\n",
    "    test_set_v2 = json.loads(f.read())\n",
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\validset_v2.json', 'r', encoding='utf-8') as f:\n",
    "    validset_v2 = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\pre_labeled_test_set_v2.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(generate_pre_labeled_dataset(test_set_v2)))\n",
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\pre_labeled_validset_v2.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(generate_pre_labeled_dataset(validset_v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_set = set()\n",
    "for i, labels in enumerate(first_epoch_trainset['labels']):\n",
    "    entitys = [label['span'] for label in labels['output']]\n",
    "    entitys_type = [label['type'] for label in labels['output']]\n",
    "    for item in dict(zip(entitys, entitys_type)).items():\n",
    "        entity_set.add(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = list(entity_set)\n",
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\first_epoch_entities.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(entity_list))\n",
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\first_epoch_entities.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(str(entity_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_dict = process_labels(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\datasets_backup.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(dataset_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'G:\\python_codes\\Principles_of_Computer_Construction_KG\\output\\datasets.json', 'r', encoding='utf-8') as f:\n",
    "    test = json.loads(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
