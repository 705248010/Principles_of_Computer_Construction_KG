分布式系统：
简介播报编辑分布式系统示例在一个分布式系统中，一组独立的计算机展现给用户的是一个统一的整体，就好像是一个系统似的。系统拥有多种通用的物理和逻辑资源，可以动态的分配任务，分散的物理和逻辑资源通过计算机网络实现信息交换。系统中存在一个以全局的方式管理计算机资源的分布式操作系统。通常，对用户来说，分布式系统只有一个模型或范型。在操作系统之上有一层软件中间件（middleware）负责实现这个模型。一个著名的分布式系统的例子是万维网（World Wide Web），在万维网中，所有的一切看起来就好像是一个文档（Web页面）一样。 [1]在计算机网络中，这种统一性、模型以及其中的软件都不存在。用户看到的是实际的机器，计算机网络并没有使这些机器看起来是统一的。如果这些机器有不同的硬件或者不同的操作系统，那么，这些差异对于用户来说都是完全可见的。如果一个用户希望在一台远程机器上运行一个程序，那么，他必须登陆到远程机器上，然后在那台机器上运行该程序。 [1]分布式系统和计算机网络系统的共同点是：多数分布式系统是建立在计算机网络之上的，所以分布式系统与计算机网络在物理结构上是基本相同的。 [1]他们的区别在于：分布式操作系统的设计思想和网络操作系统是不同的，这决定了他们在结构、工作方式和功能上也不同。网络操作系统要求网络用户在使用网络资源时首先必须了解网络资源，网络用户必须知道网络中各个计算机的功能与配置、软件资源、网络文件结构等情况，在网络中如果用户要读一个共享文件时，用户必须知道这个文件放在哪一台计算机的哪一个目录下；分布式操作系统是以全局方式管理系统资源的，它可以为用户任意调度网络资源，并且调度过程是“透明”的。当用户提交一个作业时，分布式操作系统能够根据需要在系统中选择最合适的处理器，将用户的作业提交到该处理程序，在处理器完成作业后，将结果传给用户。在这个过程中，用户并不会意识到有多个处理器的存在，这个系统就像是一个处理器一样。 [1]内聚性是指每一个数据库分布节点高度自治，有本地的数据库管理系统。透明性是指每一个数据库分布节点对用户的应用来说都是透明的，看不出是本地还是远程。在分布式数据库系统中，用户感觉不到数据是分布的，即用户不须知道关系是否分割、有无副本、数据存于哪个站点以及事务在哪个站点上执行等。 [1]分类播报编辑分布式计算机系统的体系结构可用处理机之间的耦合度为主要标志来加以描述。耦合度是系统模块之间互联的紧密程度，它是数据传输率、响应时间、并行处理能力等性能指标的综合反映，主要取决于所选用体系结构的互联拓扑结构和通信链路的类型。 [2]按地理环境衡量耦合度，分布式系统可以分为机体内系统、建筑物内系统、建筑物间系统和不同地理范围的区域系统等，它们的耦合度依次由高到低按应用领域的性质决定耦合度，可以分成三类： [2]第一种是面向计算任务的分布并行计算机系统和分布式多用户计算机系统，它们要求尽可能高的耦合度，以便发展成为能分担大型计算机和分时计算机系统所完成的工作。 [2]第二种是面向管理信息的分布式数据处理系统。耦合度可以适当降低。 [2]第三种是面向过程控制的分布式计算机控制系统。耦合度要求适中，当然对于某些实时应用，其耦合度的要求可能很高。 [2]特征播报编辑分布式系统是多个处理机通过通信线路互联而构成的松散耦合的系统。从系统中某台处理机来看，其余的处理机和相应的资源都是远程的，只有它自己的资源才是本地的。至今，对分布式系统的定义尚未形成统一的见解。一般认为，分布式系统应具有以下四个特征： [3](1)分布性。分布式系统由多台计算机组成，它们在地域上是分散的，可以散布在一个单位、一个城市、一个国家，甚至全球范围内。整个系统的功能是分散在各个节点上实现的，因而分布式系统具有数据处理的分布性。 [3](2)自治性。分布式系统中的各个节点都包含自己的处理机和内存，各自具有独立的处理数据的功能。通常，彼此在地位上是平等的，无主次之分，既能自治地进行工作，又能利用共享的通信线路来传送信息，协调任务处理。 [3](3)并行性。一个大的任务可以划分为若干个子任务，分别在不同的主机上执行。 [3](4)全局性。分布式系统中必须存在一个单一的、全局的进程通信机制，使得任何一个进程都能与其他进程通信，并且不区分本地通信与远程通信。同时，还应当有全局的保护机制。系统中所有机器上有统一的系统调用集合，它们必须适应分布式的环境。在所有CPU上运行同样的内核，使协调工作更加容易。 [3]优缺点播报编辑优点(1)资源共享。若干不同的节点通过通信网络彼此互联，一个节点上的用户可以使用其他节点上的资源，如分布式系统允许设备共享，使众多用户共享昂贵的外部设备，如彩色打印机；允许数据共享，使众多用户访问共用的数据库；可以共享远程文件，使用远程特有的硬件设备（如高速阵列处理器），以及执行其他操作。 [3](2)加快计算速度。如果一个特定的计算任务可以划分为若干个并行运行的子任务，则可把这些子任务分散到不同的节点上，使它们同时在这些节点上运行，从而加快计算速度。另外，分布式系统具有计算迁移功能，如果某个节点上的负载太重，则可把其中一些作业移到其他节点去执行，从而减轻该节点的负载。这种作业迁移称为负载平衡。 [3](3)可靠性高。分布式系统具有高可靠性。如果其中某个节点失效了，则其余的节点可以继续操作，整个系统不会因为一个或少数几个节点的故障而全体崩溃。因此，分布式系统有很好的容错性能。 [3]系统必须能够检测节点的故障，采取适当的手段，使它从故障中恢复过来。系统确定故障所在的节点后，就不再利用它来提供服务，直至其恢复正常工作。如果失效节点的功能可由其他节点完成，则系统必须保证功能转移的正确实施。当失效节点被恢复或者修复时，系统必须把它平滑地集成到系统中。 [3](4)通信方便、快捷。分布式系统中各个节点通过一个通信网络互联在一起。通信网络由通信线路、调制解调器和通信处理器等组成，不同节点的用户可以方便地交换信息。在低层，系统之间利用传递消息的方式进行通信，这类似于单CPU系统中的消息机制。单CPU系统中所有高层的消息传递功能都可以在分布式系统中实现，如文件传递、登录、邮件、Web浏览和远程过程调用( Remote Procedure call，RPC)。 [3]分布式系统实现了节点之间的远距离通信，为人与人之间的信息交流提供了很大方便不同地区的用户可以共同完成一个项目，通过传送项目文件，远程登录进入对方系统来运行程序，如发送电子邮件等，协调彼此的工作。 [3]缺点尽管分布式系统具备众多优势，但它也有自身的缺点，主要是可用软件不足，系统软件、编程语言、应用程序以及开发工具都相对很少。此外，还存在通信网络饱和或信息丢失和网络安全问题，方便的数据共享同时意味着机密数据容易被窃取。虽然分布式系统存在这些潜在的问题，但其优点远大于缺点，而且这些缺点也正得到克服。因此，分布式系统仍是人们研究、开发和应用的方向。应用播报编辑分布式系统被用在许多不同类型的应用中。以下列出了一些应用。对这些应用而言，使用分布式系统要比其他体系结构如处理机和共享存储器多处理机更优越： [4]并行原则上，并行应用也可以在共享存储器多处理机上运行，但共享存储器系统不能很好地扩大规模以包括大量的处理机。HPCC（高性能计算和通信）应用一般需要一个可伸缩的设计，这种设计取决于分布式处理。 [4]容错应用因为每个PE是自治的，所以分布式系统更加可靠。一个单元或资源（软件或硬件）的故障不影响其他资源的正常功能。 [4]固有的应用许多应用是固有分布式的。这些应用是突发模式（burstmode）而非批量模式（bulk mode）。这方面的实例有事务处理和Internet Javad，程序。 [4]这些应用的性能取决于吞吐量（事务响应时间或每秒完成的事务数）而不是一般多处理机所用的执行时间。 [4]对于一组用户而言， 分布式系统有一个特别的应用称为计算机支持的协同工作（Computer Supported Cooperative Working，CSCW）或群件（groupware）， 支持用户协同工作。另一个应用是分布式会议， 即通过物理的分布式网络进行电子会议。同样，多媒体远程教学也是一个类似的应用。 [4]为了达到互操作性，用户需要一个标准的分布式计算环境，在这个环境里，所有系统和资源都可用。 [4]DCE（分布式计算环境）是OSF（开放系统基金会）开发的分布式计算技术的工业标准集。它提供保护和控制对数据访问的安全服务、容易寻找分布式资源的名字服务、以及高度可伸缩的模型用于组织极为分散的用户、服务和数据。D C E可在所有主要的计算平台上运行， 并设计成支持异型硬件和软件环境下的分布式应用。 [4]DCE已经被包括TRANSVARL在内的一些厂商实现。TRANSVARL是最早的多厂商组（multi vendor team）的成员之一，它提出的建议已成为DCE体系结构的基础。在中可以找到利用DCE开发分布式应用的指南。 [4]一些其它标准基于一个特别的模型，比如CORBA（公用对象请求代理程序体系结构），它是由OMG （对象管理组）和多计算机厂商联盟开发的一个标准。CORBA使用面向对象模型实现分布式系统中的透明服务请求。 [4]工业界有自己的标准，比如微软的分布式构件对象模型（DCOM）和Sun Microsystem公司的Java Beans。 [4]与计算机网络异同播报编辑分布式计算机系统与计算机网络既有类似之处又有不同点，其主要的异同如下： [5](1)在计算机网络中，每个用户或任务通常只使用一台计算机，若要利用网络中的另一台计算机，则需要远程注册。在分布式计算机系统中，用户进程在系统内各个计算机上动态调度，并根据运行情况由分布式操作系统动态地、透明地将机器分配给用户进程或任务。 [5](2)在计算机网络中，用户知道它们的文件存放在何处，并用显示的文件传输命令在机器之间传送文件。在分布式计算机系统中，文件的放置由操作系统管理，用户可用相同方式访问系统中的所有文件而不管它们位于何处。 [5](3)在计算机网络中，各结点计算机均有自己的操作系统，资源归局部所有并被局部控制，网络内的进程调度是通过进程迁移和数据迁移实现的。在分布式计算机系统中，每个场点上运行一个局部操作系统，执行的任务可以是独立的，可以是某任务的一个部分，也可以是其他场点上的(部分)任务，且各场点相互协同，合作平衡系统内的负载。 [5](4)在计算机网络中，系统几乎无容错能力。在分布式计算机系统中有系统自动重构、适度降级使用及错误恢复功能。 [5](5)两者透明性的程度和级别不同。 [5](6)就资源共享而言，计算机网络和分布式计算机系统是类似的。 [5]系统设计难点播报编辑虽然分布式系统具有很多优点，然而由于分布式系统自身的特点及应用环境的复杂性，分布式系统设计有如下的很多难题需要解决: [6]1.部分失效问题由于分布式系统通常由若干部分组成，各个部分由于各种原因可能发生故障，如硬件故障、软件错误及错误操作等。如果一个分布式系统不对这些故障进行有效的处理，系统某一组成部分的故障可能导致整个系统的瘫痪。 [6]2.性能和可靠性过分依赖于网络由于分布式系统是建立在网络之上的，而网络本身是不可靠的，可能经常发生故障，网络故障可能导致系统服务的终止。另外，网络超负荷会导致性能的降低，增加系统的响应时间。 [6]3.缺乏统一控制一个分布式系统的控制通常是一个典型的分散控制，没有统一的中心控制。因此，分布式系统通常需要相应的同步机制来协调系统中各个部分的工作。设计与实现一个对用户来说是透明的且具有容错能力的分布式系统是一项具有挑战性的工作，而且所需的机制和策略尚未成熟。因此什么样的程序设计模型、什么样的控制机制最适合分布式系统仍是需要继续研究的课题。 [6]4.难以合理设计资源分配策略在集中式系统中，所有的资源都由操作系统管理和分配，但在分布式系统中，资源属于各节点，所以调度的灵活性不如集中式系统，资源的物理分布可能与用户请求的分布不匹配，某些资源可能空闲，而另一些资源可能超载。 [6]5.安全保密性问题开放性使得分布式系统中的许多软件接口都提供给用户，这样的开放式结构对于开发人员非常有价值，但同时也为破坏者打开了方便之门。 [6]针对分布式系统存在的上述难点，要保证一个分布式系统的正常运行，就必须对系统资源进行有效的管理，对计算机之间的通信、故障、安全等问题提供有效的处理手段和支持机制。 [6]用户对分布式系统的要求是透明性、安全性、灵活性、简单性、可靠性，也要求方便在局部失效时重构系统，以及集成不均匀子系统的能力。 [6]资源的分布性、缺乏全局状态信息及传输延迟，意味着集中式操作系统的某些方法和技术不能应用于分布式系统中。即使集中式系统中的某些技术满足上面的要求，其实现通常也是要付出很大代价的。 [6]

总线冲突：
通常，连接到总线的集成电路是预先被设计好的，以便总线冲突的可能性降低为零，芯片是在它们的速率设置时间之内被操作等等。然而，如果这个总线故意被驱动太快，这些设置时间可能被干扰而导致无法正常连接。连接也可能出现在那些内存映射是不可编程的系统中，不合法的值被写入到这个寄存器中来控制这个映射。使得计算机总线无法正常工作，进而可能会导致电脑死机系统崩溃。

汇编指令：
编译背景播报编辑任何一种微处理器（CPU）在设计时，就已规定好自己特定的指令系统，这种指令系统的功能也就决定了由该微处理器构成的计算机系统及其基本功能。指令系统中所设计的每条指令都对应着微处理器要完成的一种规定功能操作，即这些指令功能的实现都是由微处理器中的物理器件完成的。要使计算机完成一个完整的任务，就需要执行一组指令，这组指令通常称为程序。计算机能够执行的各种不同指令的集合就称为处理器（CPU）的指令系统。 [3]一台计算机只能识别由二进制编码表示的指令，称之为机器指令。一条机器指令应包括两部分内容：一部分给出该指令应完成何种操作，称为指令操作码部分；另一部分给出参与操作的操作数的值，或指出操作数存放在何处、操作的结果应送往何处等，这一部分称为指令的操作数部分。处理器可根据指令中给出的地址信息求出存放操作数的地址称为有效地址EA（Effective Address），然后对存放在有效地址中的操作数进行存取操作。指令中关于如何求岀存放操作数有效地址的方法称为操作数的寻址方式。计算机按照指令给出的寻址方式求出操作数有效地址进行存取操作数的过程，称为指令的寻址操作。 [3]指令的格式播报编辑指令是计算机能够识别和执行的操作命令，由二进制数“0”、“1”组成。每条指令的编码格式由机器指令系统规定。通常，一条指令包含操作码和操作数两部分内容，格式如下图所示：汇编指令格式操作码（Operation Code）用来说明指令操作的性质与功能，常用OP表示。操作码是指令中不可缺少的部分，通常由1~2个字节组成，机器通过译码电路来识别指令。操作数用于提供指令中要处理的数据或数据所在的地址信息。以MOV指令作为例子，MOV指令的格式为：MOV dst，src。其中：MOV为指令助记符，表示传送，dst为目标操作数，src表示源操作数，该指令的功能是将源操作数传送到目标单元。例如：MOV......AL，20H；将8位立即数20H传送到AL中。MOV......AX，1234H；将16位立即数1234H传送到AX中。MOV......EAX，34568020H；将32位立即数34568020H传送到EAX中。 [4]寻址方式播报编辑寄存器寻址操作数的值在寄存器中，指令中的地址码字段指出的是寄存器编号，指令执行时直接取出寄存器值来操作。 [5]立即寻址立即寻址指令中的操作码字段后面的地址码部分即是操作数本身，也就是说，数据就包含在指令当中，取出指令也就取出了可以立即使用的操作数（立即数）。 [5]寄存器移位寻址寄存器移位寻址是ARM指令集特有的寻址方式。当第2个操作数是寄存器移位方式时，第2个寄存器操作数在与第1个操作数结合之前，选择进行移位操作。 [5]寄存器间接寻址寄存器间接寻址指令中的地址码给出的是一个通用寄存器的编号，所需的操作数保存在寄存器指定地址的存储单元中，即寄存器为操作数的地址指针。 [5]基址寻址基址寻址就是将基址寄存器的内容与指令中给出的偏移量相加，形成操作数的有效地址。基址寻址用于访问基址附近的存储单元，常用于査表、数组操作、功能部件寄存器访问等。 [5]多寄存器寻址多寄存器寻址一次可以传送几个寄存器值，允许一条指令传送16个寄存器的任何子集或所有寄存器。 [5]堆栈寻址堆栈寻址是一个按特定顺序进行存取的存储区，操作顺序为”后进先出“。堆栈寻址是隐含的，它使用一个专门的寄存器（堆栈指针）指向一块存储区域（堆栈），指针所指向的存储单元即是堆栈的栈顶。 [5]数据传输指令播报编辑数据传输类指令主要包括数据传送、数据交换、堆栈操作、查表转换、地址传送、标志位传送、I/O数据传送指令。这类指令的主要特点是大部分指令操作完成后，对FR中的标志位不产生影响。它们在存储器和寄存器、寄存器和输入输出端口之间传送数据。 [4]数据传送指令MOV：传送字或字节。MOVSX：先符号扩展，再传送。MOVZX：先零扩展，再传送。PUSH：把字压入堆栈。POP： 把字弹出堆栈。PUSHA： 把AX，CX，DX，BX，SP，BP，SI，DI依次压入堆栈。POPA： 把DI，SI，BP，SP，BX，DX，CX，AX依次弹出堆栈。PUSHAD： 把EAX，ECX，EDX，EBX，ESP，EBP，ESI，EDI依次压入堆栈。POPAD： 把EDI，ESI，EBP，ESP，EBX，EDX，ECX，EAX依次弹出堆栈。BSWAP： 交换32位寄存器里字节的顺序。XCHG： 交换字或字节。( 至少有一个操作数为寄存器,段寄存器不可作为操作数)CMPXCHG： 比较并交换操作数。（第二个操作数必须为累加器AL/AX/EAX）XADD： 先交换再累加。( 结果在第一个操作数里 )XLAT： 字节查表转换── BX 指向一张 256 字节的表的起点，AL 为表的索引值（0-255，即0-FFH）； 返回 AL 为查表结果 （ [BX+AL]->AL） [6]输入输出端口传送指令IN： I/O端口输入。（ 语法：IN 累加器，{端口号│DX} ）OUT： I/O端口输出. （语法：OUT {端口号│DX}，累加器）输入输出端口由立即方式指定时，其范围是 0-255；由寄存器DX 指定时，其范围是 0-65535。 [6]目的地址传送指令LEA： 装入有效地址。例：LEA DX，string；把偏移地址存到DX。LDS： 传送目标指针，把指针内容装入DS。例： LDS SI，string；把段地址：偏移地址存到DS：SI。LES： 传送目标指针，把指针内容装入ES。例： LES DI，string；把段地址：偏移地址存到ES：DI。LFS： 传送目标指针，把指针内容装入FS。例： LFS DI，string；把段地址：偏移地址存到FS：DI。LGS： 传送目标指针，把指针内容装入GS。例： LGS DI，string；把段地址：偏移地址存到GS：DI。LSS： 传送目标指针，把指针内容装入SS。例： LSS DI，string；把段地址：偏移地址存到SS：DI。 [6]标志传送指令LAHF：标志寄存器传送，把标志装入AH。SAHF： 标志寄存器传送，把AH内容装入标志寄存器。PUSHF： 标志入栈。POPF： 标志出栈。PUSHD： 32位标志入栈。POPD： 32位标志出栈。 [6]算术运算指令播报编辑算数运算类指令包括加减乘除、比较与调整指令。它们可进行8位、16位和32位的运算。参加运算的操作数可以说二进制数和十进制数（BCD码），这些数可以是无符号数，也可以是带符号数。算术运算指令的主要特点是执行结果影响标志寄存器的状态标志位OF、SF、ZF、AF、PF、CF。 [4]ADD： 加法。ADC： 带进位加法。INC： 加 1。AAA： 加法的ASCII码调整。DAA： 加法的十进制调整。SUB： 减法。SBB： 带借位减法。DEC： 减 1。NEG： 取补。CMP： 比较。（两操作数作减法，仅修改标志位，不回送结果）AAS： 减法的ASCII码调整。DAS： 减法的十进制调整。MUL： 无符号乘法。结果回送AH和AL（字节运算），或DX和AX（字运算）IMUL： 整数乘法。结果回送AH和AL（字节运算），或DX和AX（字运算）AAM： 乘法的ASCII码调整。DIV： 无符号除法：商回送AL，余数回送AH，（字节运算）；或商回送AX，余数回送DX（字运算）IDIV： 整数除法：商回送AL，余数回送AH，（字节运算）；或商回送AX，余数回送DX（字运算）AAD： 除法的ASCII码调整。CBW： 字节转换为字。（把AL中字节的符号扩展到AH中去）CWD： 字转换为双字。（把AX中的字的符号扩展到DX中去）CWDE： 字转换为双字。 （把AX中的字符号扩展到EAX中去）CDQ： 双字扩展。（把EAX中的字的符号扩展到EDX中去） [6]逻辑运算类指令播报编辑逻辑运算类指令分为逻辑运算指令和移位指令两大类。 [4]逻辑运算指令AND： 与运算。or： 或运算。XOR： 异或运算。NOT： 取反。TEST： 测试。（两操作数作与运算，仅修改标志位，不回送结果） [6]移位指令SHL： 逻辑左移。SAL： 算术左移。(=SHL)SHR： 逻辑右移。（每位右移，低位进 CF，高位补 0）SAR： 算术右移。（每位右移， 低位进 CF，高位不变）ROL： 循环左移。ROR： 循环右移。RCL： 通过进位的循环左移。RCR： 通过进位的循环右移。以上八种移位指令，其移位次数可达255次。移位一次时，可直接用操作码，如：SHL AX，1；移位>1次时，则由寄存器CL给出移位次数，如：MOV CL，04；SHL AX，CL。 [6]串操作指令播报编辑串操作指令用于处理存放在存储器中的数据串，有串传送、串比较、串扫描、串装入、串存储。其中，仅有串比较和串扫描指令对标志位OF、SF、ZF、AF、PF、CF有影响。 [4]DS：SI ——源串段寄存器：源串变址。ES：DI ——目标串段寄存器：目标串变址。CX： 重复次数计数器。AL/AX： 扫描值。D标志： 0表示重复操作中SI和DI应自动增量；1表示应自动减量。Z标志： 用来控制扫描或比较操作的结束。MOVS： 串传送。（MOVSB 传送字符， MOVSW 传送字， MOVSD 传送双字）CMPS： 串比较。（CMPSB 比较字符， CMPSW 比较字）SCAS： 串扫描。把AL或AX的内容与目标串作比较，比较结果反映在标志位。LODS： 装入串。把源串中的元素（字或字节）逐一装入AL或AX中。（LODSB 传送字符， LODSW 传送字，LODSD 传送双字）STOS： 保存串。是LODS的逆过程。REP： 当CX/ECX<>0时重复。REPE/REPZ： 当ZF=1或比较结果相等，且CX/ECX<>0时重复。REPNE/REPNZ： 当ZF=0或比较结果不相等，且CX/ECX<>0时重复。REPC： 当CF=1且CX/ECX<>0时重复。REPNC： 当CF=0且CX/ECX<>0时重复。 [6]程序转移指令播报编辑控制转移类指令包括无条件转移指令、条件转移指令、循环控制指令、中断指令、子程序调用和返回指令。 [4]无条件转移指令（长转移）JMP： 无条件转移指令。CALL： 过程调用。RET/RETF： 过程返回。 [6]条件转移指令（短转移，-128到+127的距离内；当且仅当(SF、XOR、OF)=1时，OP1<OP2 ）JA/JNBE： 大于转移。JAE/JNB： 大于或等于转移。JB/JNAE： 小于转移。JBE/JNA： 小于或等于转移。以上四条，测试无符号整数运算的结果（标志C和Z）JG/JNLE： 大于转移。JGE/JNL： 大于或等于转移。JL/JNGE： 小于转移。JLE/JNG： 小于或等于转移。以上四条，测试带符号整数运算的结果（标志S，O和Z）JE/JZ： 等于转移。JNE/JNZ： 不等于时转移。JC： 有进位时转移。JNC： 无进位时转移。JNO： 不溢出时转移。JNP/JPO： 奇偶性为奇数时转移。JNS： 符号位为 "0" 时转移。JO： 溢出转移。JP/JPE： 奇偶性为偶数时转移。JS： 符号位为 "1" 时转移。 [6]循环控制指令（短转移）LOOP： CX不为零时循环。LOOPE/LOOPZ： CX不为零且标志Z=1时循环。LOOPNE/LOOPNZ： CX不为零且标志Z=0时循环。JCXZ： CX为零时转移。JECXZ： ECX为零时转移。 [6]中断指令INT： 中断指令。INTO： 溢出中断。IRET： 中断返回。 [6]其他指令播报编辑伪指令DB： 定义字节（1字节）DW： 定义字（2字节）DD： 定义双字（4字节）PROC： 定义过程。ENDP： 过程结束。SEGMENT： 定义段。ASSUME： 建立段寄存器寻址。ENDS： 段结束。END： 程序结束。 [6]处理机控制指令即标志处理指令，处理机控制指令完成简单的控制功能。CLC： （进位位置0指令）CMC： （进位位求反指令）CLC： （进位位置为0指令）STC： （进位位置为1指令）CLD： （方向标志位置0指令）STD： （方向标志位置1指令）CLI： （中断标志置0指令）STI： （中断标志置1指令）NOP： （无操作）HLT： （停机）WAIT： （等待）ESC： （换码）LOCK： （封锁） [6]

冯诺依曼体系结构：
发展历史播报编辑在计算机诞生之前，人们在计算的精度和数量上出现了瓶颈，对于计算机这样的机器的需求就十分强烈，冯·诺依曼的逻辑和计算机思想指导他设计并制造出历史上的第一台通用电子计算机。他的计算机理论主要受自身数学基础影响，且具有高度数学化、逻辑化特征，对于该理论，他自己一般会叫作“计算机的逻辑理论”。而他的计算机存储程序的思想，则是他的另一伟大创新，通过内部存储器安放存储程序，成功解决了当时计算机存储容量太小，运算速度过慢的问题。 [1]冯·诺依曼第二次世界大战期间，美军要求实验室为其提供计算量庞大的计算结果。于是便有了研制电子计算机的设想。面对这种需求，美国立即组建研发团队，包括许多工程师与物理学家，试图开发全球首台计算机（后世称作ENIAC机）。虽然采取了最先进的电子技术，但缺少原理上的指导。这时，冯·诺依曼出现了。他提出了一个至关重要的方面：计算机的逻辑结构。冯·诺依曼从逻辑入手，带领团队对ENIAC进行改进。他的逻辑设计具有以下特点：（1）将电路、逻辑两种设计进行分离，给计算机建立创造最佳条件；（2）将个人神经系统、计算机结合在一起，提出全新理念，即生物计算机。即便ENIAC机是通过当时美国乃至全球顶尖技术实现的，但它采用临时存储，将运算器确定成根本，故而缺点较多，比如存储空间有限、程序无法存储等，且运行速度较慢，具有先天不合理性。冯·诺依曼以此为前提制定以下优化方案：（1）用二进制进行运算，大大加快了计算机速度；（2）存储程序，也就是通过计算机内部存储器保存运算程序。如此一来，程序员仅仅通过存储器写入相关运算指令，计算机便能立即执行运算操作，大大加快运算效率。 [1]冯·诺依曼结构示意图特点及局限播报编辑特点现代计算机发展所遵循的基本结构形式始终是冯·诺依曼机结构。这种结构特点是“程序存储，共享数据，顺序执行”，需要 CPU 从存储器取出指令和数据进行相应的计算。 [2]主要特点有：（1）单处理机结构，机器以运算器为中心；（2）采用程序存储思想；（3）指令和数据一样可以参与运算；（4） 数据以二进制表示；（5）将软件和硬件完全分离；（6） 指令由操作码和操作数组成；（7）指令顺序执行。 [3]局限CPU 与共享存储器间的信息交换的速度成为影响系统性能的主要因素，而信息交换速度的提高又受制于存储元件的速度、存储器的性能和结构等诸多条件。传统冯·诺依曼计算机体系结构的存储程序方式造成了系统对存储器的依赖，CPU 访问存储器的速度制约了系统运行的速度。集成 电路 IC 芯片的技术水平决定了存储器及其他硬件的性能。为了提高硬件的性能， 以英特尔公司为代表的芯片制造企业在集成电路生产方面做出了极大的努力，且获得了巨大的技术成果。 现在每隔 18 个 月 IC 的集成度翻一倍，性能也提升一倍，产品价格降低一半，这就是所谓的“摩尔定律”。 这个规律已经持续了40 多年，估计还将延续若干年。然而，电子产品面临的二个基本限制是客观存在的：光的速度和材料的原子特性。首先，信息传播的速度最终将取决于电子流动的速度，电子信号在元件和导线里流动会产生时间延迟，频率过高会造成信号畸变，所以元件的速度不可能无限的提高直至达到光速。第二，计算机的电子信号存储在以硅晶体材料为代表晶体管上，集成度的提高在于晶体管变小，但是晶体管不可能小于一个硅原子的体积。 随着半导体技术逐渐逼近硅工艺尺寸极限，摩尔定律原导出的规律将不再适用。对冯·诺依曼计算机体系结构缺陷的分析：（1）指令和数据存储在同一个存储器中，形成系统对存储器的过分依赖。如果储存器件的发展受阻，系统的发展也将受阻。（2）指令在存储器中按其执行顺序存放，由指令计数器PC指明要执行的指令所在的单元地址。 然后取出指令执行操作任务。所以指令的执行是串行。影响了系统执行的速度。（3）存储器是按地址访问的线性编址，按顺序排列的地址访问，利 于存储和执行的机器语言指令，适用于作数值计算。但是高级语言表示的存储器则是一组有名字的变量，按名字调用变量，不按地址访问。机器语言同高级语言在语义上存在很大的间隔， 称之为冯·诺依曼语 义间隔。消除语义间隔成了计算机发展面临的一大难题。（4）冯·诺依曼体系结构计算机是为算术和逻辑运算而诞生的，目前在数值处理方面已经到达较高的速度和精度，而非数值处理应用领域发展缓慢，需要在体系结构方面有重大的突破。（5）传统的冯·诺依曼型结构属于控制驱动方式。它是执行指令代码对数值代码进行处理，只要指令明确，输入数据准确，启动程序后自动运行而且结果是预期的。一旦指令和数据有错误，机器不会主动修改指令并完善程序。而人类生活中有许多信息是模糊的，事件的发生、发展和结果是不能预期的，现代计算机的智能是无法应对如此复杂任务的。 [2]哈佛结构以及两者区别播报编辑哈佛结构哈佛结构的计算机分为三大部件：（1）CPU；（2）程序存储器；（3）数据存储器。它的特点是将程序指令和数据分开存储，由于数据存储器与程序存储器采用不同的总线，因而较大的提高了存储器的带宽，使之数字信号处理性能更加优越。 [3]哈佛结构是一种将程序指令存储和数据存储分开的存储器结构。中央处理器首先到程序指令存储器中读取程序指令内容，解码后得到数据地址，再到相应的数据存储器中读取数据，并进行下一步的操作（通常是执行）。程序指令存储和数据存储分开，可以使指令和数据有不同的数据宽度，如Microchip公司的PIC16芯片的程序指令是14位宽度，而数据是8位宽度。为避免将程序和指令共同存储在存储器中，并共用同一条总线，使得 CPU 和内存的信息流访问存取成为系统的瓶颈，人们设计了哈佛结构，原则是将程序和指令分别存储在不同的存储器中，分别访问。如此设计克服了数据流传输瓶颈，提高了运算速度，但结构复杂，对外围设备的连接与处理要求高，不适合外围存储器的扩展， 实现成本高，所以哈佛结构未能得到大范围的应用。但是作为冯式存储程序的改良手段，哈佛结构在CPU 内的高速缓存 Cache中得到了应用。通过设置指令缓存和数据缓存，指令和数据分开读取，提高了数据交换速度，极大克服了计算机的数据瓶颈。通过增加处理器数量，中央处理单元从最初的单核向双核、四核的方向发展，在冯氏计算机的简单结构下，增加处理器数量，也极大提高了计算机的运算性能。存储程序的方式使得计算机擅长数值处理而限制了其在非数值处理方面的发展。 [4]哈佛结构处理器有两个明显的特点：使用两个独立的存储器模块，分别存储指令和数据，每个存储模块都不允许指令和数据并存；使用独立的两条总线，分别作为CPU与每个存储器之间的专用通信路径，而这两条总线之间毫无关联。改进的哈佛结构，其结构特点为：以便实现并行处理；具有一条独立的地址总线和一条独立的数据总线，利用公用地址总线访问两个存储模块（程序存储模块和数据存储模块），公用数据总线则被用来完成程序存储模块或数据存储模块与CPU之间的数据传输。两者区别冯·诺依曼理论的要点是：数字计算机的数制采用二进制；计算机应该按照程序顺序执行。人们把冯诺依曼的这个理论称为冯诺依曼体系结构。从ENIAC到当前最先进的计算机都采用的是冯诺依曼体系结构。所以冯诺依曼是当之无愧的数字计算机之父。根据冯诺依曼体系结构构成的计算机，必须具有如下功能：把需要的程序和数据送至计算机中；必须具有长期记忆程序、数据、中间结果及最终运算结果的能力；能够完成各种算术、逻辑运算和数据传送等数据加工处理的能力；能够根据需要控制程序走向，并能根据指令控制机器的各部件协调操作；能够按照要求将处理结果输出给用户。哈佛结构是为了高速数据处理而采用的，因为可以同时读取指令和数据（分开存储的）。大大提高了数据吞吐率，缺点是结构复杂。通用微机指令和数据是混合存储的，结构上简单，成本低。假设是哈佛结构：你就得在电脑安装两块硬盘，一块装程序，一块装数据，内存装两根，一根储存指令，一根存储数据……是什么结构要看总线结构的。51单片机虽然数据指令存储区是分开的，但总线是分时复用的，所以顶多算改进型的哈佛结构。ARM9虽然是哈佛结构，但是之前的版本也还是冯·诺依曼结构。早期的X86能迅速占有市场，一条很重要的原因，正是靠了冯·诺依曼这种实现简单，成本低的总线结构。处理器虽然外部总线上看是诺依曼结构的，但是由于内部CACHE的存在，因此实际上内部来看已经算是改进型哈佛结构的了。展望播报编辑冯·诺依曼结构开启了计算机系统结构发展的先河，但是因为其集中、顺序的的控制而成为性能提高的瓶颈，因此各国科学家仍然在探索各种非冯·诺依曼结构，比如，数据流计算机，函数式编程语言计算机等都是较为著名的非冯·诺依曼结构。 [3]近几年来人们努力谋求突破传统冯·诺依曼体制的局限，各类非诺依曼化计算机的研究如雨后春笋蓬勃发展，主要表现在以下四个方面：（1）对传统冯·诺依曼机进行改良，如传统体系计算机只有一个处理部件是串行执行的，改成多处理部件形成流水处理，依靠时间上的重叠提高处理效率。（2）由多个处理器构成系统，形成多指令流多数据流支持并行算法结构。这方面的研究目前已经取得一些成功。（3）否定冯·诺依曼机的控制流驱动方式。设计数据流驱动工作方式的数据流计算机，只要数据已经准备好，有关的指令就可并行地执行。这是真正非诺依曼化的计算机，这样的研究还在进行中，已获得阶段性的成果，如神经计算机。（4）彻底跳出电子的范畴，以其它物质作为信息载体和执行部件，如光子、生物分子、量子等。 众多科学家正在进行这些前瞻性的研究。 [2]

虚拟存储器：
作用播报编辑虚拟存储器虚拟内存的作用 内存在计算机中的作用很大，电脑中所有运行的程序都需要经过内存来执行，如果执行的程序很大或很多，就会导致内存消耗殆尽。为了解决这个问题，Windows中运用了虚拟内存技术，即拿出一部分硬盘空间来充当内存使用，当内存占用完时，电脑就会自动调用硬盘来充当内存，以缓解内存的紧张。举一个例子来说，如果电脑只有128MB物理内存的话，当读取一个容量为200MB的文件时，就必须要用到比较大的虚拟内存，文件被内存读取之后就会先储存到虚拟内存，等待内存把文件全部储存到虚拟内存之后，跟着就会把虚拟内存里储存的文件释放到原来的安装目录里了。设置播报编辑虚拟存储器虚拟内存的设置 对于虚拟内存主要设置两点，即内存大小和分页位置，内存大小就是设置虚拟内存最小为多少和最大为多少；而分页位置则是设置虚拟内存应使用那个分区中的硬盘空间。对于内存大小的设置，如何得到最小值和最大值呢？你可以通过下面的方法获得：选择“开始→程序→附件→系统工具→系统监视器”（如果系统工具中没有，可以通过“添加/删除程序”中的Windows安装程序进行安装）打开系统监视器，然后选择“编辑→添加项目”，在“类型”项中选择“内存管理程序”，在右侧的列表选择“交换文件大小”。这样随着你的操作，会显示出交换文件值的波动情况，你可以把经常要使用到的程序打开，然后对它们进行使用，这时查看一下系统监视器中的表现值，由于用户每次使用电脑时的情况都不尽相同，因此，最好能够通过较长时间对交换文件进行监视来找出最符合您的交换文件的数值，这样才能保证系统性能稳定以及保持在最佳的状态。 找出最合适的范围值后，在设置虚拟内存时，用鼠标右键点击“我的电脑”，选择“属性”，弹出系统属性窗口，选择“性能”标签，点击下面“虚拟内存”按钮，弹出虚拟内存设置窗口，点击“用户自己指定虚拟内存设置”单选按钮，“硬盘”选较大剩余空间的分区，然后在“最小值”和“最大值”文本框中输入合适的范围值。如果您感觉使用系统监视器来获得最大和最小值有些麻烦的话，这里完全可以选择“让Windows管理虚拟内存设置”。调整分页位置播报编辑虚拟存储器Windows 9x的虚拟内存分页位置，其实就是保存在C盘根目录下的一个虚拟内存文件（也称为交换文件）Win386.swp，它的存放位置可以是任何一个分区，如果系统盘C容量有限，我们可以把Win386.swp调到别的分区中，方法是在记事本中打开System.ini（C:\\Windows下）文件，在[386Enh]小节中，将“PagingDrive=C:WindowsWin 386.swp”，改为其他分区的路径，如将交换文件放在D:中，则改为“PagingDrive=D:Win386.swp”，如没有上述语句可以直接键入即可。 而对于使用Windows 2000和Windows XP的，可以选择“控制面板→系统→高级→性能”中的“设置→高级→更改”，打开虚拟内存设置窗口，在驱动器[卷标]中默认选择的是系统所在的分区，如果想更改到其他分区中，首先要把原先的分区设置为无分页文件，然后再选择其他分区。如果你的硬盘够大，那就请你打开”控制面板“中的“系统”，在“性能”选项中打开“虚拟内存”，选择第二项：用户自己设定虚拟内存设置，指向一个较少用的硬盘，并把最大值和最小值都设定为一个固定值，大小为物理内存的2倍左右。这样，虚拟存储器在使用硬盘时，就不用迁就其忽大忽小的差别，而将固定的空间作为虚拟内存，加快存取速度。虚拟内存的设置最好在“磁盘碎片整理”之后进行，这样虚拟内存就分布在一个连续的、无碎片文件的空间上，可以更好的发挥作用。使用技巧播报编辑虚拟内存使用技巧虚拟存储器对于虚拟内存如何设置的问题，微软已经给我们提供了官方的解决办法，对于一般情况下，我们推荐采用如下的设置方法：(1)在Windows系统所在分区设置页面文件，文件的大小由你对系统的设置决定。具体设置方法如下：打开"我的电脑"的"属性"设置窗口，切换到"高级"选项卡，在"启动和故障恢复"窗口的"写入调试信息"栏，如果你采用的是"无"，则将页面文件大小设置为2MB左右，如果采用"核心内存存储"和"完全内存存储"，则将页面文件值设置得大一些，跟物理内存差不多就可以了。小提示：对于系统分区是否设置页面文件，这里有一个矛盾：如果设置，则系统有可能会频繁读取这部分页面文件，从而加大系统盘所在磁道的负荷，但如果不设置，当系统出现蓝屏死机(特别是STOP错误)的时候，无法创建转储文件 (Memory.dmp)，从而无法进行程序调试和错误报告了。所以折中的办法是在系统盘设置较小的页面文件，只要够用就行了。虚拟存储器(2)单独建立一个空白分区，在该分区设置虚拟内存，其最小值设置为物理内存的1.5倍，最大值设置为物理内存的3倍，该分区专门用来存储页面文件，不要再存放其它任何文件。之所以单独划分一个分区用来设置虚拟内存，主要是基于两点考虑：其一，由于该分区上没有其它文件，这样分区不会产生磁盘碎片，这样能保证页面文件的数据读写不受磁盘碎片的干扰；其二，按照Windows对内存的管理技术，Windows会优先使用不经常访问的分区上的页面文件，这样也减少了读取系统盘里的页面文件的机会，减轻了系统盘的压力。(3)其它硬盘分区不设置任何页面文件。当然，如果你有多个硬盘，则可以为每个硬盘都创建一个页面文件。当信息分布在多个页面文件上时，硬盘控制器可以同时在多个硬盘上执行读取和写入操作。这样系统性能将得到提高。提示：允许设置的虚拟内存最小值为2MB，最大值不能超过当前硬盘的剩余空间值，同时也不能超过32位操作系统的内存寻址范围——4GB。相关播报编辑虚拟存储器virtual memory为了给用户提供更大的随机存取空间而采用的一种存储技术。它将内存与外存结合使用，好像有一个容量极大的内存储器，工作速度接近于主存，每位成本又与辅存相近，在整机形成多层次存储系统。虚拟存储器源出于英国ATLAS计算机的一级存储器概念。这种系统的主存为16千字的磁芯存储器，但中央处理器可用20位逻辑地址对主存寻址。到1970年，美国RCA公司研究成功虚拟存储器系统。IBM公司于1972年在IBM370系统上全面采用了虚拟存储技术。虚拟存储器已成为计算机系统中非常重要的部分。虚拟存储器是由硬件和操作系统自动实现存储信息调度和管理的。它的工作过程包括6个步骤：①中央处理器访问主存的逻辑地址分解成组号a和组内地址b，并对组号a进行地址变换，即将逻辑组号a作为索引，查地址变换表，以确定该组信息是否存放在主存内。②如该组号已在主存内，则转而执行④；如果该组号不在主存内，则检查主存中是否有空闲区，如果没有，便将某个暂时不用的组调出送往辅存，以便将这组信息调入主存。③从辅存读出所要的组，并送到主存空闲区，然后将那个空闲的物理组号a和逻辑组号a登录在地址变换表中。④从地址变换表读出与逻辑组号a对应的物理组号a。⑤从物理组号a和组内字节地址b得到物理地址。⑥根据物理地址从主存中存取必要的信息。调度方式有分页式、分段式、段页式3种。页式调度是将逻辑和物理地址空间都分成固定大小的页。主存按页顺序编号，而每个独立编址的程序空间有自己的页号顺序，通过调度辅存中程序的各页可以离散装入主存中不同的页面位置，并可据表一一对应检索。页式调度的优点是页内零头小，页表对程序员来说是透明的，地址变换快，调入操作简单；缺点是各页不是程序的独立模块，不便于实现程序和数据的保护。段式调度是按程序的逻辑结构划分地址空间，段的长度是随意的，并且允许伸长，它的优点是消除了内存零头，易于实现存储保护，便于程序动态装配；缺点是调入操作复杂。将这两种方法结合起来便构成段页式调度。在段页式调度中把物理空间分成页，程序按模块分段，每个段再分成与物理空间页同样小的页面。段页式调度综合了段式和页式的优点。其缺点是增加了硬件成本，软件也较复杂。大型通用计算机系统多数采用段页式调度。替换方法播报编辑随机算法用软件或硬件随机数产生器确定替换的页面。先进先出先调入主存的页面先替换。最近最少使用算法替换最长时间不用的页面。虚拟存储器模型虚实地址播报编辑存储模型1、实地址与虚地址用户编制程序时使用的地址称为虚地址或逻辑地址，其对应的存储空间称为虚存空间或逻辑地址空间；而计算机物理内存的访问地址则称为实地址或物理地址，其对应的存储空间称为物理存储空间或主存空间。程序进行虚地址到实地址转换的过程称为程序的再定位。示意图2、虚存的访问过程虚存空间的用户程序按照虚地址编程并存放在辅存中。程序运行时，由地址变换机构依据当时分配给该程序的实地址空间把程序的一部分调入实存。每次访存时，首先判断该虚地址所对应的部分是否在实存中：如果是，则进行地址转换并用实地址访问主存；否则，按照某种算法将辅存中的部分程序调度进内存，再按同样的方法访问主存。由此可见，每个程序的虚地址空间可以远大于实地址空间，也可以远小于实地址空虚拟存储器模型间。前一种情况以提高存储容量为目的，后一种情况则以地址变换为目的。后者通常出现在多用户或多任务系统中：实存空间较大，而单个任务并不需要很大的地址空间，较小的虚存空间则可以缩短指令中地址字段的长度。异构体系播报编辑从虚存的概念可以看出，主存-辅存的访问机制与cache-主存的访问机制是类似的。这是由cache存储器、主存和辅存构成的三级存储体系中的两个层次。cache和主存之间以及主存和辅存之间分别有辅助硬件和辅助软硬件负责地址变换与管理，以便各级存储器能够组成有机的三级存储体系。cache和主存构成了系统的内存，而主存和辅存依靠辅助软硬件的支持构成了虚拟存储器。地址在三级存储体系中，cache-主存和主存-辅存这两个存储层次有许多相同点：(1)出发点相同：二者都是为了提高存储系统的性能价格比而构造的分层存储体系，都力图使存储系统的性能接近高速存储器，而价格和容量接近低速存储器。(2)原理相同：都是利用了程序运行时的局部性原理把常用的信息块从相对慢速而大容量的存储器调入相对高速而小容量的存储器。存储体系但cache-主存和主存-辅存这两个存储层次也有许多不同之处：存储体系(1)侧重点不同：cache主要解决主存与CPU的速度差异问题；而就性能价格比的提高而言，虚存主要是解决存储容量问题，另外还包括存储管理、主存分配和存储保护等方面。(2)数据通路不同：CPU与cache和主存之间均有直接访问通路，cache不命中时可直接访问主存；而虚存所依赖的辅存与CPU之间不存在直接的数据通路，当主存不命中时只能通过调页解决，CPU最终还是要访问主存。(3)透明性不同：cache的管理完全由硬件完成，对系统程序员和应用程序员均透明；而虚存管理由软件（操作系统）和硬件共同完成，由于软件的介入，虚存对实现存储管理的系统程序员不透明，而只对应用程序员透明（段式和段页式管理对应用程序员“半透明”）。存储体系(4)未命中时的损失不同：由于主存的存取时间是cache的存取时间的5～10倍，而主存的存取速度通常比辅存的存取速度快上千倍，故主存未命中时系统的性能损失要远大于cache未命中时的损失。虚存机制要解决的关键问题 (1)调度问题：决定哪些程序和数据应被调入主存。(2)地址映射问题：在访问主存时把虚地址变为主存物理地址（这一过程称为内地址变换）；在访问辅存时把虚地址变成辅存的物理地址（这一过程称为外地址变换），以便换页。此外还要解决主存分配、存储保护与程序再定位等问题。(3)替换问题：决定哪些程序和数据应被调出主存。(4)更新问题：确保主存与辅存的一致性。在操作系统的控制下，硬件和系统软件为用户解决了上述问题，从而使应用程序的编程大大简化。页式调度播报编辑1、页式虚存地址映射页式虚拟存储系统页式虚拟存储系统中，虚地址空间被分成等长大小的页，称为逻辑页；主存空间也被分成同样大小的页，称为物理页。相应地，虚地址分为两个字段：高字段为逻辑页号，低字段为页内地址（偏移量）；实存地址也分两个字段：高字段为物理页号，低字段为页内地址。通过页表可以把虚地址（逻辑地址）转换成物理地址。 在大多数系统中，每个进程对应一个页表。页表中对应每一个虚存页面有一个表项，表项的内容包含该虚存页面所在的主存页面的地址（物理页号），以及指示该逻辑页是否已调入主存的有效位。地址变换时，用逻辑页号作为页表内的偏移地址索引页表（将虚页号看作页表数组下标）并找到相应物理页号，用物理页号作为实存地址的高字段，再与虚地址的页内偏移量拼接，就构成完整的物理地址。现代的中央处理机通常有专门的硬件支持地址变换。2、转换后援缓冲器由于页表通常在主存中，因而即使逻辑页已经在主存中，也至少要访问两次物理存储器才能实现一次访存，这将使虚拟存储器的存取时间加倍。为了避免对主存访问次数的增多，可以对页表本身实行二级缓存，把页表中的最活跃的部分存放在高速存储器中，组成快表。这个专用于页表缓存的高速存储部件通常称为转换后援缓冲器(TLB)。保存在主存中的完整页表则称为慢表。3、内页表和外页表页表是虚地址到主存物理地址的变换表，通常称为内页表。与内页表对应的还有外页表，用于虚地址与辅存地址之间的变换。当主存缺页时，调页操作首先要定位辅存，而外页表的结构与辅存的寻址机制密切相关。例如对磁盘而言，辅存地址包括磁盘机号、磁头号、磁道号和扇区号等。段式调度播报编辑段式虚拟存储系统页式虚拟存储系统段是按照程序的自然分界划分的长度可以动态改变的区域。通常，程序员把子程序、操作数和常数等不同类型的数据划分到不同的段中，并且每个程序可以有多个相同类型的段。在段式虚拟存储系统中，虚地址由段号和段内地址（偏移量）组成。虚地址到实主存地址的变换通过段表实现。每个程序设置一个段表，段表的每一个表项对应一个段。每个表项至少包含下面三个字段： (1)有效位：指明该段是否已经调入实存。(2)段起址：指明在该段已经调入实存的情况下，该段在实存中的首地址。(3)段长：记录该段的实际长度。设置段长字段的目的是为了保证访问某段的地址空间时，段内地址不会超出该段长度导致地址越界而破坏其他段。段表本身也是一个段，可以存在辅存中，但一般驻留在主存中。段式虚拟存储器有许多优点：①段的逻辑独立性使其易于编译、管理、修改和保护，也便于多道程序共享。②段长可以根据需要动态改变，允许自由调度，以便有效利用主存空间。段式虚拟存储器也有一些缺点：①因为段的长度不固定，主存空间分配比较麻烦。②容易在段间留下许多外碎片，造成存储空间利用率降低。③由于段长不一定是2的整数次幂，因而不能简单地像分页方式那样用虚地址和实地址的最低若干二进制位作为段内偏移量，并与段号进行直接拼接，必须用加法操作通过段起址与段内偏移量的求和运算求得物理地址。因此，段式存储管理比页式存储管理方式需要更多的硬件支持。段页式调度播报编辑段页式虚拟存储器 [1]段页式虚拟存储器是段式虚拟存储器和页式虚拟存储器的结合。实存被等分成页。每个程序则先按逻辑结构分段，每段再按照实存的页大小分页，程序按页进行调入和调出操作，但可按段进行编程、保护和共享。它把程序按逻辑单位分段以后，再把每段分成固定大小的页。程序对主存的调入调出是按页面进行的，但它又可以按段实现共享和保护，兼备页式和段式的优点。缺点是在映象过程中需要多次查表。在段页式虚拟存储系统中，每道程序是通过一个段表和一组页表来进行定位的。段表中的每个表目对应一个段，每个表目有一个指向该段的页表起始地址及该段的控制保护信息。由页表指明该段各页在主存中的位置以及是否已装入、已修改等状态信息。如果有多个用户在机器上运行，多道程序的每一道需要一个基号,由它指明该道程序的段表起始地址。虚拟地址格式如下：基号段号页号页内地址变换算法播报编辑段式管理图虚拟存储器地址变换基本上有3种形虚拟存储器工作过程式：全相联变换、直接变换和组相联变换。任何逻辑空间页面能够变换到物理空间任何页面位置的方式称为全相联变换。每个逻辑空间页面只能变换到物理空间一个特定页面的方式称为直接变换。组联想变换是指各组之间是直接变换，而组内各页间则是全相联变换。替换规则用来确定替换主存中哪一部分，以便腾空部分主存，存放来自辅存要调入的那部分内容。常见的替换算法有4种。 ①随机算法：用软件或硬件随机数产生器确定替换的页面。②先进先出：先调入主存的页面先替换。③最少使用算法：替换最长时间不用的页面。④最优算法：替换最长时间以后才使用的页面。这是理想化的算法，只能作为衡量其他各种算法优劣的标准。虚拟存储器的效率是系统性能评价的重要内容，它与主存容量、页面大小、命中率，程序局部性和替换算法等因素有关。

流水线技术：
简介播报编辑借鉴了工业流水线制造的思想，现代CPU也采用了流水线设计。在工业制造中采用流水线可以提高单位时间的生产量；同样在CPU中采用流水线设计也有助于提高CPU的频率。流水线技术先以汽车装配为例来解释流水线的工作方式。假设装配一辆汽车需要4个步骤：1.冲压：制作车身外壳和底盘等部件；2.焊接：将冲压成形后的各部件焊接成车身；3.涂装：将车身等主要部件清洗、化学处理、打磨、喷漆和烘干；4.总装：将各部件（包括发动机和向外采购的零部件）组装成车；同时对应地需要冲压、焊接、涂装和总装四个工人。采用流水线的制造方式，同一时刻四辆汽车在装配。如果不采用流水线，那么第一辆汽车依次经过上述四个步骤装配完成之后，下一辆汽车才开始进行装配，最早期的工业制造就是采用的这种原始的方式。未采用流水线的原始制造方式，同一时刻只有一辆汽车在装配。流水线技术不久之后就发现，某个时段中一辆汽车在进行装配时，其它三个工人处于闲置状态，显然这是对资源的极大浪费。于是开始思考能有效利用资源的方法：在第一辆汽车经过冲压进入焊接工序的时候，立刻开始进行第二辆汽车的冲压，而不是等到第一辆汽车经过全部四个工序后才开始。之后的每一辆汽车都是在前一辆冲压完毕后立刻进入冲压工序，这样在后续生产中就能够保证四个工人一直处于运行状态，不会造成人员的闲置。这样的生产方式就好似流水川流不息，因此被称为流水线。CPU的工作也可以大致分为指令的获取、解码、运算和结果的写入四个步骤，采用流水线设计之后，指令（好比待装配的汽车）就可以连续不断地进行处理。在同一个较长的时间段内，显然拥有流水线设计的CPU能够处理更多的指令。分类播报编辑流水线技术流水线功能繁杂，种类也非常多；如果按照处理级别来分类，流水线可以有操作部件级、指令级和处理机级；如果按照流水线可以完成的动作的数量来分类，又可以分为单功能和多功能流水线；如果按照流水线内部的功能部件的连接方式来分类，则有线性流水线和非线性流水线；按照可处理对象来分类，还可以有标量流水线和向量流水线。按处理级别功能部件级：在实现较为复杂的运算时采用指令级：将一条指令执行过程分为多个阶段处理器间级：每个处理器完成其专门的任务。按完成的功能单功能流水线：只完成一种如乘法或浮点运算等，多用于数字信号处理器（DSP），各处理器可并行完成各自的功能，加快整机处理速度。流水线技术多功能流水线：在不同情况下可完成不同功能按连接的方式分类静态流水线：同一时间内，多功能结构只能按一种功能的连接方式工作。动态流水线：同一时间内，可以有多种功能的连接方式同时工作。按处理的数据类型标量流水线：一般数据向量流水线：矢量数据。X+Y=Z每一个代表一维数据。按流水线结构线性流水线：指各功能模块顺序串行连接，无反馈回路，如前面介绍的。非线性流水线：带有反馈回路的流水线。性能指标播报编辑衡量一种流水线处理方式的性能高低的书面数据主要由吞吐率、效率和加速比这三个参数来决定。吞吐率流水线技术指的是计算机中的流水线在特定的时间内可以处理的任务或输出数据的结果的数量。流水线的吞吐率可以进一步分为最大吞吐率和实际吞吐率。它们主要和流水段的处理时间、缓存寄存器的延迟时间有关，流水段的处理时间越长，缓存寄存器的延迟时间越大，那么，这条流水线的吞吐量就越小。因为，在线性流水线中，最大吞吐率Tpmax=流水线时钟周期△T=1/max（T1,...Ti,..Tm）+T1/1，而其中，m是流水线的段数，i是特定过程段执行时间。如果，一条流水线的段数越多，过程执行时间越长，那么，这条流水线的理论吞吐率就越小。由此，要对于流水线的瓶颈部分的处理主要在于减少流水段的处理时间。实现的方法一般有两种：1、把瓶颈部分的流水线分拆，以便任务可以充分流水处理。流水段的处理时间过长，一般是由于任务堵塞造成的，而任务的堵塞会导致流水线不能在同一个时钟周期内启动另一个操作，可以把流水段划分，在各小流水段中间设置缓存寄存器，缓冲上一个流水段的任务，使流水线充分流水。假如X流水段的处理时间为3T，可以把X流水段再细分成3小段，这样，每小段的功能相同，但是处理时间已经变成3T/3=T了。流水线技术2、在瓶颈部分设置多条相同流水段，并行处理。对付流水段的处理时间过长，还有另外一种方法，那就是把瓶颈流水段用多个相同的并联流水段代替，在前面设一个分派单元来对各条流水段的任务进行分派。仍然假设瓶颈流水段的处理时间是△3T，那么经过3条并联流水段的同时处理，实际需要的时间只是△T。这样，就达到了缩短流水段处理时间，但这种方法比较少以采用，因为要3段相同的流水段并联，成本较高，而且，分派单元会比较麻烦处理。加速比是指某一流水线如果采用串行模式之后所用的时间T0和采用流水线模式后所用时间T的比值，数值越大，说明这条流水线的工作安排方式越好。效率使用效率：指流水线中，各个部件的利用率。由于流水线在开始工作时存在建立时间；在结束时存在排空时间，各个部件不可能一直在工作，总有某个部件在某一个时间处于闲置状态。用处于工作状态的部件和总部件的比值来说明这条流水线的工作效率。影响因素播报编辑流水线技术流水线处理方式是一种时间重叠并行处理的处理技术，具体地说，就是流水线可以在同一个时间启动2个或以上的操作，借此来提高性能。为了实现这一点，流水线必须要时时保存畅通，让任务充分流水，但在实际中，会出现2种情况使流水线停顿下来或不能启动：1、多个任务在同一时间周期内争用同一个流水段。例如，假如在指令流水线中，如果数据和指令是放在同一个储存器中，并且访问接口也只有一个，那么，两条指令就会争用储存器；在一些算数流水线中，有些运算会同时访问一个运算部件。2、数据依赖。比如，A运算必须得到B运算的结果，但是，B运算还没有开始，A运算动作就必须等待，直到B运算完成，两次运算不能同时执行。解决方案：第一种情况，增加运算部件的数量来使他们不必争用同一个部件；第二种情况，用指令调度的方法重新安排指令或运算的顺序。技术规范播报编辑超级流水线超级流水线（SuperPipeline)又叫做深度流水线，它是提高cpu速度通常采取的一种技术。CPU处理指令是通过Clock来驱动的，每个clock完成一级流水线操作。每个周期所做的操作越少，需要的时间就越短，时间越短，频率就可以提得越高。超级流水线就是将cpu处理指令是得操作进一步细分，增加流水线级数来提高频率。频率高了，当流水线开足马力运行时平均每个周期完成一条指令（单发射情况下），这样cpu处理得速度就提高了。当然，这是理想情况下，一般是流水线级数越多，重叠执行的执行就越多，那么发生竞争冲突得可能性就越大，对流水线性能有一定影响现在很多cpu都是将超标量和超级流水线技术一起使用，例如pentiumIV，流水线达到20级，频率最快已经超过3GHZ。教科书上用于教学的经典MIPS只有5级流水。超标量流水线技术将一条指令分成若干个周期处理以达到多条指令重叠处理,从而提高cpu部件利用率的技术叫做标量流水技术。超级标量是指cpu内一般能有多条流水线,这些流水线能够并行处理。在单流水线结构中，指令虽然能够重叠执行，但仍然是顺序的,每个周期只能发射(issue)或退休(retire)一条指令。超级标量结构的cpu支持指令级并行，每个周期可以发射多条指令(2-4条居多)。可以使得cpu的IPC(InstructionPerClock)>，从而提高cpu处理速度。超级标量机能同时对若干条指令进行译码，将可以并行执行的指令送往不同的执行部件，在程序运行期间，由硬件(通常是状态记录部件和调度部件)来完成指令调度。超级标量机主要是借助硬件资源重复(例如有两套译码器和ALU等)来实现空间的并行操作。熟知的pentium系列(可能是p-II开始),还有SUNSPARC系列的较高级型号,以及MIPS若干型号等都采用了超级标量技术。 [1]超长指令字超长指令字（VLIW：VeryLongInstructionWord）是由美国Yale大学教授Fisher提出的。它有点类似于超级标量，是一条指令来实现多个操作的并行执行，之所以放到一条指令是为了减少内存访问。通常一条指令多达上百位，有若干操作数，每条指令可以做不同的几种运算。那些指令可以并行执行是由编译器来选择的。通常VLIW机只有一个控制器，每个周期启动一条长指令，长指令被分为几个字段，每个字段控制相应的部件。由于编译器需要考虑数据相关性，避免冲突，并且尽可能利用并行，完成指令调度，所以硬件结构较简单。VLIW机器较少，可能不太容易实现，业界比较有名的VLIW公司之一是Transmeta，在加州硅谷SantaClara（硅谷圣地之一，还有SanJose，PaloAlto）。它做的机器采用X86指令集，VLIW实现，具体资料可以去访问公司的网站。 [2]向量机平时接触的计算机都是标量机，向量机都是大型计算机，一般用于军事工业，气象预报，以及其他大型科学计算领域，这也说明了向量机都很贵。国产的银河计算机就是向量机普通的计算机所做的计算，例如加减乘除，只能对一组数据进行操作，被称为标量运算。向量运算一般是若干同类型标量运算的循环。向量运算通常是对多组数据成批进行同样运算，所得结果也是一组数据。很多做科学计算的大（巨）型机都是向量机。SIMD技术单指令多数据（SingleInstructionMultipleData）简称SIMD。SIMD结构的CPU有多个执行部件，但都在同一个指令部件的控制下。SIMD在性能优势呢：以加法指令为例，单指令单数据（SISD）的CPU对加法指令译码后，执行部件先访问内存，取得第一个操作数；之后再一次访问内存，取得第二个操作数；随后才能进行求和运算。而在SIMD型CPU中，指令译码后几个执行部件同时访问内存，一次性获得所有操作数进行运算。这个特点使得SIMD特别适合于多媒体应用等数据密集型运算。AMD公司的3DNOW！技术其实质就是SIMD，这使K6－2处理器在音频解码、视频回放、3D游戏等应用中显示出优异性能。

总线频率：
1. 含义北桥芯片负责联系内存、显卡等数据吞吐量最大的部件，并和南桥芯片连接。CPU就是通过前端总线连接到北桥芯片，进而通过北桥芯片和内存、显卡交换数据。前端总线是CPU和外界交换数据的最主要通道，因此前端总线的数据传输能力对计算机整体性能作用很大，如果没有足够快的前端总线，再强的CPU也不能明显提高计算机整体速度。数据传输最大带宽取决于所有同时传输的数据的宽度和传输频率，即数据带宽=总线频率×（数据位宽÷8）。PC机上所能达到的前端总线频率有266MHz、333MHz、400MHz、533MHz、800MHz、1066MHz、1333MHz、1600MHz、2000MHz几种，前端总线频率越大，代表着CPU与北桥芯片之间的数据传输能力越大，更能充分发挥出CPU的功能。CPU技术发展很快，运算速度提高很快，而足够大的前端总线可以保障有足够的数据供给给CPU，较低的前端总线将无法供给足够的数据给CPU，这样就限制了CPU性能得发挥，成为系统瓶颈。主板支持的前端总线是由芯片组决定的，一般都带有足够的向下兼容性，如865PE主板支持800MHz前端总线，那安装的CPU的前端总线可以是800MHz，也可以是533MHz，但这样就无法发挥出主板的全部功效。2. 相对差异前端总线的速度指的是CPU和北桥芯片间总线的速度，更实质性的表示了CPU和外界数据传输的速度。而外频的概念是建立在数字脉冲信号震荡速度基础之上的，也就是说，100MHz外频特指数字脉冲信号在每秒钟震荡一万万次，它更多的影响了PCI及其他总线的频率。之所以前端总线与外频这两个概念容易混淆，主要的原因是在以前的很长一段时间里（主要是在Pentium 4出现之前和刚出现Pentium 4时），前端总线频率与外频是相同的，因此往往直接称前端总线为外频，最终造成这样的误会。随着计算机技术的发展，人们发现前端总线频率需要高于外频，因此采用了QDR（Quad Date Rate）技术，或者其他类似的技术实现这个目的。这些技术的原理类似于AGP的2X或者4X，它们使得前端总线的频率成为外频的2倍、4倍甚至更高，从此之后前端总线和外频的区别才开始被人们重视起来。此外，在前端总线中比较特殊的是AMD64的HyperTransport。3. 支持频率3.1 Intel平台系列Intel芯片组：　845、845D、845GL所支持的前端总线频率是400MHz，845E、845G、845GE、845PE、845GV以及865P、910GL所支持的前端总线频率是533MHz，而865PE、865G、865GV、848P、875P、915P、915G、915GV、915PL、915GL、925X、945PL、945GZ所支持的前端总线频率是800MHz。定位于欢跃(VIIV)平台的945GT所支持的前端总线频率是533MHz和667MHz，高端的925XE、945P、945G、955X、975X所支持的前端总线频率是1066MHz。946PL和946GZ所支持的前端总线频率是800MHz，而P965、G965、Q965和Q963所支持的前端总线频率则都是1066MHz。VIA芯片组：　P4X266、P4X266A、P4M266所支持的前端总线频率是400MHz，P4X266E、P4X333、P4X400、P4X533所支持的前端总线频率是533MHz，PT800、PT880、PM800、PM880、P4M800、P4M800 Pro、PT880 Pro所支持的前端总线频率是800MHz，PT880 Ultra、PT894、PT894 Pro、PT890所支持的前端总线频率也高达1066MHz。P4M890所支持的前端总线频率是800MHz，而P4M900所支持的前端总线频率则是1066MHz。SIS芯片组：　SIS645、SIS645DX、SIS650所支持的前端总线频率是400MHz，SIS651、SIS655、SIS648、SIS661GX所支持的前端总线频率是533MHz，SIS648FX、SIS661FX、SIS655FX、SIS655TX、SIS649、SIS656、SIS662所支持的前端总线频率是800MHz，SIS649FX和SIS656FX所支持的前端总线频率则高达1066MHz。ATI芯片组：　Radeon 9100 IGP、Radeon 9100 Pro IGP、RX330、Radeon Xpress 200 IE(RC410)、Radeon Xpress 200 IE(RXC410)所支持的前端总线频率是800MHz，Radeon Xpress 200 IE(RS400)、Radeon Xpress 200 CrossFire IE(RD400)、CrossFire Xpress 1600 IE所支持的前端总线频率则高达1066MHz。　ULI芯片组：　M1683和M1685所支持的前端总线频率是800MHz。NVIDIA芯片组：　nForce4 SLI IE、nForce4 SLI X16 IE、nForce4 SLI XE、nForce4 Ultra IE所支持的前端总线频率全部都高达1066MHz。nForce 590 SLI IE、nForce 570 SLI IE和nForce 570 Ultra IE所支持的前端总线频率全部都是1066MHz。3.2 AMD平台系列VIA芯片组：　KT266、KT266A、KM266所支持的前端总线频率是266MHz，KT333、KT400、KT400A、KM400、KN400所支持的前端总线频率是333MHz，KT600和KT880所支持的前端总线频率是400MHz。SIS芯片组：　SIS735、SIS745、SIS746、SIS740所支持的前端总线频率是266MHz，SIS741GX和SIS746FX所支持的前端总线频率是333MHz，SIS741和SIS748所支持的前端总线频率是400MHz。Uli芯片组：　M1647所支持的前端总线频率是266MHz。nVidia芯片组：　nForce2 IGP、nForce2 400和nForce2 Ultra 400所支持的前端总线频率是400MHz。此外，由于AMD64系列CPU内部整合了内存控制器，其HyperTransport频率只与CPU接口类型有关，而与主板芯片组无关，所以其HyperTransport频率的区分是相当简单的。

总线带宽：
概念简介播报编辑从电子电路角度出发，带宽（Bandwidth）本意指的是电子电路中存在一个固有通频带，各类复杂的电子电路无一例外都存在电感、电容或相当功能的储能元件，即使没有采用现成的电感线圈或电容，导线自身就是一个电感，而导线与导线之间、导线与地之间便可以组成电容——这就是通常所说的杂散电容或分布电容；不管是哪种类型的电容、电感，都会对信号起着阻滞作用从而消耗信号能量，严重的话会影响信号品质。这种效应与交流电信号的频率成正比关系，当频率高到一定程度、令信号难以保持稳定时，整个电子电路自然就无法正常工作。为此，电子学上就提出了带宽的概念，它指的是电路可以保持稳定工作的频率范围。而属于该体系的有显示器带宽、通讯/网络中的带宽等等。而第二种带宽的概念指的其实是数据传输率，譬如内存带宽、总线带宽、网络带宽等等，都是以字节/秒为单位。对于电子电路中的带宽，决定因素在于电路设计。它主要是由高频放大部分元件的特性决定，而高频电路的设计是比较困难的部分，成本也比普通电路要高很多。这部分内容涉及到电路设计的知识，对此我们就不做深入的分析。而对于总线、内存中的带宽，决定其数值的主要因素在于工作频率和位宽，在这两个领域，带宽等于工作频率与位宽的乘积，因此带宽和工作频率、位宽两个指标成正比。不过工作频率或位宽并不能无限制提高，它们受到很多因素的制约 [1]。总线带宽简介播报编辑在计算机系统中，总线的作用就好比是人体中的神经系统，它承担的是所有数据传输的职责，而各个子系统间都必须藉由总线才能通讯，例如，CPU和北桥间有前端总线、北桥与显卡间为AGP总线、芯片组间有南北桥总线，各类扩展设备通过PCI、PCI-X总线与系统连接；主机与外部设备的连接也是通过总线进行，如流行的USB 2.0、IEEE1394总线等等，一句话，在一部计算机系统内，所有数据交换的需求都必须通过总线来实现！按照工作模式不同，总线可分为两种类型，一种是并行总线，它在同一时刻可以传输多位数据，好比是一条允许多辆车并排开的宽敞道路，而且它还有双向单向之分；另一种为串行总线，它在同一时刻只能传输一个数据，好比只容许一辆车行走的狭窄道路，数据必须一个接一个传输、看起来仿佛一个长长的数据串，故称为“串行”。并行总线和串行总线的描述参数存在一定差别。对并行总线来说，描述的性能参数有以下三个：总线宽度、时钟频率、数据传输频率。其中，总线宽度就是该总线可同时传输数据的位数，好比是车道容许并排行走的车辆的数量；例如，16位总线在同一时刻传输的数据为16位，也就是2个字节；而32位总线可同时传输4个字节，64位总线可以同时传输8个字节......显然，总线的宽度越大，它在同一时刻就能够传输更多的数据。不过总线的位宽无法无限制增加。总线的带宽指的是这条总线在单位时间内可以传输的数据总量，它等于总线位宽与工作频率的乘积。例如，对于64位、800MHz的前端总线，它的数据传输率就等于64bit×800MHz÷8(Byte)=6.4GB/s；32位、33MHz PCI总线的数据传输率就是32bit×33MHz÷8=132MB/s，等等，这项法则可以用于所有并行总线上面——看到这里，读者应该明白我们所说的总线带宽指的就是它的数据传输率。对串行总线来说，带宽和工作频率的概念与并行总线完全相同，只是它改变了传统意义上的总线位宽的概念。在频率相同的情况下，并行总线比串行总线快得多，那么，为什么各类并行总线反而要被串行总线接替呢？原因在于并行总线虽然一次可以传输多位数据，但它存在并行传输信号间的干扰现象，频率越高、位宽越大，干扰就越严重，因此要大幅提高现有并行总线的带宽是非常困难的；而串行总线不存在这个问题，总线频率可以大幅向上提升，这样串行总线就可以凭借高频率的优势获得高带宽。而为了弥补一次只能传送一位数据的不足，串行总线常常采用多条管线（或通道）的做法实现更高的速度——管线之间各自独立，多条管线组成一条总线系统，从表面看来它和并行总线很类似，但在内部它是以串行原理运作的。对这类总线，带宽的计算公式就等于“总线频率×管线数”，这方面的例子有PCI Express和HyperTransport，前者有×1、×2、×4、×8、×16和×32多个版本，在第一代PCI Express技术当中，单通道的单向信号频率可达2.5GHz，我们以×16举例，这里的16就代表16对双向总线，一共64条线路，每4条线路组成一个通道，二条接收，二条发送。这样可以换算出其总线的带宽为2.5GHz×16/10=4GB/s（单向）。除10是因为每字节采用10位编码。内存带宽播报编辑除总线之外，内存也存在类似的带宽概念。其实所谓的内存带宽，指的也就是内存总线所能提供的数据传输能力，但它决定于内存芯片和内存模组而非纯粹的总线设计，加上地位重要，往往作为单独的对象讨论。SDRAM、DDR和DDRⅡ的总线位宽为64位，RDRAM的位宽为16位。而这两者在结构上有很大区别：SDRAM、DDR和DDRⅡ的64位总线必须由多枚芯片共同实现，计算方法如下：内存模组位宽=内存芯片位宽×单面芯片数量（假定为单面单物理BANK）；如果内存芯片的位宽为8位，那么模组中必须、也只能有8颗芯片，多一枚、少一枚都是不允许的；如果芯片的位宽为4位，模组就必须有16颗芯片才行，显然，为实现更高的模组容量，采用高位宽的芯片是一个好办法。而对RDRAM来说就不是如此，它的内存总线为串联架构，总线位宽就等于内存芯片的位宽。和并行总线一样，内存的带宽等于位宽与数据传输频率的乘积，例如，DDR400内存的数据传输频率为400MHz，那么单条模组就拥有64bit×400MHz÷8(Byte)=3.2GB/s的带宽；PC 800标准RDRAM的频率达到800MHz，单条模组带宽为16bit×800MHz÷ 8=1.6GB/s。为了实现更高的带宽，在内存控制器中使用双通道技术是一个理想的办法，所谓双通道就是让两组内存并行运作，内存的总位宽提高一倍，带宽也随之提高了一倍！带宽可以说是内存性能最主要的标志，业界也以内存带宽作为主要的分类标准，但它并非决定性能的要素，在实际应用中，内存延迟的影响并不亚于带宽。如果延迟时间太长的话相当不利，此时即便带宽再高也无济于事 [2]。带宽匹配播报编辑计算机系统中存在形形色色的总线，这不可避免带来总线速度匹配问题，其中最常出问题的地方在于前端总线和内存、南北桥总线和PCI总线。前端总线与内存匹配与否对整套系统影响最大，最理想的情况是前端总线带宽与内存带宽相等，而且内存延迟要尽可能低。在Pentium4刚推出的时候，Intel采用RDRAM内存以达到同前端总线匹配，但RDRAM成本昂贵，严重影响推广工作，Intel曾推出搭配PC133 SDRAM的845芯片组，但SDRAM仅能提供1.06GB/s的带宽，仅相当于400MHz前端总线带宽的1/3，严重不匹配导致系统性能大幅度下降；后来，Intel推出支持DDR266的845D才勉强好转，但仍未实现与前端总线匹配；接着，Intel将P4前端总线提升到533MHz、带宽增长至4.26GB/s，虽然配套芯片组可支持DDR333内存，可也仅能满足2/3而已；P4的前端总线提升到800MHz，而配套的865/875P芯片组可支持双通道DDR400——这个时候才实现匹配的理想状态，当然，这个时候继续提高内存带宽意义就不是特别大，因为它超出了前端总线的接收能力。南北桥总线带宽曾是一个尖锐的问题，早期的芯片组都是通过PCI总线来连接南北桥，而它所能提供的带宽仅仅只有133MB/s，若南桥连接两个ATA-100硬盘、100M网络、IEEE1394接口......区区133MB/s带宽势必形成严重的瓶颈，为此，各芯片组厂商都发展出不同的南北桥总线方案，如Intel的Hub-Link、VIA的V-Link、SiS 的MuTIOL，还有AMD的 HyperTransport等等，它们的带宽都大大超过了133MB/s，最高纪录已超过1GB/s，瓶颈效应已不复存在。PCI总线带宽不足还是比较大的矛盾，PC上使用的PCI总线均为32位、33MHz类型，带宽133MB/s，而这区区133MB/s必须满足网络、硬盘控制卡（如果有的话）之类的扩展需要，一旦使用千兆网络，瓶颈马上出现，业界打算自2004年开始以PCI Express总线来全面取代PCI总线，届时PCI带宽不足的问题将成为历史。显示器播报编辑以上我们所说的“带宽”指的都是速度概念，但对CRT显示器来说，它所指的带宽则是频率概念、属于电路范畴，更符合“带宽”本来的含义。要了解显示器带宽的真正含义，必须简单介绍一下CRT显示器的工作原理——由灯丝、阴极、控制栅组成的电子枪，向外发射电子流，这些电子流被拥有高电压的加速器加速后获得很高的速度，接着这些高速电子流经过透镜聚焦成极细的电子束打在屏幕的荧光粉层上，而被电子束击中的地方就会产生一个光点；光点的位置由偏转线圈产生的磁场控制，而通过控制电子束的强弱和通断状态就可以在屏幕上形成不同颜色、不同灰度的光点——在某一个特定的时刻，整个屏幕上其实只有一个点可以被电子束击中并发光。为了实现满屏幕显示，这些电子束必须从左到右、从上到下一个一个象素点进行扫描，若要完成800×600分辨率的画面显示，电子枪必须完成800×600=480000个点的顺序扫描。由于荧光粉受到电子束击打后发光的时间很短，电子束在扫描完一个屏幕后必须立刻再从头开始——这个过程其实十分短暂，在一秒钟时间电子束往往都能完成超过85个完整画面的扫描、屏幕画面更新85次，人眼无法感知到如此小的时间差异会“误以为”屏幕处于始终发亮的状态。而每秒钟屏幕画面刷新的次数就叫场频，或称为屏幕的垂直扫描频率、以Hz（赫兹）为单位，也就是我们俗称的“刷新率”。以800×600分辨率、85Hz刷新率计算，电子枪在一秒钟至少要扫描800×600×85=40800000个点的显示；如果将分辨率提高到1024×768，将刷新率提高到100Hz，电子枪要扫描的点数将大幅提高。按照业界公认的计算方法，显示器带宽指的就是显示器的电子枪在一秒钟内可扫描的最高点数总和，它等于“水平分辨率×垂直分辨率×场频（画面刷新次数）”，单位为MHz(兆赫)；由于显像管电子束的扫描过程是非线性的，为避免信号在扫描边缘出现衰减影响效果、保证图像的清晰度，总是将边缘扫描部分忽略掉，但在电路中它们依然是存在的。因此，我们在计算显示器带宽的时候还应该除一个取值为0.6~0.8 的“有效扫描系数”，故得出带宽计算公式如下：“带宽=水平像素（行数）×垂直像素（列数）×场频（刷新频率）÷扫描系数”。扫描系数一般取为0.744。例如，要获得分辨率1024×768、刷新率85Hz的画面，所需要的带宽应该等于：1024×768×85÷0.744，结果大约是90MHz。不过，这个定义并不符合带宽的原意，称之为“像素扫描频率”似乎更为贴切。带宽的 最初概念确实也是电路中的问题——简单点说就是：在“带宽”这个频率宽度之内，放大器可以处于良好的工作状态，如果超出带宽范围，信号会很快出现衰减失真现象。从本质上说，显示器的带宽描述的也是控制电路的频率范围，带宽高低直接决定显示器所能达到的性能等级。由于前文描述的“像素扫描频率”与控制电路的“带宽”基本是成正比关系，显示器厂商就干脆把它当作显示器的“带宽”——这种做法当然没有什么错，只是容易让人产生认识上的误区。当然，从用户的角度考虑没必要追究这么多，毕竟以“像素扫描频率”作为“带宽”是很合乎人们习惯的，大家可方便使用公式计算出达到某种显示状态需要的最低带宽数值。但是反过来说，“带宽数值完全决定着屏幕的显示状态”是否也成立呢？答案是不完全成立，因为屏幕的显示状态除了与带宽有关系之外，还与一个重要的概念相关——它就是“行频”。行频又称为“水平扫描频率”，它指的是电子枪每秒在荧光屏上扫描过的水平线数量，计算公式为：“行频=垂直分辨率×场频（画面刷新率）×1.07”，其中1.07为校正参数，因为显示屏上下方都存在我们看不到的区域。可见，行频是一个综合分辨率和刷新率的参数，行频越大，显示器就可以提供越高的分辨率或者刷新率。例如，1台17寸显示器要在1600×1200分辨率下达到75Hz的刷新率，那么带宽值至少需要221MHz，行频则需要96KHz，两项条件缺一不可；要达到这么高的带宽相对容易，而要达到如此高的行频就相当困难，后者成为主要的制约因素，而出于商业因素考虑，显示器厂商会突出带宽而忽略行频，这种宣传其实是一种误导。通讯带宽播报编辑在通讯和网络领域，带宽的含义又与上述定义存在差异，它指的是网络信号可使用的最高频率与最低频率之差、或者说是“频带的宽度”，也就是所谓的“Bandwidth”、“信道带宽”——这也是最严谨的技术定义。在100M以太网之类的铜介质布线系统中，双绞线的信道带宽通常用MHz为单位，它指的是信噪比恒定的情况下允许的信道频率范围，不过，网络的信道带宽与它的数据传输能力（单位Byte/s）存在一个稳定的基本关系。我们也可以用高速公路来作比喻：在高速路上，它所能承受的最大交通流量就相当于网络的数据运输能力，而这条高速路允许形成的宽度就相当于网络的带宽。显然，带宽越高、数据传输可利用的资源就越多，因而能达到越高的速度；除此之外，我们还可以通过改善信号质量和消除瓶颈效应实现更高的传输速度。网络带宽与数据传输能力的正比关系最早是由贝尔实验室的工程师Claude Shannon所发现，因此这一规律也被称为Shannon定律。而通俗起见普遍也将网络的数据传输能力与“网络带宽”完全等同起来，这样“网络带宽”表面上看与“总线带宽”形成概念上的统一，但这两者本质上就不是一个意思、相差甚远。总结播报编辑对总线和内存来说，带宽高低对系统性能有着举足轻重的影响——倘若总线、内存的带宽不够高的话，处理器的工作频率再高也无济于事，因此带宽可谓是与频率并立的两大性能决定要素。而对CRT显示器而言，带宽越高，往往可以获得更高的分辨率、显示精度越高，不过CRT显示器的带宽都能够满足标准分辨率下85Hz刷新率或以上的显示需要（相信没有太多的朋友喜欢用非常高的分辨率去运行程序或者游戏），这样带宽高低就不是一个太敏感的参数了，当然，如果你追求高显示品质那是另一回事了 [3]。

寄存器：
基本含义播报编辑寄存器是CPU内部用来存放数据的一些小型存储区域，用来暂时存放参与运算的数据和运算结果。其实寄存器就是一种常用的时序逻辑电路，但这种时序逻辑电路只包含存储电路。寄存器的存储电路是由锁存器或触发器构成的，因为一个锁存器或触发器能存储1位二进制数，所以由N个锁存器或触发器可以构成N位寄存器。寄存器是中央处理器内的组成部分。寄存器是有限存储容量的高速存储部件，它们可用来暂存指令、数据和位址。 [2]在计算机领域，寄存器是CPU内部的元件，包括通用寄存器、专用寄存器和控制寄存器。寄存器拥有非常高的读写速度，所以在寄存器之间的数据传送非常快。 [2]Cortex-M4总共有18个寄存器，相比传统ARM（如ARM7/ARM9/Cortex-A系列）的38个寄存器已减少很多，减少了内核核心面积(Die-size)。 [2]寄存器对于编译器非常友好易用，例如：包含灵活的寄存器配置，任意寄存器之间可实现单周期乘法，任意寄存器可以作为数据、结构或数组的指针。此外，Cortex-M4还包含4个特殊功能寄存器PRIMASK、FAUI。TMASK、BASEPRI和CONTROL。 [2]基本概念播报编辑寄存器最起码具备以下4种功能。①清除数码：将寄存器里的原有数码清除。 [3]②接收数码：在接收脉冲作用下，将外输入数码存入寄存器中。 [3]③存储数码：在没有新的写入脉冲来之前，寄存器能保存原有数码不变。 [3]④输出数码：在输出脉冲作用下，才通过电路输出数码。 [3]仅具有以上功能的寄存器称为数码寄存器；有的寄存器还具有移位功能，称为移位寄存器。 [3]PORT1的控制寄存器(2张)寄存器有串行和并行两种数码存取方式。将n位二进制数一次存入寄存器或从寄存器中读出的方式称为并行方式。将n位二进制数以每次1位，分成n次存入寄存器并从寄存器读出，这种方式称为串行方式。并行方式只需一个时钟脉冲就可以完成数据操作，工作速度快，但需要n根输入和输出数据线。串行方式要使用几个时钟脉冲完成输入或输出操作，工作速度慢，但只需要一根输入或输出数据线，传输线少，适用于远距离传输。 [3]结构播报编辑在数字电路中，用来存放二进制数据或代码的电路称为寄存器。寄存器是由具有存储功能的触发器组合起来构成的。一个触发器可以存储1位二进制代码，存放门位二进制代码的寄存器需用逐个触发器来构成。 [4]对寄存器中的触发器只要求它们具有置1，置0的功能即可，因而无论是用电平触发的锁存器（latch-up），还是用脉冲触发或边沿触发的触发器（flip-flop），都可以组成寄存器。 [4]由电平触发的动作特点可知，在CLK高电平期间，Q端的状态跟随D端状态的改变而改变；CLK变成低电平以后，Q端将保持CLK变为低电平时刻D端的状态。 [4]74HC175则是用CMOS边沿触发器组成的4位寄存器，根据边沿触发的动作特点可知，触发器输出端的状态仅仅取决于CLK上升沿到达时刻D端的状态。可见，虽然74LS75和74HC175都是4位寄存器，但由于采用了不同结构类型的触发器，所以动作特点是不同的。 [4]为了增加使用的灵活性，在有些寄存器电路中还附加了一些控制电路，使寄存器又增添了异步置零、输出三态控制和保持等功能。这里所说的保持，是指CLK信号到达时触发器不随D端的输入信号而改变状态，保持原来的状态不变。 [4]上面介绍的两个寄存器电路中，接收数据时所有各位代码都是同时输入的，而且触发器中的数据是并行地出现在输出端的，因此将这种输入、输出方式称为并行输入、并行输出方式。 [4]基本寄存器逻辑图(2张)工作原理播报编辑在计算机及其他计算系统中，寄存器是一种非常重要的、必不可少的数字电路构件，它通常由触发器（D触发器）组成，主要作用是用来暂时存放数码或指令。一个触发器可以存放一位二进制代码，若要存放N位二进制数码，则需用N个触发器。 [6]寄存器应具有接收数据、存放数据和输出数据的功能，它由触发器和门电路组成。只有得到“存入脉冲”（又称“存入指令”、“写入指令”）时，寄存器才能接收数据；在得到“读出”指令时，寄存器才将数据输出。 [6]寄存器存放数码的方式有并行和串行两种。并行方式是数码从各对应位输入端同时输入到寄存器中；串行方式是数码从一个输入端逐位输入到寄存器中。 [6]寄存器读出数码的方式也有并行和串行两种。在并行方式中，被读出的数码同时出现在各位的输出端上；在串行方式中，被读出的数码在一个输出端逐位出现。 [6]寄存器(3张)类型播报编辑1.通用寄存器组通用寄存器组包括AX、BX、CX、DX4个16位寄存器，用以存放16位数据或地址。也可用作8位寄存器。用作8位寄存器时分别记为AH、AL、BH、BL、CH、CL、DH、DL。只能存放8位数据，不能存放地址。它们分别是AX、BX、CX、DX的高八位和低八位。若AX=1234H，则AH=12H，AL=34H。通用寄存器通用性强，对任何指令，它们具有相同的功能。为了缩短指令代码的长度，在8086中，某些通用寄存器用作专门用途。例如，串指令中必须用CX寄存器作为计数寄存器，存放串的长度，这样在串操作指令中不必给定CX的寄存器号，缩短了串操作指令代码的长度。下面一一介绍：AX(AH、AL)：累加器。有些指令约定以AX(或AL)为源或目的寄存器。输入/输出指令必须通过AX或AL实现，例如：端口地址为43H的内容读入CPU的指令为INAL，43H或INAX，43H。目的操作数只能是AL/AX，而不能是其他的寄存器。 [5]BX(BH、BL)：基址寄存器。BX可用作间接寻址的地址寄存器和基地址寄存器，BH、BL可用作8位通用数据寄存器。 [5]CX(CH、CL)：计数寄存器。CX在循环和串操作中充当计数器，指令执行后CX内容自动修改，因此称为计数寄存器。 [5]DX(DH、DL)：数据寄存器。除用作通用寄存器外，在I/O指令中可用作端口地址寄存器，乘除指令中用作辅助累加器。 [5]2.指针和变址寄存器BP( Base Pointer Register)：基址指针寄存器。 [5]SP( Stack Pointer Register)：堆栈指针寄存器。 [5]SI( Source Index Register)：源变址寄存器。 [5]DI( Destination Index Register)：目的变址寄存器。 [5]这组寄存器存放的内容是某一段内地址偏移量，用来形成操作数地址，主要在堆栈操作和变址运算中使用。BP和SP寄存器称为指针寄存器，与SS联用，为访问现行堆栈段提供方便。通常BP寄存器在间接寻址中使用，操作数在堆栈段中，由SS段寄存器与BP组合形成操作数地址即BP中存放现行堆栈段中一个数据区的“基址”的偏移量，所以称BP寄存器为基址指针。 [5]SP寄存器在堆栈操作中使用，PUSH和POP指令是从SP寄存器得到现行堆栈段的段内地址偏移量，所以称SP寄存器为堆栈指针，SP始终指向栈顶。 [5]寄存器SI和DI称为变址寄存器，通常与DS一起使用，为访问现行数据段提供段内地址偏移量。在串指令中，其中源操作数的偏移量存放在SⅠ中，目的操作数的偏移量存放在DI中，SI和DI的作用不能互换，否则传送地址相反。在串指令中，SI、DI均为隐含寻址，此时，SI和DS联用，Dl和ES联用。 [5]3.段寄存器8086/8088CPU可直接寻址1MB的存储器空间，直接寻址需要20位地址码，而所有内部寄存器都是16位的，只能直接寻址6KB，因此采用分段技术来解决。将1MB的存储空间分成若干逻辑段，每段最长64KB，这些逻辑段在整个存储空间中可浮动。 [5]8086/8088CPU内部设置了4个16位段寄存器，它们分别是代码段寄存器CS、数据段寄存器DS、堆栈段寄存器SS、附加段寄存器ES、由它们给出相应逻辑段的首地址，称为“段基址”。段基址与段内偏移地址组合形成20位物理地址，段内偏移地址可以存放在寄存器中，也可以存放在存储器中。 [5]例如：代码段寄存器CS存放当前代码段基地址，IP指令指针寄存器存放了下一条要执行指令的段内偏移地址，其中CS=2000H，IP=001AH。通过组合，形成20位存储单元的寻址地址为2001AH。 [5]代码段内存放可执行的指令代码，数据段和附加段内存放操作的数据，通常操作数在现行数据段中，而在串指令中，目的操作数指明必须在现行附加段中。堆栈段开辟为程序执行中所要用的堆栈区，采用先进后出的方式访问它。各个段寄存器指明了一个规定的现行段，各段寄存器不可互换使用。程序较小时，代码段、数据段、堆栈段可放在一个段内，即包含在64KB之内，而当程序或数据量较大时，超过了64KB，那么可以定义多个代码段或数据段、堆栈段、附加段。现行段由段寄存器指明段地址，使用中可以修改段寄存器内容，指向其他段。有时为了明确起见，可在指令前加上段超越的前缀，以指定操作数所在段。 [5]4.指令指针寄存器IP8086/8088CPU中设置了一个16位指令指针寄存器IP，用来存放将要执行的下一条指令在现行代码段中的偏移地址。程序运行中，它由BIU自动修改，使IP始终指向下一条将要执行的指令的地址，因此它是用来控制指令序列的执行流程的，是一个重要的寄存器。8086程序不能直接访问IP，但可以通过某些指令修改IP的内容。例如，当遇到中断指令或调用子程序指令时，8086自动调整IP的内容，将IP中下一条将要执行的指令地址偏移量入栈保护，待中断程序执行完毕或子程序返回时，可将保护的内容从堆栈中弹出到IP，使主程序继续运行。在跳转指令时，则将新的跳转目标地址送入IP，改变它的内容，实现了程序的转移。 [5]5.标志寄存器FR标志寄存器FR也称程序状态字寄存器。 [5]寄存器(2张)FR是16位寄存器，其中有9位有效位用来存放状态标志和控制标志。状态标志共6位，CF、PF、AF、ZF、SF和OF，用于寄存程序运行的状态信息，这些标志往往用作后续指令判断的依据。控制标志有3位，IF、DF和TF，用于控制CPU的操作，是人为设置的。 [5]存放代码满足条件播报编辑(1)代码要存得进；(2)代码要记得住；(3)代码要取得出。 [7]寄存器是由具有存储功能的触发器组合起来构成的。一个触发器可以存储1位2进制代码，存放n位2进制代码的寄存器，需用n个触发器来构成。对寄存器中的触发器只要求它具有置1、置0的功能即可，因而无论用何种类型的触发器都可组成寄存器。 [7]按照功能的不同，寄存器可分为基本寄存器和移位寄存器两大类。基本寄存器只能并行送入数据，需要时也只能并行输出。移位寄存器中的数据可以在移位脉冲作用下依次逐位右移或左移，数据既可以并行输入、并行输出，也可以串行输入、串行输出，还可以并行输入、串行输出或串行输入、并行输出，十分灵活，用途也很广。 [7]寄存器组织播报编辑ARM微处理器共有37个32位寄存器，其中31个为通用寄存器，6个为状态寄存器。但是这些寄存器不能被同时访问，具体哪些寄存器是可编程访问的，取决于微处理器的工作状态及具体的运行模式。但在任何时候，通用寄存器R14~R0、程序计数器PC、一个或两个状态寄存器都是可访问的。 [8]ARM9处理器共有37个32位长的寄存器，这些寄存器包括：(1) RO～R12：均为32位通用寄存器，用于数据操作。但是注意：绝大多数16位Thumb指令只能访问R0～R7，而32位Thumb -2指令可以访问所有寄存器。 [9](2)堆栈指针：堆栈指针的最低两位永远是O，这意味着堆栈总是4字节对齐的。 [9](3)链接寄存器：当呼叫一个子程序时，由R14存储返回地址。 [9](4)程序计数器：指向当前的程序地址，如果修改它的值，就能改变程序的执行流。 [9](5)6个状态寄存器（1个CPSR、5个SPSR），用以标识CPU的工作状态及程序的运行状态，均为32位，目前只使用了其中的一部分。 [9]Cortex-A8处理器有40个32位长的寄存器，多了监控模式下的寄存器，如RO～R12、R15、CPSR通用，R13_ mon、R14_mon、SPSR_mon三个专用寄存器。 [9]寄存器寻址播报编辑寄存器寻址就是利用寄存器中的数值作为操作数，这种寻址方式是各类微处理器经常采用的一种方式，也是一种执行效率较高的寻址方式。 [10]寄存器寻址是指操作数存放在CPU内部的寄存器中，指令中给出操作数所在的寄存器名。寄存器操作数可以是8位寄存器AH、AL、BH、BL、CH、CL、DH、DL，也可以是16位寄存器AX、BX、CX、DX、SP、BP、SI、DI等。因为寄存器寻址不需要通过总线操作访问存储器，所以指令执行速度比较快。 [11]寄存器寻址( Register Addressing)是以通用寄存器的内容作为操作数的寻址方式，在该寻址方式下，操作数存放在寄存器中。寄存器寻址方式的寻址对象为：A，B，DPTR，R0~R7。其中，B仅在乘除法指令中为寄存器寻址，在其他指令中为直接寻址。A可以按寄存器寻址又可以直接寻址，直接寻址时写成ACC。 [12]

地址寄存器：
简介播报编辑地址寄存器（Address Register,AR）用来保存当前CPU所访问的内存单元的地址。由于在内存和CPU之间存在着操作速度上的差别，所以必须使用地址寄存器来保持地址信息，直到内存的读/写操作完成为止。 [1]数据寄存器DR用来暂存微处理器与存储器或输人/输出接口电路之间待传送的数据。地址寄存器AR和数据寄存器DR在微处理器的内部总线和外部总线之间，还起着隔离和缓冲的作用。结构播报编辑地址寄存器采用单纯的寄存器结构。在对主存或I/O端口进行访问时，地址寄存器存放当前访问的地址，数据缓冲器实现数据的缓冲。CPU通过修改地址寄存器中的值，就可访问不同的存储器单元及不同的I/O端口。 [2]地址寄存器可用LPM库中的元件lpm_latch锁存器来完成。图是地址寄存器的结构图。地址寄存器的数据宽度应当与程序计数器的数据宽度一致。data[7…0]是地址寄存器的数据输入端，q[7…0]是地址寄存器的数据输出端，gate是地址锁存器的控制端。gate的作用是当锁存控制脉冲到来时，高电平时数据进入锁存器，低电平时锁存数据，保持输出数据稳定不变。 [3]特点播报编辑当CPU和内存进行信息交换，即CPU向内存存/取数据时，或者CPU从内存中读出指令时，都要使用地址寄存器和数据缓冲寄存器。同样，如果我们把外围设备的设备地址作为像内存的地址单元那样来看待，那么，当CPU和外围设备交换信息时，我们同样使用 地址寄存器和数据缓冲寄存器。地址寄存器的结构和数据缓冲寄存器、指令寄存器一样，通常使用单纯的寄存器结构。信息的存入一般采用电位-脉冲方式，即电位输入端对应数据信息位，脉冲输入端对应控制信号，在控制信号作用下，瞬时地将信息打入寄存器。8086地址寄存器播报编辑8086有8个16比特的寄存器，包括栈寄存器SP与BP，但不包括指令寄存器IP、控制寄存器FLAGS以及四个段寄存器。AX, BX, CX, DX,这四个寄存器可以按照字节访问；但BP, SI, DI, SP,这四个地址寄存器只能按照16位宽访问。 [2]8086以8080和8085（它与8080有汇编语言上的源代码兼容性）的设计为基础，拥有类似的暂存器集合，但是扩充为16位。总线接口单元（Bus Interface Unit）通过6字节预存（prefetch）的贮列（queue）将指令送给运行单元（Execution Unit），所以取指令和运行是同步的－一种流水线的原始形式（8086指令长度变化从1到6字节）。 [2]8086有四个完全一样的16位暂存器，但也能够当作八个8位暂存器来访问；以及四个16位变址寄存器（包含堆栈索引）。数据暂存器通常由指令隐含地使用，针对暂存值需要复杂的暂存器配置。它提供64K 8位的输出输入（或32K 16位）端口，以及固定的矢量中断。大部分的指令只能够访问一个存储器地址，所以其中一个运算符必须是一个暂存器。运算结果会存储在运算符中的一个。 [2]64-bit地址寄存器可存储2个地址，存储器的基本单位是Byte，换言之最大支持16EiB存储器，1EiB则相等于1024GiB。但是，现在的64-bit CPU并没有64位地址总线。 [2]

输出设备：
输出设备的作用播报编辑输出设备是对将外部世界信息发送给计算机的设备和将处理结果返回给外部世界的设备的总称。这些返回结果可能是作为使用者能够视觉上体验的，或是作为该计算机所控制的其他设备的输入：对于一台机器人，控制计算机的输出基本上就是这台机器人本身，如做出各种行为。输出设备的功能播报编辑输出设备的功能是将内存中计算机处理后的信息以能为人或其它设备所接收的形式输出。输出设备的种类播报编辑输出设备种类也很多．计算机常用的输出设备有各种打印机、凿孔输出设备、显示设备和绘图机等。打印机和显示设备已成为每台计算机和大多数终端所必需的设备。纸带凿孔输出机　计算机用纸带凿孔输出设备。计算机输出信息用凿孔纸带上的小孔表示。这既可将信息长期保存于纸带上,又可利用凿孔纸带再输入计算机。卡片凿孔输出机　计算机用卡片凿孔输出设备。凿孔卡片阅读方便，可长期保存，也可作为计算机的输入。输出设备的分类播报编辑显示器黑白显示器显示器（Display）又称监视器,是实现人机对话的主要工具。它既可以显示键盘输入的命令或数据,也可以显示计算机数据处理的结果。常用的显示器主要有两种类型。一种是CRT（Cath-odeRayTube,阴极射线管）显示器,用於一般的台式微机；另一种是液晶（LiquidCrystalDisplay,简称LCD）显示器,用於便携式微机．下面主要介绍CRT显示器．按颜色区分,可以分为单色（黑白）显示器和彩色显示器。彩色显示器又称图形显示器。它有两种基本工作方式:字符方式和图形方式。在字符方式下,显示内容以标准字符为单位,字符的字形由点阵构成，字符点阵存放在字形发生器中。在图形方式下，显示内容以像素为单位,屏幕上的每个点（像素）均可由程序控制其亮度和颜色，因此能显示出较高质量的图形或图像。显示器的分辨率分为高中低三种。分辨率的指标是用屏幕上每行的像素数与每帧（每个屏幕画面）行数的乘积表示的．乘积越大，也就是像素点越小，数量越多，分辨率就越高,图形就越清晰美观。显示器适配器彩色显示器显示器适配器又称显示器控制器，是显示器与主机的接口部件，以硬件插卡的形式插在主机板上。显示器的分辨率不仅决定於阴极射线管本身，也与显示器适配器的逻辑电路有关。常用的适配器有：（1）CGA（ColourGraphicAdapter）彩色图形适配器，俗称CGA卡,适用於低分辨率的彩色和单色显示器。它支持的显示方式为：字符方式下，40列×25行，80列×25行，4色或2色。图形方式下，320×200，4色；640×200，2色。（2）EGA（EnhancedGraphicAdapter）增强型图形适配器，俗称EGA卡，适用於中分辨率的彩色图形显示器．它支持的显示方式为：字符方式下，80×25列，256色 图形方式下，640×350，16色超级EGA卡，支持800×600，16色。液晶显示器（3）VGA（VideoGraphicArray）视频图形阵列，俗称VGA卡，适用於高分辨率的彩色图形显示器。标准的分辨率为640×480，256色。使用的多是增强型的VGA卡，比如SuperVGA卡等,分辨率为800×600，1024×768等，256种颜色。（4）中文显示器适配器中国在开发汉字系统过程中，研制了一些支持汉字的显示器适配器，比如GW-104卡、CEGA卡、CVGA卡等，解决了汉字的快速显示问题。打印机打印机打印机（Printer）是将计算机的处理结果打印在纸张上的输出设备。人们常把显示器的输出称为软拷贝，把打印机的输出称为硬拷贝。将计算机输出数据转换成印刷字体的设备。从使用角度看可分为两类。一类具有键盘输入功能，速度较慢，但与计算机有对话能力。它价格低廉，除计算机和终端常用外，通信系统也把它用作常规设备。另一类没有键盘输入功能。这类打印机又可分为条式打印机、窄行式打印机、串行打印机、行式打印机和页式打印机等。按照物理结构，打印机又可分为击打式和非击打式两类。打印机分类按传输方式，可以分为一次打印一个字符的字符打印机、一次打印一行的行式打印机和一次打印一页的页式打印机。打印机按工作机构，可以分为击打式打印机和非击打式印字机。其中击打式又分为字模式打印机和点阵式打印机。非击打式又分为喷墨印字机、激光印字机、热敏印字机和静电印字机。微型计算机最常用的是点阵式打印机。点阵针式打印机特点：结构简单，体积小，价格低，字符种类不受限制，对打印介质要求不高，可以打印多层介质。结构：打印头与字车；输纸机构；色带机构；控制器：与显示控制器类似。它的打印头上安装有若干个针，打印时控制不同的针头通过色带打印纸面即可得到相应的字符和图形，因此,又常称之为针式打印机。日常使用的多为9针或24针的打印机,主要是24针打印机。喷墨印字机和激光印字机也得到广泛应用。喷墨式是通过磁场控制一束很细墨汁的偏转，同时控制墨汁的喷与不喷，即可得到相应的字符或图形。激光式则是利用电子照相原理,由受到控制的激光束射向感光鼓表面，在不同位置吸附上厚度不同的碳粉，通过温度与压力的作用把相应的字符或图形印在纸上．它与静电复印机的方式很相似。激光印字机分辨率高，印出字形清晰美观,但价格较高。喷墨打印机喷墨打印机是类似于用墨水写字一样的打印机,可直接将墨水喷射到普通纸上实现印刷,如喷射多种颜色墨水则可实现彩色硬拷贝输出。喷墨打印机的喷墨技术有连续式和随机式两种,目前市场上流行的各种型号打印机,大多采用随机式喷墨技术。而早年的喷墨打印机以及当前输出的大幅面打印机采用连续式喷墨技术。激光打印机的性能普通激光印字机的印字分辨率都能达到300DPI (每英寸300个点)或400DPI,甚至600DPI。特别是对汉字或图形/图像输出,是理想的输出设备。激光打印机称为“页式输出设备”，用每分钟输出的页数(pages per minute，简称PPM)来表示。高速的在100PPM以上,中速为30~60PPM,它们主要用于大型计算机系统。低速为10~20PPM，甚至10PPM以下，主要用于办公自动化系统和文字编辑系统。 [1]热转印打印机热转印打印机的印字质量优于点阵针式打印机,与喷墨打印机相当,印字速度比较快,串式一般可超过6页/分,分辨率达到360DPI。印字原理热转印打印机中的印字头是用半导体集成电路技术制成的薄膜头,头中有发热电阻,它由一种能耐高功率密度和耐高温的薄膜材料组成。将具有热敏性能的油墨涂在涤纶基膜上便构成热转印色带,色带位于热印字头与记录纸之间。印字时,脉冲信号将印字头中的发热电阻加热到几百度(如300℃),而印字头又压在涤纶膜上,使膜基上的油墨熔化而转移到记录纸上留下色点由色点组成字符,图形或图像。 [1]打印机若打印汉字，对於装有汉字库的打印机，可直接打印，打印速度快．如无汉字库，在微机中则需安装该种打印机的汉字驱动程序，使用微机的汉字库，打印速度较慢。工作方式打印机有联机和脱机两种工作方式。所谓联机，就是与主机接通，能够接收及打印主机传送的信息；所谓脱机，就是切断与主机的联系。在脱机状态下，可以进行自检或自动进/退纸，这两种状态由打印机面板上的联机键控制。打印机控制器打印机控制器亦称打印机适配器,是打印机的控制机构。也是打印机与主机的接口部件，以硬件插卡的形式插在主机板上。标准接口是并行接口，它可以同时传送多个数据，比串行接口传输速度快。绘图机绘图机自动绘图机是直接由电子计算机或数字信号控制，用以自动输出各种图形、图像和字符的绘图设备。可采用联机或脱机的工作方式。是计算机辅助制图和计算机辅助设计中广泛使用的一种外围设备。常见的按绘图方式分为跟踪式绘图机（如笔式绘图机）和扫描式绘图机（如静电扫描绘图机、激光扫描绘图机、喷墨式扫描绘图机）等。按机械结构分为滚筒式（鼓式）绘图机和平台式绘图机两大类。数控绘图机的传动方式有钢丝或钢带传动；有滚珠丝杠或齿轮齿条传动；有电机传动，如采用开环控制方式的直线步进电机和采用闭环控制的伺服电机等。绘图仪能按照人们要求自动绘制图形的设备。它可将计算机的输出信息以图形的形式输出。主要可绘制各种管理图表和统计图、大地测量图、建筑设计图、电路布线图、各种机械图与计算机辅助设计图等。最常用的是X-Y绘图仪。现代的绘图仪已具有智能化的功能，它自身带有微处理器，可以使用绘图命令，具有直线和字符演算处理以及自检测等功能。这种绘图仪一般还可选配多种与计算机连接的标准接口。绘图仪绘图仪是一种输出图形的硬拷贝设备。绘图仪在绘图软件的支持下课绘制出复杂、精确的图形，是各种计算机辅助设计不可缺少的工具。绘图仪的性能指标主要有绘图笔数、图纸尺寸、分辨率、接口形式及绘图语言等。绘图仪一般是由驱动电机、插补器、控制电路、绘图台、笔架、机械传动等部分组成。绘图仪除了必要的硬设备之外，还必须配备丰富的绘图软件。只有软件与硬件结合起来，才能实现自动绘图。绘图仪的种类很多，按结构和工作原理可以分为滚筒式和平台式两大类：①滚筒式绘图仪。当X向步进电机通过传动机构驱动滚筒转动时，链轮就带动图纸移动，从而实现X方向运动。Y方向的运动，是由Y向步进电机驱动笔架来实现的。这种绘图仪结构紧凑，绘图幅面大。但它需要使用两侧有链孔的专用绘图纸。②平台式绘图仪。绘图平台上装有横梁，笔架装在横梁上，绘图纸固定在平台上。X向步进电机驱动横梁连同笔架，作X方向运动；Y向步进电机驱动笔架沿着横梁导轨，作Y方向运动。图纸在平台上的固定方法有3种，即真空吸附、静电吸附和磁条压紧。平台式绘图仪绘图精度高，对绘图纸无特殊要求，应用比较广泛。发展前景播报编辑输出设备第一代计算机的输出设备种类非常有限。通常的输入设备是打孔卡片的读卡机，用来将指令和数据导入内存；而用于存储结果的输出设备则一般是磁带。随着科技的进步，输入输出设备的丰富性得到提高。以个人计算机为例：键盘和鼠标是用户向计算机直接输入信息的主要工具，而显示器、打印机、扩音器、耳机则返回处理结果。此外还有许多输入设备可以接受其他不同种类的信息，如数码相机可以输入图像。在输入输出设备中，有两类很值得注意：第一类是二级存储设备，如硬盘，光碟或其他速度缓慢但拥有很高容量的设备。第二个是计算机网络访问设备，通过他们而实现的计算机间直接数据传送极大地提升了计算机的价值。国际互联网成就了数以千万计的计算机彼此间传送各种类型的数据。

存储单元：
介绍播报编辑存储单元存储单元：在存储器中有大量的存储元，把它们按相同的位划分为组，组内所有的存储元同时进行读出或写入操作，这样的一组存储元称为一个存储单元。一个存储单元通常可以存放一个字节；存储单元是CPU访问存储器的基本单位。 [1-2]存储单元播报编辑地址上存储单元的过程在计算机中最小的信息单位是bit，也就是一个二进制位，8个bit组成一个Byte，也就是字节。一个存储单元可以存储一个字节，也就是8个二进制位。计算机的存储器容量是以字节为最小单位来计算的，对于一个有128个存储单元的存储器，可以说它的容量为128字节。如果有一个1KB的存储器则它有1024个存储单元，它的编号为从0－1023。存储器被划分成了若干个存储单元，每个存储单元都是从0开始顺序编号，如一个存储器有128个存储单元，则它的编号就是从0-127。存储地址一般用十六进制数表示，而每一个存储器地址中又存放着一组二进制（或十六进制）表示的数，通常称为该地址的内容。值得注意的是，存储单元的地址和地址中的内容两者是不一样的。前者是存储单元的编号，表示存储器中的一个位置，而后者表示这个位置里存放的数据。正如一个是房间号码，一个是房间里住的人一样。存放一个机器字的存储单元，通常称为字存储单元，相应的单元地址叫字地址。而存放一个字节的单元，称为字节存储单元，相应的地址称为字节地址。如果计算机中可以编址的最小单元是字存储单元，则该计算机称为按字寻址的计算机。如果计算机中可编址的最小单位是字节，则该计算机称为按字节寻址的计算机。如果机器字长等于存储器单元的位数，一个机器字可以包含数个字节，所以一个存储单元也可以包含数个能够单独编址的字节地址。例如一个16位二进制的字存储单元可存放两个字节，可以按字地址寻址，也可以按字节地址寻址。当用字节地址寻址时，16位的存储单元占两个字节地址。最小静态存储单元播报编辑世界上最小的静态存储单元2008年8月18日，美国IBM公司、AMD以及纽约州立大学Albany分校的纳米科学与工程学院（CNSE）等机构共同宣布，世界上首个22纳米节点有效静态随机存储器（SRAM）研制成功。这也是全世界首次宣布在300毫米研究设备环境下，制造出有效存储单元。SRAM芯片是更复杂的设备，比如微处理器的“先驱”。SRAM单元的尺寸更是半导体产业中的关键技术指标。最新的SRAM单元利用传统的六晶体管设计，仅占0.1平方微米，打破了此前的SRAM尺度缩小障碍。新的研究工作是在纽约州立大学Albany分校的纳米科学与工程学院（CNSE）完成的，IBM及其他伙伴的许多顶尖的半导体研究都在这里进行。IBM科技研发部副总裁T.C.Chen博士称，“我们正在可能性的终极边缘进行研究，朝着先进的下一代半导体技术前进。新的研究成果对于不断驱动微电子设备小型化的追求，可以说至关重要。”22纳米是芯片制造的下两代，而下一代是32纳米。在这方面，IBM及合作伙伴正在发展它们无与伦比的32纳米高K金属栅极工艺（high-Kmetalgatetechnology）。从传统上而言，SRAM芯片通过缩小基本构建单元，来制造得更加紧密。IBM联盟的研究人员优化了SRAM单元的设计和电路图，从而提升了稳定性，此外，为了制造新型SRAM单元，他们还开发出几种新的制作工艺流程。研究人员利用高NA浸没式光刻（high-NAimmersionlithography）技术刻出了模式维度和密度，并且在先进的300毫米半导体研究环境中制作了相关部件。与SRAM单元相关的关键技术包括：边带高K金属栅极、<25纳米栅极长度晶体管、超薄隔离结构（spacer）、共同掺杂、先进激活技术、极薄硅化物膜以及嵌入式铜触点等。据悉，在2008年12月15至17日美国旧金山将要举行的IEEE国际电子设备（IEDM）年会上，还会有专门的报告来介绍最新成果的细节。相关应用播报编辑在计算机中，由控制器解释，运算器执行的指令集是一个精心定义的数目十分有限的简单指令集合。一般可以分为四类：1）、数据移动 （如：将一个数值从存储单元A拷贝到存储单元B）2）、数逻运算（如：计算存储单元A与存储单元B之和，结果返回存储单元C）3）、 条件验证（如：如果存储单元A内数值为100，则下一条指令地址为存储单元F）4）、指令序列改易（如：下一条指令地址为存储单元F） [1]

内存碎片：
产生播报编辑内存分配有静态分配和动态分配两种。 静态分配在程序编译链接时分配的大小和使用寿命就已经确定，而应用上要求操作系统可以提供给进程运行时申请和释放任意大小内存的功能，这就是内存的动态分配。因此动态分配将不可避免会产生内存碎片的问题，那么什么是内存碎片？内存碎片即“碎片的内存”描述一个系统中所有不可用的空闲内存，这些碎片之所以不能被使用，是因为负责动态分配内存的分配算法使得这些空闲的内存无法使用，这一问题的发生，原因在于这些空闲内存以小且不连续方式出现在不同的位置。因此这个问题的或大或小取决于内存管理算法的实现上。为什么会产生这些小且不连续的空闲内存碎片呢？实际上这些空闲内存碎片存在的方式有两种：a.内部碎片 b.外部碎片 。内部碎片的产生：因为所有的内存分配必须起始于可被 4、8 或 16 整除（视处理器体系结构而定）的地址或者因为MMU的分页机制的限制，决定内存分配算法仅能把预定大小的内存块分配给客户。假设当某个客户请求一个 43 字节的内存块时，因为没有适合大小的内存，所以它可能会获得 44字节、48字节等稍大一点的字节，因此由所需大小四舍五入而产生的多余空间就叫内部碎片。外部碎片的产生： 频繁的分配与回收物理页面会导致大量的、连续且小的页面块夹杂在已分配的页面中间，就会产生外部碎片。假设有一块一共有100个单位的连续空闲内存空间，范围是0~99。如果你从中申请一块内存，如10个单位，那么申请出来的内存块就为0~9区间。这时候你继续申请一块内存，比如说5个单位大，第二块得到的内存块就应该为10~14区间。如果你把第一块内存块释放，然后再申请一块大于10个单位的内存块，比如说20个单位。因为刚被释放的内存块不能满足新的请求，所以只能从15开始分配出20个单位的内存块。现在整个内存空间的状态是0~9空闲，10~14被占用，15~34被占用，25~99空闲。其中0~9就是一个内存碎片了。如果10~14一直被占用，而以后申请的空间都大于10个单位，那么0~9就永远用不上了，变成外部碎片。 [1]分类播报编辑内存碎片分为：内部碎片和外部碎片。内部碎片内部碎片就是已经被分配出去（能明确指出属于哪个进程）却不能被利用的内存空间；内部碎片是处于区域内部或页面内部的存储块。占有这些区域或页面的进程并不使用这个存储块。而在进程占有这块存储块时，系统无法利用它。直到进程释放它，或进程结束时，系统才有可能利用这个存储块。单道连续分配只有内部碎片。多道固定连续分配既有内部碎片，又有外部碎片。外部碎片外部碎片指的是还没有被分配出去（不属于任何进程），但由于太小了无法分配给申请内存空间的新进程的内存空闲区域。外部碎片是出于任何已分配区域或页面外部的空闲存储块。这些存储块的总和可以满足当前申请的长度要求，但是由于它们的地址不连续或其他原因，使得系统无法满足当前申请。多道可变连续分配只有外部碎片 [2]。减少方法播报编辑内存碎是因为在分配一个内存块后，使之空闲，但不将空闲内存归还给最大内存块而产生的。最后这一步很关键。如果内存分配程序是有效的，就不能阻止系统分配内存块并使之空闲。即使一个内存分配程序不能保证返回的内存能与最大内存块相连接（这种方法可以彻底避免内存碎片问题），但你可以设法控制并限制内存碎片。所有这些作法涉及到内存块的分割。每当系统减少被分割内存块的数量，确保被分割内存块尽可能大时，你就会有所改进。这样做的目的是尽可能多次反复使用内存块，而不要每次都对内存块进行分割，以正好符合请求的存储量。分割内存块会产生大量的小内存碎片，犹如一堆散沙。以后很难把这些散沙与其余内存结合起来。比较好的办法是让每个内存块中都留有一些未用的字节。留有多少字节应看系统要在多大程度上避免内存碎片。对小型系统来说，增加几个字节的内部碎片是朝正确方向迈出的一步。当系统请求1字节内存时，你分配的存储量取决于系统的工作状态。如果系统分配的内存存储量的主要部分是 1 ～ 16 字节，则为小内存也分配 16 字节是明智的。只要限制可以分配的最大内存块，你就能够获得较大的节约效果。但是，这种方法的缺点是，系统会不断地尝试分配大于极限的内存块，这使系统可能会停止工作。减少最大和最小内存块存储量之间内存存储量的数量也是有用的。采用按对数增大的内存块存储量可以避免大量的碎片。例如，每个存储量可能都比前一个存储量大 20%。在嵌入式系统中采用“一种存储量符合所有需要”对于嵌入式系统中的内存分配程序来说可能是不切实际的。这种方法从内部碎片来看是代价极高的，但系统可以彻底避免外部碎片，达到支持的最大存储量。将相邻空闲内存块连接起来是一种可以显著减少内存碎片的技术。如果没有这一方法，某些分配算法（如最先适合算法）将根本无法工作。然而，效果是有限的，将邻近内存块连接起来只能缓解由于分配算法引起的问题，而无法解决根本问题。而且，当内存块存储量有限时，相邻内存块连接可能很难实现。有些内存分配器很先进，可以在运行时收集有关某个系统的分配习惯的统计数据，然后，按存储量将所有的内存分配进行分类，例如分为小、中和大三类。系统将每次分配指向被管理内存的一个区域，因为该区域包括这样的内存块存储量。较小存储量是根据较大存储量分配的。这种方案是最先适合算法和一组有限的固定存储量算法的一种有趣的混合，但不是实时的。有效地利用暂时的局限性通常是很困难的，但值得一提的是，在内存中暂时扩展共处一地的分配程序更容易产生内存碎片。尽管其它技术可以减轻这一问题，但限制不同存储量内存块的数目仍是减少内存碎片的主要方法。现代软件环境业已实现各种避免内存碎片的工具。例如，专为分布式高可用性容错系统开发的 OSE 实时操作系统可提供三种运行时内存分配程序：内核 alloc()，它根据系统或内存块池来分配；堆 malloc()，根据程序堆来分配； OSE 内存管理程序 alloc_region，它根据内存管理程序内存来分配。从 许多方面来看，Alloc就是终极内存分配程序。它产生的内存碎片很少，速度很快，并有判定功能。你可以调整甚至去掉内存碎片。只是在分配一个存储量后，使之空闲，但不再分配时，才会产生外部碎片。内部碎片会不断产生，但对某个给定的系统和八种存储量来说是恒定不变的。Alloc 是一种有八个自由表的固定存储量内存分配程序的实现方法。系统程序员可以对每一种存储量进行配置，并可决定采用更少的存储量来进一步减少碎片。除开始时以外，分配内存块和使内存块空闲都是恒定时间操作。首先，系统必须对请求的存储量四舍五入到下一个可用存储量。就八种存储量而言，这一目标可用三个 如果 语句来实现。其次，系统总是在八个自由表的表头插入或删除内存块。开始时，分配未使用的内存要多花几个周期的时间，但速度仍然极快，而且所花时间恒定不变。堆 malloc() 的内存开销（8 ～ 16 字节/分配）比 alloc小，所以你可以停用内存的专用权。malloc() 分配程序平均来讲是相当快的。它的内部碎片比alloc()少，但外部碎片则比alloc()多。它有一个最大分配存储量，但对大多数系统来说，这一极限值足够大。可选的共享所有权与低开销使 malloc() 适用于有许多小型对象和共享对象的 C++ 应用程序。堆是一种具有内部堆数据结构的伙伴系统的实现方法。在 OSE 中，有 28 个不同的存储量可供使用，每种存储量都是前两种存储量之和，于是形成一个斐波那契（Fibonacci）序列。实际内存块存储量为序列数乘以 16 字节，其中包括分配程序开销或者 8 字节/分配（在文件和行信息启用的情况下为 16 字节）。当你很少需要大块内存时，则OSE内存管理程序最适用。典型的系统要把存储空间分配给整个系统、堆或库。在有 MMU 的系统中，有些实现方法使用 MMU 的转换功能来显著降低甚至消除内存碎片。在其他情况下，OSE 内存管理程序会产生非常多的碎片。它没有最大分配存储量，而且是一种最先适合内存分配程序的实现方法。内存分配被四舍五入到页面的偶数——典型值是 4 k 字节。

网络系统：
简介播报编辑NOS与运行在工作站上的单用户操作系统(如WINDOWS系列)或多用户操作系统（UNIX、Linux）由于提供的服务类型不同而有差别。一般情况下，NOS是以使网络相关特性达到最佳为目的的，如共享数据文件、软件应用，以及共享硬盘、打印机、调制解调器、扫描仪和传真机等。一般计算机的操作系统，如DOS和OS/2等，其目的是让用户与系统及在此操作系统上运行的各种应用之间的交互作用最佳。为防止一次有一个以上的用户对文件进行访问，一般网络操作系统都具有文件加锁功能。如果系统没有这种功能，用户将不会正常工作。文件加锁功能可跟踪使用中的每个文件，并确保一次只能一个用户对其进行编辑。文件也可由用户的口令加锁，以维持专用文件的专用性。NOS还负责管理LAN用户和LAN打印机之间的连接。NOS总是跟踪每一个可供使用的打印机，以及每个用户的打印请求，并对如何满足这些请求进行管理，使每个端用户感到进行操作的打印机犹如与其计算机直接相连。由于网络计算的出现和发展，现代操作系统的主要特征之一就是具有上网功能，因此，除了在20世纪90年代初期，Novell公司的Netware等系统被称为网络操作系统之外，人们一般不再特指某个操作系统为网络操作系统。模式分类播报编辑集中模式图示 [1]集中式网络操作系统是由分时操作系统加上网络功能演变的。系统的基本单元是由一台主机和若干台与主机相连的终端构成，信息的处理和控制是集中的。UNIX就是这类系统的典型。客户机/服务器模式这种模式是最流行的网络工作模式。服务器是网络的控制中心，并向客户提供服务。客户是用于本地处理和访问服务器的站点。对等模式采用这种模式的站点都是对等的，既可以作为客户访问其它站点，又可以作为服务器向其他站点提供服务。这种模式具有分布处理和分布控制的功能。LAN中的网络操作系统分类播报编辑Windows类对于这类操作系统相信用过电脑的人都不会陌生，这是全球最大的软件开发商--Microsoft（微软）公司开发的。微软公司的Windows系统不仅在个人操作系统中占有绝对优势，它在网络操作系统中也是具有非常强劲的力量。这类操作系统配置在整个局域网配置中是最常见的，但由于它对服务器的硬件要求较高，且稳定性能不是很高，所以微软的网络操作系统一般只是用在中低档服务器中，高端服务器通常采用UNIX、LINUX或Solaris等非Windows操作系统。在局域网中，微软的网络操作系统主要有：Windows NT 4.0 Serve、Windows 2000 Server/Advance Server，以及最新的Windows 2003 Server/ Advance Server等，工作站系统可以采用任一Windows或非Windows操作系统，包括个人操作系统，如Windows 9x/ME/XP等。在整个Windows网络操作系统中最为成功的还是要算了Windows NT4.0这一套系统，它几乎成为中、小型企业局域网的标准操作系统，一则是它继承了Windows家族统一的界面，使用户学习、使用起来更加容易。再则它的功能也的确比较强大，基本上能满足所有中、小型企业的各项网络需求。虽然相比Windows 2000/2003 Server系统来说在功能上要逊色许多，但它对服务器的硬件配置要求要低许多，可以更大程度上满足许多中、小企业的PC服务器配置需求。图示 [2]NetWare类NetWare操作系统虽然远不如早几年那么风光，在局域网中早已失去了当年雄霸一方的气势，但是NetWare操作系统仍以对网络硬件的要求较低（工作站只要是286机就可以了）而受到一些设备比较落后的中、小型企业，特别是学校的青睐。人们一时还忘不了它在无盘工作站组建方面的优势，还忘不了它那毫无过分需求的大度。且因为它兼容DOS命令，其应用环境与DOS相似，经过长时间的发展，具有相当丰富的应用软件支持，技术完善、可靠。目前常用的版本有3.11、3.12和4.10 、V4.11，V5.0等中英文版本，NetWare服务器对无盘站和游戏的支持较好，常用于教学网和游戏厅。目前这种操作系统有市场占有率呈下降趋势，这部分的市场主要被Windows NT/2000和Linux系统瓜分了。Unix系统目前常用的UNIX系统版本主要有：Unix SUR4.0、HP-UX 11.0，SUN的Solaris8.0等。支持网络文件系统服务，提供数据等应用，功能强大，由AT&T和SCO公司推出。这种网络操作系统稳定和安全性能非常好，但由于它多数是以命令方式来进行操作的，不容易掌握，特别是初级用户。正因如此，小型局域网基本不使用Unix作为网络操作系统，UNIX一般用于大型的网站或大型的企、事业局域网中。UNIX网络操作系统历史悠久，其良好的网络管理功能已为广大网络 用户所接受，拥有丰富的应用软件的支持。目前UNIX网络操作系统的版本 有：AT&T和SCO的UNIXSVR3.2、SVR4.0和SVR4.2等。UNIX本是针对小型机 主机环境开发的操作系统，是一种集中式分时多用户体系结构。因其体系 结构不够合理，UNIX的市场占有率呈下降趋势。Linux这是一种新型的网络操作系统，它的最大的特点就是源代码开放，可以免费得到许多应用程序。目前也有中文版本的Linux，如REDHAT(红帽子)，红旗Linux等。在国内得到了用户充分的肯定，主要体现在它的安全性和稳定性方面，它与Unix有许多类似之处。但目前这类操作系统目前使仍主要应用于中、高档服务器中。总的来说，对特定计算环境的支持使得每一个操作系统都有适合于自己的工作场合，这就是系统对特定计算环境的支持。例如，Windows 2000 Professional适用于桌面计算机，Linux目前较适用于小型的网络，而Windows 2000 Server和UNIX则适用于大型服务器应用程序。因此，对于不同的网络应用，需要我们有目的有选择合适地网络操作系统。网络服务器的架设播报编辑客户端与服务器又称主从式架构(Client/Server)，基本的架构为：客户端（Client），用户将所需的数据通过网络联系服务端，服务器（Server）收到消息后，搜索数据库内符合的数据，再通过网络回应给客户端，所有的需求都须经过网络，所以网络在主从式架构中，扮演着极重的角色。 [2]点对点技术（Peer to Peer）又称非中心式网络，每台电脑同时是客户端也是服务器，拥有平等的地位，此技术最大的特性就是资源共享，一般来说，在网络上同一个文件越多人下载就越难下载的到，因为服务端的流量是固定的，导致供不应求，但是P2P通过访问电脑上的带宽，再下载文件的同时利用未使用的上传带宽，分享文件，越多人下载则速度越快。 [2]与其他操作系统区别播报编辑网络操作系统是网络上各计算机能方便而有效地共享网络资源，为网络用户提供所需的各种服务的软件和有关规程的集合。网络操作系统与通常的操作系统有所不同，它除了应具有通常操作系统应具有的处理机管理、存储器管理、设备管理和文件管理外，还应具有以下两大功能：(1)提供高效、可靠的网络通信能力；(2)提供多种网络服务功能，如：远程作业录入并进行处理的服务功能；文件转输服务功能；电子邮件服务功能；远程打印服务功能

大型计算机：
简介播报编辑大型机是用来处理大容量数据的机器。欧盟委员会称，全球绝大多数企业数据依然存储在大型机上，2009年新大型机硬件销售额便达到了85亿欧元，其中仅在欧洲经济区销售额就达到30亿欧元。 [1]特征播报编辑现代大型计算机并非主要通过每秒运算次数MIPS来衡量性能，而是可靠性、安全性、向后兼容性和极其高效的I/O性能。主机通常强调大规模的数据输入输出，着重强调数据的吞吐量。 [1]大型计算机可以同时运行多操作系统，因此不像是一台计算机而更像是多台虚拟机，因此一台主机可以替代多台普通的服务器，是虚拟化的先驱。同时主机还拥有强大的容错能力。 [1]主机的投资回报率取决于处理数据的规模、减少人力开支、实现不间断服务和其他成本的缩减。由于主机的平台与操作系统并不开放，因而很难被攻破，安全性极强。 [1]大型机使用专用的操作系统和应用软件，在主机上编程采用COBOL，同时采用的数据库为IBM自行开发的DB2。在大型机上工作的DB2数据库管理员能够管理比其他平台多3～4倍的数据量。 [1]历史播报编辑在1960年代，大多数主机没有交互式的界面，通常使用打孔卡、磁带等等。 [1]1964年，IBM引入了System/360，它是由5种功能越来越强大的计算机所组成的系列，这些计算机运行同一操作系统并能够使用相同的44个外围设备。 [1]1972年，SAP公司为System/360开发了革命性的“企业资源计划”系统。 [1]1999年，Linux出现在System/390中，第一次将开放式源代码计算的灵活性与主机的传统可伸缩性和可靠性相结合。 [1]与超级计算机的区别播报编辑超级计算机有极强的计算速度，通常由于科学与工程上的计算，这些计算的速度受运算速度与内存大小所限制；而主机运算任务主要受数据传输与转移、可靠性及并发处理性能所限制。 [1]主机更倾向于整数运算，如订单数据、银行数据等，同时在安全性、可靠性和稳定性方面优于超级计算机。而超级计算机更强调浮点运算性能，如天气预报。主机在处理数据的同时需要读写或传输大量信息，如海量的交易信息、航班信息等等。 [1]优势播报编辑IBM mainframe Z10大型机体系结构的最大好处是无与伦比的I/O处理能力。虽然大型机处理器并不总是拥有领先优势，但是它们的I/O体系结构使它们能处理好几个PC服务器放一起才能处理的数据。大型机的另一些特点包括它们的大尺寸和使用液体冷却处理器阵列。在使用大量中心化处理的组织中，它们仍有重要的地位。 [1]由于小型计算机的到来，新型大型机的销售已经明显放慢。在电子商务系统中，如果数据库服务器或电子商务服务器需要高性能、高I/O处理能力，可以采用大型机。 [1]在20世纪60－80年代，信息处理主要是采用主机+终端的方式，即主机集中式处理方式。大型机的主要厂商有IBM/日立等。但无论是大型机本身还是它的维护成本都相当昂贵。因此，能够使用大型机的企业寥寥可数。进入80年代以后，随着PC和各种服务器的高速发展，大型机的市场变的越来越小，很多企业都放弃了原来的大型机改用小型机和服务器。 [1]另外，客户机/服务器（client/server）技术的飞速发展也是大型机市场萎缩的一个重要原因。这时的大型机就象濒临灭绝的恐龙逐渐走向灭亡。进入90年代后，经济进入全球化，信息技术得以高速的发展，随着企业规模的扩大，信息分散管理的弊端越来越多，运营成本迅速的增长。信息集中成了不可逆转的潮流。这时，人们又把目光集中到大型机的身上，大型机的市场逐渐的恢复了活力，直至今天，大型机还占有了不可替代的市场份额。90年代后期，大型机的技术得以飞速的发展，其处理能力也大踏步的提高，在民用领域，IBM已经完全占据了大型机的市场。 [1]发展历史播报编辑1948年，IBM开发制造了基于电子管的计算机SSEC。1952年IBM公司的第一台用于科学计算的大型机IBM701问世，1953年又推出了第一台用于数据处理的大型机IBM702和小型机IBM650，这样第一代商用计算机诞生了。此后，IBM于1965年又推出了701于702的后续产品704和705。1956年，IBM又推出了第一台随机存储系统RAMAC305,RAMAC是”计算与控制随机访问方法”的英文缩写。它是现代磁盘系统的先驱。1958年IBM又推出了7090，1960年又推出7040、7044大型数据处理机。总之，在1955年到1965年，美国名牌大学与大公司使用的计算机大多数是IBM704到IBM7094这些机器。 [1]1964年4月7日，IBM公布了360系统，成为计算机发展史上的一个重要了里程碑。System/360系统的主要贡献是：从应用的角度来看，克服了第二代计算机性能单调的弱点，集科学计算，数据处理和实时控制功能于一身，确定了通用性。从生产的角度讲，实现了系列化，360系统的主要型号有：20型、25型和30型小型机44型和50型小型机，65型，75型和85型大型机，以及91型和105型超级计算机。型号虽多，但采用了标准化措施，统一指令格式，统一数据格式，统一字符编码，统一I/O接口，统一中断系统以及统一人机对话方式等。由于确定了兼容性。是同一程序在不同型号的机器语言级上的兼容，促进了计算机工业能力的规模和发展。从发展的角度来看，既采用了新的技术，有留有发展的余地。 [1]日后S/370和S/390都是从S/360上发展而来的，他们各自标定着相应的系统体系机构。从使用的角度来看，360系统配有操作系统、汇编语言和FORTRAN、COBOL等高级语言，使用十分方便，更重要的是360在建立计算机系统的继承性上起来开创性的作用。1981年，IBM公布了扩展的System/370体系结构（370－XA）。System/370-XA将地址线位数增加到32位，大大增加了System/370的寻址能力，同时保留了24位的兼容方式（向上兼容）。同时370还增加了扩展存储器（EXPANDEDSTORAGE）。1988年，IBM对System/370做了进一步的改进，ESA（Enterprise SystemArchitecture）/370。ESA/370增加了访问寄存器，改进了虚存的性能。通过这项技术，应用可以访问称之为数据空间（dataspace）的另一个虚存空间。因此，ESA/370允许应用访问多个2G的数据空间。基于该体系结构的产品系列使得多用户可以更方便得共享系统资源。 [1]1990年9月，IBM开发了ESA/390以及ES/9000System/390系列计算机系统，其性价比远远高于System/370系统。IBMS/390系列与以前得S/370系统相比，在体系结构上作了如下改进： [1]⑴、企业系统连接结构ESCON：这是一种新得输入输出结构，它定义了一个规则，使存储器子系统、控制部件、通讯控制部件等I/O设备都通过这套规则与处理器进行通讯。ESCON得通讯速度是17M/S。 [1]⑵加密结构：在S/390中，计算机通过集成密码特征来实现对计算机中得信息进行加密或解密，以防止被非法访问。 [1]⑶子系统存贮保护：防止诸如CICS等子系统对系统存储器的干扰。这个功能由操作系统和子系统共同提供。 [1]⑷数据压缩：S/390在硬件级上提供了数据压缩。其压缩速度是软件压缩的5倍。 [1]⑸异步数据转移结构（ADMF）：利用I/O处理器去更有效的实现中央处理器与扩展存储器之间的信息转移，以空出处理器来处理其他任务。 [1]⑹DB2排序增强：使用硬件完成DB2的排序算法。 [1]工作原理播报编辑架构2000年后，IBM推出Z/Architecture架构主机，Z系列主机的旗舰产品为Z/990，这一体系用来减少由于缺少可寻址的内存而带来的瓶颈，并通过智能资源导向器（Intelligent Resource Director，IRD）自动将资源分配给高优先级的工作报。z/ArchitectureTM是ESA/390的64位扩展集。z990利用新的超标量体系结构的微型处理器和CMOS9S-SOI技术，它进一步扩展并集成了主要的平台特性，例如混合和无法预测的负载环境中的动态灵活分区和资源管理，为新兴的电子商务应用（例如IBM WebSphereTM，JavaTM和Linux┨峁┝死┱剐浴⒏呖捎眯院头务质量。z990将系统的扩展性以及服务器整合的机会进行了相当大的改进，提供了一个多区域（multi-book）系统结构，可支持配置一到四个区域。每个区域中包括一个多芯片模块（MultiChipModule，MCM）、内存卡—每个区域最多可支持64GB内存—以及新的高性能的自定时互连 (Self-TimedInterconnect）。为支持高度可扩展的多区域的系统设计，z990对通道子系统（Channel SubSystem，CSS）进行了改进，引入了多个逻辑通道子系统（Logical Channel SubSystem,LCSS），利用这些LCSS，在三个I/O箱中最多可以安装512个通道。TCP/IP通讯的高速互连（称作HiperSockets）使分区之间的TCP/IP传输速度达到内存的速度，而不是受限于网络的速度。支持光纤和铜连线的高速千兆以太网(Gigabit Ethernet,GbE）是业内线速最先达到每秒千兆的实现之一。 [1]技术发展S/390的系统运行方式为了满足各种不同应用业务的需求，S/390可以运行在不同的模式下。 [1]S/370本机模式在这种模式下，S/390同样可以执行S/370的217条指令，但是它利用S/370的扩展实寻址的26为地址，可以将重要存储器扩展到64位。中央存储器的容量的增大意味着页面调度次数（PAGING）的减少，因而提高了整个系统的性能。 [1]ESA/390方式这个方式的运行是按企业系统结构ESA/390所确定的功能来进行的，其指令集扩展到了229条指令。它使用32位地址，因此虚拟地址空间扩展大了2GB，但仍与24位的程序相兼容。在最大的S/390计算机系统中，总共可以配置256个通道。它支持并行通道，ESCON通道和集成I/O适配器。另外，它在硬件中还有自动处理“通道占线”条件，可以十分有效的减少与I/O动作有关的整个等待或延迟时间。 [1]ESA/390LPAR方式，所有的S/390系列计算机都可以在这种方式下运行，在这种模式下系统虽然物理在一个机柜里。但是在逻辑上，它最多可以分成16个部分，这种在逻辑上的划分是由PR/SM微码来完成的。每个系统可以拥有自己的通道CPU和内存，每个部分都可独立工作独立安装一个系统。在这种模式下一个物理CPU和一个物理ESCON通道可以同时划分给不同的LPAR。内存不可以。 [1]COUPLE FACILITY方式在这中方式下，S/390主机被作为了一个单独管理数据的主机。这种模式要运行CFCC微码，没有任何应用程序在CFCC里运行。主要是在并行耦合系统中应用。 [1]S/390的操作系统在S/390上可以同时运行多个操作系统，每个操作系统都有各自不同的应用程序环境。 [1]MVS/ESAMVS/ESA操作系统是S/390上运行的综合能力较强、可靠性较高的操作系统。它实际是1964年IBM在其S/360上运行的OS/360操作系统的后代。1972年IBM又推出了新的操作系统OS/VS2也就是单虚存系统。它最大的特点是提供对虚存的支持。1988年，IBM宣布并发行了MVS/ESA操作系统。同样，MVS/ESA也是为1988年问世的ESA/370的新功能而设计的。MVS/ESA最多可支持2GB的中央存储器并能很好的应用扩展存储器。1990年9月，IBM开发了MVS/ESA SPV4。MVS/ESA的这个版本管理存储器的方法与早期的MVS/ESA发行版一样，但它改进了对ESCON通道的复合系统时钟的支持。1994年，IBM发布了MVS/ESA的第5版，实现了操作系统并行耦合的功能，它最大可以支持32个MVS/ESA系统。每个系统都可以是一个多处理器。 [1]OS/390OS/390是一个集成的企业服务器操作系统。它将开发的通讯服务器、分布式数据和文件服务、并行耦合系统的支持、面向对象程序设计、DCE以及开发应用程序接口集成成为一个产品。由于它是MVS操作系统基础上发展起来的，因而保留了MVS的高可*性、持续可用性及安全性等优异性能，为用户提供具有可扩充性的系统。但MVS是封闭性的，而OS/390转变为开发性的。它包括许多服务器软件，具有整合的功能。 [1]Z/OS-z/OS的内核由OS/390发展而来，同时它又提供了一系列与z/900硬件与微码紧密结合的创新功能。其中的核心之一是智能资源导向器（简称IRD）技术，也曾被称为“LPAR集群”技术。IRD技术的实质是将工作量管理器（简称WLM）目标管理模式，结合并行系统综合体资源共享以及分区资源/系统管理（简称PR/SM）等多种技术，进行有机的整合以产生最大的效益，帮助用户将宝贵的系统资源在合适的时间分配给最需要的任务。-z/Series各操作系统所采用的新技术都为在该平台混合运行多种工作负荷提供了更好的支持。z/OS为传统主机应用和需要最高服务品质的Java及UNⅨ应用提供理想的运行环境。 [1]VM和LINUXVM（ⅥSUALMACHINE）是IBM早期在大型机上安装的底层操作系统，在VM上可以同时安装很多其他的操作系统。进入90年代由于S/390LPAR模式的诞生，VM几乎将被淘汰，但由于LINUX的兴起，VM再次被利用起来，在一台S/390的主机上，VM上可以同时运行上千个LINUX。当然LINUX也可以独自运行在S/390的一个分区上。运行在主机上的LINUX大都是TRUBO和SUSE的LINUX。后两家在LINUX进行合作，推出了UNITED LINUX。 [1]组成部分OS/390操作系统由其基本的部分和各个子系统组成，本节就基本部分和各个子系统进行简单介绍。 [1]MVS基本控制程序BCP [1]MVS基本控制程序BCP与JES2或JES3组成了OS/390的主干部分。它提供了基本的服务，使得OS/390能够更可靠、完全、完整的处理用户数据。与MVS相比OS/390增强了对LINKLST的处理、系统日志及APPC的支持。作业进入子系统（JES） [1]作业进入子系统接受要处理的作业并处理作业的输出。作业进入子系统共有两个，JES2和JES3。他们基本上提供相同的功能。通常使用的是JES2。JES工作过程如下：作业通过读卡机、分时终端系统或网络进入系统，或者由程序生成后传递给系统。所有的作业都要经过扫描检验其正确性，然后排入适当的队列。JES提供一种手段，使系统以优先权分级结构为基础，通过有序的方式调度工作。转换程序将作业控制语句（JCL）转换成系统能读的内部形式。同时进行一些其他的校验。如果发现错误，将该作业从系统中清除，并向用户发出相应的信息。下一步，作业又一次根据分级与优先权送至系统执行。这时JES放弃对作业的控制权，直到又打印输出被JES截获时，JES把它交付给假脱机磁盘，作业终止系统通知JES，作业进入下一个JES操作输出步骤，输出经过打印或穿孔后从系统中撤销，该作业用过的假脱机空间可以重新使用。JES对优先权进行动态调整，作业等待运行的时间越长，系统将它的优先权提升的越高。 [1]存储管理子系统（SMS）存储管理子系统主要完成如下功能：管理外存资源，存储管理子系统可以让你为操作系统定义自动管理外存储系统的策略（主要通过定义适当的SG，MC，SC，DC，ACSROUTE），系统能够安装定义的策略进行自动的管理外存系统。提供编目机制（VTOC,VVDS，VCAT），对磁盘、磁带上的数据进行编目（RMM），以便与用户能够方便迅速的访问数据。把程序存储在程序库中，以便于读出执行。定义系统中的输入输出设备并控制这些设备的操作。支持从工作站、个人计算机或基于SNA LU6.2网络的其他系统通过分布式文件管理（DFM）访问主机系统的数据. [1]分时系统（TSO）TSO是支持分时系统的软件，终端用户发出的每一条指令都由TSO处理，用户通过TSO命令和系统进行交互式工作，但这样作不太方便，IBM又在TSO开发了用户程序产品ISPF/PDF（INTERACTⅣE PRODUCTIⅥTY FACILITY/PROGRAMDEVELOPMENTFACILITY）。其中ISPF支持回话功能，PDF支持程序开发功能，从而使终端用户与TSO会话更加简单直观，提高用户的应用开发效率。ISPF/PDF是以屏幕为单位的菜单输入方式，用户只需进行一些简单的菜单选择就可以和系统进行交互了。在TSO下工作了另一个软件是SDSF（SYSTEM DISPLAY AND SEARCHFACILITY），用户利用SDSF可以非常方便的查看用户用户提交给系统的作业的返回信息，也能够修改作业的属性。 [1]设备支持机制（ICKDSF）ICKDSF可用于执行对IBM直接访问设备（DASD/硬盘）的安装和使用的任务。例如，你可以使用ICKDSF对DASD进行错误检查，格式化，碎片整理等维护。 [1]硬件配置定义（HCD）HCD拥有定义操作系统硬件配置的定义以及处理器硬件配置的定义。由于HCD是在设备定义时验证其数据的有效性，而不是在设备被访问的时候验证，设备定义的不一致性可以得到避免。 [1]SMP/ESMP/E是一个安装和维护软件的工具。它提供了一个可靠的方法用于安装维护OS/390中的软件。 [1]VTAMVTAM是实现SNA和APPN的网络通讯访问方法，它为在主机处理器上的应用程序和SNA网络上的其他资源之间提供了一个接口。VTAM为网络上的用户建立和终止会话。为了建立和终止这些会话，VTAM按照其控制激活和不激活资源，这些资源包括包括应用程序、网络控制程序（NCP）及其控制的设备以及VTAM直接连接的设备。VTAM也维护网络的配置信息、活动和网络条件。为了帮助用户控制网络，VTAM从操作员接受命令然后执行网络服务。它通过操作员通知操作员网络的状况。Anynet实现了多协议传换网络体系结构。它运行应用和传输服务之间消除强制联网协议约束。换句话说，应用和它们的服务能用超过一个协议通讯而非原始的实现。AnyNet的这一特性使得SNA应用可以在TCP/IP网络上通讯，统一建筑与TCP/IP之上的SNA对逻辑单元通讯提供了支持。它支持所有LU类型，包括LU6.29.TCP/IPTCP/IP是一组工业标准和应用，它允许你与其他的计算机共享数据和计算机资源不管这些机器是IBM或非IBM的。标准的TCP/IP应用包括电子邮件、文件传输、远程登陆等。TCP/IP CICSSOCKET应用提供了在COBOL,PL/1及汇编语言中使用通用的应用程序接口的能力。 [1]OS/390安全服务器（RACF）RACF是OS/390中的安全管理服务，它可以保护系统中的所有资源，认证用户的登陆，有效的记录系统中的安全事件。RACF提供：灵活可变的资源保护方式；保护所有资源；可以选择集中保护或分散保护；提供一个ISPF菜单；对最终用户的透明。资源度量设备（RMF）RMF是反应OS/390资源使用情况的一个窗口。它收集在SYSPLEX级和单个系统级和地址级的信息，在SYSPLEX中任何系统上产生报告。用户能在这些报告中选择所关心的活动，比如能专门选择关注的存储器、I/O或处理器数据、RMF的管理器1可以产生长期的报告，用于对RMF收集的长期数据进行详细的分析。这些报告能被打印或显示出来，你可以用RMF电子数据表转换器，从屏幕或MVS数据集下载这些报告。产生工作站上的电子数据表并对其进行详细的分析。 [1]DFSMSDFSMS主要由三个部分组成：DFSMSdss是一个DASD数据和空间的管理工具，它可用于卷之间的拷贝，移动数据集；DUMP、恢复数据集及完整的卷或磁道；使数据集和卷改变为SMS管理的或非SMS管理的；压缩分区式的数据集；释放数据集中未使用的工具。DFSMShsm是一个DASD存储管理工具，用于管理较少活动和不活跃的数据。它通过自动管理空间的数据的有效性，在存储体系中改善DASD使用情况。DFSMShsm和SMS一起工作，按照数据集的managementclass对数据集进行空间管理和有效性管理。DFSMSrmm管理磁带卷以及其上的数据集。DFSMSrmm可以管理所有的磁带介质以及其他可移动的介质。例如，DFSMSrmm能记录光盘架的位置，追踪它们的必不可少的记录状态，但不管理光盘的数据。 [1]系统显示和查设施（SDSF）系统显示和查找设施（SDSF）提供给用户有关监视、管理和控制OS/390系统信息。SDSF提供一个简单而高效的方法。控制作业的处理和设备的运行。比如你可以在作业运行时监视作业，浏览作业的输出而不打印，你可以浏览包括整个SYSPLEX范围的操作日志和系统日志。SDSF提供对信息进行分类、过滤，查找和打印的功能，帮助你定位和组织信息。菜单和弹出式窗口，使SDSF的使用非常简单。SDSF提供了完整的联机帮助和交互式的入门指导。 [1]OS/390作业管理和文件目录系统（JES和CATALOG） [1]在大型服务器系统中，当用户需要使用计算机完成某项任务时，用户必须准备一个作业流。一个作业流中包含一个或多个作业。作业是用户在完成该任务时要求计算机所要完成的工作的集合。JES（JOB ENTERSUBSYSTEM）是在OS/390里管理作业的子系统。JES能够从各种途径接受作业，并根据作业的语句和特点向操作系统申请资源完成作业的处理，最后处理作业的输出。JES主要是JES2和JES3,JES2是常用的系统，他是OS/390中不可缺少的子系统，JES2处理主要主要分为如下几个步骤：⑴接收作业⑵处理作业⑶ 申请资源 [1]

时钟周期：
简介播报编辑时钟周期是同步电路中时钟基础频率的倒数。它以时间动作重复的最小周期来度量，度量单位采用时间单位。在单个时钟周期内（现代非嵌入式微处理器的这个时间一般都短于1纳秒），逻辑零状态与逻辑一状态来回切换。由于发热和电气规格的限制，周期里逻辑零状态的持续时间历来要长于逻辑一状态。应用播报编辑时钟周期是由CPU时钟定义的定长时间间隔，是CPU工作的最小时间单位，也称节拍脉冲或T周期。时钟周期表示了SDRAM所能运行的最高频率。更小的时钟周期就意味着更高的工作频率。对于PC100规格的内存来说，它的运行时钟周期应该不高于10纳秒。纳秒与工作频率之间的转换关系为：1 / 时钟周期 =工作频率。例如，标称10纳秒的PC100内存芯片，其工作频率的表达式就应该是1/ 10 = 100MHZ，这说明此内存芯片的额定工作频率为100MHZ。市场上一些质量优秀的内存通常可以工作在比额定频率高的频率下，这为一些喜欢超频的朋友带来了极大的方便。例如KingMAX的PC100内存，此类内存多采用8纳秒的芯片，相对于其100MHZ的频率来说，频率提高的余地还很大，许多用户都可以让它们工作在133MHZ甚至更高的频率下。能不能超频使用很大程度上反应了内存芯片以及PCB板的质量。不过，仅仅凭借时钟周期来判断内存的速度还是不够的，内存CAS的存取时间和延迟时间也在一定程度上决定了内存的性能。 [1]单片机时间单位播报编辑在MCS-51中时钟周期也称振荡周期，振荡周期也称为晶振周期，振荡周期是单片机的基本时间单位。8051把一个振荡周期定义为一个节拍（用P表示），两个节拍为一个状态周期。振荡器脉冲信号经过时钟电路二分频之后产生的单片机时钟信号的周期（用S表示）称为状态周期。故一个状态周期S包含2个节拍，前一时钟周期称为P1节拍，后一时钟周期称为P2节拍。若时钟晶振的振荡频率为fosc，则振荡周期Tosc=（1/fosc）。如：晶振频率为12MHZ，则振荡周期Tosc=（1/12us）。相互关系播报编辑1、时钟周期=振荡周期，名称不同而已，都是等于单片机晶振频率的倒数，如常见的外接12M晶振，那它的时钟周期=1/12M。2、机器周期，8051系列单片机的机器周期=12*时钟周期，之所以这样分是因为单个时钟周期根本干不了一件完整的事情（如取指令、写寄存器、读寄存器等），而12个时钟周期就能基本完成一项基本操作了。3、指令周期。一个机器周期能完成一项基本操作，但一条指令常常是需要多项基本操作结合才能完成，完成一条指令所需的时间就是指令周期，当然不同的指令，其指令周期就不一样的了。 [2]

易失性存储器：
简介播报编辑它在任何时候都可以读写，RAM通常是作为操作系统或其他正在运行程序的临时存储介质（可称作系统内存）。不过，当电源关闭时RAM不能保留数据，如果需要保存数据，就必须把它们写入到一个长期的存储器中（例如硬盘）。正因为如此，有时也将RAM称作“可变存储器”。RAM内存可以进一步分为静态RAM（SRAM）和动态内存（DRAM）两大类。DRAM由于具有较低的单位容量价格，所以被大量的采用作为系统的主记忆。应用灵活性播报编辑不同应用在不同的容性负载下需要不同的工作频率，这项要求与芯片组的性能以及电路板布局和复杂性紧密相关。例如，高频工作环境通常对电性能的优化要求严格，设计工程师需要考虑整个电路板上的电噪声，以降低线路的寄生电容。在这种情况下，降低存储器输出驱动器的强度更加受欢迎。此外，还必须根据工作频率优化指令执行速度。有时候，要想在发送命令后取得适合的高效的吞吐量，就必须减少空时钟周期次数。最终使用播报编辑在应用电路板测试阶段，为了正确地激励存储器、查看存储器的响应，微控制器需要全套的命令和功能。这项操作灵活性测试通常用于检测全部系统组件，以确保产品在生命周期内的功能。相反，标准的客户最终应用只使用一个精简的指令集。例如，在使用SPI闪存时，最终应用通常使用读指令(正常、快速和/或4位I/O输入输出)，把启动代码下载到RAM存储器。设计人员应该优化非易失性存储器，以缩减系统上电期间的代码读取和下载时间。在新的先进的平台上，如车用电子、计算机光驱或蓝牙模块，SPI闪存可能用于直接从非易失性存储器读取部分系统固件，以缩短系统固件下载到高速易失性存储器的过程。当然，目前出现的最新应用对存储器的灵活性要求更加严格，本文稍后再做详解。自适应模式播报编辑SPI闪存的的优点是引脚数量少而且固定不变(8个或16个)。串口闪存的这个特性可简化电路板布局，无需更改硬件即可升级固件，从而可以降低系统开发的总体成本。由于在简易性和成本方面的强大优势，PC机和消费电子市场出现了并口闪存改用SPI闪存的发展趋势。只要达到性能要求时，设备厂商就会优先选用串口闪存。计算机光驱、汽车电子、蓝牙模块、机顶盒和调制解调器市场正在引入这种能够把代码直接读到非易失性存储器内的SPI闪存。XiP(片内执行)应用要求串口存储器提供一种“随机访存”仿真功能，即无需发送指令即可访问存储器内容，并准许以最大的吞吐量访问存储器。因为传统用途是存储和下载代码，所以SPI存储器是同步器件，XIP功能迫使设计人员研发灵活的存储器，能够根据芯片组特性灵活地配置串行闪存。例如，在系统上电后，具有XIP功能的器件需要基于命令、地址和数据的JEDEC协议，所以有些逻辑器件不准像管理XIP器件一样管理串口闪存。此外，有些逻辑器件只在一条线上或者最多在两条线上支持XIP模式，因为固有的硬件限制，不可能开启4位I/O输入输出模式。最后，因为实现一个混合协议、接受命令的传统存储器和不接受命令的非易失性存储器的设计困难，芯片组厂商更愿意保留原有的SPI指令结构，即命令、地址和数据。在这些情况下，高速协议结合并行化命令、地址和数据的方案更受市场欢迎。

存储器带宽：
名词释义播报编辑存储器带宽，体现数据传输速率技术指标 （单位：bps, bit per second,位/秒，或Bytes/s,字节/秒）存储器的带宽决定了以存储器为中心的机器获取信息的传输速度，它是改善机器瓶颈的一个关键因素。提高措施播报编辑为了提高存储器的带宽，可以采取以下措施：1、缩短存取周期；2、增加存储字长，使每个存取周期可读/写更多的二进制位数；3、增加存储体。计算方法播报编辑带宽我们一般用表示，若存储周期为，每次读/写个字节，则其带宽。如存取周期为500ns，每个存取周期可访问16位，则它的带宽为32M位/s

内存泄漏：
简介播报编辑内存泄漏（Memory Leak）是指程序中已动态分配的堆内存由于某种原因程序未释放或无法释放，造成系统内存的浪费，导致程序运行速度减慢甚至系统崩溃等严重后果。内存泄漏缺陷具有隐蔽性、积累性的特征，比其他内存非法访问错误更难检测。因为内存泄漏的产生原因是内存块未被释放，属于遗漏型缺陷而不是过错型缺陷。此外，内存泄漏通常不会直接产生可观察的错误症状，而是逐渐积累，降低系统整体性能，极端的情况下可能使系统崩溃。随着计算机应用需求的日益增加，应用程序的设计与开发也相应的日趋复杂，开发人员在程序实现的过程中处理的变量也大量增加，如何有效进行内存分配和释放，防止内存泄漏的问题变得越来越突出。例如服务器应用软件，需要长时间的运行，不断的处理由客户端发来的请求，如果没有有效的内存管理，每处理一次请求信息就有一定的内存泄漏。这样不仅影响到服务器的性能，还可能造成整个系统的崩溃。因此，内存管理成为软件设计开发人员在设计中考虑的主要方面 [1]。泄漏原因播报编辑在C语言中，从变量存在的时间生命周期角度上，把变量分为静态存储变量和动态存储变量两类。静态存储变量是指在程序运行期间分配了固定存储空间的变量，而动态存储变量是指在程序运行期间根据实际需要进行动态地分配存储空间的变量。在内存中供用户使用的内存空间分为三部分：程序存储区静态存储区动态存储区程序中所用的数据分别存放在静态存储区和动态存储区中。静态存储区数据在程序的开始就分配好内存区，在整个程序执行过程中它们所占的存储单元是固定的，在程序结束时就释放，因此静态存储区数据一般为全局变量。动态存储区数据则是在程序执行过程中根据需要动态分配和动态释放的存储单元，动态存储区数据有三类函数形参变量、局部变量和函数调用时的现场保护与返回地址。由于动态存储变量可以根据函数调用的需要，动态地分配和释放存储空间，大大提高了内存的使用效率，使得动态存储变量在程序中被广泛使用。开发人员进行程序开发的过程使用动态存储变量时，不可避免地面对内存管理的问题。程序中动态分配的存储空间，在程序执行完毕后需要进行释放。没有释放动态分配的存储空间而造成内存泄漏，是使用动态存储变量的主要问题。一般情况下，开发人员使用系统提供的内存管理基本函数，如malloc、realloc、calloc、free等，完成动态存储变量存储空间的分配和释放。但是，当开发程序中使用动态存储变量较多和频繁使用函数调用时，就会经常发生内存管理错误，例如：分配一个内存块并使用其中未经初始化的内容；释放一个内存块，但继续引用其中的内容；子函数中分配的内存空间在主函数出现异常中断时、或主函数对子函数返回的信息使用结束时，没有对分配的内存进行释放；程序实现过程中分配的临时内存在程序结束时，没有释放临时内存。内存错误一般是不可再现的，开发人员不易在程序调试和测试阶段发现，即使花费了很多精力和时间，也无法彻底消除。产生方式的分类以产生的方式来分类，内存泄漏可以分为四类：1.常发性内存泄漏：发生内存泄漏的代码会被多次执行到，每次被执行时都会导致一块内存泄漏。2.偶发性内存泄漏：发生内存泄漏的代码只有在某些特定环境或操作过程下才会发生。常发性和偶发性是相对的。对于特定的环境，偶发性的也许就变成了常发性的。所以测试环境和测试方法对检测内存泄漏至关重要。3.一次性内存泄漏：发生内存泄漏的代码只会被执行一次，或者由于算法上的缺陷，导致总会有一块且仅有一块内存发生泄漏。4.隐式内存泄漏：程序在运行过程中不停的分配内存，但是直到结束的时候才释放内存。严格的说这里并没有发生内存泄漏，因为最终程序释放了所有申请的内存。但是对于一个服务器程序，需要运行几天，几周甚至几个月，不及时释放内存也可能导致最终耗尽系统的所有内存。所以，我们称这类内存泄漏为隐式内存泄漏。从用户使用程序的角度来看，内存泄漏本身不会产生什么危害，作为一般的用户，根本感觉不到内存泄漏的存在。真正有危害的是内存泄漏的堆积，这会最终耗尽系统所有的内存。从这个角度来说，一次性内存泄漏并没有什么危害，因为它不会堆积，而隐式内存泄漏危害性则非常大，因为较之于常发性和偶发性内存泄漏它更难被检测到。检测方法播报编辑无论是C还是C++程序，运行时候的变量主要有三种分配方式：堆分配、栈分配、全局和静态内存分配。内存泄漏主要是发生在堆内存分配方式中，即“配置了内存后，所有指向该内存的指针都遗失了”，若缺乏语言这样的垃圾回收机制，这样的内存片就无法归还系统。因为内存泄漏属于程序运行中的问题，无法通过编译识别，所以只能在程序运行过程中来判别和诊断。下面将介绍几种常用的内存检测方法，每种方法均以现有的内存检测工具为分析范例，并对各种方法进行比较。静态分析技术静态分析技术就是直接分析程序的源代码或机器代码，获得一些有用的信息，而并不运行程序本身。目前有许多静态分析的工具，编译器就属于这一类，它读入源程序代码，对源程序进行词法和语法分析，进行数据类型的检查以及一些优化的分析等，以此来提高程序的质量与运行效率。这类静态的分析工具仅仅是读入程序代码进行相关的分析，而并不进行其它额外的操作，如修改源程序代码等。LCLink是一种通过对源代码及添加到源代码中特定格式的注释说明进行静态分析的程序理解和检错工具，的检查对象是源程序，能检查出的内存错误有内存分配释放故障、空指针的错误使用、使用未定义或已被释放的内存等程序错误。LCLink重点分析两类内存释放错误：试图释放某内存块，该内存块有两个或两个以上的有效指针指向它。试图释放某内存块，该内存块没有任何有效指针指向它。解决此类内存错误的方法是规定分配某块内存时返回的指针必须释放该内存。使用注释表示某指针是唯一指向某内存块的指针，使用注释表示被调用函数可能释放函数参数指向的内存块或创建新的指针指向该内存块。源代码插装技术图1为了获得被测程序的动态执行信息，需要对其进行跟踪，一般使用插装方法。所谓插装就是在保持被测程序的逻辑完整性的基础上，在被测程序的特定部位插入一段检测程序又称探针函数，通过探针的执行抛出程序的运行特征数据。基于这些特征数据分析，可以获得程序的控制流及数据流信息，进而获得逻辑覆盖等动态信息，这样就可以在被测程序执行的过程中动态地同步执行程序的检测工作。插装方法又分为源代码级程序插装和目标代码级程序插装。源代码插装测试必须在静态测试部分获得的被测程序的结构信息、静态数据信息、控制流信息等基础上，应用插装技术向被测程序中的适当位置植入相应类型的探针，通过运行带有探针的被测程序而获得程序运行的动态数据。源代码插装要通过运行被测程序来测定程序的各种指标，如覆盖率、时间性能、内存使用等等，实现源代码插装的关键技术是借助于插入到源程序中的监控语句来收集执行信息，以达到揭示程序内部行为和特性的目的，如图1所示。基于源代码插装的动态测试框架分为个主要的阶段：插装交互与动态测试信息分析；插装阶段；插装库制作阶段；测试实施阶段。插装交互与动态测试信息分析是软件测试工具与用户交互的界面。用户通过该界面选择要进行动态测试的程序模块，拓扑产生相应的插装选择记录文件。用户还可以通过该交互界而浏览动态测试结果信息，在软件测试工具的实现上，采用可视化的方式显示这些动态信息。插装阶段实现了在被测程序中植入探针，并生成带有插装信息的源文件。在此过程中，首先将被测程序经过预处理展开为不包含宏、条件编译和头文件的文件格式。然后，按照一定的插装策略，根据前面生成的插装选择记录文件，将探针函数加载到该文件中，最后生成插装后的程序。插装库制作阶段的目的是生成插装库中的探针函数，它含有插装语句调用的函数及其函数的定义。显然，插装过程中生成的目标文件中含有探针函数的桩，而探针函数的实现恰恰在本过程完成。需要指出的是，插装库的制作过程是独立于动态测试过程之外的，可以与软件测试工具开发同步。测试实施阶段将插装过程生成的文件与插装库制作过程生成的插装静态库连接生成带有插装信息的可执行文件，选取测试用例，运行该程序，可以获得被测程序的动态跟踪信息。在以上四个阶段中，其中的插装交互与动态测试信息分析与测试实施阶段是测试人员的可视部分，通过这两部分，用户与系统交互，完成测试工作。而插装阶段与插装库制作阶段对测试人员是不可见的，在后台完成，对于用户而言，这两部分是完全透明的。在性能方面，采用插装方法应尽量减少插装开销。为了达到不同的统计目的如语句覆盖、分支覆盖等，应尽量减少插装次数。若能仅仅插装一次就能完成多种类型的统计，则可使插装代码得到优化。此外，应尽量减少插装代码的数量，减少插装代码的运行次数，从而达到减小插装代码运行开销的目的。特别是对于一些实时系统的测试，在这方面的要求尤为苛刻。一个运行时错误检测工具，能够自动检测一应用中大量的编程和运行时错误。通过使用源码插装和运行时指针跟踪的专利技术，在编译时，附十插入测试和分析代码，它建立一个有关程序中各种对象的数据库。然后在运行时通过检查数据值和内存引用验证对象的一致性和正确性。使用这些技术，包括变异测试技术等，一能够检查和测试用户的代码，精确定位错误的准确位置并给出详细的诊断信息。能够可视化实时内存操作，优化内存算法。还能执行覆盖性分析，清楚地指示那些代码已经测试过。将集成到开发环境中，能够极大地减少调试时间并有效地防止错误。检验每一次内存操作的有效性，包括静态全局和堆栈以及动态分配内存的操作。叶有两种运行模式。监护模式下用户可以快速检测代码中的错误，不需要对代码作任何插装和处理源码插装模式则进行彻底地代码检测。目标代码插装技术图2目标代码插装实现主要分为预处理、测试执行和结果汇总个阶段，工作流程如图2所示，系统主要工作是围绕断点而进行的。在预处理阶段，首先静态分析被测程序的目标代码，查找待测程序中源代码各语句、函数入口点在目标代码中的对`应位置，然后在相应位置插入断点在测试执行阶段，启动调试进程，当被测程序执行到断点处时，响应断点信息，在相应的断点处完成相应的统计操作在结果汇总阶段，根据各断点处的统计结果，按不同的统计角度进行归并、综合得到最终的统计数据。被测代码预处理图3在测试预处理阶段对被测程序的目标代码进行分析，可以获得目标代码与源代码中语句、函数的对应关系。在目标代码中为相对应的源代码的每条语句及每个函数的入口点插入断点。对于第三方代码，只要其目标代码格式与下生成的目标代码格式一致，我们就可以用与分析用户代码同样的方法获取信息。获取断点的信息后，为所有的断点建立断点链表，同时建立语句及函数的信息链表，供随后的测试执行阶段存储信息。预处理流程如图3所示。测试执行阶段图4利用OCI技术，我们把测试执行看作是一个在被测进程和检测进程间不断切换的过程。每当被测进程遇到断点，就会将自身挂起，同时发送消息唤醒检测进程，检测进程根据当前断点的地址在断点链表中查找相应节点，并查找对应的语句或函数信息，记录该语句或函数的执行次数、到达或离开的时刻，供以后统计之用。然后，将插入的断点信息去除，恢复原来的指令，转入被测进程继续执行。在转入被测进程之前，必须将上一个断点处的断点恢复上一个断点处的断点在指令运行时被去除了。具体流程如图4所示。数据统计与结果汇总图5根据各断点处的统计结果，按不同的统计角度进行归并、综合，进行覆盖率及各种时间的计算，得到最终的统计数据。是公司出品的一种软件测试和质量保证工具，它能检测程序内存泄漏和内存访问冲突等错误。使用目标码插装技术，在编译器生成的目标码中直接插入特殊的检查指令实现对内存错误的检测。在程序的所有代码中插入这些检查逻辑，包括第三方目标码库，并且验证系统调用的接口。目标码插装技术分为链接前插装和链接后插装两种插装方法。使用如图5所示的链接前插装法。检查插装后程序的每个内存读写动作，跟踪内存使用情况，使用类似垃圾收集器的技术来检查内存泄漏。垃圾收集机制分为两阶段垃圾检测和垃圾回收。为了不影响程序的执行速度，提供了一个可调用的垃圾检测器，使用类似于保守式垃圾收集算法，即标记一清除算法。在标记阶段，递归地从数据段、堆栈段到数据堆跟踪分析指针，并使用标准保守式方法为所有被引用的内存块做标记。在清除阶段，逐步访问数据堆，并报告已分配但程序不再引用的内存块，即程序的内存泄漏。检测工具播报编辑部分工具1.ccmalloc－Linux和Solaris下对C和C++程序的简单的使用内存泄漏和malloc调试库。2.Dmalloc－Debug Malloc Library.3.Electric Fence－Linux分发版中由Bruce Perens编写的malloc()调试库。4.Leaky－Linux下检测内存泄漏的程序。5.LeakTracer－Linux、Solaris和HP-UX下跟踪和分析C++程序中的内存泄漏。6.MEMWATCH－由Johan Lindh编写，是一个开放源代码C语言内存错误检测工具，主要是通过gcc的precessor来进行。7.Valgrind－Debugging and profiling Linux programs, aiming at programs written in C and C++.8.KCachegrind－A visualization tool for the profiling data generated by Cachegrind and Calltree.9.IBM Rational PurifyPlus－帮助开发人员查明C/C++、托管.NET、Java和VB6代码中的性能和可靠性错误。PurifyPlus 将内存错误和泄漏检测、应用程序性能描述、代码覆盖分析等功能组合在一个单一、完整的工具包中。10.ParasoftInsure++－针对C/C++应用的运行时错误自动检测工具，它能够自动监测C/C++程序，发现其中存在着的内存破坏、内存泄漏、指针错误和I/O等错误。并通过使用一系列独特的技术（SCI技术和变异测试等），彻底的检查和测试我们的代码，精确定位错误的准确位置并给出详细的诊断信息。能作为MicrosoftVisual C++的一个插件运行。11.Compuware DevPartner for Visual C++ BoundsChecker Suite－为C++开发者设计的运行错误检测和调试工具软件。作为Microsoft Visual Studio和C++ 6.0的一个插件运行。12.Electric Software GlowCode－包括内存泄漏检查，code profiler，函数调用跟踪等功能。给C++和.Net开发者提供完整的错误诊断，和运行时性能分析工具包。13.Compuware DevPartner Java Edition－包含Java内存检测,代码覆盖率测试,代码性能测试,线程死锁,分布式应用等几大功能模块。14.Quest JProbe－分析Java的内存泄漏。15.ej-technologies JProfiler－一个全功能的Java剖析工具，专用于分析J2SE和J2EE应用程序。它把CPU、执行绪和内存的剖析组合在一个强大的应用中。16.BEAJRockit－用来诊断Java内存泄漏并指出根本原因，专门针对Intel平台并得到优化，能在Intel硬件上获得最高的性能。

存储器管理：
操作系统的职能之一，主要任务是为多道程序的运行提供良好的环境，方便用户使用存储器，提高存储器的利用率以及能从逻辑上扩充内存。主要功能：1、内存分配2、内存保护3、地址映射4、内存扩充

缓存一致性：
简介播报编辑内存系统的本质是，一个内存系统应该能提供一组保存值的存储单元，当对一个存储单元执行读操作时，应该能返回“最近”一个对该存储单元的写操作所写入的值。在串行程序中，程序员利用内存来将程序中某一点计算出来的值，传递到该值的使用点，实际上就是利用了以上的基本性质。同样，运行在单处理器上的多个进程或线程利用共享地址空间进行通信，实际上也是利用了内存系统的这个性质。一个读操作应返回最近的向那个位置的写操作所写的值，而不管是哪个线程写的。当所有的线程运行在同一个物理处理器上时，它们通过相同的高速缓存层次来看内存，因此在这种情况下，高速缓存不会引起问题。当在共享存储的多处理器系统上运行一个具有多个进程的程序时，希望不管这些进程是运行在同一个处理器上，还是在不同的处理器上，程序的运行结果都是相同的。然而，当两个运行在不同的物理处理器上的进程通过不同的高速缓存层次来看共享内存时，其中一个进程可能会看到在它的高速缓存中的新值，而另一个则可能会看到旧值，这样就引起了一致性问题 [1]。高速缓冲存储器一致性的比较正式的一个定义是当一个共享存储的机器满足以下条件时，则可以认为该系统是高速缓冲存储器一致的：1.任何进程所发出的访存操作被存储器所观察到的顺序必须与进程发出操作的顺序相同；2.每个读操作所返回的值必须是最后一次对该存储位置的写操作的值。以上定义保证了两个属性写广播和写串行化。写广播指的是写操作被其他所有处理器所观察到。定串行化是指对某一存储位置的所有写操作的顺序，在所有处理器看来都是一样的。高速缓冲存储器一致性可以分为三个层级：1.在进行每个写入运算时都立刻采取措施保证数据一致性；2.每个独立的运算，假如它造成数据值的改变，所有进程都可以看到一致的改变结果；3.在每次运算之后，不同的进程可能会看到不同的值（这也就是没有一致性的行为）。一致性播报编辑关于一致性的概念，直观上可以定义为每个读操作必须返回最后写入此位置的值。对于每个处理器单元的操作都有一个总体的、串行的操作序是我们希望在任何一个一致性存储系统中见到的。因此，可以对高速缓存一致性进行一个更加形式化的定义方法：如果某个程序的任何执行结果都满足下列条件：对于任何单元，有可能建立一个假想的操作序列（也就是说，将所有进程发出的读写操作排成一个全序，此序列与执行结果一致，并且在序列中）：任何特定进程发出的操作所表现出来的序和该进程向存储系统发出它们的序相同，且每个读操作返回的值是对相应单元按照串行顺序写入的最后一值，那么此多处理器存储系统是一致性。此外，关于多个私有缓存还存在另外一方面的问题：如果数据是由一个处理器核对某个单元写入，而另一个处理器从中读出这样的方式来进行传递的话，那么我们前面所关注的一致性将是非常重要的。最终，写在一个单元中的数据将对所有的读取者都会是可见的，但这种一致性并没有指明所写入的数据何时会成为可见的。通常，在编写一个并行程序时，我们希望在写和读之间建立一种序，即我们需要定义一个序模型，依照该模型，程序员能推断他们程序的执行结果及其正确性。这个模型就是存储同一性。一个完整的一致性模型包括高速缓存一致性及存储同一性两个方面，且这两个是互补的：高速缓存一致性定义了对同一个存储地址进行的读写操作行为，而存储同一性模型定义了访问所有存储地址的读写行为。在共享存储空间中，多个进程对存储的不同单元做并发的读写操作，每个进程都会看到一个这些操作被完成的序。一个存储同一性模型规定了对这种序的若干约束，值得一提的是，这里所涉及到的并发存储操作既包括对相同单元的，也包括对不同单元的；即可以来自同一进行，也可以来自不同的进程。在这个意义上，存储同一性包含了高速缓存一致性。实现播报编辑高速缓存一致性协议是解决缓存一致性问题的主要方案，同时也是保证存储同一性的重要手段。它定义了共享缓存块在各个私有缓存中的存在形式，并详细定义了各个私有缓存之间的通信行为。学术界及工业界已经提出了多种高速缓存一致性协议模型，但所有模型的出发点都是一样的，它们都是为了保证存储模型SWMR（single-writer，multiple-reader）属性，即对于一个给定的缓存块，在系统运行的任意时刻，保证（1）同时只能有一个处理器核拥有对此缓存的写权限；或者（2）同时可以有零个或者多个处理器核拥有对此缓存块的读权限。根据共享数据的修改方式的不同，可以将高速缓存一致性协议的实现分为两种形式：写无效协议、写更新协议。其中，在写无效协议中，处理器核在对某个存储块进行写操作之前，必须保证当前的处理器核拥有对此缓存块的读写权。如果两个或者多个处理器核试图同时访问同一个数据项并执行写操作，那么它们中同时只能有一个在进行，另一个访问请求会被阻塞；在某个处理器执行写操作时，其它所有私有缓存中该数据的副本都会被置为无效状态。在当前处理器完成写操作后，后续的对此数据的所有操作，都必须首先获得此新写入数据的副本。因此，写无效型的协议强制执行了写操作的串行化。写更新协议也称为写广播协议，它是指处理器核在对某个数据进行写操作时，同时更新当前数据在其它所有缓存中的数据副本。有两种主要的缓存一致性协议：基于侦听形式的高速缓存一致性协议（或侦听协议）及基于目录结构的高速缓存一致性协议（或目录协议）。在这两种协议中，侦听协议的实现依赖于一个总线或者类总线形式的网络连接，使用此网络，单个处理器核的私有缓存所发出的所有请求会被广播到系统中所有的其它处理器核的私有缓存中，而所有处理器的访问请求也可以在此总线上进行定序操作，以实现缓存一致性模型及存储同一性模型中对访存序的要求。此外，此协议还可以通过总线结构来很好的处理对同一数据块的多个冲突请求，而且多个处理器的私有缓存之间可以通过此总线结构进行直接的通信，减少了通信延迟。但是，由于所有请求都是通过总线来进行传输的，但总线带宽资源有限，因此它会影响整个系统的扩展性。目录协议则是采用一个目录结构来实现对缓存块的管理的。在目录协议中，处理器核的私有缓存发出的访存请求，会首先发送到拥有相应缓存块的目录结构中，此目录结构中记录了当前缓存块的共享情况，目录结构控制器会根据当前缓存块的状态，选取响应此请求或者转发此请求到其它相应的私有缓存中。此种方法不需要依赖于特定拓扑结构的网络，并通过点对点的直接通信形式降低了网络中的带宽消耗，因此这种协议易于扩展。但由于此协议的实现中，所有请求都必须通过目录结构进行处理，因此会引入额外的延迟 [2]。MESI协议播报编辑单核Cache中每个Cache line有2个标志：dirty和valid标志，它们很好的描述了Cache和Memory（内存）之间的数据关系（数据是否有效，数据是否被修改），而在多核处理器中，多个核会共享一些数据，MESI协议就包含了描述共享的状态。在MESI协议中，每个Cache line有4个状态，可用2个bit表示，它们分别是：M（Modified）：这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。E（Exclusive）：这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中。S（Shared）：这行数据有效，数据和内存中的数据一致，数据存在于很多Cache中。I（Invalid）：这行数据无效。在该协议的作用下，虽然各cache控制器随时都在监听系统总线，但能监听到的只有读未命中、写未命中以及共享行写命中三种情况。读监听命中的有效行都要进入S态并发出监听命中指示，但M态行要抢先写回主存；写监听命中的有效行都要进入I态，但收到RWITM时的M态行要抢先写回主存。总之监控逻辑并不复杂，增添的系统总线传输开销也不大，但MESI协议却有力地保证了主存块脏拷贝在多cache中的一致性，并能及时写回，保证cache主存存取的正确性 [3]。

内存屏障：
简介播报编辑大多数现代计算机为了提高性能而采取乱序执行，这使得内存屏障成为必须。语义上，内存屏障之前的所有写操作都要写入内存；内存屏障之后的读操作都可以获得同步屏障之前的写操作的结果。因此，对于敏感的程序块，写操作之后、读操作之前可以插入内存屏障。底层体系结构相关的原语播报编辑大多数处理器提供了内存屏障指令:完全内存屏障（full memory barrier）保障了早于屏障的内存读写操作的结果提交到内存之后，再执行晚于屏障的读写操作。内存读屏障（read memory barrier）仅确保了内存读操作；内存写屏障(write memory barrier)仅保证了内存写操作。内存屏障是底层原语，是内存排序的一部分，在不同体系结构下变化很大而不适合推广。需要认真研读硬件的手册以确定内存屏障的办法。x86指令集中的内存屏障指令是：lfence (asm), void _mm_lfence (void) 读操作屏障sfence (asm), void _mm_sfence (void)[1] 写操作屏障mfence (asm), void _mm_mfence (void)[2] 读写操作屏障常见的x86/x64，通常使用lock指令前缀加上一个空操作来实现，注意当然不能真的是nop指令，但是可以用来实现空操作的指令其实是很多的，比如Linux中采用的addl $0, 0 (%esp)存储器也提供了另一套语义的内存屏障指令:acquire semantics: 该操作结果可利用要早于代码中后续的所有操作的结果。release semantics: 该操作结果可利用要晚于代码中之前的所有操作的结果。fence semantics: acquire与release两种语义的共同有效。即该操作结果可利用要晚于代码中之前的所有操作的结果，且该操作结果可利用要早于代码中后续的所有操作的结果。Intel Itanium处理器，具有内存屏障mf的指令，具有下述modifiers [1]:acq (acquire)rel (release).Windows API的内存屏障实现播报编辑下述同步函数使用适当的屏障来确保内存有序：进出临界区(critical section)的函数触发(signaled)同步对象的函数等待函数(Wait function)互锁函数(Interlocked function) [1]多线程编程与内存可见性播报编辑多线程程序通常使用高层程序设计语言中的同步原语，如Java与.NET Framework，或者API如pthread或Windows API。因此一般不需要明确使用内存屏障。内存可见性问题，主要是高速缓存与内存的一致性问题。一个处理器上的线程修改了某数据，而在另一处理器上的线程可能仍然使用着该数据在专用cache中的老值，这就是可见性出了问题。解决办法是令该数据为volatile属性，或者读该数据之前执行内存屏障。 [2]乱序执行与编译器重排序优化的比较播报编辑C与C++语言中，volatile关键字意图允许内存映射的I/O操作。这要求编译器对此的数据读写按照程序中的先后顺序执行，不能对volatile内存的读写重排序。因此关键字volatile并不保证是一个内存屏障。对于Visual Studio 2003，编译器保证对volatile的操作是有序的，但是不能保证处理器的乱序执行。因此，可以使用InterlockedCompareExchange或InterlockedExchange函数。对于Visual Studio 2005及以后版本，编译器对volatile变量的读操作使用acquire semantics，对写操作使用release semantics。 [1]编译器内存屏障播报编辑编译器会对生成的可执行代码做一定优化，造成乱序执行甚至省略（不执行）。gcc编译器在遇到内嵌汇编语句：asm volatile("" ::: "memory");将以此作为一条内存屏障，重排序内存操作。即此语句之前的各种编译优化将不会持续到此语句之后。也可用内建的__sync_synchronizeMicrosoft Visual C++的编译器内存屏障为：_ReadWriteBarrier() MemoryBarrier()Intel C++编译器的内存屏障为 [1]：__memory_barrier()

程序计数器：
简介播报编辑程序计数器是计算机处理器中的寄存器，它包含当前正在执行的指令的地址（位置）。当每个指令被获取，程序计数器的存储地址加一。在每个指令被获取之后，程序计数器指向顺序中的下一个指令。当计算机重启或复位时，程序计数器通常恢复到 [1]零。冯 ·诺伊曼计算机体系结构的主要内容之一就是“程序预存储，计算机自动执行”！处理器要执行的程序（指令序列）都是以二进制代码序列方式预存储在计算机的存储器中，处理器将这些代码逐条地取到处理器中再译码、执行，以完成整个程序的执行。为了保证程序能够连续地执行下去，CPU必须具有某些手段来确定下一条取指指令的地址。程序计数器（PC ）正是起到这种作用，所以通常又称之为‘指令计数器’。在程序开始执行前，将程序指令序列的起始地址，即程序的第一条指令所在的内存单元地址送入PC，CPU按照 PC的指示从内存读取第一条指令（取指）。当执行指令时，CPU自动地修改PC的内容，即每执行一条指令PC增加一个量，这个量等于指令所含的字节数（指令字节数），使 PC总是指向下一条将要取指的指令地址。由于大多数指令都是按顺序来执行的，所以修改PC 的过程通常只是简单的对PC 加“指令字节数”。当程序转移时，转移指令执行的最终结果就是要改变PC的值，此PC值就是转去的目 标地址。处理器总是按照PC 指向取指、译码、执行，以此实现了程序转移。ARM 处理器中使用R15 作为PC，它总是指向取指单元，并且ARM 处理器中只有一个PC 寄存器，被各模式共用。R15 有32 位宽度（下述标记为R15[31:0]，表示R15 的‘第31位’到‘第0位'），ARM 处理器可以直接寻址4GB的地址空间（2^32 = 4G ）。 [2]特点播报编辑为了保证程序（在操作系统中理解为进程）能够连续地执行下去，处理器必须具有某些手段来确定下一条指令的地址。而程序计数器正是起到这种作用，所以通常又称为指令计数器。在程序开始执行前，必须将它的起始地址，即程序的第一条指令所在的内存单元地址送入程序计数器，因此程序计数器的内容即是从内存提取的一条指令的地址。当执行指令时，处理器将自动修改PC的内容，即每执行一条指令PC增加一个量，这个量等于指令所含的字节数，以便使其保持的总是将要执行的下一条指令的地址。由于大多数指令都是按顺序来执行的，所以修改的过程通常只是简单的对PC加1。 [3]但是，当遇到转移指令如JMP（跳转、外语全称：JUMP）指令时，后继指令的地址（即PC的内容）必须从指令寄存器中的地址字段取得。在这种情况下，下一条从内存取出的指令将由转移指令来规定，而不像通常一样按顺序来取得。因此程序计数器的结构应当是具有寄存信息和计数两种功能的结构。 [3]

机器指令：
机器指令是CPU能直接识别并执行的指令，它的表现形式是二进制编码。机器指令通常由操作码和操作数两部分组成，操作码指出该指令所要完成的操作，即指令的功能，操作数指出参与运算的对象，以及运算结果所存放的位置等。 [1]由于机器指令与CPU紧密相关，所以，不同种类的CPU所对应的机器指令也就不同，而且它们的指令系统往往相差很大。但对同一系列的CPU来说，为了满足各型号之间具有良好的兼容性，要做到：新一代CPU的指令系统必须包括先前同系列CPU的指令系统。只有这样，先前开发出来的各类程序在新一代CPU上才能正常运行。机器语言是用来直接描述机器指令、使用机器指令的规则等。它是CPU能直接识别的唯一一种语言，也就是说，CPU能直接执行用机器语言描述的程序。用机器语言编写程序是早期经过严格训练的专业技术人员的工作，普通的程序员一般难以胜任，而且用机器语言编写的程序不易读、出错率高、难以维护，也不能直观地反映用计算机解决问题的基本思路。由于用机器语言编写程序有以上诸多的不便，几乎没有程序员这样编写程序了。

主存：
产品介绍播报编辑内存在计算机的组成结构中有一个很重要的部分是存储器。它是用来存储程序和数据的部件。对于计算机来说，有了存储器，才有记忆功能，才能保证正常工作。存储器的种类很多。按其用途可分为主存储器和辅助存储器，主存储器又称内存储器（简称内存，港台称之为记忆体）。 [2]内存又称主存。它是CPU能直接寻址的存储空间，由半导体器件制成。特点是存取速率快。内存是电脑中的主要部件，它是相对于外存而言的。我们平常使用的程序，如：Windows操作系统、打字软件、游戏软件等。一般安装在硬盘等外存上，但仅此是不能使用其功能，必须把它们调入内存中运行，才能真正使用其功能。我们平时输入一段文字或玩一个游戏，其实是在内存中进行。好比在一个书房，存放书籍的书架和书柜相当于电脑的外存，我们工作的办公桌相当于内存。通常，我们把要永久保存、大量数据存储在外存上，把一些临时或少量的数据和程序放在内存上。当然，内存的好坏会直接影响电脑的运行速度。 [2]内存是暂时存储程序以及数据的地方。当我们使用WPS处理文稿时，当你在键盘上敲入字符时，它被存入内存中。当你选择存盘时，内存中的数据才会被存入硬（磁）盘。 [2]发展播报编辑计算机诞生初期并不存在内存条的概念。最早的内存是以磁芯的形式排列在线路上，每个磁芯与晶体管组成的一个双稳态电路作为一比特（BIT）的存储器。每一比特都要有玉米粒大小，可以想象一间机房只能装下不超过百k字节左右的容量。后来才出现了焊接在主板上的集成内存芯片，以内存芯片的形式为计算机的运算提供直接支持。那时的内存芯片容量都特别小，最常见的莫过于256K×1bit、1M×4bit。虽然如此，但对于那时的运算任务来说却绰绰有余了。 [3]内存条内存芯片的状态一直沿用到286初期。鉴于它存在着无法拆卸更换的弊病，这对计算机的发展造成了现实的阻碍。有鉴于此，内存条便应运而生了。将内存芯片焊接到事先设计好的印刷线路板上，电脑主板上也改用内存插槽。这样，把内存难以安装和更换的问题彻底解决了。 [3]在80286主板发布之前，内存没有被世人重视。这个时候的内存直接固化在主板上，容量只有64 ～256KB。对于当时PC所运行的工作程序来说，这种内存的性能以及容量足以满足当时软件程序的处理需要。随着软件程序和新一代80286硬件平台的出现，程序和硬件对内存性能提出了更高要求。为了提高速度并扩大容量，内存必须以独立的封装形式出现，因而诞生了“内存条”的概念。 [3]80286主板刚推出时，内存条采用了SIMM（Single In-lineMemory Modules，单边接触内存模组）接口，容量为30pin、256kb，必须是由8 片数据位和1 片校验位组成1 个bank。正因如此，我们见到的30pin SIMM一般是四条一起使用。自1982年PC进入民用市场一直到现在，搭配80286处理器的30pin SIMM内存是内存领域的开山鼻祖。 [3]随后，在1988 ～1990 年当中，PC 技术迎来另一个发展高峰，也就是386和486时代。此时，CPU 已经向16bit 发展，所以30pin SIMM内存再也无法满足需求，其较低的内存带宽已经成为急待解决的瓶颈，所以此时72pin SIMM 内存出现了。72pin SIMM支持32bit快速页模式内存，内存带宽得以大幅度提升。72pin SIMM内存单条容量一般为512KB ～2MB，而且仅要求两条同时使用。由于其与30pin SIMM 内存无法兼容，因此这个时候PC业界毅然将30pin SIMM 内存淘汰出局了。 [3]EDO DRAM（Extended Date Out RAM 外扩充数据模式存储器）内存，这是1991 年到1995 年之间盛行的内存条。EDO DRAM同FPM DRAM（Fast Page Mode RAM 快速页面模式存储器）极其相似，它取消了扩展数据输出内存与传输内存两个存储周期之间的时间间隔，在把数据发送给CPU的同时去访问下一个页面。故而速度要比普通DRAM快15~30%。工作电压为一般为5V，带宽32bit，速度在40ns以上，其主要应用在当时的486及早期的Pentium电脑上。 [3]1991 年至1995 年期间，内存技术发展比较缓慢，几乎停滞不前。我们看到此时EDO DRAM有72 pin和168 pin并存的情况，事实上EDO内存也属于72pin SIMM 内存的范畴。不过它采用了全新的寻址方式。EDO 在成本和容量上有所突破，凭借着制作工艺的飞速发展。此时单条EDO内存的容量已经达到4 ～16MB。由于Pentium及更高级别的CPU数据总线宽度都是64bit甚至更高，所以EDO DRAM与FPM DRAM都必须成对使用。 [3]SDRAM自Intel Celeron系列以及AMD K6处理器以及相关的主板芯片组推出后，EDO DRAM内存性能再也无法满足需要了。内存技术必须彻底得到革新才能满足新一代CPU架构的需求，此时内存开始进入比较经典的SDRAM时代。 [3]第一代SDRAM内存为PC66 规范，但很快由于Intel 和AMD的频率之争将CPU外频提升到了100MHz。所以PC66内存很快就被PC100内存取代，接着，133MHz 外频的PIII以及K7时代的来临，PC133规范也以相同的方式进一步提升SDRAM 的整体性能，带宽提高到1GB/sec以上。由于SDRAM 的带宽为64bit，正好对应CPU 的64bit 数据总线宽度，因此，它只需要一条内存便可工作，便捷性进一步提高。在性能方面，由于其输入输出信号保持与系统外频同步，速度明显超越EDO 内存。 [3]SDRAM内存由早期的66MHz，发展至后来的100MHz、133MHz。尽管没能彻底解决内存带宽的瓶颈问题，但此时的CPU超频已成为DIY用户永恒的话题。不少用户将品牌好的PC100品牌内存超频到133MHz使用以获得CPU超频成功。为了方便一些超频用户的需求，市场上出现了一些PC150、PC166规范的内存。 [3]SDRAM PC133内存的带宽可提高到1064MB/S，加上Intel已开始着手最新的Pentium 4计划，所以SDRAM PC133内存不能满足日后的发展需求。Intel为了达到独占市场的目的，与Rambus联合在PC市场推广Rambus DRAM内存（称为RDRAM内存）。与SDRAM不同的是，其采用了新一代高速简单内存架构，基于一种类RISC(Reduced Instruction Set Computing，精简指令集计算机)理论，这个理论可以减少数据的复杂性，使得整个系统性能得到提高。 [3]在AMD与Intel的竞争中，这属于频率竞备时代。这时CPU的主频不断提升，Intel为了盖过AMD，推出高频PentiumⅢ以及Pentium 4 处理器。Rambus DRAM内存被Intel看着是未来自己的竞争杀手锏。Rambus DRAM内存以高时钟频率来简化每个时钟周期的数据量，内存带宽在当时相当出色。如：PC 1066 1066 MHz 32 bits带宽可达到4.2G Byte/sec，Rambus DRAM曾一度被认为是Pentium 4 的绝配。 [3]Rambus RDRAM内存生不逢时，依然要被更高速度的DDR“掠夺”其宝座地位。当时，PC600、PC700的Rambus RDRAM 内存因出现Intel820芯片组“失误事件”、PC800 Rambus RDRAM因成本过高而让Pentium 4平台高高在上，无法获得大众用户拥戴。发生的种种问题让Rambus RDRAM胎死腹中，Rambus曾希望具有更高频率的PC1066 规范RDRAM来力挽狂澜，但最终也是拜倒在DDR 内存面前。 [3]DDR时代DDRSDRAM(Double Data Rate SDRAM)简称DDR，也是“双倍速率SDRAM”的意思。DDR可说是SDRAM的升级版本。DDR在时钟信号上升沿与下降沿各传输一次数据，使得DDR的数据传输速度为传统SDRAM的两倍。由于仅多采用了下降缘信号，不会造成能耗增加。至于定址与控制信号则与传统SDRAM相同，仅在时钟上升缘传输。 [3]DDR内存作为一种性能与成本间折中的解决方案，其目的是迅速建立起牢固的市场空间，继而一步步在频率上高歌猛进，最终弥补内存带宽上的不足。第一代DDR200 规范没有得到普及，第二代PC266 DDR SRAM（133MHz时钟×2倍数据传输=266MHz带宽）是由PC133SDRAM内存所衍生出的。它将DDR 内存带向第一个高潮。2017年还有不少赛扬和AMD K7处理器都在采用DDR266规格的内存，其后来的DDR333内存也属于一种过渡。而DDR400内存成为目前的主流平台选配，双通道DDR400内存已经成为800FSB处理器搭配的基本标准，随后的DDR533 规范则成为超频用户的选择对象。 [3]DDR2时代随着CPU 性能的不断提高，大众对内存性能的要求也逐步提高。依高频率提升带宽的DDR迟早会力不从心，因此JEDEC 组织很早就开始酝酿DDR2 标准，加上LGA775接口的915/925以及最新的945等新平台开始对DDR2内存的支持，所以DDR2内存将开始演义内存领域的今天。 [3]DDR2 能够在100MHz 的发信频率基础上提供每插脚最少400MB/s 的带宽，而且其接口将运行于1.8V 电压上，从而进一步降低发热量，以便提高频率。此外，DDR2 将融入CAS、OCD、ODT 等新性能指标和中断指令，提升内存带宽的利用率。从JEDEC组织者阐述的DDR2标准来看，针对PC等市场的DDR2内存将拥有400、533、667MHz等不同的时钟频率。高端的DDR2内存将拥有800、1000MHz两种频率。DDR-II内存将采用200-、220-、240-针脚的FBGA封装形式。最初的DDR2内存将采用0.13微米的生产工艺，内存颗粒的电压为1.8V，容量密度为512MB。 [3]内存技术在2005年将会毫无悬念，SDRAM为代表的静态内存在五年内不会普及。QBM与RDRAM内存也难以挽回颓势，因此DDR与DDR2共存时代将是铁定的事实。 [3]PC-100的“接班人”除了PC一133以外，VCM（VirXual Channel Memory）也是很重要的一员。VCM即“虚拟通道存储器”，这也是目前大多数较新的芯片组支持的一种内存标准。VCM内存主要根据由NEC公司开发的一种“缓存式DRAM”技术制造而成。它集成了“通道缓存”，由高速寄存器进行配置和控制。在实现高速数据传输的同时，VCM还维持着对传统SDRAM的高度兼容性，所以通常也把VCM内存称为VCM SDRAM。VCM与SDRAM的差别在于不论是否经过CPU处理的数据，都可先交于VCM进行处理，而普通的SDRAM就只能处理经CPU处理以后的数据，所以VCM要比SDRAM处理数据的速度快20%以上。目前可以支持VCM SDRAM的芯片组很多，包括：Intel的815E、VIA的694X等。 [3]RDRAM时代Intel推出PC-100后，由于技术的发展，PC-100内存的800MB/s带宽不能满足更大的需求。而PC-133的带宽提高并不大（1064MB/s），同样不能满足日后的发展需求。Intel为了达到独占市场的目的，与Rambus公司联合在PC市场推广Rambus DRAM(DirectRambus DRAM)。 [3]Rambus DRAM是：Rambus公司最早提出的一种内存规格，采用了新一代高速简单内存架构，基于一种RISC(Reduced Instruction Set Computing，精简指令集计算机)理论，从而可以减少数据的复杂性，使得整个系统性能得到提高。Rambus使用400MHz的16bit总线，在一个时钟周期内，可以在上升沿和下降沿的同时传输数据，这样它的实际速度就为400MHz×2=800MHz，理论带宽为（16bit×2×400MHz/8）1.6GB/s，相当于PC-100的两倍。另外，Rambus也可以储存9bit字节，额外的一比特是属于保留比特，可能以后会作为：ECC (ErroI Checking and Correction，错误检查修正)校验位。Rambus的时钟可以高达400MHz，而且仅使用了30条铜线连接内存控制器和RIMM(Rambus In-line MemoryModules，Rambus内嵌式内存模块），减少铜线的长度和数量就可以降低数据传输中的电磁干扰，从而快速地提高内存的工作频率。不过在高频率下，其发出的热量肯定会增加，因此第一款Rambus内存甚至需要自带散热风扇。 [3]DDR3时代DDR3相比起DDR2有更低的工作电压，从DDR2的1.8V降落到1.5V，性能更好更为省电；DDR2的4bit预读升级为8bit预读。DDR3目前最高能够达到2000Mhz的速度，尽管目前最为快速的DDR2内存速度已经提升到800Mhz / 1066Mhz的速度，但是DDR3内存模组仍会从1066Mhz起跳。 [3]DDR3在DDR2基础上采用的新型设计： [3]1．8bit预取设计，而DDR2为4bit预取，这样DRAM内核的频率只有接口频率的1/8，DDR3-800的核心工作频率只有100MHz。2．采用点对点的拓朴架构，以减轻地址/命令与控制总线的负担。3．采用100nm以下的生产工艺，将工作电压从1.8V降至1.5V，增加异步重置（Reset）与ZQ校准功能。部分厂商已经推出1.35V的低压版DDR3内存。 [3]DDR4时代2012年，DDR4时代将开启，工作电压降至1.2V，而频率提升至2133MHz，次年进一步将电压降至1.0V，频率则实现2667MHz。新一代的DDR4内存将会拥有两种规格。根据多位半导体业界相关人员的介绍，DDR4内存将会是Single-endedSignaling（ 传统SE信号）方式DifferentialSignaling（差分信号技术）方式并存。其中AMD公司的PhilHester先生也对此表示了确认。现在有3200Mhz的ddr4和4266Mhz的LPDDR4预计这两个标准将会推出不同的芯片产品，因此在DDR4内存时代我们将会看到两个互不兼容的内存产品。 [3]第二代HBM32023年7月消息，美光宣布已出样业界首款8层堆叠的24GB容量第二代HBM3内存，基于1β DRAM制程节点高带宽内存（HBM）解决方案，带宽超过1.2TB/s，引脚速率超过9.2Gb/s，比现有HBM3解决方案性能可提升最高50%。美光介绍，第二代HBM3产品与前一代产品相比，每瓦性能提高2.5倍，可帮助缩短大型语言模型（如GPT-4及更高版本）的训练时间，降低总体拥有成本（TCO）。 [8]分类播报编辑内存一般采用半导体存储单元，包括随机存储器（RAM），只读存储器（ROM），以及高速缓存（CACHE）。只不过因为RAM是其中最重要的存储器。（synchronous）SDRAM同步动态随机存取存储器：SDRAM为168脚，这是目前PENTIUM及以上机型使用的内存。SDRAM将CPU与RAM通过一个相同的时钟锁在一起，使CPU和RAM能够共享一个时钟周期，以相同的速度同步工作，每一个时钟脉冲的上升沿便开始传递数据，速度比EDO内存提高50%。DDR（DOUBLE DATA RATE）RAM ：SDRAM的更新换代产品，他允许在时钟脉冲的上升沿和下降沿传输数据，这样不需要提高时钟的频率就能加倍提高SDRAM的速度。 [4]按工作原理分类●只读存储器（ROM）ROM表示只读存储器（Read Only Memory），在制造ROM的时候，信息（数据或程序）就被存入并永久保存。这些信息只能读出，一般不能写入，即使机器停电，这些数据也不会丢失。ROM一般用于存放计算机的基本程序和数据，如BIOS ROM。其物理外形一般是双列直插式（DIP）的集成块。 [4]现在比较流行的只读存储器是闪存( Flash Memory)，它属于 EEPROM(电擦除可编程只读存储器)的升级，可以通过电学原理反复擦写。现在大部分BIOS程序就存储在 FlashROM芯片中。U盘和固态硬盘(SSD)也是利用闪存原理做成的。 [4]●随机存储器（RAM）内存随机存储器（Random Access Memory）表示既可以从中读取数据，也可以写入数据。当机器电源关闭时，存于其中的数据就会丢失。我们通常购买或升级的内存条就是用作电脑的内存，内存条（SIMM）就是将RAM集成块集中在一起的一小块电路板，它插在计算机中的内存插槽上，以减少RAM集成块占用的空间。目前市场上常见的内存条有4G,8G,16G,32G等。RAM分为两种：DRAM和SRAM。 [4]1.DRAM( Dynamic RAM，动态随机存储器)的存储单元是由电容和相关元件做成的，电容内存储电荷的多寡代表信号0和1。电容存在漏电现象，电荷不足会导致存储单元数据出错，所以DRAM需要周期性刷新，以保持电荷状态。DRAM结构较简单且集成度高，通常用于制造内存条中的存储芯片。 [4]2.SRAM( Static RAM，静态随机存储器)的存储单元是由晶体管和相关元件做成的锁存器，每个存储单元具有锁存“0”和“1”信号的功能。它速度快且不需要刷新操作，但集成度差和功耗较大，通常用于制造容量小但效率高的CPU缓存。 [4]●高速缓冲存储器（Cache）Cache也是我们经常遇到的概念，也就是平常看到的一级缓存（L1 Cache）、二级缓存（L2 Cache）、三级缓存（L3 Cache）这些数据，它位于CPU与内存之间，是一个读写速度比内存更快的存储器。当CPU向内存中写入或读出数据时，这个数据也被存储进高速缓冲存储器中。当CPU再次需要这些数据时，CPU就从高速缓冲存储器读取数据，而不是访问较慢的内存，当然，如需要的数据在Cache中没有，CPU会再去读取内存中的数据。 [4]按内存技术标准分类按内存技术标准可分为 SDRAM， DDR SDRAM，DDR2 SDRAM和DDR3 SDRAM。 [4]1)SDRAM(Synchronous Dynamic RAM，同步动态随机存储器)采用3.3V工作电压，内存数据位宽64位。 SDRAM与CPU通过一个相同的时钟频率锁在一起，使两者以相同的速度同步工作。 SDRAM它在每一个时钟脉冲的上升沿传输数据SDRAM内存金手指为168脚。 [4]SDRAM内存有以下几种：PC66/100/133150/166，核心频率分别为66MHz，100Mz133MHz，150MHz，166MHz。时钟频率、等效频率与核心频率相等单根 SDRAM内存数据传输带宽最高为 166MHz × 64bit ÷ 8 = 1.3GB/s。 [4]相关概念核心频率：是内存颗粒内部存储单元的工作频率，即电容的刷新频率。它是内存工作的基础频率，其他频率都是建立在它基础之上的。 [4]时钟频率：又称内存总线频率，它是主板时钟芯片提供给内存的工作频率。 [4]等效频率：又称等效数据传输频率，它是内存与外界据交换的实际频率。通常内存标签上贴的就是等效效率。 [4]2)DDR SDRAM( Double data Rate SDRAM，双倍速率同步动态随机存储器)采用2.5V工作电压，内存数据位宽64位。 DDR SDRAM (简称DDR内存)一个时钟脉冲传输两次数据，分别在时钟脉冲的上升沿和下降沿各传输一次数据，因此称为双倍速率的SDRAM。 [4]DDR内存金手指为184脚。DDR内存有以下几种:：DDR 200 / 266 / 333/ 400 / 500。核心频率与时钟频率相等，分别为100 MHz， 133 MHz， 166 MHz， 200 MHz， 250 MHz，等效频分别为200 MHz， 266 MHz， 333 MHz， 400 MHz， 500 MHz，请注意， DDR内存的等效频率是时钟频率的两倍，因为DDR内存是双倍速率工作的。DDR内存核心采用2位数据预读取，也就是一次(一个脉冲)取2位。 [4]而DDR内存核心频率等于时钟频率，等效频率是时钟频率的2倍，所以内存核心一次(一个脉冲)取出的数能及时地一次(一个脉冲)传输出去。单根DDR内存数据传输带宽最高为500 MHz×64 bit 8-4 GB/s。 [4]3)DDR2 SDRAM(Double Data Rate 2 SDRAM)采用1.8V工作电压，内存数据位宽64位。 DDR2内存和DDR内存一样，一个时钟脉冲传输两次数据，但DDR2内存却拥有两倍于上一代DDR内存的预读取能力，即4位数据预读取。 [4]DDR 2内存金手指为240脚。DDR2内存有以下几种: DDR2 533 / 667 / 800 / 1066。核心频率分别为133 MHz， 166 MHz， 200 MHz， 266 MHz，时钟频率分别为: 266 MHz，333 MHz， 400 MHz， 533 MHz，等效频率分别为533 MHz， 667 MHz， 800 MHz， 1066 MHz。 [4]前面已经说过， DDR2内存核心采用4位数据预读取，也就是一次(一个脉冲)取4位，如果和上一代DDR内存一样，时钟频率与核心频率相等，等效频率是时钟频率2倍的话，就无法及时地将取出的数传输出去；所以DDR 2内存的时钟频率是核心频率的2倍，这样才能将相同时间间隔内从内存核心取出的数，在相同时间间隔内传输出去。 [4]单根DDR2内存的数据传输带宽最高为1066 MH2z X 64 bit 8 - 8.6 GB/s。 [4]4)DDR3 SDRAM(Double Data Rate 3 SDRAM)采用1.5 V工作电压，内存数据位宽64位。同样， DDR3内存拥有两倍于上一代DDR2内存的预读取能力，即8位数据预读取。 [4]对于DDR 3内存，可以得出以下关系：时钟频率是核心频率的4倍，等效频率是时钟频率的2倍，也就是说DDR3内存等效频率是核心频率的8倍。 [4]DDR 3内存有以下几种: DDR3 1066 / 1333 / 1600 / 1800 / 2000。核心频率分别为133 MHz，166 MHz， 200 MHz， 225 MHz， 250 MHz，时钟频率分别分533 MHz， 667 MHz， 800 MHz，900 MHz， 1000 MHz，等效频率分别为: 1066 MHz， 1333 MHz， 1600 MHz， 1800 MHz，2000 MHz。单根DDR3内存的数据传输带宽最高为2000 MHz × 64 bit÷ 8 -16 GB/s。 [4]5) DDR4 SDRAM(Double Data Rate 4 SDRAM)采用1.2V工作电压，内存数据位宽64位， 16位数据预读取。取消双通道机制，一条内存即为一条通道。工作频率最高可达4266 MHz，单根DDR4内存的数据传输带宽最高为34 GB/s。 [4]按系统逻辑分类1）扩充内存到1984年，即286被普遍接受不久，人们越来越认识到640KB的限制已成为大型程序的障碍，这时，Intel和Lotus，这两家硬、软件的杰出代表，联手制定了一个由硬件和软件相结合的方案，此方法使所有PC机存取640KB以上RAM成为可能。而Microsoft刚推出Windows不久，对内存空间的要求也很高，因此它也及时加入了该行列。 [5]在1985年初，Lotus、Intel和Microsoft三家共同定义了LIM－EMS，即扩充内存规范，通常称EMS为扩充内存。当时，EMS需要一个安装在I/O槽口的内存扩充卡和一个称为EMS的扩充内存管理程序方可使用。但是I/O插槽的地址线只有24位（ISA总线），这对于386以上档次的32位机是不能适应的。所以，现在已很少使用内存扩充卡。现在微机中的扩充内存通常是用软件如DOS中的EMM386把扩展内存模拟或扩充内存来使用。所以，扩充内存和扩展内存的区别并不在于其物理存储器的位置，而在于使用什么方法来读写它。下面将作进一步介绍。 [5]前面已经说过扩充存储器也可以由扩展存储器模拟转换而成。EMS的原理和XMS不同，它采用了页帧方式。页帧是在1MB空间中指定一块64KB空间（通常在保留内存区内，但其物理存储器来自扩展存储器），分为4页，每页16KB。EMS存储器也按16KB分页，每次可交换4页内容，以此方式可访问全部EMS存储器。符合EMS的驱动程序很多，常用的有EMM386.EXE、QEMM、TurboEMS、386MAX等。DOS和Windows中都提供了EMM386 . EXE。 [5]2）扩展内存扩展内存图解我们知道，286有24位地址线，它可寻址16MB的地址空间，而386有32位地址线，它可寻址高达4GB的地址空间，为了区别起见，我们把1MB以上的地址空间称为扩展内存XMS（eXtend memory）。 [5]在386以上档次的微机中，有两种存储器工作方式，一种称为实地址方式或实方式，另一种称为保护方式。在实方式下，物理地址仍使用20位，所以最大寻址空间为1MB，以便与8086兼容。保护方式采用32位物理地址，寻址范围可达4GB。DOS系统在实方式下工作，它管理的内存空间仍为1MB，因此它不能直接使用扩展存储器。为此，Lotus、Intel、AST及Microsoft公司建立了MS－DOS下扩展内存的使用标准，即扩展内存规范XMS。我们常在Config.sys文件中看到的Himem.sys就是管理扩展内存的驱动程序。 [6]扩展内存管理规范的出现迟于扩充内存管理规范。3）高端内存区在实方式下，内存单元的地址可记为：段地址：段内偏移 [5]内存通常用十六进制写为XXXX：XXXX。实际的物理地址由段地址左移4位再和段内偏移相加而成。若地址各位均为1时，即为FFFF：FFFF。其实际物理地址为：FFF0+FFFF=10FFEF，约为1088KB（少16字节），这已超过1MB范围进入扩展内存了。这个进入扩展内存的区域约为64KB，是1MB以上空间的第一个64KB。我们把它称为高端内存区HMA（High Memory Area）。HMA的物理存储器是由扩展存储器取得的。因此要使用HMA，必须要有物理的扩展存储器存在。此外HMA的建立和使用还需要XMS驱动程序HIMEM.SYS的支持，因此只有装入了HIMEM.SYS之后才能使用HMA。 [5]4）上位内存为了解释上位内存的概念，我们还得回过头看看保留内存区。保留内存区是指640KB～1024KB（共384KB）区域。这部分区域在PC诞生之初就明确是保留给系统使用的，用户程序无法插足。但这部分空间并没有充分使用，因此大家都想对剩余的部分打主意，分一块地址空间（注意：是地址空间，而不是物理存储器）来使用。于是就得到了又一块内存区域UMB。 [5]UMB（Upper Memory Blocks）称为上位内存或上位内存块。它是由挤占保留内存中剩余未用的空间而产生的，它的物理存储器仍然取自物理的扩展存储器，它的管理驱动程序是EMS驱动程序。 [5]5）影子内存对于装有1MB或1MB以上物理存储器的机器，其640KB～1024KB这部分物理存储器如何使用的问题。由于这部分地址空间已分配为系统使用，所以不能再重复使用。为了利用这部分物理存储器，在某些386系统中，提供了一个重定位功能，即把这部分物理存储器的地址重定位为1024KB～1408KB。这样，这部分物理存储器就变成了扩展存储器，当然可以使用了。但这种重定位功能在当今高档机器中不再使用，而把这部分物理存储器保留作为Shadow存储器。Shadow存储器可以占据的地址空间与对应的ROM是相同的。Shadow由RAM组成，其速度大大高于ROM。当把ROM中的内容（各种BIOS程序）装入相同地址的Shadow RAM中，就可以从RAM中访问BIOS，而不必再访问ROM。这样将大大提高系统性能。因此在设置CMOS参数时，应将相应的Shadow区设为允许使用（Enabled）。 [5]总结经过上面分析，内存储器的划分可归纳如下： [5]●基本内存占据0～640KB地址空间。内存●保留内存占据640KB～1024KB地址空间。分配给显示缓冲存储器、各适配卡上的ROM和系统ROM BIOS，剩余空间可作上位内存UMB。UMB的物理存储器取自物理扩展存储器。此范围的物理RAM可作为Shadow RAM使用。 [5]●上位内存（UMB）利用保留内存中未分配使用的地址空间建立，其物理存储器由物理扩展存储器取得。UMB由EMS管理，其大小可由EMS驱动程序设定。 [5]●高端内存（HMA）扩展内存中的第一个64KB区域（1024KB～1088KB）。由HIMEM.SYS建立和管理。 [5]●XMS内存符合XMS规范管理的扩展内存区。其驱动程序为HIMEM.SYS。 [5]●EMS内存符合EMS规范管理的扩充内存区。其驱动程序为EMM386.EXE等。 [5]其他类型SRAMSRAM（Static RAM）意为静态随机存储器。SRAM数据不需要通过不断地刷新来保存，因此速度比DRAM（动态随机存储器）快得多。但是SRAM具有的缺点是：同容量相比DRAM需要非常多的晶体管，发热量也非常大。因此SRAM难以成为大容量的主存储器，通常只用在CPU、GPU中作为缓存，容量也只有几十K至几十M。 [5]SRAM目前发展出的一个分支是eSRAM（Enhanced SRAM），为增强型SRAM，具备更大容量和更高运行速度。 [5]RDRAMRDRAM是由RAMBUS公司推出的内存。RDRAM内存条为16bit，但是相比同期的SDRAM具有更高的运行频率，性能非常强。 [5]然而它是一个非开放的技术，内存厂商需要向RAMBUS公司支付授权费。并且RAMBUS内存的另一大问题是不允许空通道的存在，必须成对使用，空闲的插槽必须使用终结器。因此，除了短寿的Intel i820和i850芯片组对其提供支持外，PC平台没有支持RAMBUS内存的芯片组。可以说，它是一个优秀的技术，但不是一个成功的商业产品。 [5]XDR RAMXDR内存是RDRAM的升级版。依旧由RAMBUS公司推出。XDR就是“eXtreme Data Rate”的缩写。XDR依旧存在RDRAM不能大面普及的那些不足之处。因此，XDR内存的应用依旧非常有限。比较常见的只有索尼的PS3游戏机。 [5]Fe-RAM铁电存储器是一种在断电时不会丢失内容的非易失存储器，具有高速、高密度、低功耗和抗辐射等优点。由于数据是通过铁元素的磁性进行存储，因此，铁电存储器无需不断刷新数据。其运行速度将会非常乐观。而且它相比SRAM需要更少的晶体管。它被业界认为是SDRAM的最有可能的替代者。 [5]MRAM磁性存储器。它和Fe-RAM具有相似性，依旧基于磁性物质来记录数据。 [5]OUM相变存储器。奥弗辛斯基（Stanford Ovshinsky）在1968年发表了第一篇关于非晶体相变的论文，创立了非晶体半导体学。一年以后，他首次描述了基于相变理论的存储器：材料由非晶体状态变成晶体，再变回非晶体的过程中，其非晶体和晶体状态呈现不同的反光特性和电阻特性，因此可以利用非晶态和晶态分别代表“0”和“1”来存储数据。 [5]接口类型播报编辑内存的接口类型分DIP， SIMM和DIMM三种(RDRAM又增加了RMM)，其中后两种就是我们要重点论述的内容。 [7]DIPDIP是"Dual n-Line Package"的缩写，即双列直插内存芯片，它的常见单片容量有256KB，IMB等几种。但现在内存发展这么快，哪里还会是几百KB和几兆容量的内存? 因此DIP接口早已经是淘汰了的内存接口。 [7]在SIMM和DIMM接口类型的内存条上，多个RAM芯片焊在一块小电路板上，通过专用插座装在主板或内存扩充板上，因此它们也可以看作是一个内存芯片。 [7]SIMMSIMM是"Singleln-Line Memory Module"的缩写，即单列直插内存模块，这是5x86及较早的PC机中常用的内存接口方式。在更早的PC机中(486以前)，多采用30针的SIMM接口，而在Pentium级别的机器中，应用更多的则是72针的SIMM接口，或者是与DIMM接口类型并存。72线的内存条体积稍大，并提供32位的有效数据位，常见容量有4MB.8MB， 16MB和32MB。 [7]DIMMDIMM是"Dual In-Line Memory Module"缩写，即双列直插内存模块，也就是说这种类型接口的内存的插板的两边都有数据接口触片(俗称为金手指)。 [7]这种接口模式的内存广泛应用于现在的计算机中，通常为84针，但由于是双边的，所以一共有168针，也就是人们常说的168线内存条。168线内存条的体积较大，提供64位有效数据位。 [7]DRAM内存通常为72线的， SDRAM内存通常为168线的，而EDO RAM内存则既有72线的，也有168线的。人们经常用内存的管线数来称呼内存。但需要注意的是，并非只有SDRAM内存是168线的，某些SIMM型内存也具有168线。SIMM的工作电压是5v，DIMM的工作电压是3.3v。 [7]技术指标播报编辑内存的技术指标一般包括奇偶校验、引脚数、容量、速度等。引脚数可以归为内存的接口类型，这里不再论述。 [7]奇偶校验奇/偶校验（ECC）是数据传送时采用的一种校正数据错误的一种方式，分为奇校验和偶校验两种。 [7]如果是采用奇校验，在传送每一个字节的时候另外附加一位作为校验位，当原来数据序列中“1”的个数为奇数时，这个校验位就是“0”，否则这个校验位就是“1”，这样就可以保证传送数据满足奇校验的要求。在接收方收到数据时，将按照奇校验的要求检测数据中“1”的个数，如果是奇数，表示传送正确，否则表示传送错误。 [7]同理偶校验的过程和奇校验的过程一样，只是检测数据中“1”的个数为偶数。 [7]内存容量内存容量同硬盘、软盘等存储器容量单位都是相同的，它们的基本单位都是字节（B），并且：内存1024B=1KB=1024字节=210字节1024KB=1MB=1048576字节=220字节1024MB=1GB=1073741824字节=230字节1024GB=1TB=1099511627776字节=240字节1024TB=1PB=1125899906842624字节=250字节1024PB=1EB=115 292150 4606846976字节=260字节1024EB=1ZB=1180591620717411303424字节=270字节1024ZB=1YB=1208925819614629174706176字节=280字节 [7]内存条是否能以完整的存储体(Bank)为单位安装将决定内存能否正常工作，这与计算机的数据总线位数是相关的，不同机型的计算机，其数据总线的位数也是不同的。内存条通常有 64MB、128MB、256MB等容量级别。从这个级别可以看出，内存条的容量都是翻倍增加的，也就是若内存条容量为512MB，则意味着再往下发展就将为1024MB了。 [7]目前，8GB，16GB内存已成了主流配置。SDRAM内存条有双面和单面两种设计，每一面采用8颗或者9颗(多出的一颗为ECC验) SDRAM芯片。 [7]存取时间存取时间是内存的另一个重要指标，其单位为纳秒(ns)，常见的SDRAM有6ns，7ns， 8ns， 10ns等几种，相应在内存条上标为-6，-7，-8，-10等字样。这个数值越小，存取速度越快，但价格也越高。在选配内存时，应尽量挑选与CPU的时钟周期相匹配的内存条，这将有利于最大限度地发挥内存条的效率。 [7]内存慢而主板快，会影响CPU的速度，还有可能导致系统崩溃；内存快而主板慢，结果只能是大材小用造成资源浪费。当内存的存取时间是10ns时，它的时钟频率最高可达100MHz，也就是说可以配合100MHz外频的主板使用；当存取时间是7ns时，时钟频率最高可达142MHz，这时主板的外频可以上到133MHz以上。不过目前市场上印有“-8"、“-7"甚至“-6"的内存条，不少都达不到它所标称的指标。 [7]CL延迟内存CL反应时间是衡定内存的另一个标志。CL是CAS Latency的缩写，指的是内存存取数据所需的延迟时间，简单的说，就是内存接到CPU的指令后的反应速度。一般的参数值是2和3两种。数字越小，代表反应所需的时间越短。在早期的PC133内存标准中，这个数值规定为3，而在Intel重新制订的新规范中，强制要求CL的反应时间必须为2。这样在一定程度上，对于内存厂商的芯片及PCB的组装工艺要求相对较高，同时也保证了更优秀的品质。因此在选购品牌内存时，这是一个不可不察的因素。 [7]还有另的诠释：内存延迟基本上可以解释成是系统进入数据进行存取操作就序状态前等待内存响应的时间。打个形象的比喻，就像你在餐馆里用餐的过程一样。你首先要点菜，然后就等待服务员给你上菜。同样的道理，内存延迟时间设置的越短，电脑从内存中读取数据的速度也就越快，进而电脑其他的性能也就越高。这条规则双双适用于基于英特尔以及AMD处理器的系统中。由于没有比2-2-2-5更低的延迟，因此国际内存标准组织认为以现在的动态内存技术还无法实现0或者1的延迟。 [7]通常情况下，我们用4个连着的阿拉伯数字来表示一个内存延迟，例如2-2-2-5。其中，第一个数字最为重要，它表示的是CAS Latency，也就是内存存取数据所需的延迟时间。第二个数字表示的是RAS-CAS延迟，接下来的两个数字分别表示的是RAS预充电时间和Act-to-Precharge延迟。而第四个数字一般而言是它们中间最大的一个。 [7]频率内存频率测试图内存主频和CPU主频一样，习惯上被用来表示内存的速度。它代表该内存能达到的最高工作频率。内存主频是以MHz（兆赫）为单位来计量的。内存主频越高在一定程度上代表内存能达到的速度越快。内存主频决定该内存最高能在什么样的频率正常工作。目前主流的内存频率是DDR4，以及内存频率更高的DDR5。 [7]计算机系统的时钟速度是以频率来衡量的。晶体振荡器控制着时钟速度，在石英晶片上加上电压，其就以正弦波的形式震动起来，这一震动可以通过晶片的形变和大小记录下来。晶体的震动以正弦调和变化的电流的形式表现出来，这一变化的电流就是时钟信号。而内存本身并不具备晶体振荡器，因此内存工作时的时钟信号是由主板芯片组的北桥或直接由主板的时钟发生器提供的，也就是说内存无法决定自身的工作频率，其实际工作频率是由主板来决定的。 [7]DDR内存和DDR2内存的频率可以用工作频率和等效频率两种方式表示，工作频率是内存颗粒实际的工作频率，但是由于DDR内存可以在脉冲的上升和下降沿都传输数据，因此传输数据的等效频率是工作频率的两倍；而DDR2内存每个时钟能够以四倍于工作频率的速度读/写数据，因此传输数据的等效频率是工作频率的四倍。例如DDR 200/266/333/400的工作频率分别是100/133/166/200MHz，而等效频率分别是200/266/333/400MHz；DDR2 400/533/667/800的工作频率分别是100/133/166/200MHz，而等效频率分别是400/533/667/800MHz。 [7]带宽从功能上理解，我们可以将内存看作是内存控制器（一般位于北桥芯片中）与CPU之间的桥梁或与仓库。显然，内存的容量决定“仓库”的大小，而内存的带宽决定“桥梁”的宽窄，两者缺一不可，这也就是我们常常说道的“内存容量”与“内存速度”。除了内存容量与内存速度，延时周期也是决定其性能的关键。当CPU需要内存中的数据时，它会发出一个由内存控制器所执行的要求，内存控制器接著将要求发送至内存，并在接收数据时向CPU报告整个周期（从CPU到内存控制器，内存再回到CPU）所需的时间。 [7]毫无疑问，缩短整个周期也是提高内存速度的关键，这就好比在桥梁上工作的警察，其指挥疏通能力也是决定通畅度的因素之一。更快速的内存技术对整体性能表现有重大的贡献，但是提高内存带宽只是解决方案的一部分，数据在CPU以及内存间传送所花的时间通常比处理器执行功能所花的时间更长，为此缓冲区被广泛应用。其实，所谓的缓冲器就是CPU中的一级缓存与二级缓存，它们是内存这座“大桥梁”与CPU之间的“小桥梁”。事实上，一级缓存与二级缓存采用的是SRAM，我们也可以将其宽泛地理解为“内存带宽”，不过现在似乎更多地被解释为“前端总线”，所以我们也只是简单的提一下。事先预告一下，“前端总线”与“内存带宽”之间有着密切的联系，我们将会在后面的测试中有更加深刻的认识。 [7]带宽重要性基本上当CPU接收到指令后，它会最先向CPU中的一级缓存（L1Cache）去寻找相关的数据，虽然一级缓存是与CPU同频运行的，但是由于容量较小，所以不可能每次都命中。这时CPU会继续向下一级的二级缓存（L2Cache）寻找，同样的道理，当所需要的数据在二级缓存中也没有的话，会继续转向L3Cache（如果有的话，如K6-2+和K6-3）、内存和硬盘。 [7]由于目前系统处理的数据量都是相当巨大的，因此几乎每一步操作都得经过内存，这也是整个系统中工作最为频繁的部件。如此一来，内存的性能就在一定程度上决定了这个系统的表现，这点在多媒体设计软件和3D游戏中表现得更为明显。3D显卡的内存带宽（或许称为显存带宽更为合适）的重要性也是不言而喻的，甚至其作用比系统的内存带宽更为明显。大家知道，显示卡在进行像素渲染时，都需要从显存的不同缓冲区中读写数据。这些缓冲区中有的放置描述像素ARGB（阿尔法通道，红，绿，蓝）元素的颜色数据，有的放置像素Z值（用来描述像素的深度或者说可见性的数据）。显然，一旦产生Z轴数据，显存的负担会立即陡然提升，在加上各种材质贴图、深度复杂性渲染、3D特效。 [7]提高内存带宽内存带宽的计算方法并不复杂，大家可以遵循如下的计算公式：带宽=总线宽度×总线频率×一个时钟周期内交换的数据包个数。很明显，在这些乘数因子中，每个都会对最终的内存带宽产生极大的影响。然而，如今在频率上已经没有太大文章可作，毕竟这受到制作工艺的限制，不可能在短时间内成倍提高。而总线宽度和数据包个数就大不相同了，简单的改变会令内存带宽突飞猛进。DDR技术就使我们感受到提高数据包个数的好处，它令内存带宽疯狂地提升一倍。当然，提高数据包个数的方法不仅仅局限于在内存上做文章，通过多个内存控制器并行工作同样可以起到效果，这也就是如今热门的双通道DDR芯片组（如nForce2、I875/865等）。 [7]事实上，双通道DDR内存控制器并不能算是新发明，因为早在RAMBUS时代，RDRAM就已经使用了类似技术，只不过当时RDRAM的总线宽度只有16Bit，无法与DDR的64Bit相提并论。内存技术发展到如今这一阶段，四通道内存控制器的出现也只是时间问题，VIA的QBM技术以及SiS支持四通道RDRAM的芯片组，这些都是未来的发展方向。至于显卡方面，我们对其显存带宽更加敏感，这甚至也是很多厂商用来区分高低端产品的重要方面。同样是使用DDR显存的产品，128Bit宽度的产品会表现出远远胜过64Bit宽度的产品。当然提高显存频率也是一种解决方案，不过其效果并不明显，而且会大幅度提高成本。值得注意的是，目前部分高端显卡甚至动用了DDRII技术，不过至少在目前看来，这项技术还为时过早。 [7]识别内存带宽对于内存而言，辨别内存带宽是一件相当简单的事情，因为SDRAM、DDR、RDRAM这三种内存在外观上有着很大的差别，大家通过下面这副图就能清楚地认识到。唯一需要我们去辨认的便是不同频率的DDR内存。目前主流DDR内存分为DDR266、DDR333以及DDR400，其中后三位数字代表工作频率。 [7]通过内存条上的标识，自然可以很方便地识别出其规格。相对而言，显卡上显存带宽的识别就要困难一些。在这里，我们应该抓住“显存位宽”和“显存频率”两个重要的技术指标。显存位宽的计算方法是：单块显存颗粒位宽×显存颗粒总数，而显存频率则是由"1000/显存颗粒纳秒数"来决定。一般来说，我们可以从显存颗粒上一串编号的最后2两位看出其纳秒数，从中也就得知其显存频率。至于单块显存颗粒位宽，我们只能在网上查询。HY、三星、EtronTech（钰创）等都提供专用的显存编号查询网站，相当方便。如三星的显存就可以到如下的地址下载，只要输入相应的显存颗粒编号即可。此外，使用RivaTuner也可以检测显卡上显存的总位宽，大家打开RivaTuner在MAIN菜单即可看到。 [7]选购方法播报编辑做工要精良对于选择内存来说，最重要的是稳定性和性能，而内存的做工水平直接会影响到性能、稳定以及超频。内存颗粒的好坏直接影响到内存的性能，可以说也是内存最重要的核心元件。所以大家在购买时，尽量选择大厂生产出来的内存颗粒。一般常见的内存颗粒厂商有三星、现代、镁光、南亚、茂矽等，它们都是经过完整的生产工序，因此在品质上都更有保障。而采用这些顶级大厂内存颗粒的内存条品质性能，必然会比其他杂牌内存颗粒的产品要高出许多。 [7]内存PCB电路板的作用是连接内存芯片引脚与主板信号线，因此其做工好坏直接关系着系统稳定性。目前主流内存PCB电路板层数一般是6层，这类电路板具有良好的电气性能，可以有效屏蔽信号干扰。而更优秀的高规格内存往往配备了8层PCB电路板，以起到更好的效能。 [7]SPD隐藏信息SPD信息可以说非常重要，它能够直观反映出内存的性能及体制。它里面存放着内存可以稳定工作的指标信息以及产品的生产，厂家等信息。不过，由于每个厂商都能对SPD进行随意修改，因此很多杂牌内存厂商会将SPD参数进行修改或者直接COPY名牌产品的SPD，但是一旦上机用软件检测就会原形毕露。 [7]因此，大家在购买内存以后，回去用常用的Everest、CPU-Z等软件一查即可明白。不过需要注意的是，对于大品牌内存来说SPD参数是非常重要的，但是对于杂牌内存来说，SPD的信息并不值得完全相信。 [7]假冒返修产品目前有一些内存往往使用了不同品牌、型号的内存颗粒，大家一眼就可以看出区别。同时有些无孔不入的JS也会采用打磨内存颗粒的作假手段，然后再加印上新的编号参数。不过仔细观察，就会发现打磨过后的芯片比较暗淡无光，有起毛的感觉，而且加印上的字迹模糊不清晰。这些一般都是假冒的内存产品，需要注意。 [7]此外，大家还要观察PCB电路板是否整洁，有无毛刺等等，金手指是否很明显有经过插拔所留下的痕迹，如果有，则很有可能是返修内存产品（当然也不排除有厂家出厂前经过测试，不过比较少数）。需要提醒大家的是，返修和假冒内存无论多么便宜都不值得购买，因为其安全隐患十分严重。 [7]故障修复播报编辑1、开机无显示由于内存条原因出现此类故障一般是因为内存条与主板内存插槽接触不良造成，只要用橡皮擦来回擦试其金手指部位即可解决问题（不要用酒精等清洗），还有就是内存损坏或主板内存槽有问题也会造成此类故障。 [1]内存 [1]由于内存条原因造成开机无显示故障，主机扬声器一般都会长时间蜂鸣（针对Award Bios而言）2、windows系统运行不稳定，经常产生非法错误出现此类故障一般是由于内存芯片质量不良或软件原因引起，如若确定是内存条原因只有更换一途。 [1]3、windows注册表经常无故损坏，提示要求用户恢复此类故障一般都是因为内存条质量不佳引起，很难予以修复，唯有更换一途。 [1]4、windows经常自动进入安全模式此类故障一般是由于主板与内存条不兼容或内存条质量不佳引起，常见于PC133内存用于某些不支持PC133内存条的主板上，可以尝试在CMOS设置内降低内存读取速度看能否解决问题，如若不行，那就只有更换内存条了。 [1]5、随机性死机此类故障一般是由于采用了几种不同芯片的内存条，由于各内存条速度不同产生一个时间差从而导致死机，对此可以在CMOS设置内降低内存速度予以解决，否则，唯有使用同型号内存。还有一种可能就是内存条与主板不兼容，此类现象一般少见，另外也有可能是内存条与主板接触不良引起电脑随机性死机，此类现象倒是比较常见。 [1]6、内存加大后系统资源反而降低此类现象一般是由于主板与内存不兼容引起，常见于PC133内存条用于某些不支持PC133内存条的主板上，即使系统重装也不能解决问题。 [1]7、windows启动时，在载入高端内存文件himem.sys时系统提示某些地址有问题此问题一般是由于内存条的某些芯片损坏造成，解决方法可参见下面内存维修一法。 [1]8、运行某些软件时经常出现内存不足的提示此现象一般是由于系统盘剩余空间不足造成，可以删除一些无用文件，多留一些空间即可，一般保持在300M左右为宜。 [1]9、从硬盘引导安装windows进行到检测磁盘空间时，系统提示内存不足此类故障一般是由于用户在config.sys文件中加入了emm386.exe文件，只要将其屏蔽掉即可解决问题。 [1]10、安装windows进行到系统配置时产生一个非法错误此类故障一般是由于内存条损坏造成，可以按内存维修一法来解决，如若不行，那就只有更换内存条了。 [1]11、启动windows时系统多次自动重新启动此类故障一般是由于内存条或电源质量有问题造成，当然，系统重新启动还有可能是CPU散热不良或其他人为故障造成，对此，唯有用排除法一步一步排除。 [1]12、内存维修一法出现上面几种故障后，倘若内存损坏或芯片质量不行，如条件不允许可以用烙铁将内存一边的各芯片卸下，看能否解决问题，如若不行再换卸另一边的芯片，直到成功为止（如此焊工只怕要维修手机的人方可达到）。当然，有条件用示波器检测那就事半功倍了），采用此法后，因为已将内存的一边芯片卸下，所以内存只有一半可用，例如，64M还有32M可用，为此，对于小容量内存就没有维修的必要了。 [1]常见误解播报编辑内部外存储器这种情况主要是发生在描述移动设备的内部集成的数据存放空间时。比如一台手机具备512G的数据存储空间，不少人将其描述为“512G内存”，事实上，这种表述是错误的，因为所谓的“512G内存”是一个外存储器。不能将“内部的外存储器”简称为”内存，因为内存是一个特定的概念，为内存储器的简称。 [1]存储卡的容量存储卡的容量不应当简称为“内存”，因其也是外存储器。 [1]

异步总线：
定义播报编辑不设时钟信号，利用请求、响应、就绪、等待等握手信号传递数据的总线。出处播报编辑《计算机科学技术名词 》第三版。 [1]

内存映射文件：
基本概述播报编辑文件操作是应用程序最为基本的功能之一，Win32 API和MFC均提供有支持文件处理的函数和类，常用的有Win32 API的CreateFile（）、WriteFile（）、ReadFile（）和MFC提供的CFile类等。一般来说，以上这些函数可以满足大多数场合的要求，但是对于某些特殊应用领域所需要的动辄几十GB、几百GB、乃至几TB的海量存储，再以通常的文件处理方法进行处理显然是行不通的。对于上述这种大文件的操作一般是以内存映射文件的方式来加以处理的。 [1]内存映射文件是由一个文件到进程地址空间的映射。Win32中，每个进程有自己的地址空间，一个进程不能轻易地访问另一个进程地址空间中的数据，所以不能像16位Windows那样做。Win32系统允许多个进程（运行在同一计算机上）使用内存映射文件来共享数据。实际上，其他共享和传送数据的技术，诸如使用SendMessage或者PostMessage，都在内部使用了内存映射文件。图1Windows对内存映射文件提供的API如图1中右部所示：数据共享播报编辑文件数据共享这种数据共享是让两个或多个进程映射同一文件映射对象的视图，即它们在共享同一物理存储页。这样，当一个进程向内存映射文件的一个视图写入数据时，其他的进程立即在自己的视图中看到变化。注意，对文件映射对象要使用同一名字。访问方法这样，文件内的数据就可以用内存读/写指令来访问，而不是用ReadFile和WriteFile这样的I/O系统函数，从而提高了文件存取速度。范围应用播报编辑适用范围这种函数最适用于需要读取文件并且对文件内包含的信息做语法分析的应用程序，如：对输入文件进行语法分析的彩色语法编辑器，编译器等。把文件映射后进行读和分析，能让应用程序使用内存操作来操纵文件，而不必在文件里来回地读、写、移动文件指针。应用有些操作，如放弃“读”一个字符，在以前是相当复杂的，用户需要处理缓冲区的刷新问题。在引入了映射文件之后，就简单的多了。应用程序要做的只是使指针减少一个值。映射文件的另一个重要应用就是用来支持永久命名的共享内存。要在两个应用程序之间共享内存，可以在一个应用程序中创建一个文件并映射之，然后另一个应用程序可以通过打开和映射此文件把它作为共享的内存来使用。VC++使用内存映射文件处理大文件内存文件播报编辑内存映射文件与虚拟内存有些类似，通过内存映射文件可以保留一个地址空间的区域，同时将物理存储器提交给此区域，只是内存文件映射的物理存储器来自一个已经存在于磁盘上的文件，而非系统的页文件，而且在对该文件进行操作之前必须首先对文件进行映射，就如同将整个文件从磁盘加载到内存 [2]。由此可以看出，使用内存映射文件处理存储于磁盘上的文件时，将不必再对文件执行I/O操作，这意味着在对文件进行处理时将不必再为文件申请并分配缓存，所有的文件缓存操作均由系统直接管理，由于取消了将文件数据加载到内存、数据从内存到文件的回写以及释放内存块等步骤，使得内存映射文件在处理大数据量的文件时能起到相当重要的作用。另外，实际工程中的系统往往需要在多个进程之间共享数据，如果数据量小，处理方法是灵活多变的，如果共享数据容量巨大，那么就需要借助于内存映射文件来进行。实际上，内存映射文件正是解决本地多个进程间数据共享的最有效方法。内存映射文件并不是简单的文件I/O操作，实际用到了Windows的核心编程技术--内存管理。所以，如果想对内存映射文件有更深刻的认识，必须对Windows操作系统的内存管理机制有清楚的认识，下面给出使用内存映射文件的一般方法：首先要通过CreateFile（）函数来创建或打开一个文件内核对象，这个对象标识了磁盘上将要用作内存映射文件的文件。在用CreateFile（）将文件映像在物理存储器的位置通告给操作系统后，只指定了映像文件的路径，映像的长度还没有指定。为了指定文件映射对象需要多大的物理存储空间还需要通过CreateFileMapping（）函数来创建一个文件映射内核对象以告诉系统文件的尺寸以及访问文件的方式。在创建了文件映射对象后，还必须为文件数据保留一个地址空间区域，并把文件数据作为映射到该区域的物理存储器进行提交。由MapViewOfFile（）函数负责通过系统的管理而将文件映射对象的全部或部分映射到进程地址空间。此时，对内存映射文件的使用和处理同通常加载到内存中的文件数据的处理方式基本一样，在完成了对内存映射文件的使用时，还要通过一系列的操作完成对其的清除和使用过资源的释放。这部分相对比较简单，可以通过UnmapViewOfFile（）完成从进程的地址空间撤消文件数据的映像、通过CloseHandle（）关闭前面创建的文件映射对象和文件对象。相关函数播报编辑在使用内存映射文件时，所使用的API函数主要就是前面提到过的那几个函数，下面分别对其进行介绍：HANDLE CreateFile(LPCTSTR lpFileName,DWORD dwDesiredAccess,DWORD dwShareMode,LPSECURITY_ATTRIBUTES lpSecurityAttributes,DWORD dwCreationDisposition,DWORD dwFlagsAndAttributes,HANDLE hTemplateFile);函数CreateFile（）即使是在普通的文件操作时也经常用来创建、打开文件，在处理内存映射文件时，该函数来创建/打开一个文件内核对象，并将其句柄返回，在调用该函数时需要根据是否需要数据读写和文件的共享方式来设置参数dwDesiredAccess和dwShareMode，错误的参数设置将会导致相应操作时的失败。HANDLE CreateFileMapping(HANDLE hFile,LPSECURITY_ATTRIBUTES lpFileMappingAttributes,DWORD flProtect,DWORD dwMaximumSizeHigh,DWORD dwMaximumSizeLow,LPCTSTR lpName);CreateFileMapping（）函数创建一个文件映射内核对象，通过参数hFile指定待映射到进程地址空间的文件句柄（该句柄由CreateFile（）函数的返回值获取）。由于内存映射文件的物理存储器实际是存储于磁盘上的一个文件，而不是从系统的页文件中分配的内存，所以系统不会主动为其保留地址空间区域，也不会自动将文件的存储空间映射到该区域，为了让系统能够确定对页面采取何种保护属性，需要通过参数flProtect来设定，保护属性PAGE_READONLY、PAGE_READWRITE和PAGE_WRITECOPY分别表示文件映射对象被映射后，可以读取、读写文件数据。在使用PAGE_READONLY时，必须确保CreateFile（）采用的是GENERIC_READ参数；PAGE_READWRITE则要求CreateFile（）采用的是GENERIC_READ|GENERIC_WRITE参数；至于属性PAGE_WRITECOPY则只需要确保CreateFile（）采用了GENERIC_READ和GENERIC_WRITE其中之一即可。DWORD型的参数dwMaximumSizeHigh和dwMaximumSizeLow也是相当重要的，指定了文件的最大字节数，由于这两个参数共64位，因此所支持的最大文件长度为16EB，几乎可以满足任何大数据量文件处理场合的要求。LPVOID MapViewOfFile(HANDLE hFileMappingObject,DWORD dwDesiredAccess,DWORD dwFileOffsetHigh,DWORD dwFileOffsetLow,DWORD dwNumberOfBytesToMap);MapViewOfFile（）函数负责把文件数据映射到进程的地址空间，参数hFileMappingObject为CreateFileMapping（）返回的文件映像对象句柄。参数dwDesiredAccess则再次指定了对文件数据的访问方式，而且同样要与CreateFileMapping（）函数所设置的保护属性相匹配。虽然这里一再对保护属性进行重复设置看似多余，但却可以使应用程序能更多的对数据的保护属性实行有效控制。MapViewOfFile（）函数允许全部或部分映射文件，在映射时，需要指定数据文件的偏移地址以及待映射的长度。其中，文件的偏移地址由DWORD型的参数dwFileOffsetHigh和dwFileOffsetLow组成的64位值来指定，而且必须是操作系统的分配粒度的整数倍，对于Windows操作系统，分配粒度固定为64KB。当然，也可以通过如下代码来动态获取当前操作系统的分配粒度：SYSTEM_INFO sinf;GetSystemInfo(&sinf);DWORD dwAllocationGranularity = sinf.dwAllocationGranularity;参数dwNumberOfBytesToMap指定了数据文件的映射长度，这里需要特别指出的是，对于Windows 9x操作系统，如果MapViewOfFile（）无法找到足够大的区域来存放整个文件映射对象，将返回空值（NULL）；但是在Windows 2000下，MapViewOfFile（）只需要为必要的视图找到足够大的一个区域即可，而无须考虑整个文件映射对象的大小。在完成对映射到进程地址空间区域的文件处理后，需要通过函数UnmapViewOfFile（）完成对文件数据映像的释放，该函数原型声明如下：BOOL UnmapViewOfFile(LPCVOID lpBaseAddress);唯一的参数lpBaseAddress指定了返回区域的基地址，必须将其设定为MapViewOfFile（）的返回值。在使用了函数MapViewOfFile（）之后，必须要有对应的UnmapViewOfFile（）调用，否则在进程终止之前，保留的区域将无法释放。除此之外，前面还曾由CreateFile（）和CreateFileMapping（）函数创建过文件内核对象和文件映射内核对象，在进程终止之前有必要通过CloseHandle（）将其释放，否则将会出现资源泄漏的问题。除了前面这些必须的API函数之外，在使用内存映射文件时还要根据情况来选用其他一些辅助函数。例如，在使用内存映射文件时，为了提高速度，系统将文件的数据页面进行高速缓存，而且在处理文件映射视图时不立即更新文件的磁盘映像。为解决这个问题可以考虑使用FlushViewOfFile（）函数，该函数强制系统将修改过的数据部分或全部重新写入磁盘映像，从而可以确保所有的数据更新能及时保存到磁盘。应用示例播报编辑下面结合一个具体的实例来进一步讲述内存映射文件的使用方法。该实例从端口接收数据，并实时将其存放于磁盘，由于数据量大（几十GB），在此选用内存映射文件进行处理。下面给出的是位于工作线程MainProc中的部分主要代码，该线程自程序运行时启动，当端口有数据到达时将会发出事件hEvent[0]，WaitForMultipleObjects（）函数等待到该事件发生后将接收到的数据保存到磁盘，如果终止接收将发出事件hEvent[1]，事件处理过程将负责完成资源的释放和文件的关闭等工作。下面给出此线程处理函数的具体实现过程：// 创建文件内核对象，其句柄保存于hFileHANDLE hFile = CreateFile("Recv1.zip",GENERIC_WRITE | GENERIC_READ,FILE_SHARE_READ,NULL,CREATE_ALWAYS,FILE_FLAG_SEQUENTIAL_SCAN,NULL);// 创建文件映射内核对象，句柄保存于hFileMappingHANDLE hFileMapping = CreateFileMapping(hFile,NULL,PAGE_READWRITE,0,0x4000000,NULL);// 释放文件内核对象CloseHandle(hFile);// 设定大小、偏移量等参数 // 尽量把文件设置大一些， 如果写的数据超过，设定的值，再次映射文件会报错 getlasterror = 183;__int64 qwFileSize = 0x4000000;__int64 qwFileOffset = 0;__int64 T = 600 * sinf.dwAllocationGranularity;DWORD dwBytesInBlock = 1000 * sinf.dwAllocationGranularity;// 将文件数据映射到进程的地址空间PBYTE pbFile = (PBYTE)MapViewOfFile(hFileMapping,FILE_MAP_ALL_ACCESS,(DWORD)(qwFileOffset>>32),(DWORD)(qwFileOffset&0xFFFFFFFF),dwBytesInBlock);while(bLoop){// 捕获事件hEvent[0]和事件hEvent[1]DWORD ret = WaitForMultipleObjects(2,hEvent,FALSE,INFINITE);ret -= WAIT_OBJECT_0;switch (ret){// 接收数据事件触发case 0:// 从端口接收数据并保存到内存映射文件nReadLen=syio_Read(port[1],pbFile + qwFileOffset,QueueLen);qwFileOffset += nReadLen;// 当数据写满60%时，为防数据溢出，需要在其后开辟一新的映射视图if (qwFileOffset > T){T = qwFileOffset + 600 * sinf.dwAllocationGranularity;UnmapViewOfFile(pbFile);pbFile = (PBYTE)MapViewOfFile(hFileMapping,FILE_MAP_ALL_ACCESS,(DWORD)(qwFileOffset>>32),(DWORD)(qwFileOffset&0xFFFFFFFF),dwBytesInBlock);}break;// 终止事件触发case 1:bLoop = FALSE;// 从进程的地址空间撤消文件数据映像UnmapViewOfFile(pbFile);// 关闭文件映射对象CloseHandle(hFileMapping);break;}}…在终止事件触发处理过程中如果只简单的执行UnmapViewOfFile（）和CloseHandle（）函数将无法正确标识文件的实际大小，即如果开辟的内存映射文件为30GB，而接收的数据只有14GB，那么上述程序执行完后，保存的文件长度仍是30GB。也就是说，在处理完成后还要再次通过内存映射文件的形式将文件恢复到实际大小，下面是实现此要求的主要代码：// 创建另外一个文件内核对象hFile2 = CreateFile("Recv.zip",GENERIC_WRITE | GENERIC_READ,FILE_SHARE_READ,NULL,CREATE_ALWAYS,FILE_FLAG_SEQUENTIAL_SCAN,NULL);// 以实际数据长度创建另外一个文件映射内核对象hFileMapping2 = CreateFileMapping(hFile2,NULL,PAGE_READWRITE,0,(DWORD)(qwFileOffset&0xFFFFFFFF),NULL);// 关闭文件内核对象CloseHandle(hFile2);// 将文件数据映射到进程的地址空间pbFile2 = (PBYTE)MapViewOfFile(hFileMapping2,FILE_MAP_ALL_ACCESS,0,0,qwFileOffset);// 将数据从原来的内存映射文件复制到此内存映射文件memcpy(pbFile2,pbFile,qwFileOffset);file://从进程的地址空间撤消文件数据映像UnmapViewOfFile(pbFile);UnmapViewOfFile(pbFile2);// 关闭文件映射对象CloseHandle(hFileMapping);CloseHandle(hFileMapping2);// 删除临时文件DeleteFile("Recv1.zip");

并行处理：
特点播报编辑只有部分应用程序在满足以下条件的情况下可利用并行处理：具有充足的能充分利用多处理机的应用程序； 并行化目标应用程序或用户需进行新的编码来利用并行程序。传统上，多处理机专为“并行计算机”所设计，沿着这样的思路，当前 Linux 支持 SMP 奔腾系统，在该系统中多处理机共享单个计算机中的单个存储器和总线接口。每个运行 Linux 的机器组都有可能通过网络互相连接形成并行处理群。第三种选择是使用 Linux 系统作为“主机”，提供专门的相关并行处理机（attached parallel processor）。第四种新选择是寄存器内 SIMD 并行，应用于多媒体扩展（MMX） [1]。并行处理所需要提供的典型硬件环境有：单处理机上的单个区；多处理机（SMP）中的单个区；多区配置一个处理机（MPP）中的各区 ；多处理机（SMP 群）中的各区；逻辑数据库区（在 AIX 第1版的 DB2 并行版 － DB2 PE 中也称之为多逻辑代码或 MLN）并行计算机具有代表性的应用领域有：天气预报建摸、VLSI电路的计算机辅助设计、大型数据库管理、人工智能、犯罪控制和国防战略研究等，而且它的应用范围还在不断地扩大。并行处理技术主要是以算法为核心，并行语言为描述，软硬件作为实现工具的相互联系而又相互制约的一种结构技术。算法基本策略播报编辑在并行处理技术中所使用的算法主要遵循三种策略：1．分而治之法：也就是把多个任务分解到多个处理器或多个计算机中，然后再按照一定的拓扑结构来进行求解。2．重新排序法：分别采用静态或动态的指令词度方式。3．显式/隐式并行性结合：显式指的是并行语言通过编译形成并行程序，隐式指的是串行语言通过编译形成并行程序，显式/隐式并行性结合的关键就在于并行编译，而并行编译涉及到语句、程序段、进程以及各级程序的并行性。并行性描述定义播报编辑利用计算机语言进行并行性描述的时候主要有三种方案：1．语言扩展方案：也就是利用各种语言的库函数来进行并行性功能的扩展。2．编译制导法：也称为智能编译，它是隐式并行策略的体现，主要是由并行编译系统进行程序表示、控制流的分析、相关分析、优化分析和并行化划分，由相关分析得到方法库管理方案，由优化分析得到知识库管理方案，由并行化划分得到程序重构，从而形成并行程序。3．新的语言结构法：这是显式并行策略的体现。也就是建立一种全新的并行语言的体系，而这种并行语言通过编译就能直接形成并行程序 [2]。并行软件播报编辑并行软件可分成并行系统软件和并行应用软件两大类，并行系统软件主要指并行编译系统和并行操作系统，并行应用软件主要指各种软件工具和应用软件包。在软件中所牵涉到的程序的并行性主要是指程序的相关性和网络互连两方面。1．程序的相关性：程序的相关性主要分为数据相关、控制相关和资源相关三类。数据相关说明的是语句之间的有序关系，主要有流相关、反相关、输出相关、I/O相关和求知相关等，这种关系在程序运行前就可以通过分析程序确定下来。数据相关是一种偏序关系，程序中并不是每一对语句的成员都是相关联的。可以通过分析程序的数据相关，把程序中一些不存在相关性的指令并行地执行，以提高程序运行的速度。控制相关指的是语句执行次序在运行前不能确定的情况。它一般是由转移指令引起的，只有在程序执行到一定的语句时才能判断出语句的相关性。控制相关常使正在开发的并行性中止，为了开发更多的并行性，必须用编译技术克服控制相关。而资源相关则与系统进行的工作无关，而与并行事件利用整数部件、浮点部件、寄存器和存储区等共享资源时发生的冲突有关。软件的并行性主要是由程序的控制相关和数据相关性决定的。在并行性开发时往往把程序划分成许多的程序段——颗粒。颗粒的规模也称为粒度，它是衡量软件进程所含计算量的尺度，一般用细、中、粗来描述。划分的粒度越细，各子系统间的通信时延也越低，并行性就越高，但系统开销也越大。因此，我们在进行程序组合优化的时候应该选择适当的粒度，并且把通讯时延尽可能放在程序段中进行，还可以通过软硬件适配和编译优化的手段来提高程序的并行度。2．网络互连：将计算机子系统互连在一起或构造多处理机或多计算机时可使用静态或动态拓扑结构的网络。静态网络由点一点直接相连而成，这种连接方式在程序执行过程中不会改变，常用来实现集中式系统的子系统之间或分布式系统的多个计算结点之间的固定连接。动态网络是用开关通道实现的，它可动态地改变结构，使之与用户程序中的通信要求匹配。动态网络包括总线、交叉开关和多级网络，常用于共享存储型多处理机中。在网络上的消息传递主要通过寻径来实现。常见的寻径方式有存储转发寻径和虫蚀寻径等。在存储转发网络中以长度固定的包作为信息流的基本单位，每个结点有一个包缓冲区，包从源结点经过一系列中间结点到达目的结点。存储转发网络的时延与源和目的之间的距离(段数)成正比。而在新型的计算机系统中采用虫蚀寻径，把包进一步分成一些固定长度的片，与结点相连的硬件寻径器中有片缓冲区。消息从源传送到目的结点要经过一系列寻径器。同一个包中所有的片以流水方式顺序传送，不同的包可交替地传送，但不同包的片不能交叉，以免被送到错误的目的地。虫蚀寻径的时延几乎与源和目的之间的距离无关。在寻径中产生的死锁问题可以由虚拟通道来解决。虚拟通道是两个结点间的逻辑链，它由源结点的片缓冲区、结点间的物理通道以及接收结点的片缓冲区组成。物理通道由所有的虚拟通道分时地共享。虚拟通道虽然可以避免死锁，但可能会使每个请求可用的有效通道频宽降低。因此，在确定虚拟通道数目时，需要对网络吞吐量和通信时延折衷考虑。硬件技术播报编辑硬件技术在硬件技术方面主要从处理机、存储器和流水线三个方面来实现并行。1．处理机：主要的处理机系列包括CISC、RISC、超标量、VL1W、超流水线、向量以及符号处理机。传统的处理机属于复杂指令系统计算(CISC)结构。指令系统大，指令格式可变，通用寄存器个数较少，基本上使用合一的指令与数据高速缓存，时钟频率较低，CPI较高，大多数利用ROM 实现微码控制CPU，而当今的精简指令系统计算(RISC)处理机指令格式简单规范，面向寄存器堆，采用重叠寄存器窗口技术，具有多级Cache，多种流水线结构，强调编译优化技术，时钟频率快，CPI低，大多数用硬连线控制CPU。CISC或RISC标量处理机都可以采用超标量或向量结构来改善性能。标量处理机在每个周期内只发射一条指令并要求周期只完成从流水线来的一条指令。而在超标量处理机中，使用了多指令流水线，每个周期要发射多条指令并产生多个结果。由于希望程序中有许多的指令级并行性，因此超标量处理机更要依靠优化编译器去开发并行性。VL1W 结构是将水平微码和超标量处理这两种普遍采用的概念结合起来产生的。典型的超长指令字VL1W 机器指令字长度有数百位。在VLlW 处理机中，多个功能部件是并发工作的，所有的功能部件共享使用公用大型寄存器堆，由功能部件同时执行的各种操作是用VL1W 指令来同步的，每条指令可指定多个操作。VL1W 指令译码比超标量指令容易，但在开发不同数量的并行性时总是需要不同的指令系统。VL1W 主要是开发标量操作之间的并行性，它的成功与否很大程度取决于代码压缩的效率，其结构和任何传统的通用处理机完全不兼容。即使同一结构的不同实现也不大可能做到彼此二进制兼容。VL1W 的主要优点在于它的硬件结构和指令系统简单，在科学应用领域可以发挥良好作用，但在一般应用场合可能并不很好用。向量处理机对数组执行向量指令，每条指令都包含一串重复的操作。它是专门设计用来完成向量运算的协处理机，通常用于多流水线超级计算机中。向量处理机可以利用循环级展开所得的并行性，它可以附属于任何标量处理机。专用的向量流水线可以在循环控制中消除某些软件开销，它的效果与优化编译器将顺序代码向量化的性能很有关系。从理论上说，向量机可以具有和超标量处理机同样的性能，因此可以说向量机的并行性与超标量机相同。符号处理机是为AI应用而研制的，已用于定理证明、模式识别、专家系统、知识工程、文本检索、科学以及机器智能等许多应用领域。在这些应用中，数据和知识表达式、原语操作、算法特性、存储器、I/0和通信以及专用的结构特性与数值计算是不一样的，符号处理机也称为逻辑程序设计语言处理机、表处理语言处理机或符号变换器。符号处理并不和数值数据打交道，它处理的是逻辑程序、符号表、对象、剧本、黑板、产生式系统、语义网络、框架以及人工神经网络等问题。这些操作需要专门的指令系统，通常不使用浮点操作。2．存储器：存储设备按容量和存取时间从低到高可分为寄存器、高速缓存、主存储器、磁盘设备和磁带机五个层次。较低层存储设备与较高层的相比，存取速度较快、容量较小，每字节成本较高、带宽较宽、传输单位较小。存放在存储器层次结构中的信息满足三个重要特性：包含性、一致性和局部性。所谓包含性，指的是一个信息字的复制品可以在比它高的所有层中找到，而如果在高层中丢失了一个信息，则在比它低的所有层中此信息也将丢失。CPU 和高速缓存之间的信息传送是按字进行的，高速缓存和主存储器间用块作为数据传送的基本单位，主存和磁盘之间又是以页面为基本单位来传送信息的，而在磁盘和磁带机之间的数据传送则是按文件级处理的。所谓一致性要求的是同一个信息项与后继存储器层次上的副本是一致的。也就是说，如果在高速缓存中的一个字被修改过，那么在所有更高层上该字的副本也必须立即或最后加以修改。为了尽量减少存储器层次结构的有效存取时间，通常把频繁使用的信息放在较低层次。维护存储器层次结构一致性一般有两种策略，一种是写直达策略，也就是如果，则立即在所有高层存储器中进行同样的修改；另一种是写回策略，也就是在较低层中对信息进行修改后并不立即在高层存储器中进行相应的修改，而是等到该信息将被替换或将从低层中消失时才在所有高层存储器中进行同样的修改。甚至可以将写直达和写回策略的优点结合起来，形成写一次协议来维护存储器的一致性。存储器的层次结构是在一种程序行为——访问的局部性基础上开发出来的。主要有时间局部性、空间局部性和顺序局部性。时间局部性指的是最近的访问项很可能在不久的将来再次被访问。它往往会引起对最近使用区域的集中访问。空间局部性表示一种趋势，指的是一个进程访问的各项其地址彼此很近。顺序局部性指的是在典型程序中，除非是转移指令，一般指令都是顺序执行的。在多处理机系统中一般使用共享存储器。对共享存储器的组织一般采用低位交叉、高位交叉、高低位交叉三种方法。低位交叉又称并发存取，它是把相邻的地址放在相邻的存储器模块中，在访问时不容易产生冲突，并行性较好，但可靠性容错能力和扩展性均较差。高位交叉又称允许同时存取，它是把相邻地址分配到同一个存储器模块中，可靠性、容错能力和扩展性均较强，但访问时易产生冲突，带宽较窄，并行性较差。高低位交叉存取又称C—s存取，它是结合了高位交叉和低位交叉两种方法的优点，既解决了冲突问题，又能有效地提高容错能力和并行性，最适合于向量处理机结构。3．流水线：流水线技术主要有指令流水线技术和运算流水线技术两种。指令流水线技术主要目的是要提高计算机的运行效率和吞吐率。它主要通过设置预取指令缓冲区、设置多功能部件、进行内部数据定向、采取适当的指令调度策略来实现。指令调度的策略主要有静态和动态两种，静态词度是基于软件的，主要由编译器完成，动态词度是基于硬件的，主要是通过硬件技术进行。运算流水线主要有单功能流水线和多功能流水线两种。其中多功能流水线又可分为静态流水线和动态流水线。静态流水线技术只用来实现确定的功能，而动态流水线可以在不同时间重新组合，实现不同的功能，它除流线连接外，还允许前馈和反馈连接，因此也称为非线性流水线。这些前馈和反馈连接使得进入流水线的相继事件的词度变得很不简单。由于这些连接，流水线不一定从最后一段输出。根据不同的数据流动模式，人们可以用同一条流水线求得不同功能的值 [1]。并行计算机发展简述播报编辑40 年代开始的现代计算机发展历程可以分为两个明显的发展时代：串行计算时代、并行计算时代。每一个计算时代都从体系结构发展开始，接着是系统软件（特别是编译器与操作系统）、应用软件，最后随着问题求解环境的发展而达到顶峰。创建和使用并行计算机的主要原因是因为并行计算机是解决单处理器速度瓶颈的最好方法之一。并行计算机是由一组处理单元组成的，这组处理单元通过相互之间的通信与协作，以更快的速度共同完成一项大规模的计算任务。因此，并行计算机的两个最主要的组成部分是计算节点和节点间的通信与协作机制。并行计算机体系结构的发展也主要体现在计算节点性能的提高以及节点间通信技术的改进两方面。60 年代初期，由于晶体管以及磁芯存储器的出现，处理单元变得越来越小，存储器也更加小巧和廉价。这些技术发展的结果导致了并行计算机的出现，这一时期的并行计算机多是规模不大的共享存储多处理器系统，即所谓大型主机（Mainframe）。IBM360 是这一时期的典型代表。到了60 年代末期，同一个处理器开始设置多个功能相同的功能单元，流水线技术也出现了。与单纯提高时钟频率相比，这些并行特性在处理器内部的应用大大提高了并行计算机系统的性能。伊利诺依大学和Burroughs 公司此时开始实施IlliacIV 计划，研制一台64 个CPU 的SIMD 主机系统，它涉及到硬件技术、体系结构、I/O 设备、操作系统、程序设计语言直至应用程序在内的众多研究课题。不过，当一台规模大大缩小了的16CPU 系统终于在1975 年面世时，整个计算机界已经发生了巨大变化。首先是存储系统概念的革新，提出虚拟存储和缓存的思想。IBM360/85 系统与360/91是属于同一系列的两个机型，360/91 的主频高于360/85，所选用的内存速度也较快，并且采用了动态调度的指令流水线；但是，360/85 的整体性能却高于360/91，唯一的原因就是前者采用了缓存技术，而后者则没有。其次是半导体存储器开始代替磁芯存储器。最初，半导体存储器只是在某些机器被用作缓存，而CDC7600 则率先全面采用这种体积更小、速度更快、可以直接寻址的半导体存储器，磁芯存储器从此退出了历史舞台。与此同时，集成电路也出现了，并迅速应用到了计算机中。元器件技术的这两大革命性突破，使得IlliacIV 的设计者们在底层硬件以及并行体系结构方面提出的种种改进都大为逊色。1976 年CRAY-1 问世以后，向量计算机从此牢牢地控制着整个高性能计算机市场15 年。CRAY-1 对所使用的逻辑电路进行了精心的设计，采用了我们如今称为RISC 的精简指令集，还引入了向量寄存器，以完成向量运算。这一系列全新技术手段的使用，使CRAY-1 的主频达到了80MHz。微处理器随着机器的字长从4 位、8 位、16 位一直增加到32 位，其性能也随之显著提高。正是因为看到了微处理器的这种潜力，卡内基- 梅隆大学开始在当时流行的DECPDP11 小型计算机的基础上研制成功一台由16 个PDP11/40 处理机通过交叉开关与16 个共享存储器模块相连接而成的共享存储多处理器系统C.mmp。从80 年代开始，微处理器技术一直在高速前进。稍后又出现了非常适合于SMP 方式的总线协议，而伯克利加州大学则对总线协议进行了扩展，提出了Cache 一致性问题的处理方案。从此，C.mmp 开创出的共享存储多处理器之路越走越宽；现在，这种体系结构已经基本上统治了服务器和桌面工作站市场。同一时期，基于消息传递机制的并行计算机也开始不断涌现。80 年代中期，加州理工成功地将64 个i8086/i8087 处理器通过超立方体互连结构连结起来。此后，便先后出现了Intel iPSC 系列、INMOS Transputer 系列，Intel Paragon 以及IBM SP 的前身Vulcan 等基于消息传递机制的并行计算机。80 年代末到90 年代初，共享存储器方式的大规模并行计算机又获得了新的发展。IBM将大量早期RISC 微处理器通过蝶形互连网络连结起来。人们开始考虑如何才能在实现共享存储器缓存一致的同时，使系统具有一定的可扩展性（Scalability）。90 年代初期，斯坦福大学提出了DASH 计划，它通过维护一个保存有每一缓存块位置信息的目录结构来实现分布式共享存储器的缓存一致性。后来，IEEE 在此基础上提出了缓存一致性协议的标准。90 年代以来，主要的几种体系结构开始走向融合。属于数据并行类型的CM-5 除大量采用商品化的微处理器以外，也允许用户层的程序传递一些简单的消息；CRAY T3D是一台NUMA 结构的共享存储型并行计算机，但是它也提供了全局同步机制、消息队列机制，并采取了一些减少消息传递延迟的技术。随着商品化微处理器、网络设备的发展，以及MPI/PVM 等并行编程标准的发布，机群架构的并行计算机出现。IBM SP2 系列机群系统就是其中的典型代表。在这些系统中，各个节点采用的都是标准的商品化计算机，它们之间通过高速网络连接起来。越来越多的并行计算机系统采用商品化的微处理器加上商品化的互连网络构造，这种分布存储的并行计算机系统称为机群。国内几乎所有的高性能计算机厂商都生产这种具有极高性能价格比的高性能计算机，并行计算机就进入了一个新的时代，并行计算的应用达到了前所未有的广度和深度。并行计算机随着微处理芯片的发展，已经进入了一个新时代。并行计算机的性能已经突破20PFLOPS，正在向百亿亿次发展。我国并行计算机的研制已经走在世界前列。2003年由联想公司生产的深腾6800 在2003 年11 月世界TOP500 排名中位列第14 名，2004 年曙光公司生产的曙光4000A 在2004 年6 月的世界TOP500 排名中位列第10 名，这是我国公开发布的高性能计算机在世界TOP500 中首次进入前十名，这标志着我国在并行计算机系统的研制和生产中已经赶上了国际先进水平，为提高我国的科学研究水平奠定了物质基础。2013年国际超级计算机大会最新发布的世界超级计算机500强排名中，国防科技大学研制的天河二号超级计算机系统，以峰值计算速度每秒5.49亿亿次、持续计算速度每秒3.39亿亿次双精度浮点运算的优异性能位居榜首。从TOP500 的前10 名来看，美国仍然是超级计算机的最大拥有者。按照世界TOP500 的统计数据来分析，美国在计算能力上占有近全世界的一半，在TOP500 中的所有计算机中拥有的数量超过50% [3]。

指令集：
简介播报编辑在计算机中，指示计算机硬件执行某种运算、处理功能的命令称为指令。指令是计算机运行的最小的功能单位，而硬件的作用是完成每条指令规定的功能。一台计算机上全部指令的集合，就是这台计算机的指令系统。指令系统也称指令集，是这台计算机全部功能的体现。而人们设计计算机首要考虑的是它拥有的功能，也就是首先要按功能档次设计指令集，然后按指令集的要求在硬件上实现。指令系统不仅仅是指令的集合，还包括全部指令的指令格式、寻址方式和数据形式。所以，各计算机执行的指令系统不仅决定了机器所要求的能力，而且也决定了指令的格式和机器的结构。反过来说，不同结构的机器和不同的指令格式应该具有与之相匹配的指令系统。为此，设计指令系统时，要对指令格式、类型及操作功能给予应有的重视。软件是为了使用计算机而编写的各种系统和用户的程序，程序由一个序列的计算机指令组成。从这个角度上说，指令是用于设计程序的一种计算机语言单位 [2]。计算机的指令系统是指一台计算机上全部指令的集合，也称计算机的指令集。指令系统包括指令格式、寻址方式和数据形式。一台计算机的指令系统反映了该计算机的全部功能，机器类型不同，其指令系统也不同，因而功能也不同。指令系统的设置和机器的硬件结构密切相关，一台计算机要有较好的性能，必须设计功能齐全、通用性强、内含丰富的指令系统，这就需要复杂的硬件结构来支持 [2]。常见的指令集有：Intel的x86，EM64T，MMX，SSE，SSE2，SSE3，SSSE3 (Super SSE3)，SSE4A，SSE4.1，SSE4.2，AVX，AVX2，AVX-512，VMX等指令集；和AMD的x86，x86-64，3D-Now!指令集。类型播报编辑SSE指令集由于MMX指令并没有带来3D游戏性能的显著提升，1999年Intel公司在Pentium IIICPU产品中推出了数据流单指令序列扩展指令（SSE）。SSE兼容MMX指令，它可以通过SIMD（单指令多数据技术）和单时钟周期并行处理多个浮点来有效地提高浮点运算速度。在MMX指令集中,借用了浮点处理器的8个寄存器，这样导致了浮点运算速度降低。而在SSE指令集推出时，Intel公司在Pentium III CPU中增加了8个128位的SSE指令专用寄存器。而且SSE指令寄存器可以全速运行，保证了与浮点运算的并行性。SSE2指令集在Pentium 4 CPU中，Intel公司开发了新指令集SSE2。这一次新开发的SSE2指令一共144条，包括浮点SIMD指令、整形SIMD指令、SIMD浮点和整形数据之间转换、数据在MMX寄存器中转换等几大部分。其中重要的改进包括引入新的数据格式，如：128位SIMD整数运算和64位双精度浮点运算等。为了更好地利用高速缓存。另外，在Pentium 4中还新增加了几条缓存指令，允许程序员控制已经缓存过的数据。SSE3指令集相对于SSE2，SSE3又新增加了13条新指令，此前它们被统称为pni(prescott new instructions)。13条指令中，一条用于视频解码，两条用于线程同步，其余用于复杂的数学运算、浮点到整数转换和SIMD浮点运算。SSE4指令集SSE4又增加了50条新的增加性能的指令，这些指令有助于编译、媒体、字符/文本处理和程序指向加速。SSE4指令集将作为Intel公司未来“显著视频增强”平台的一部分。该平台的其他视频增强功能还有Clear Video技术（CVT）和统一显示接口（UDI）支持等，其中前者是对ATi AVIVO技术的回应，支持高级解码、后处理和增强型3D功能。3D Now!扩展指令集3D Now!指令集是AMD公司1998年开发的多媒体扩展指令集，共有21条指令。针对MMX指令集没有加强浮点处理能力的弱点，重点提高了AMD公司K6系列CPU对3D图形的处理能力。由于指令有限，3D Now!指令集主要用于3D游戏，而对其他商业图形应用处理支持不足。3DNow!+指令集：在原有的指令集基础上，增加到52条指令，其中包含了部分SSE指令，该指令集主要用于新型的AMDCPU上。X86指令集要知道什么是指令集，要从X86架构的CPU说起。X86指令集是Intel为其第一块16位CPU(i8086)专门开发的，IBM1981年推出的世界第一台PC机中的CPU—i8088(i8086简化版)使用的也是X86指令，同时电脑中为提高浮点数据处理能力而增加的X87芯片系列数学协处理器则另外使用X87指令，图示以后就将X86指令集和X87指令集统称为X86指令集。虽然随着CPU技术的不断发展，Intel陆续研制出更新型的i80386、i80486，但为了保证电脑能继续运行以往开发的各类应用程序以保护和继承丰富的软件资源，所以Intel公司所生产的所有CPU仍然继续使用X86指令集，所以它的CPU仍属于X86系列。由于Intel X86系列及其兼容CPU都使用X86指令集，所以就形成了庞大的X86系列及兼容CPU阵容。EM64T指令集Intel公司的EM64T（Extended Memory 64 Technology）即64位内存扩展技术。该技术为服务器和工作站平台应用提供扩充的内存寻址能力，拥有更多的内存地址空间，可带来更大的应用灵活性，特别有利于提升音频视频编辑、CAD设计等复杂工程软件及游戏软件的应用。常说的64位指的是AMD公司出的64位CPU，而EM64T则是Intel公司按照自己的意思理解出来的64位，也就是和AMD公司的64位对应的另一种叫法。RISC指令集RISC指令集是以后高性能CPU的发展方向。它与传统的CISC(复杂指令集)相对。相比而言，RISC的指令格式统一，种类比较少，寻址方式也比复杂指令集少。使用RISC指令集的体系结构主要有ARM、MIPS。MIPS 指令集是最早实现商用的精简指令集（RISC）之一，上个世纪80年代初由斯坦福大学的研究小组研发，并在1984年成立MIPS计算机公司 [3]。随后MIPS 成为上世纪90年代最流行的指令集，一度与 x86 和ARM 指令集齐名。RISC具有设计更简单、设计周期更短等优点，并可以应用更多先进的技术，开发更快的下一代处理器。MIPS是出现最早的商业RISC架构芯片之一，新的架构集成了所有原来MIPS指令集，并增加了许多更强大的功能。随着移动互联网的兴起，MIPS 指令集逐渐衰落，公司也多次辗转被收购。AVX指令集Intel AVX指令集在SIMD计算性能增强的同时也沿用了的MMX/SSE指令集。不过MMX/SSE的不同点在于增强的AVX指令，从指令的格式上就发生了很大的变化。x86 (IA-32/Intel 64)架构的基础上增加了prefix (Prefix)，所以实现了新的命令，也使更加复杂的指令得以实现，从而提升了x86 CPU的性能。AVX并不是x86 CPU的扩展指令集，可以实现更高的效率，同时和CPU硬件兼容性也好，并且也有着足够的扩展空间，这都和其全新的命令格式系统有关。更加流畅的架构就是AVX发展的方向，换言之，就是摆脱传统x86的不足，在SSE指令的基础上AVX也使SSE指令接口更加易用。针对AVX的最新的命令编码系统，Intel也给出了更加详细的介绍，其中包括了大幅度扩充指令集的可能性。比如Sandy Bridge所带来的融合了乘法的双指令支持。从而可以更加容易地实现512bits和1024bits的扩展。而在2008年末到2009年推出的meniikoa CPU“Larrabee (LARAB)”处理器，就会采用AVX指令集。从地位上来看AVX也开始了Intel处理器指令集的新篇章。AT指令集在移动卫星通信中的应用播报编辑AT命令处理器的实现架构AT 命令集是由贺氏公司（Hayes）发明，贺氏公司起初是一家生产拨号调制解调器的公司，而 AT 命令集最初的用途正是为了控制拨号调制解调器，其控制协议采用文本格式，且每条指令以 AT 打头，AT 指令集因此得名。随着技术的不断进步，低速的拨号调制解调器逐步开始满足不了高带宽、高速率的应用需求，因此逐步被市场所淘汰。贺氏公司也在这一技术升级换代的浪潮中所消失。但是 AT 指令却得以保存，其后，当时几家主要的移动电话生产商诺基亚、摩托罗拉、HP和爱立信基于贺氏AT指令加以延伸扩展，针对移动电话中的 GSM模块控制，研制出了一套完整的 AT 指令。由此，之后GSM 07.05标准、GSM07.07标准均将AT指令纳入其中。并且工业上常用PDU、GPRS控制等也均采用AT 指令来进行实际的控制。因此，AT 指令也成为了这些产品的事实标准。ATCoP，是 AT Command Processor的缩写，它是负责软件实现 AT 指令的模块，我们对 AT 指令的新增和修改都是通过 AT 命令处理器来实现的。其具体流程为 ：当 AT 命令处理器接收到串口的 AT 命令，进行相应的解析工作，并根据具体的解析结果去 AT 命令表查找是否存在对应的处理选项，若找到对应的项，则继续执行相应的处理过程，并在处理结束后将得到的响应数据返回到串口，具体如图《AT命令处理器的实现架构》所示。SIO数据预处理模块的主要工作是将串口收到的AT命令先进行一个数据预处理，同时，将预处理所产生的非中断（null-terminated）命令行发送给 AT 命令解析模块。AT 命令解析模块对传送来的非中断（null-terminated）命令行进行解析，并将每一个非中断命令行映射成一个 token 结构，并将此token结构放入到队列中，形成 AT命令表，等待AT命令处理模块进行查找调用。AT命令处理模块处理AT命令时，对AT命令表中的token结构逐一进行查找，如果查找到匹配选项，则继续执行具体的处理函数，并将此token结构删除。AT命令响应产生模块主要是格式化解析AT命令产生的响应数据，并将此格式化的响应传送给数据终端设备（Data Terminal Equipment ：数据终端设备）。AT命令处理器的容错机制为 ：一次只进行一条AT指令的处理，并且如果AT命令存在错误，在SIO 数据预处理模块就会给出一个错误响应，并产生一个错误代码，不再对其进行处理。常规的卫星移动通信系统主要由卫星、卫星天线、功放及射频模块、信道模块以及用户组成。其中，地面站网络管理控制中心（Network Control Center，NCC）负责对整个卫星网内的各卫星地面站设备进行入网、退网、建立卫星业务通道、各种业务流程等进行统一的管理控制。卫星地面站设备包括卫星控制信道、卫星业务信道、射频及功放设备、卫星收发天线等。它负担着整个卫星业务的业务流程控制，业务数据采集、调制解调等工作。卫星控制信道主要负责整个卫星地面站设备的入网、退网等控制信令的传输控制，卫星业务信道负责对需要发送的卫星业务数据或者卫星话音数据进行加密、调制解调成射频信号传输给射频设备，或者对接收到的射频信号进行调制解调、解密转变成卫星业务数据或话音数据。射频设备以及卫星收发天线主要负责对经过信道处理的卫星数据进行发送或者接收对端传输来的卫星射频信号。当卫星地面站设备1的卫星用户1想和卫星地面站设备 N 的用户 N 进行卫星通信时，用户1通过卫星电话终端或者卫星数据终端进行卫星业务发起，这时，卫星控制信道将对业务发起的控制信令进行处理，通过地面站网络管理控制中心，为两个卫星地面站设备建立空中链路业务通道，之后两个地面站的用户就可以进行需要的业务通信了。当通信结束时，一方用户进行挂机操作，卫星控制终端将会发起业务结束控制信令，拆除两个卫星地面站设备之间的卫星链路。通过前面的简介可以知道，在整个卫星移动通信过程中，由于卫星通信天生的时延等特性，要进行正常的卫星业务通信，对每个卫星地面站设备的入退网管控、话音或者卫星数据流程的发起、结束，卫星业务链路的建立、拆除等控制流程起着至关重要的作用，因此这里我们将简单可靠的 AT 指令集引入，作为卫星移动通信系统的控制协议。这里我们将卫星控制信道称之为 AT命令解析器（AT Command Processor，AP），将卫星业务信道称之为信道处理器（Channel Processor，CP）在卫星控制信道中使用 AT 指令来进行具体对本地面站设备的的控制与解析、对卫星业务流程的发起管理与结束、以及对 CP 的设置与查询等指令。在 CP 中主要接收来自 AP 的一些参数的设置与查询命令，以及根据来自 AP 的 AT 指令进行业务通信的具体流程 [4]。

集成电路：
综述播报编辑集成电路，英文为Integrated Circuit，缩写为IC；顾名思义，就是把一定数量的常用电子元件，如电阻、电容、晶体管等，以及这些元件之间的连线，通过半导体工艺集成在一起的具有特定功能的电路。是20世纪50年代后期到60年代发展起来的一种新型半导体器件。它是经过氧化、光刻、扩散、外延、蒸铝等半导体制造工艺，把构成具有一定功能的电路所需的半导体、电阻、电容等元件及它们之间的连接导线全部集成在一小块硅片上，然后焊接封装在一个管壳内的电子器件。其封装外壳有圆壳式、扁平式或双列直插式等多种形式。集成电路技术包括芯片制造技术与设计技术，主要体现在加工设备，加工工艺，封装测试，批量生产及设计创新的能力上。为什么会产生集成电路？我们知道任何发明创造背后都是有驱动力的，而驱动力往往来源于问题。那么集成电路产生之前的问题是什么呢？我们看一下1946年在美国诞生的世界上第一台电子计算机，它是一个占地150平方米、重达30吨的庞然大物，里面的电路使用了17468只电子管、7200只电阻、10000只电容、50万条线，耗电量150千瓦 [1]。显然，占用面积大、无法移动是它最直观和突出的问题；如果能把这些电子元件和连线集成在一小块载体上该有多好！我们相信，有很多人思考过这个问题，也提出过各种想法。典型的如英国雷达研究所的科学家达默，他在1952年的一次会议上提出：可以把电子线路中的分立元器件，集中制作在一块半导体晶片上，一小块晶片就是一个完整电路，这样一来，电子线路的体积就可大大缩小，可靠性大幅提高。这就是初期集成电路的构想，晶体管的发明使这种想法成为了可能，1947年在美国贝尔实验室制造出来了第一个晶体管，而在此之前要实现电流放大功能只能依靠体积大、耗电量大、结构脆弱的电子管。晶体管具有电子管的主要功能，并且克服了电子管的上述缺点，因此在晶体管发明后，很快就出现了基于半导体的集成电路的构想，也就很快发明出来了集成电路。杰克·基尔比（Jack Kilby）和罗伯特·诺伊斯（Robert Noyce）在1958~1959期间分别发明了锗集成电路和硅集成电路。现在，集成电路已经在各行各业中发挥着非常重要的作用，是现代信息社会的基石。集成电路的含义，已经远远超过了其刚诞生时的定义范围，但其最核心的部分，仍然没有改变，那就是“集成”，其所衍生出来的各种学科，大都是围绕着“集成什么”、“如何集成”、“如何处理集成带来的利弊”这三个问题来开展的。硅集成电路是主流，就是把实现某种功能的电路所需的各种元件都放在一块硅片上，所形成的整体被称作集成电路。对于“集成”，想象一下我们住过的房子可能比较容易理解：很多人小时候都住过农村的房子，那时房屋的主体也许就是三两间平房，发挥着卧室的功能，门口的小院子摆上一副桌椅，就充当客厅，旁边还有个炊烟袅袅的小矮屋，那是厨房，而具有独特功能的厕所，需要有一定的隔离，有可能在房屋的背后，要走上十几米……后来，到了城市里，或者乡村城镇化，大家都住进了楼房或者套房，一套房里面，有客厅、卧室、厨房、卫生间、阳台，也许只有几十平方米，却具有了原来占地几百平方米的农村房屋的各种功能，这就是集成。当然现如今的集成电路，其集成度远非一套房能比拟的，或许用一幢摩登大楼可以更好地类比：地面上有商铺、办公、食堂、酒店式公寓，地下有几层是停车场，停车场下面还有地基——这是集成电路的布局，模拟电路和数字电路分开，处理小信号的敏感电路与翻转频繁的控制逻辑分开，电源单独放在一角。每层楼的房间布局不一样，走廊也不一样，有回字形的、工字形的、几字形的——这是集成电路器件设计，低噪声电路中可以用折叠形状或“叉指”结构的晶体管来减小结面积和栅电阻。各楼层直接有高速电梯可达，为了效率和功能隔离，还可能有多部电梯，每部电梯能到的楼层不同——这是集成电路的布线，电源线、地线单独走线，负载大的线也宽；时钟与信号分开；每层之间布线垂直避免干扰；CPU与存储之间的高速总线，相当于电梯，各层之间的通孔相当于电梯间……特点播报编辑集成电路或称微电路（microcircuit）、微芯片（microchip）、芯片（chip）在电子学中是一种把电路（主要包括半导体装置，也包括被动元件等）小型化的方式，并通常制造在半导体晶圆表面上。前述将电路制造在半导体芯片表面上的集成电路又称薄膜（thin-film）集成电路。另有一种厚膜（thick-film）混成集成电路（hybrid integrated circuit）是由独立半导体设备和被动元件，集成到衬底或线路板所构成的小型化电路。本文是关于单片（monolithic）集成电路，即薄膜集成电路。集成电路具有体积小，重量轻，引出线和焊接点少，寿命长，可靠性高，性能好等优点，同时成本低，便于大规模生产。它不仅在工、民用电子设备如收录机、电视机、计算机等方面得到广泛的应用，同时在军事、通讯、遥控等方面也得到广泛的应用。用集成电路来装配电子设备，其装配密度比晶体管可提高几十倍至几千倍，设备的稳定工作时间也可大大提高。分类播报编辑功能结构集成电路集成电路，又称为IC，按其功能、结构的不同，可以分为模拟集成电路、数字集成电路和数/模混合集成电路三大类。模拟集成电路又称线性电路,用来产生、放大和处理各种模拟信号（指幅度随时间变化的信号。例如半导体收音机的音频信号、录放机的磁带信号等），其输入信号和输出信号成比例关系。而数字集成电路用来产生、放大和处理各种数字信号（指在时间上和幅度上离散取值的信号。例如5G手机、数码相机、电脑CPU、数字电视的逻辑控制和重放的音频信号和视频信号）。制作工艺集成电路按制作工艺可分为半导体集成电路和膜集成电路。膜集成电路又分类厚膜集成电路和薄膜集成电路。集成度高低集成电路按集成度高低的不同可分为：SSIC 小规模集成电路(Small Scale Integrated circuits)MSIC 中规模集成电路(Medium Scale Integrated circuits)LSIC 大规模集成电路(Large Scale Integrated circuits)VLSIC 超大规模集成电路(Very Large Scale Integrated circuits)ULSIC特大规模集成电路(Ultra Large Scale Integrated circuits)GSIC 巨大规模集成电路也被称作极大规模集成电路或超特大规模集成电路(Giga Scale Integration)。导电类型不同集成电路按导电类型可分为双极型集成电路和单极型集成电路，他们都是数字集成电路。双极型集成电路的制作工艺复杂，功耗较大，代表集成电路有TTL、ECL、HTL、LST-TL、STTL等类型。单极型集成电路的制作工艺简单，功耗也较低，易于制成大规模集成电路，代表集成电路有CMOS、NMOS、PMOS等类型。按用途集成电路集成电路按用途可分为电视机用集成电路、音响用集成电路、影碟机用集成电路、录像机用集成电路、电脑（微机）用集成电路、电子琴用集成电路、通信用集成电路、照相机用集成电路、遥控集成电路、语言集成电路、报警器用集成电路及各种专用集成电路。1.电视机用集成电路包括行、场扫描集成电路、中放集成电路、伴音集成电路、彩色解码集成电路、AV/TV转换集成电路、开关电源集成电路、遥控集成电路、丽音解码集成电路、画中画处理集成电路、微处理器（CPU）集成电路、存储器集成电路等。2.音响用集成电路包括AM/FM高中频电路、立体声解码电路、音频前置放大电路、音频运算放大集成电路、音频功率放大集成电路、环绕声处理集成电路、电平驱动集成电路，电子音量控制集成电路、延时混响集成电路、电子开关集成电路等。3.影碟机用集成电路有系统控制集成电路、视频编码集成电路、MPEG解码集成电路、音频信号处理集成电路、音响效果集成电路、RF信号处理集成电路、数字信号处理集成电路、伺服集成电路、电动机驱动集成电路等。4.录像机用集成电路有系统控制集成电路、伺服集成电路、驱动集成电路、音频处理集成电路、视频处理集成电路。5.计算机集成电路，包括中央控制单元（CPU）、内存储器、外存储器、I/O控制电路等。6.通信集成电路7.专业控制集成电路按应用领域分集成电路按应用领域可分为标准通用集成电路和专用集成电路。按外形分集成电路按外形可分为圆形（金属外壳晶体管封装型，一般适合用于大功率）、扁平型（稳定性好，体积小）和双列直插型。简史播报编辑世界集成电路发展历史1947年：美国贝尔实验室的约翰·巴丁、布拉顿、肖克莱三人发明了晶体管，这是微电子技术发展中第一个里程碑；集成电路1950年：结型晶体管诞生1950年：R Ohl和肖克莱发明了离子注入工艺1951年：场效应晶体管发明1956年：C S Fuller发明了扩散工艺1958年：仙童公司Robert Noyce与德仪公司基尔比间隔数月分别发明了集成电路，开创了世界微电子学的历史；1960年：H H Loor和E Castellani发明了光刻工艺1962年：美国RCA公司研制出MOS场效应晶体管1963年：F.M.Wanlass和C.T.Sah首次提出CMOS技术，今天，95%以上的集成电路芯片都是基于CMOS工艺1964年：Intel摩尔提出摩尔定律，预测晶体管集成度将会每18个月增加1倍1966年：美国RCA公司研制出CMOS集成电路，并研制出第一块门阵列（50门），为现如今的大规模集成电路发展奠定了坚实基础，具有里程碑意义1967年：应用材料公司（Applied Materials）成立，现已成为全球最大的半导体设备制造公司1971年：Intel推出1kb动态随机存储器（DRAM），标志着大规模集成电路出现1971年：全球第一个微处理器4004由Intel公司推出，采用的是MOS工艺，这是一个里程碑式的发明1974年：RCA公司推出第一个CMOS微处理器18021976年：16kb DRAM和4kb SRAM问世1978年：64kb动态随机存储器诞生，不足0.5平方厘米的硅片上集成了14万个晶体管，标志着超大规模集成电路（VLSI）时代的来临1979年：Intel推出5MHz 8088微处理器，之后，IBM基于8088推出全球第一台PC1981年：256kb DRAM和64kb CMOS SRAM问世1984年：日本宣布推出1Mb DRAM和256kb SRAM1985年：80386微处理器问世，20MHz1988年：16M DRAM问世，1平方厘米大小的硅片上集成有3500万个晶体管，标志着进入超大规模集成电路（VLSI）阶段1989年：1Mb DRAM进入市场1989年：486微处理器推出，25MHz，1μm工艺，后来50MHz芯片采用0.8μm工艺1992年：64M位随机存储器问世1993年：66MHz奔腾处理器推出，采用0.6μm工艺集成电路1995年：Pentium Pro, 133MHz，采用0.6-0.35μm工艺；1997年：300MHz奔腾Ⅱ问世，采用0.25μm工艺1999年：奔腾Ⅲ问世，450MHz，采用0.25μm工艺，后采用0.18μm工艺2000年：1Gb RAM投放市场2000年：奔腾4问世，1.5GHz，采用0.18μm工艺2001年：Intel宣布2001年下半年采用0.13μm工艺。2003年：奔腾4 E系列推出，采用90nm工艺。2005年：intel 酷睿2系列上市，采用65nm工艺。2007年：基于全新45纳米High-K工艺的intel酷睿2 E7/E8/E9上市。2009年：intel酷睿i系列全新推出，创纪录采用了领先的32纳米工艺，并且下一代22纳米工艺正在研发。我国集成电路发展历史我国集成电路产业诞生于六十年代，共经历了三个发展阶段：1965年-1978年：以计算机和军工配套为目标，以开发逻辑电路为主要产品，初步建立集成电路工业基础及相关设备、仪器、材料的配套条件1978年-1990年：主要引进美国二手设备，改善集成电路装备水平，在“治散治乱”的同时，以消费类整机作为配套重点，较好地解决了彩电集成电路的国产化1990年-2000年：以908工程、909工程为重点，以CAD为突破口，抓好科技攻关和北方科研开发基地的建设，为信息产业服务，集成电路行业取得了新的发展。集成电路产业是对集成电路产业链各环节市场销售额的总体描述，它不仅仅包含集成电路市场，也包括IP核市场、EDA市场、芯片代工市场、封测市场，甚至延伸至设备、材料市场。集成电路产业不再依赖CPU、存储器等单一器件发展，移动互联、三网融合、多屏互动、智能终端带来了多重市场空间，商业模式不断创新为市场注入新活力。目前我国集成电路产业已具备一定基础，多年来我国集成电路产业所聚集的技术创新活力、市场拓展能力、资源整合动力以及广阔的市场潜力，为产业在未来5年～10年实现快速发展、迈上新的台阶奠定了基础。检测常识1、检测前要了解集成电路及其相关电路的工作原理检查和修理集成电路前首先要熟悉所用集成电路的功能、内部电路、主要电气参数、各引脚的作用以及引脚的正常电压、波形与外围元件组成电路的工作原理。2、测试避免造成引脚间短路电压测量或用示波器探头测试波形时，避免造成引脚间短路，最好在与引脚直接连通的外围印刷电路上进行测量。任何瞬间的短路都容易损坏集成电路，尤其在测试扁平型封装的CMOS集成电路时更要加倍小心。3、严禁在无隔离变压器的情况下，用已接地的测试设备去接触底板带电的电视、音响、录像等设备严禁用外壳已接地的仪器设备直接测试无电源隔离变压器的电视、音响、录像等设备。虽然一般的收录机都具有电源变压器，当接触到较特殊的尤其是输出功率较大或对采用的电源性质不太了解的电视或音响设备时，首先要弄清该机底盘是否带电，否则极易与底板带电的电视、音响等设备造成电源短路，波及集成电路，造成故障的进一步扩大。4、要注意电烙铁的绝缘性能不允许带电使用烙铁焊接，要确认烙铁不带电，最好把烙铁的外壳接地，对MOS电路更应小心，能采用6~8V的低压电烙铁就更安全。5、要保证焊接质量焊接时确实焊牢，焊锡的堆积、气孔容易造成虚焊。焊接时间一般不超过3秒钟，烙铁的功率应用内热式25W左右。已焊接好的集成电路要仔细查看，最好用欧姆表测量各引脚间有否短路，确认无焊锡粘连现象再接通电源。6、不要轻易断定集成电路的损坏不要轻易地判断集成电路已损坏。因为集成电路绝大多数为直接耦合，一旦某一电路不正常，可能会导致多处电压变化，而这些变化不一定是集成电路损坏引起的，另外在有些情况下测得各引脚电压与正常值相符或接近时，也不一定都能说明集成电路就是好的。因为有些软故障不会引起直流电压的变化。7、测试仪表内阻要大测量集成电路引脚直流电压时，应选用表头内阻大于20KΩ/V的万用表，否则对某些引脚电压会有较大的测量误差。8、要注意功率集成电路的散热功率集成电路应散热良好，不允许不带散热器而处于大功率的状态下工作。9、引线要合理如需要加接外围元件代替集成电路内部已损坏部分，应选用小型元器件，且接线要合理以免造成不必要的寄生耦合，尤其是要处理好音频功放集成电路和前置放大电路之间的接地端。集成电路型号各部分的意义第0部分第一部分第二部分第三部分第四部分符号意义符合意义意义符号意义符合意义CC表示中国制造TTTL电路用数字表示器件的系列代号C0~70℃F多层陶瓷扁平HHTL电路G‐25~70℃B塑料扁平EECL电路L‐24~85℃H黑瓷扁平CCMOS电路E‐40~85℃D多层陶瓷双列直插M存储器R‐55~85℃J黑瓷双列直插&micro;微型机电路M‐55~125℃P塑料双列直插F线性放大器S塑料单列直插W稳定器K金属菱形B非线性电路T金属圆形J接口电路C陶瓷芯片载体ADA/D转换器E塑料芯片载体DAD/A转换器G网络针栅陈列D音响、电视电路SC通信专用电路SS敏感电路SW钟表电路例如：肖特基4输入与非门CT54S20MDC—符合国家标准T—TTL电路54S20—肖特基双4输入与非门M—‐55~125℃D—多层陶瓷双列直插封装1、BGA(ball grid array)集成电路球形触点阵列，表面贴装型封装之一。在印刷基板的背面按阵列方式制作出球形凸点用以代替引脚，在印刷基板的正面装配LSI芯片，然后用模压树脂或灌封方法进行密封。也称为凸点阵列载体(PAC)。引脚可超过200，是多引脚LSI 用的一种封装。封装本体也可做得比QFP(四侧引脚扁平封装)小。例如，引脚中心距为1.5mm的360引脚BGA仅为31mm见方；而引脚中心距为0.5mm的304 引脚QFP为40mm见方。而且BGA不用担心QFP那样的引脚变形问题（见有图所示）。2、BQFP(quad flat package with bumper)带缓冲垫的四侧引脚扁平封装。QFP封装之一，在封装本体的四个角设置突起(缓冲垫)以防止在运送过程中引脚发生弯曲变形。美国半导体厂家主要在微处理器和ASIC等电路中采用此封装。引脚中心距0.635mm，引脚数从84到196左右(见QFP)。3、C－(ceramic)表示陶瓷封装的记号。例如，CDIP表示的是陶瓷DIP。是在实际中经常使用的记号。4、Cerdip用玻璃密封的陶瓷双列直插式封装，用于ECL RAM，DSP(数字信号处理器)等电路。带有玻璃窗口的Cerdip用于紫外线擦除型EPROM以及内部带有EPROM的微机电路等。引脚中心距2.54mm，引脚数从8到42。在日本，此封装表示为DIP－G(G即玻璃密封的意思)。5、Cerquad集成电路表面贴装型封装之一，即用下密封的陶瓷QFP，用于封装DSP等的逻辑LSI电路。带有窗口的Cerquad用于封装EPROM电路。散热性比塑料QFP好，在自然空冷条件下可容许1.5～2W的功率。但封装成本比塑料QFP高3～5倍。引脚中心距有1.27mm、0.8mm、0.65mm、0.5mm、0.4mm等多种规格。引脚数从32到368。带引脚的陶瓷芯片载体，表面贴装型封装之一，引脚从封装的四个侧面引出，呈丁字形。带有窗口的用于封装紫外线擦除型EPROM以及带有EPROM的微机电路等。此封装也称为QFJ、QFJ－G(见QFJ)。6、COB(chip on board)板上芯片封装，是裸芯片贴装技术之一，半导体芯片交接贴装在印刷线路板上，芯片与基板的电气连接用引线缝合方法实现，芯片与基板的电气连接用引线缝合方法实现，并用树脂覆盖以确保可靠性。虽然COB是最简单的裸芯片贴装技术，但它的封装密度远不如TAB和倒片焊技术。7、DFP(dual flat package)双侧引脚扁平封装。是SOP的别称(见SOP)。以前曾有此称法，80年代后期已基本上不用。8、DIC(dual in-line ceramic package)陶瓷DIP(含玻璃密封)的别称(见DIP).9、DIL(dual in-line)DIP的别称(见DIP)。欧洲半导体厂家多用此名称。10、DIP(dual in-line package)双列直插式封装。插装型封装之一，引脚从封装两侧引出，封装材料有塑料和陶瓷两种。DIP是最普及的插装型封装，应用范围包括标准逻辑IC，存贮器LSI，微机电路等。引脚中心距2.54mm，引脚数从6到64。封装宽度通常为15.2mm。有的把宽度为7.52mm和10.16mm的封装分别称为skinny DIP 和slim DIP(窄体型DIP)。但多数情况下并不加区分，只简单地统称为DIP。另外，用低熔点玻璃密封的陶瓷DIP也称为cerdip(见cerdip)。11、DSO(dual small out-lint)双侧引脚小外形封装。SOP的别称(见SOP)。部分半导体厂家采用此名称。12、DICP(dual tape carrier package)集成电路双侧引脚带载封装。TCP(带载封装)之一。引脚制作在绝缘带上并从封装两侧引出。由于利用的是TAB(自动带载焊接)技术，封装外形非常薄。常用于液晶显示驱动LSI，但多数为定制品。另外，0.5mm厚的存储器LSI簿形封装正处于开发阶段。在日本，按照EIAJ(日本电子机械工业)会标准规定，将DICP命名为DTP。13、DIP(dual tape carrier package)同上。日本电子机械工业会标准对DTCP 的命名(见DTCP)。14、FP(flat package)扁平封装。表面贴装型封装之一。QFP或SOP(见QFP和SOP)的别称。部分半导体厂家采用此名称。15、flip-chip倒焊芯片。裸芯片封装技术之一，在LSI芯片的电极区制作好金属凸点，然后把金属凸点与印刷基板上的电极区进行压焊连接。封装的占有面积基本上与芯片尺寸相同。是所有封装技术中体积最小、最薄的一种。但如果基板的热膨胀系数与LSI芯片不同，就会在接合处产生反应，从而影响连接的可靠性。因此必须用树脂来加固LSI芯片，并使用热膨胀系数基本相同的基板材料。16、FQFP(fine pitch quad flat package)小引脚中心距QFP。通常指引脚中心距小于0.65mm的QFP(见QFP)。部分导导体厂家采用此名称。17、CPAC(globe top pad array carrier)美国Motorola公司对BGA的别称(见BGA)。18、CQFP(quad fiat package with guard ring)带保护环的四侧引脚扁平封装。塑料QFP之一，引脚用树脂保护环掩蔽，以防止弯曲变形。在把LSI组装在印刷基板上之前，从保护环处切断引脚并使其成为海鸥翼状(L形状)。这种封装在美国Motorola公司已批量生产。引脚中心距0.5mm，引脚数最多为208左右。19、H-(with heat sink)表示带散热器的标记。例如，HSOP表示带散热器的SOP。20、pin grid array(surface mount type)集成电路表面贴装型PGA。通常PGA为插装型封装，引脚长约3.4mm。表面贴装型PGA在封装的底面有陈列状的引脚，其长度从1.5mm到2.0mm。贴装采用与印刷基板碰焊的方法，因而也称为碰焊PGA。因为引脚中心距只有1.27mm，比插装型PGA小一半，所以封装本体可制作得不怎么大，而引脚数比插装型多(250～528)，是大规模逻辑LSI用的封装。封装的基材有多层陶瓷基板和玻璃环氧树脂印刷基数。以多层陶瓷基材制作封装已经实用化。21、JLCC(J-leaded chip carrier)J形引脚芯片载体。指带窗口CLCC和带窗口的陶瓷QFJ的别称(见CLCC和QFJ)。部分半导体厂家采用的名称。22、LCC(Leadless chip carrier)无引脚芯片载体。指陶瓷基板的四个侧面只有电极接触而无引脚的表面贴装型封装。是高速和高频IC用封装，也称为陶瓷QFN或QFN－C(见QFN)。23、LGA(land grid array)触点陈列封装。即在底面制作有阵列状态坦电极触点的封装。装配时插入插座即可。现已实用的有227触点(1.27mm中心距)和447触点(2.54mm中心距)的陶瓷LGA，应用于高速逻辑LSI电路。LGA与QFP相比，能够以比较小的封装容纳更多的输入输出引脚。另外，由于引线的阻抗小，对于高速LSI是很适用的。但由于插座制作复杂，成本高，90年代基本上不怎么使用。预计今后对其需求会有所增加。24、LOC(lead on chip)芯片上引线封装。LSI封装技术之一，引线框架的前端处于芯片上方的一种结构，芯片的中心附近制作有凸焊点，用引线缝合进行电气连接。与原来把引线框架布置在芯片侧面附近的结构相比，在相同大小的封装中容纳的芯片达1mm左右宽度。25、LQFP(low profile quad flat package)薄型QFP。指封装本体厚度为1.4mm的QFP，是日本电子机械工业会根据制定的新QFP外形规格所用的名称。26、L－QUAD陶瓷QFP之一。封装基板用氮化铝，基导热率比氧化铝高7～8倍，具有较好的散热性。封装的框架用氧化铝，芯片用灌封法密封，从而抑制了成本。是为逻辑LSI开发的一种封装，在自然空冷条件下可容许W3的功率。现已开发出了208引脚(0.5mm中心距)和160引脚(0.65mm中心距)的LSI逻辑用封装，并于1993年10月开始投入批量生产。27、MCM(multi-chip module)多芯片组件。将多块半导体裸芯片组装在一块布线基板上的一种封装。根据基板材料可分为MCM－L，MCM－C 和MCM－D三大类。MCM－L是使用通常的玻璃环氧树脂多层印刷基板的组件。布线密度不怎么高，成本较低。 MCM－C是用厚膜技术形成多层布线，以陶瓷(氧化铝或玻璃陶瓷)作为基板的组件，与使用多层陶瓷基板的厚膜混合IC类似。两者无明显差别。布线密度高于MCM－L。MCM－D是用薄膜技术形成多层布线，以陶瓷(氧化铝或氮化铝)或Si、Al作为基板的组件。布线密谋在三种组件中是最高的，但成本也高。28、MFP(mini flat package)小形扁平封装。塑料SOP或SSOP的别称(见SOP和SSOP)。部分半导体厂家采用的名称。29、MQFP(metric quad flat package)按照JEDEC(美国联合电子设备委员会)标准对QFP进行的一种分类。指引脚中心距为0.65mm、本体厚度为3.8mm～2.0mm的标准QFP(见QFP)。30、MQUAD(metal quad)美国Olin公司开发的一种QFP封装。基板与封盖均采用铝材，用粘合剂密封。在自然空冷条件下可容许2.5W～2.8W的功率。日本新光电气工业公司于1993年获得特许开始生产。31、MSP(mini square package)QFI的别称(见QFI)，在开发初期多称为MSP。QFI是日本电子机械工业会规定的名称。34、OPMAC(over molded pad array carrier)模压树脂密封凸点陈列载体。美国Motorola公司对模压树脂密封BGA采用的名称(见BGA)。32、P－(plastic)表示塑料封装的记号。如PDIP表示塑料DIP。33、PAC(pad array carrier)凸点陈列载体，BGA的别称(见BGA)。34、PCLP(printed circuit board leadless package)印刷电路板无引线封装。日本富士通公司对塑料QFN(塑料LCC)采用的名称(见QFN)。引脚中心距有0.55mm和0.4mm两种规格。35、PFPF(plastic flat package)塑料扁平封装。塑料QFP的别称(见QFP)。部分LSI厂家采用的名称。36、PGA(pin grid array)集成电路陈列引脚封装。插装型封装之一，其底面的垂直引脚呈陈列状排列。封装基材基本上都采用多层陶瓷基板。在未专门表示出材料名称的情况下，多数为陶瓷PGA，用于高速大规模逻辑LSI电路。成本较高。引脚中心距通常为2.54mm，引脚数从64到447左右。为了为降低成本，封装基材可用玻璃环氧树脂印刷基板代替。也有64～256引脚的塑料PGA。另外，还有一种引脚中心距为1.27mm的短引脚表面贴装型PGA(碰焊PGA)。(见表面贴装型PGA)。37、piggy back驮载封装。指配有插座的陶瓷封装，形关与DIP、QFP、QFN相似。在开发带有微机的设备时用于评价程序确认操作。例如，将EPROM插入插座进行调试。这种封装基本上都是定制品，市场上不怎么流通。38、PLCC(plastic leaded chip carrier)带引线的塑料芯片载体。表面贴装型封装之一。引脚从封装的四个侧面引出，呈丁字形，是塑料制品。美国德克萨斯仪器公司首先在64k位DRAM和256kDRAM中采用，90年代已经普及用于逻辑LSI、DLD(或程逻辑器件电路。引脚中心距1.27mm，引脚数从18到84。J形引脚不易变形，比QFP容易操作，但焊接后的外观检查较为困难。PLCC与LCC(也称QFN)相似。以前，两者的区别仅在于前者用塑料，后者用陶瓷。但现在已经出现用陶瓷制作的J形引脚封装和用塑料制作的无引脚封装(标记为塑料LCC、PC LP、P－LCC等)，已经无法分辨。为此，日本电子机械工业会于1988年决定，把从四侧引出J形引脚的封装称为QFJ，把在四侧带有电极凸点的封装称为QFN(见QFJ和QFN)。39、P－LCC(plastic teadless chip carrier)(plastic leaded chip currier)有时候是塑料QFJ的别称，有时候是QFN(塑料LCC)的别称(见QFJ和QFN)。部分LSI厂家用PLCC表示带引线封装，用P－LCC表示无引线封装，以示区别。40、QFH(quad flat high package)四侧引脚厚体扁平封装。塑料QFP的一种，为了防止封装本体断裂，QFP本体制作得较厚(见QFP)。部分半导体厂家采用的名称。41、QFI(quad flat I-leaded packgac)四侧I形引脚扁平封装。表面贴装型封装之一。引脚从封装四个侧面引出，向下呈I字。也称为MSP(见MSP)。贴装与印刷基板进行碰焊连接。由于引脚无突出部分，贴装占有面积小于QFP。日立制作所为视频模拟IC开发并使用了这种封装。此外，日本的Motorola公司的PLL IC也采用了此种封装。引脚中心距1.27mm，引脚数从18于68。42、QFJ(quad flat J-leaded package)四侧J形引脚扁平封装。表面贴装封装之一。引脚从封装四个侧面引出，向下呈J字形。是日本电子机械工业会规定的名称。引脚中心距1.27mm。材料有塑料和陶瓷两种。塑料QFJ多数情况称为PLCC(见PLCC)，用于微机、门陈列、DRAM、ASSP、OTP 等电路。引脚数从18至84。陶瓷QFJ也称为CLCC、JLCC(见CLCC)。带窗口的封装用于紫外线擦除型EPROM以及带有EPROM的微机芯片电路。引脚数从32至84。43、QFN(quad flat non-leaded package)集成电路四侧无引脚扁平封装。表面贴装型封装之一。90年代后期多称为LCC。QFN是日本电子机械工业会规定的名称。封装四侧配置有电极触点，由于无引脚，贴装占有面积比QFP小，高度比QFP低。但是，当印刷基板与封装之间产生应力时，在电极接触处就不能得到缓解。因此电极触点难于作到QFP的引脚那样多，一般从14到100左右。材料有陶瓷和塑料两种。当有LCC标记时基本上都是陶瓷QFN。电极触点中心距1.27mm。塑料QFN是以玻璃环氧树脂印刷基板基材的一种低成本封装。电极触点中心距除1.27mm外，还有0.65mm和0.5mm两种。这种封装也称为塑料LCC、PCLC、P－LCC等。44、QFP(quad flat package)四侧引脚扁平封装。表面贴装型封装之一，引脚从四个侧面引出呈海鸥翼(L)型。基材有陶瓷、金属和塑料三种。从数量上看，塑料封装占绝大部分。当没有特别表示出材料时，多数情况为塑料QFP。塑料QFP是最普及的多引脚LSI封装。不仅用于微处理器，门陈列等数字逻辑LSI电路，而且也用于VTR信号处理、音响信号处理等模拟LSI电路。引脚中心距有1.0mm、0.8mm、0.65mm、0.5mm、0.4mm、0.3mm等多种规格。0.65mm中心距规格中最多引脚数为304。日本将引脚中心距小于0.65mm的QFP称为QFP(FP)。但2000年后日本电子机械工业会对QFP的外形规格进行了重新评价。在引脚中心距上不加区别，而是根据封装本体厚度分为QFP(2.0mm～3.6mm厚)、LQFP(1.4mm厚)和TQFP(1.0mm厚)三种。另外，有的LSI厂家把引脚中心距为0.5mm的QFP专门称为收缩型QFP或SQFP、VQFP。但有的厂家把引脚中心距为0.65mm及0.4mm的QFP也称为SQFP，至使名称稍有一些混乱。QFP的缺点是，当引脚中心距小于0.65mm时，引脚容易弯曲。为了防止引脚变形，现已出现了几种改进的QFP品种。如封装的四个角带有树指缓冲垫的BQFP(见BQFP)；带树脂保护环覆盖引脚前端的GQFP(见GQFP)；在封装本体里设置测试凸点、放在防止引脚变形的专用夹具里就可进行测试的TPQFP(见TPQFP)。在逻辑LSI方面，不少开发品和高可靠品都封装在多层陶瓷QFP里。引脚中心距最小为0.4mm、引脚数最多为348的产品也已问世。此外，也有用玻璃密封的陶瓷QFP(见Gerqa d)。45、QFP(FP)(QFP fine pitch)小中心距QFP。日本电子机械工业会标准所规定的名称。指引脚中心距为0.55mm、0.4mm、0.3mm等小于0.65mm的QFP(见QFP)。46、QIC(quad in-line ceramic package)陶瓷QFP的别称。部分半导体厂家采用的名称(见QFP、Cerquad)。47、QIP(quad in-line plastic package)塑料QFP的别称。部分半导体厂家采用的名称(见QFP)。48、QTCP(quad tape carrier package)四侧引脚带载封装。TCP封装之一，在绝缘带上形成引脚并从封装四个侧面引出。是利用TAB技术的薄型封装(见TAB、TCP)。49、QTP(quad tape carrier package)四侧引脚带载封装。日本电子机械工业会于1993年4月对QTCP所制定的外形规格所用的名称(见TCP)。50、QUIL(quad in-line)QUIP的别称(见QUIP)。51、QUIP(quad in-line package)四列引脚直插式封装。引脚从封装两个侧面引出，每隔一根交错向下弯曲成四列。引脚中心距1.27mm，当插入印刷基板时，插入中心距就变成2.5mm。因此可用于标准印刷线路板。是比标准DIP更小的一种封装。日本电气公司在台式计算机和家电产品等的微机芯片中采用了些种封装。材料有陶瓷和塑料两种。引脚数64。52、SDIP(shrink dual in-line package)收缩型DIP。插装型封装之一，形状与DIP相同，但引脚中心距(1.778mm)小于DIP(2.54mm)，因而得此称呼。引脚数从14到90。也有称为SH－DIP的。材料有陶瓷和塑料两种。53、SH－DIP(shrink dual in-line package)同SDIP。部分半导体厂家采用的名称。54、SIL(single in-line)SIP的别称(见SIP)。欧洲半导体厂家多采用SIL这个名称。55、SIMM(single in-line memory module)单列存贮器组件。只在印刷基板的一个侧面附近配有电极的存贮器组件。通常指插入插座的组件。标准SIMM 有中心距为2.54mm的30电极和中心距为1.27mm的72电极两种规格。在印刷基板的单面或双面装有用SOJ封装的1兆位及4兆位DRAM的SIMM已经在个人计算机、工作站等设备中获得广泛应用。至少有30～40%的DRAM都装配在SIMM里。56、SIP(single in-line package)单列直插式封装。引脚从封装一个侧面引出，排列成一条直线。当装配到印刷基板上时封装呈侧立状。引脚中心距通常为2.54mm，引脚数从2至23，多数为定制产品。封装的形状各异。也有的把形状与ZIP相同的封装称为SIP。57、SK－DIP(skinny dual in-line package)DIP的一种。指宽度为7.62mm、引脚中心距为2.54mm的窄体DIP。通常统称为DIP(见DIP)。58、SL－DIP(slim dual in-line package)DIP的一种。指宽度为10.16mm，引脚中心距为2.54mm的窄体DIP。通常统称为DIP。59、SMD(surface mount devices)表面贴装器件。偶而，有的半导体厂家把SOP归为SMD(见SOP)。SOP的别称。世界上很多半导体厂家都采用此别称。(见SOP)。60、SOI(small out-line I-leaded package)I形引脚小外型封装。表面贴装型封装之一。引脚从封装双侧引出向下呈I字形，中心距1.27mm。贴装占有面积小于SOP。日立公司在模拟IC(电机驱动用IC)中采用了此封装。引脚数26。61、SOIC(small out-line integrated circuit)SOP的别称(见SOP)。国外有许多半导体厂家采用此名称。62、SOJ(Small Out-Line J-Leaded Package)J形引脚小外型封装。表面贴装型封装之一。引脚从封装两侧引出向下呈J字形，故此得名。通常为塑料制品，多数用于DRAM和SRAM等存储器LSI电路，但绝大部分是DRAM。用SOJ封装的DRAM器件很多都装配在SIMM 上。引脚中心距1.27mm，引脚数从20至40(见SIMM)。63、SQL(Small Out-Line L-leaded package)按照JEDEC(美国联合电子设备工程委员会)标准对SOP所采用的名称(见SOP)。64、SONF(Small Out-Line Non-Fin)无散热片的SOP。与通常的SOP相同。为了在功率IC封装中表示无散热片的区别，有意增添了NF(non-fin)标记。部分半导体厂家采用的名称(见SOP)。65、SOP(small Out-Line package)小外形封装。表面贴装型封装之一，引脚从封装两侧引出呈海鸥翼状(L字形)。材料有塑料和陶瓷两种。另外也叫SOL和DFP。SOP除了用于存储器LSI外，也广泛用于规模不太大的ASSP等电路。在输入输出端子不超过10～40的领域，SOP是普及最广的表面贴装封装。引脚中心距1.27mm，引脚数从8～44。另外，引脚中心距小于1.27mm的SOP也称为SSOP；装配高度不到1.27mm的SOP也称为TSOP(见SSOP、TSOP)。还有一种带有散热片的SOP。66、SOW(Small Outline Package(Wide-Jype))宽体SOP。部分半导体厂家采用的名称。制造从1930年代开始，元素周期表中的化学元素中的半导体被研究者如贝尔实验室的William Shockley认为是固态真空管的最可能的原料。从氧化铜到锗，再到硅，原料在1940到1950年代被系统的研究。今天，尽管元素周期表的一些III-V价化合物如砷化镓应用于特殊用途如：发光二极管，激光，太阳能电池和最高速集成电路，单晶硅成为集成电路主流的基层。创造无缺陷晶体的方法用去了数十年的时间。半导体IC制程，包括以下步骤，并重复使用：黄光(微影)蚀刻薄膜扩散CMP使用单晶硅晶圆（或III-V族，如砷化镓）用作基层。然后使用微影、扩散、CMP等技术制成MOSFET或BJT等组件，然后利用微影、薄膜、和CMP技术制成导线，如此便完成芯片制作。因产品性能需求及成本考量，导线可分为铝制程和铜制程。IC由很多重叠的层组成，每层由图像技术定义，通常用不同的颜色表示。一些层标明在哪里不同的掺杂剂扩散进基层（成为扩散层），一些定义哪里额外的离子灌输（灌输层），一些定义导体（多晶硅或金属层），一些定义传导层之间的连接（过孔或接触层）。所有的组件由这些层的特定组合构成。在一个自排列（CMOS）过程中，所有门层（多晶硅或金属）穿过扩散层的地方形成晶体管。电阻结构，电阻结构的长宽比，结合表面电阻系数，决定电阻。电容结构，由于尺寸限制，在IC上只能产生很小的电容。更为少见的电感结构，可以制作芯片载电感或由回旋器模拟。因为CMOS设备只引导电流在逻辑门之间转换，CMOS设备比双级组件消耗的电流少很多。随机存取存储器（random access memory）是最常见类型的集成电路，所以密度最高的设备是存储器，但即使是微处理器上也有存储器。尽管结构非常复杂－几十年来芯片宽度一直减少－但集成电路的层依然比宽度薄很多。组件层的制作非常像照相过程。虽然可见光谱中的光波不能用来曝光组件层，因为他们太大了。高频光子（通常是紫外线）被用来创造每层的图案。因为每个特征都非常小，对于一个正在调试制造过程的过程工程师来说，电子显微镜是必要工具。在使用自动测试设备（ATE）包装前，每个设备都要进行测试。测试过程称为晶圆测试或晶圆探通。晶圆被切割成矩形块，每个被称为“die”。每个好的die被焊在“pads”上的铝线或金线，连接到封装内，pads通常在die的边上。封装之后，设备在晶圆探通中使用的相同或相似的ATE上进行终检。测试成本可以达到低成本产品的制造成本的25%，但是对于低产出，大型和/或高成本的设备，可以忽略不计。在2005年，一个制造厂（通常称为半导体工厂，常简称fab，指fabrication facility）建设费用要超过10亿美金，因为大部分操作是自动化的。发展趋势2001年到2010年这10年间，我国集成电路产量的年均增长率超过25%，集成电路销售额的年均增长率则达到23%。2010年国内集成电路产量达到640亿块，销售额超过1430亿元，分别是2001年的10倍和8倍。中国集成电路产业规模已经由2001年不足世界集成电路产业总规模的2%提高到2010年的近9%。中国成为过去10年世界集成电路产业发展最快的地区之一。国内集成电路市场规模也由2001年的1140亿元扩大到2010年的7350亿元，扩大了6.5倍。国内集成电路产业规模与市场规模之比始终未超过20%。如扣除集成电路产业中接受境外委托代工的销售额，则中国集成电路市场的实际国内自给率还不足10%，国内市场所需的集成电路产品主要依靠进口。近几年国内集成电路进口规模迅速扩大，2010年已经达到创纪录的1570亿美元，集成电路已连续两年超过原油成为国内最大宗的进口商品。与巨大且快速增长的国内市场相比，中国集成电路产业虽发展迅速但仍难以满足内需要求。当前以移动互联网、三网融合、物联网、云计算、智能电网、新能源汽车为代表的战略性新兴产业快速发展，将成为继计算机、网络通信、消费电子之后，推动集成电路产业发展的新动力。工信部预计，国内集成电路市场规模到2015年将达到12000亿元。我国集成电路产业发展的生态环境亟待优化，设计、制造、封装测试以及专用设备、仪器、材料等产业链上下游协同性不足，芯片、软件、整机、系统、应用等各环节互动不紧密。“十二五”期间，中国将积极探索集成电路产业链上下游虚拟一体化模式，充分发挥市场机制作用，强化产业链上下游的合作与协同，共建价值链。培育和完善生态环境，加强集成电路产品设计与软件、整机、系统及服务的有机连接，实现各环节企业的群体跃升，增强电子信息大产业链的整体竞争优势。2023年，问天实验舱元器件与组件舱外通用试验装置将开展大规模集成电路、新型半导体器件的空间环境效应试验 [6]。发展对策建议1.创新性效率超越传统的成本性静态效率从理论上讲，商务成本属于成本性的静态效率范畴，在产业发展的初级阶段作用显著。外部商务成本的上升实际上是产业升级、创新驱动的外部动力。作为高新技术产业的上海集成电路产业，需要积极利用产业链完备、内部结网度较高、与全球生产网络有机衔接等集群优势，实现企业之间的互动共生的高科技产业机体的生态关系，有效保障并促进产业创业、创新的步伐。事实表明，20世纪80年代，虽然硅谷的土地成本要远高于128公路地区，但在硅谷建立的半导体公司比美国其他地方的公司开发新产品的速度快60%，交运产品的速度快40%。具体而言，就是硅谷地区的硬件和软件制造商结成了紧密的联盟，能最大限度地降低从创意到制造出产品等相关过程的成本，即通过技术密集关联为基本的动态创业联盟，降低了创业成本，从而弥补了静态的商务成本劣势 [2]。2.准确的产品与市场定位许多归国创业的设计人才认为，中国的消费者是世界上最好的衣食父母，与欧美发达国家相比，我们的消费者对新产品充满好奇，一般不退货，基本无赔偿。这些特点为设计企业的创业、创新与发展提供了良好的市场机遇。企业要善于去发现产品应用，寻找市场 [2]。设计公司扩张主要是受限于人才与产品定位。由于在人才团队、市场和产品定义方面的不足，初创公司不可能做大项目，不适合于做集聚型大项目。现有的大多数设计企业还是适合于分散型市场，主动去支持系统厂商，提供大量的服务。人力密集型业务项目不适合欧美公司，更适合我们。例如，在国内市场上，如果一个产品能出货300万颗，那么公司就会去做，国外企业则不可能去做它 [2]。3.打造国际精英人才的“新故乡”，充分发挥海归人才优势海归人才在国外做了很多超前的技术开发研究，并且在全球一些顶尖公司内有产业经验，回国后从事很有需求的产品开发应用，容易成功。集成电路产业的研发就怕方向性错误与低水平重复，海归人才知道如何去做才能够成功 [2]。“归国人才团队+海外工作经验+优惠政策扶持+风险投资”式上海集成电路产业发展的典型模式，这在张江高科技园区尤为明显。然而，由于国际社区建设滞后、户籍政策限制、个人所得税政策缺乏国际竞争力等多方面原因综合作用，张江仍然没有成为海外高级人才的安家落户、长期扎根的开放性、国际性高科技园区。留学生短期打算、“做做看”的“候鸟”观望气氛浓厚，不利于全球高级人才的集聚。要充分发挥张江所处的区位优势以及浦东综合配套改革试点的政策优势，将单纯吸引留学生变为吸引留学生、国外精英等高层次人才。通过科学城建设以及个人所得税率的国际化调整、落户政策的优化，发挥上海“海派文化”传统，将张江建设成为世界各国人才汇集、安居乐业的新故乡，大幅提升张江在高层次人才争夺中的国际竞争力 [2]。4.重在积累，克服急功近利设计业的复杂度很高，需要强大的稳定的团队、深厚的积累。积累是一个不可逾越的发展过程。中国集成电路产业的发展如同下围棋，不能只争一时之短长，要比谁的气长，而不是谁的空多。集成电力产业人才尤其是设计人才供给问题长期以来是舆论界关注的热点，许多高校在专业与设置、人才培养方面急功近利，片面追随所谓社会热点和学业对口，导致学生的基本综合素质和人文科学方面的素养不够高，知识面过窄。事实上，众多设计企业普遍反映，他们招聘人才的标准并非是单纯的所谓专业对口，而是更注重基础知识和综合素质，他们普遍反映高校的教育太急功近利了 [2]。5.促进企业间合作，促进产业链合作国内企业之间的横向联系少，外包刚刚起步，基本上每个设计企业都有自己的芯片，都在进行全面发展。这些因素都限制了企业的快速发展。要充分运用华南一些企业为国外做的解决方案，这样终端客户就可以直接将公司产品运用到原有解决方案上去。此外，设计企业要与方案商、通路商、系统厂商形成紧密的战略合作伙伴关系 [2]。6.摒弃理想化的产学研模式产学研一体化一直被各界视为促进高新技术产业发展的良方，但实地调研结果暴露出人们在此方面存在着不切实际的幻想。笔者所调研的众多设计企业对高校帮助做产品不抱任何指望。公司项目要求的进度快，存在合作的时间问题；高校一般不具备可以使工厂能更有效利用厂房空间，也适用于研发中心的使用。新开发的空冷系统减少了对外部设施的依赖，可在任意位置安装设置，同时继续支持符合STC标准的各种T2000模块，满足各种测试的需要 [2]。其他信息播报编辑晶体管发明并大量生产之后，各式固态半导体组件如二极管、晶体管等大量使用，取代了真空管在电路中的功能与角色。到了20世纪中后期半导体制造技术进步，使得集成电路成为可能。相对于手工组装电路使用个别的分立电子组件，集成电路可以把很大数量的微晶体管集成到一个小芯片，是一个巨大的进步。集成电路的规模生产能力，可靠性，电路设计的模块化方法确保了快速采用标准化IC代替了设计使用离散晶体管。IC对于离散晶体管有两个主要优势：成本和性能。成本低是由于芯片把所有的组件通过照相平版技术，作为一个单位印刷，而不是在一个时间只制作一个晶体管。性能高是由于组件快速开关，消耗更低能量，因为组件很小且彼此靠近。2006年，芯片面积从几平方毫米到350mm²，每mm²可以达到一百万个晶体管。第一个集成电路雏形是由杰克·基尔比于1958年完成的，其中包括一个双极性晶体管，三个电阻和一个电容器。根据一个芯片上集成的微电子器件的数量，集成电路可以分为以下几类：1.小规模集成电路SSI英文全名为Small Scale Integration，逻辑门10个以下或晶体管100个以下。2.中规模集成电路MSI英文全名为Medium Scale Integration，逻辑门11~100个或晶体管101~1k个。3.大规模集成电路LSI英文全名为Large Scale Integration，逻辑门101~1k个或晶体管1,001~10k个。4.超大规模集成电路VLSI英文全名为Very large scale integration，逻辑门1,001~10k个或晶体管10,001~100k个。5.甚大规模集成电路ULSI英文全名为Ultra Large Scale Integration，逻辑门10,001~1M个或晶体管100,001~10M个。GLSI英文全名为Giga Scale Integration，逻辑门1,000,001个以上或晶体管10,000,001个以上。而根据处理信号的不同，可以分为模拟集成电路、数字集成电路、和兼具模拟与数字的混合信号集成电路。集成电路发展最先进的集成电路是微处理器或多核处理器的"核心(cores)",可以控制电脑到手机到数字微波炉的一切。存储器和ASIC是其他集成电路家族的例子，对于现代信息社会非常重要。虽然设计开发一个复杂集成电路的成本非常高，但是当分散到通常以百万计的产品上，每个IC的成本最小化。IC的性能很高，因为小尺寸带来短路径，使得低功率逻辑电路可以在快速开关速度应用。这些年来，IC持续向更小的外型尺寸发展，使得每个芯片可以封装更多的电路。这样增加了每单位面积容量，可以降低成本和增加功能－见摩尔定律，集成电路中的晶体管数量，每两年增加一倍。总之，随着外形尺寸缩小，几乎所有的指标改善了－单位成本和开关功率消耗下降，速度提高。但是，集成纳米级别设备的IC不是没有问题，主要是泄漏电流（leakage current）。因此，对于最终用户的速度和功率消耗增加非常明显，制造商面临使用更好几何学的尖锐挑战。这个过程和在未来几年所期望的进步，在半导体国际技术路线图（ITRS）中有很好的描述。越来越多的电路以集成芯片的方式出现在设计师手里，使电子电路的开发趋向于小型化、高速化。越来越多的应用已经由复杂的模拟电路转化为简单的数字逻辑集成电路。2022年，关于促进我国集成电路全产业链可持续发展的提案:集成电路产业是国民经济和社会发展的战略性、基础性、先导性产业，其全产业链中的短板缺项成为制约我国数字经济高质量发展、影响综合国力提升的关键因素之一。现阶段我国集成电路产业高端受封锁压制、中低端产能紧缺情况愈演愈烈，仍存在一些亟需解决的问题。一是国内芯片企业能力不强与市场不足并存。二是美西方对我国集成电路产业先进工艺的高端装备全面封堵，形成新的产业壁垒。三是目前我国集成电路产业人才处于缺乏状态，同时工艺研发人员的培养缺乏“产线”的支撑。为此，建议：一是发挥新型举国体制优势，持续支持集成电路产业发展。延续和拓展国家科技重大专项，集中力量重点攻克核心难点。支持首台套应用，逐步实现国产替代。拓展新的应用领域。加大产业基金规模和延长投入周期。二是坚持产业长远布局，深化人才培养改革。既要“补短板”也要“加长板”。持续加大科研人员培养力度和对从事基础研究人员的投入保障力度，夯实人才基础。三是坚持高水平对外开放，拓展和营造新兴市场。积极探索未来和集成电路有关的新兴市场，支持我国集成电路企业走出去。 [4]IC的普及仅仅在其开发后半个世纪，集成电路变得无处不在，电脑，手机和其他数字电器成为现代社会结构不可缺少的一部分。这是因为，现代计算，交流，制造和交通系统，包括互联网，全都依赖于集成电路的存在。甚至很多学者认为有集成电路带来的数字革命是人类历史中最重要的事件。IC的分类集成电路的分类方法很多，依照电路属模拟或数字，可以分为：模拟集成电路、数字集成电路和混合信号集成电路（模拟和数字在一个芯片上）。数字集成电路可以包含任何东西，在几平方毫米上有从几千到百万的逻辑门，触发器，多任务器和其他电路。这些电路的小尺寸使得与板级集成相比，有更高速度，更低功耗并降低了制造成本。这些数字IC，以微处理器，数字信号处理器（DSP）和单片机为代表，工作中使用二进制，处理1和0信号。模拟集成电路有,例如传感器，电源控制电路和运放，处理模拟信号。完成放大，滤波，解调，混频的功能等。通过使用专家所设计、具有良好特性的模拟集成电路，减轻了电路设计师的重担，不需凡事再由基础的一个个晶体管处设计起。IC可以把模拟和数字电路集成在一个单芯片上，以做出如模拟数字转换器（A/D converter）和数字模拟转换器（D/A converter）等器件。这种电路提供更小的尺寸和更低的成本，但是对于信号冲突必须小心。统计数据播报编辑《中华人民共和国2021年国民经济和社会发展统计公报》显示：2021年，集成电路产量3594.3亿块，增长37.5%。 [3]2023年2月28日，国家统计局发布《中华人民共和国2022年国民经济和社会发展统计公报》，初步核算，2022年集成电路产量3241.9亿块，比上年增长-9.8%；集成电路出口量2734亿个，比上年增长-12.0%；集成电路进口量5384亿个，比上年增长-15.3%。 [5]

光盘存储：
简介播报编辑光盘是存储信息的主要物理媒介，随着信息容量的急速增长，对信息的分类和快速提取非常重要。因此需要建立一套对光盘存储媒介的管理系统，来实现快速检索。美可达光盘档案归档管理系统播报编辑光盘档案随着光盘价格的急剧下降，光盘已成为价格最低的计算机数据存储介质，全世界百分之八十的光学头、百分之七十的光盘机小机芯在中国生产，中国已成为世界光盘产品产销大国。特别是2003年国际蓝光光盘标准的统一，单盘存储量可达50GB。越来越多的政府机构与公共事业部门、大中型企业、报刊出版社、电台和电视台、银行、学校，以及图书馆、档案馆（室）、资料室在线查询，将信息（计算机程序、数据、书刊）用光盘形式存储发行。光存储介质的物理特性决定了光盘对“恶意修改”、“病毒”和“黑客 ”入侵，先天“免疫”， 特别是，在真实不可更改的档案保管中，光盘发挥着不可替代的作用。在绝对不可修改的数据，如身份证、驾驶证、银行票据、保险票据、建筑档案、国防科技重要资料等行业应用中，以及不可存储与复制的数据（如有版权的光盘）的归档和日常数据管理，光盘已成为各级档案部门的唯一性选择。光盘档案分发归档管理解决方案，是针对国家机关、企事业单位档案管理部门电子文件光盘或公文光盘的存储归档等诸多现实问题，本解决方案有效解决了电子档案的光盘存储、分类标识、上架管理和调档利用问题。该方案不但可大幅度降低部署和维护存储归档系统的管理成本，而且充分体现了电子文档系统与传统归档系统一样的权威性和安全性。本方案可把分布在硬盘、磁带、胶片以及挂接在INTERNET/INTRANET网络上所有数据资源，迁移到光盘后进行分类归档。也可对已归档的光盘档案进行再分类归档、上架。随着中华人民共和国《GB/T 20530-2006文献档案资料数字化工作导则》国家标准的正式发布实施，奠定了光盘档案在存储与归档领域的未来法律地位主要功能1、光盘档案上架管理按照国家及行业相关标准，对每张上架光盘 进行条码赋码，系统通过读取光盘外标识的条形码，为每张光盘分配上架的物理位置。2、光盘档案著录本方案可对每张光盘进行卷宗级、案卷级和 文件级著录（建立文件索引），并可提取文件相关信息，真正做到“只需动手， 无需动脑”。3、档案查询用户通过输入关键字等信息，可快速查询到目标文件光盘的具体位置。与智能光盘柜结合进行具体光盘档案查询。可采用多种模式快速定位所查光盘档案所属的物理位置。查询方式：卷宗级、案卷级、目录级查询途径：单一条件查询、组合条件查询、模糊查询4、光盘调档管理Rimage设备可自动为用户提供双备份或者批量备份数据等服务。 依照调档工作管理规范完成审批调阅手续后，依次批量复制，及批量调档。5、系统管理可对用户权限进行分级设定和管理，支持组策 略；系统初始化设置;日志管理等功能。光盘归档管理方式具有以下优点1、符合国家、行业对档案管理的相关标准要求；2、基于光盘建立的存储与归档系统具有操作便易性、高效性及实用性，安全性高、成本低廉、维护成本低；3、对光盘档案的容灾性强；4、检索和调档手段可选择性强，满足分级管理的需求；5、光盘档案与纸质档案、电子档案互为关联，系统管理具有包容性。应用领域档案馆、图书馆、医院、城建、测绘、电信、证券、保险、银行、广电、气象、地震、石油、地质勘探、音像出版等应用领域。

微型计算机：
简介播报编辑微型计算机简称微机，俗称电脑，其准确的称谓应该是微型计算机系统。它可以简单地定义为：在微型计算机硬件系统的基础上配置必要的外部设备和软件构成的实体 [2]。组成播报编辑微型计算机系统从全局到局部存在三个层次：微型计算机系统、微型计算机、微处理器（CPU）。单纯的微处理器和单纯的微型计算机都不能独立工作，只有微型计算机系统才是完整的信息处理系统，才具有实用意义 [2]。一个完整的微型计算机系统包括硬件系统和软件系统两大部分。硬件系统由运算器、控制器、存储器（含内存、外存和缓存）、各种输入输出设备组成，采用“指令驱动”方式工作 [3]。微型计算机软件系统可分为系统软件和应用软件。系统软件是指管理、监控和维护计算机资源（包括硬件和软件）的软件。它主要包括：操作系统、各种语言处理程序、数据库管理系统以及各种工具软件等。其中操作系统是系统软件的核心，用户只有通过操作系统才能完成对计算机的各种操作。应用软件是为某种应用目的而编制的计算机程序，如文字处理软件、图形图像处理软件、网络通信软件、财务管理软件、CAD软件、各种程序包等 [3]。特点播报编辑微型计算机的特点是体积小、灵活性大、价格便宜、使用方便。自1981年美国IBM公司推出第一代微型计算机IBM-PC以来，微型机以其执行结果精确、处理速度快捷、性价比高、轻便小巧等特点迅速进入社会各个领域，且技术不断更新、产品快速换代，从单纯的计算工具发展成为能够处理数字、符号、文字、语言、图形、图像、音频、视频等多种信息的强大多媒体工具。如今的微型机产品无论从运算速度、多媒体功能、软硬件支持还是易用性等方面都比早期产品有了很大飞跃 [4]。分类播报编辑工作站工作站是一种高端的通用微型计算机，以个人计算机和分布式网络计算为基础，主要面向专业应用领域，具备强大的数据运算与图形、图像处理能力，是为满足工程设计、动画制作、科学研究、软件开发、金融管理、信息服务、模拟仿真等专业领域而设计开发的高性能计算机。它属于一种高档的计算机，一般拥有较大的屏幕显示器和大容量的内存和硬盘，也拥有较强的信息处理功能和高性能的图形、图像处理功能以及联网功能 [4]。服务器服务器服务器专指某些高性能计算机，能通过网络对外提供服务。相对于普通计算机来说，稳定性、安全性、性能等方面都要求更高，因此在CPU、芯片组、内存、磁盘系统、网络等硬件和普通计算机有所不同。服务器是网络的结点，存储、处理网络上80%的数据和信息，在网络中起到举足轻重的作用。服务器是为客户端计算机提供各种服务的高性能的计算机，其高性能主要在高速度的运算能力、长时间的可靠运行、强大的外部数据吞吐能力等方面。服务器的构成与普通计算机类似，也有处理器、硬盘、内存、系统总线等，但因为它是针对具体的网络应用特别定制的，因而服务器与微型机在处理能力、稳定性、可靠性、安全性、可扩展性、可管理性等方面存在很大差异。服务器主要有网络服务器(DNS、DHCP)、打印服务器、终端服务器、磁盘服务器、邮件服务器、文件服务器等 [4]。工业控制计算机工业控制计算机是一种采用总线结构，对生产过程及其机电设备、工艺装备进行检测与控制的计算机系统总称，简称控制机。它由计算机和过程输入/输出（I/O）两大部分组成。计算机由主机、输入/输出设备和外部磁盘机、磁带机等组成。在计算机外部又增加一部分过程输入/输出通道，用来将工业生产过程的检测数据送入计算机进行处理；另一方面，将计算机要行使对生产过程控制的命令、信息转换成工业控制对象的控制变量信号，再送往工业控制对象的控制器中。由控制器行使对生产设备的运行控制 [4]。个人计算机①台式机：台式机是应用非常广泛的微型计算机，也叫桌面机，是一种独立分离的计算机，体积相对较大，主机、显示器等设备一般都是相对独立的，需要放置在电脑桌或者专门的工作台上，因此命名为台式机。台式机的机箱空间大、通风条件好，具有很好的散热性；独立的机箱方便用户进行硬件升级，如光驱、硬盘；台式机机箱的开关键、重启键、USB、音频接口都在机箱前置面板中，方便用户的使用 [4]。②电脑一体机：电脑一体机是由一台显示器、一个键盘和一个鼠标组成的计算机。它的芯片、主板与显示器集成在一起，显示器就是一台计算机，因此只要将键盘和鼠标连接到显示器上，机器就能使用。随着无线技术的发展，电脑一体机的键盘、鼠标与显示器可实现无线连接，机器只有一根电源线，在很大程度上解决了一直为人诟病的台式机线缆多而杂的问题 [4]。③笔记本式计算机：笔记本式计算机是一种小型、可携带的个人计算机，通常质量为1-3kg。它和台式机架构类似，但是它具有更好的便携性。笔记本式计算机除了键盘外，还提供了触控板（TouchPad）或触控点（Pointing Stick），提供了更好的定位和输入功能 [4]。④掌上电脑（PDA）：PDA（Personal Digital Assistant），是个人数字助手的意思。顾名思义就是辅助个人工作的数字工具，主要提供记事、通讯录、名片交换及行程安排等功能。可以帮助人们在移动中工作、学习、娱乐等。按使用来分类，分为工业级PDA和消费品PDA。工业级PDA主要应用在工业领域，常见的有条形码扫描器、RFID读写器、POS机等；消费品PDA包括的比较多，比如智能手机、手持的游戏机等 [4]。⑤平板电脑：平板电脑也叫平板式计算机（Tablet Personal Computer，简称 Tablet Pc、 Flat Pc、Tablet、 Slates），是一种小型、方便携带的个人计算机，以触摸屏作为基本的输入设备。它拥有的触摸屏（也称为数位板技术）允许用户通过触控笔或数字笔来进行作业而不是传统的键盘或鼠标。用户可以通过内置的手写识别、屏幕上的软键盘、语音识别或者一个真正的键盘（如果该机型配备的话）实现输入 [4]。嵌入式计算机个人计算机(3张)嵌入式计算机即嵌入式系统，是一种以应用为中心、以微处理器为基础，软硬件可裁剪的，适应应用系统对功能、可靠性、成本、体积、功耗等综合性严格要求的专用计算机系统。它一般由嵌入式微处理器、外围硬件设备、嵌入式操作系统及用户的应用程序四个部分组成。它是计算机市场中增长最快的领域，也是种类繁多、形态多种多样的计算机系统。嵌入式系统几乎包括了生活中的所有电器设备，如计算器、电视机顶盒、手机、数字电视、多媒体播放器、汽车、微波炉、数字相机、家庭自动化系统、电梯、空调、安全系统、自动售货机、消费电子设备、工业自动化仪表与医疗仪器等 [4]。发展阶段播报编辑1、4位和8位低档微处理器时代（1971~1973年）1971年1月，Intel公司的霍夫工程师研制成功世界上第1个字长为4位的微处理器芯片Intel 4004，标志着第1代微处理器问世，微型计算机时代从此开始 [5]。该阶段是4位和8位低档微处理器时代，通常称为第1代，其典型产品是Intel 4004和Intel 8008微处理器和分别由它们组成的MCS-4和MCS-8微机。该阶段产品的基本特点是采用PMOS工艺，集成度低，系统结构和指令系统都比较简单，主要采用机器语言或简单的汇编语言，指令数目较少，多用于家电和简单控制场合 [5]。2、8位中高档微处理器时代（1974~1978年）该阶段通常称为第2代，典型产品有Intel公司的Intel 8080/8085、Motorola公司的MC6800及美国Zilog公司的Z80等，以及各种8位单片机，如Inte公司的8048、Motorola公司的MC6801、Zilog公司的Z8等。该阶段产品的基本特点是采用NMOS工艺，集成度提高约4倍，运算速度提高约10~15倍，指令系统比较完善，具有典型的计算机体系结构和中断、DMA等控制功能。软件方面除了汇编语言外，还有BASIC、FORTRAN等高级语言和相应的解释程序和编译程序，在后期还出现了操作系统，如CP/M就是当时流行的操作系统 [5]。3、16位微处理器时代（1978~1984年）该阶段通常称为第3代。1978年6月，Intel公司推出主频为4.77MHz的字长16位的微处理器芯片Intel 8086。8086微处理器的诞生标志着第3代微处理器问世。该阶段的典型产品包括Intel公司的8086/8088、80286，Motorola公司的M68000，Zilog公司的Z8000等微处理器。其特点是采用HMOS工艺，集成度和运算速度都比第2代提高了一个数量级。指令系统更加丰富、完善，采用多级中断、多种寻址方式、段式存储结构、硬件乘除部件，并配置了软件系统 [5]。这一时期的著名微机产品有IBM公司的个人计算机PC。1981年推出的IBM-PC机采用8088 CPU。紧接着1982年又推出了扩展型的个人计算机IBM-PC/XT，它对内存进行了扩充，并增加了一个硬盘驱动器。1984年IBM推出了以80286处理器为核心组成的16位增强型个人计算机IBM-PC/AT。由于IBM公司在发展PC机时采用了技术开放的策略，使PC机风靡世界 [5]。4、32位微处理器时代（1985~1992年）该阶段通常称为第4代。1985年10月，Intel公司推出了80386DX微处理器，标志着进入了字长为32位的数据总线时代。该阶段典型产品包括Intel公司的80386/80486，Motorola公司的M68030/68040等。其特点是采用HMOS或CMOS工艺，集成度高达100万晶体管/片，具有32位地址线和32位数据总线。每秒钟可完成600万条指令。微机的功能已经达到甚至超过超级小型计算机，完全可以胜任多任务、多用户的作业。同期，其他一些微处理器生产厂商(如AMD、TEXAS等)也推出了80386/80486系列的芯片 [5]。5、Pentium系列微处理器时代（1993年以后）该阶段通常称为第5代。典型产品是Intel公司的奔腾系列芯片及与之兼容的AMD的K6系列微处理器芯片。该阶段产品内部采用了超标量指令流水线结构，并具有相互独立的指令和数据高速缓存。随着MMX（Multi media eXtended）微处理器的出现，使微机的发展在网络化、多媒体化和智能化等方面跨上了更高的台阶。2000年3月，AMD与Intel分别推出了时钟频率达1GHz的Athlon和Pentium Ⅲ。2000年11月，Intel又推出了Pentium Ⅳ微处理器，集成度高达每片4200万个晶体管，主频1.5GHz，400MHz的前端总线，使用全新SSE2指令集。2002年11月，Intel推出的Pentium Ⅳ微处理器的时钟频率达到3.06GHz，而且微处理器还在不断地发展，性能也在不断提升 [5]。2001年Intel公司发布第一款64位的产品Itanium（安腾）微处理器。2003年4月，AMD公司推出了基于64位运算的Opteron（皓龙）微处理器。2003年9月，AMD公司的Athlon（速龙）微处理器问世，标志着64位计算时代的到来 [5]。技术指标播报编辑计算机的性能指标涉及体系结构、软硬件配置、指令系统等多种因素，一般说来主要有下列技术指标 [6]。字长字长是指计算机运算部件一次能同时处理的二进制数据的位数。字长越长，如果用作存储数据，则计算机的运算精度就越高；如果用作存储指令，则计算机的处理能力就越强。通常字长总是8的整数倍，如8、16、32、64位等。Intel 486机和Pentium 4机均属32位机 [6]。时钟主频时钟主频是指CPU的时钟频率，它的高低在一定程度上决定了计算机速度的高低。主频以兆赫兹（MHz）为单位，一般来说，主频越高，速度就越快。由于微处理器发展迅速，微机的主频也在不断提高 [6]。运算速度计算机的运算速度通常是指每秒钟所能执行指令的数目，常用百万次/秒（MIPS，MillionInstructions Per Second）来表示。这个指标更能直观地反映机器的速度 [6]。存储容量存储容量通常分内存容量和外存容量，这里主要指内存储器的容量。显然，内存容量越大，机器所能运行的程序就越大，处理能力就越强。尤其是当前多媒体PC应用多涉及图像信息处理，要求存储容量越来越大，甚至没有足够大的内存容量就无法运行某些软件。大多数微机的内存容量已达到4GB [6]。存取周期内存储器的存取周期也是影响整个计算机系统性能的主要指标之一。简单地讲，存取周期就是CPU从内存储器中存取数据所需的时间。内存的存取周期在7-70ns之间 [7]。此外，计算机的可靠性、可维护性、平均无故障时间和性能价格比也都是计算机的技术指标 [7]。

系统总线：
总线简介播报编辑系统总线上传送的信息包括数据信息、地址信息、控制信息，因此，系统总线包含有三种不同功能的总线，即数据总线DB（Data Bus）、地址总线AB（Address Bus）和控制总线CB（Control Bus）。系统总线数据总线DB用于传送数据信息。数据总线是双向三态形式（双向是指可以两个方向传输，可以A->B也可以A<-B；三态指 0，1和第三态（tri-state）。tri-state既不是一也不是零，三态门的闭合无输出高阻状态。）的总线，即他既可以把CPU的数据传送到存储器或I/O接口等其它部件，也可以将其它部件的数据传送到CPU。数据总线的位数是微型计算机的一个重要指标，通常与微处理的字长相一致。例如Intel 8086微处理器字长16位，其数据总线宽度也是16位。需要指出的是，数据的含义是广义的，它可以是真正的数据，也可以指令代码或状态信息，有时甚至是一个控制信息，因此，在实际工作中，数据总线上传送的并不一定仅仅是真正意义上的数据。地址总线AB是专门用来传送地址的，由于地址只能从CPU传向外部存储器或I/O端口，所以地址总线总是单向三态的，这与数据总线不同。地址总线的位数决定了CPU可直接寻址的内存空间大小，比如8位微机的地址总线为16位，则其最大可寻址空间为2^16=64KB，16位微型机的地址总线为20位，其可寻址空间为2^20=1MB。一般来说，若地址总线为n位，则可寻址空间为2^n（2的n次方）个地址空间（存储单元）。 举例来说：一个16位元宽度的位址总线（通常在1970年和1980年早期的8位元处理器中使用）可以寻址的内存空间为 2 的 16 次方=65536=64 KB的地址，而一个 32位元 位址总线（通常在像现今 2004年 的 PC 处理器中） 可以寻址的内存空间为4,294,967,296=4GB（前提：数据总线的宽度是8位）的位址。注释：位元=bit。上面提到的2^n=X=YGB中的B其实是bit，这个结果其实是乘以可寻址的位元8bit之后得到的。控制总线CB用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和I/O接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的，比如：中断申请信号、复位信号、总线请求信号、限备就绪信号等。因此，控制总线的传送方向由具体控制信号而定，一般是双向的，控制总线的位数要根据系统的实际控制需要而定。实际上控制总线的具体情况主要取决于CPU。工作原理播报编辑系统总线系统总线在微型计算机中的地位，如同人的神经中枢系统，CPU通过系统总线对存储器的内容进行读写，同样通过总线，实现将CPU内数据写入外设，或由外设读入CPU。微型计算机都采用总线结构。总线就是用来传送信息的一组通信线。微型计算机通过系统总线将各部件连接到一起，实现了微型计算机内部各部件间的信息交换。一般情况下，CPU提供的信号需经过总线形成电路形成系统总线。系统总线按照传递信息的功能来分，分为地址总线、数据总线和控制总线。这些总线提供了微处理器(CPU)与存储器、输入输出接口部件的连接线。可以认为，一台微型计算机就是以CPU为核心，其它部件全“挂接”在与CPU相连接的系统总线上。这种总线结构形式，为组成微型计算机提供了方便。人们可以根据自己的需要，将规模不一的内存和接口接到系统总线上，很容易形成各种规模的微型计算机。微型计算机实质上就是把CPU、存储器和输入/输出接口电路正确的连接到系统总线上，而计算机应用系统的硬件设计本质上是外部设备同系统总线之间的总线接口电路设计问题，这种总线结构设计是计算机硬件系统的一个特点。常用总线播报编辑ISA总线----ISA（industrial standard architecture）总线标准是IBM 公司1984年为推出PC/AT机而建立的系统总线标准，所以也叫AT总线。它是对XT总线的扩展，以适应8/16位数据总线要求。它在80286至80486时代应用非常广泛，以至于奔腾机中还保留有ISA总线插槽。ISA总线有98只引脚。EISA总线----EISA总线是1988年由Compaq等9家公司联合推出的总线标准。它是在ISA总线的基础上使用双层插座，在原来ISA总线的98条信号线上又增加了98条信号线，也就是在两条ISA信号线之间添加一条EISA信号线。在实用中，EISA总线完全兼容ISA总线信号。VESA总线系统总线----VESA（video electronics standard association）总线是 1992年由60家附件卡制造商联合推出的一种局部总线，简称为VL(VESA local bus)总线。它的推出为微机系统总线体系结构的革新奠定了基础。该总线系统考虑到CPU与主存和Cache 的直接相连，通常把这部分总线称为CPU总线或主总线，其他设备通过VL总线与CPU总线相连，所以VL总线被称为局部总线。它定义了32位数据线，且可通过扩展槽扩展到64 位，使用33MHz时钟频率，最大传输率达132MB/s，可与CPU同步工作。是一种高速、高效的局部总线，可支持386SX、386DX、486SX、486DX及奔腾微处理器。PCI总线----PCI（peripheral component interconnect）总线是当前最流行的总线之一，它是由Intel公司推出的一种局部总线。它定义了32位数据总线，且可扩展为64位。PCI总线主板插槽的体积比原ISA总线插槽还小，其功能比VESA、ISA有极大的改善，支持突发读写操作，最大传输速率可达132MB/s，可同时支持多组外围设备。 PCI局部总线不能兼容现有的ISA、EISA、MCA（micro channel architecture）总线，但它不受制于处理器，是基于奔腾等新一代微处理器而发展的总线。Compact PCI----以上所列举的几种系统总线一般都用于商用PC机中，在计算机系统总线中，还有另一大类为适应工业现场环境而设计的系统总线，比如STD总线、VME总线、PC/104总线等。这里仅介绍当前工业计算机的热门总线之一——Compact PCI。系统总线----Compact PCI的意思是“坚实的PCI”，是当今第一个采用无源总线底板结构的PCI系统，是PCI总线的电气和软件标准加欧式卡的工业组装标准，是当今最新的一种工业计算机标准。Compact PCI是在原来PCI总线基础上改造而来，它利用PCI的优点，提供满足工业环境应用要求的高性能核心系统，同时还考虑充分利用传统的总线产品，如ISA、STD、VME或PC/104来扩充系统的I/O和其他功能。----6.PCI-E总线----PCI Express采用的也是业内流行这种点对点串行连接，比起PCI以及更早期的计算机总线的共享并行架构，每个设备都有自己的专用连接，不需要向整个总线请求带宽，而且可以把数据传输率提高到一个很高的频率，达到PCI所不能提供的高带宽。相对于传统PCI总线在单一时间周期内只能实现单向传输，PCI Express的双单工连接能提供更高的传输速率和质量，它们之间的差异跟半双工和全双工类似。技术规范播报编辑系统总线是一类信号线的集合是模块间传输信息的公共通道，通过它，计算机各部件间可进行各种数据和命令的传送。为使不同供应商的产品间能够互换，给用户更多的选择，总线的技术规范要标准化。总线的标准制定要经周密考虑，要有严格的规定。系统总线标准（技术规范）包括：（1）机械结构规范：模块尺寸、总线插头、总线接插件以及安装尺寸均有统一规定。（2）功能规范：总线每条信号线（引脚的名称）、功能以及工作过程要有统一规定。（3）电气规范：总线每条信号线的有效电平、动态转换时间、负载能力等。技术指标播报编辑系统总线发展1、系统总线的带宽（总线数据传输速率）系统总线的带宽指的是单位时间内总线上传送的数据量，即每钞钟传送MB的最大稳态数据传输率。与总线密切相关的两个因素是总线的位宽和总线的工作频率，它们之间的关系：总线的带宽=总线的工作频率*总线的位宽/82、系统总线的位宽系统总线的位宽指的是总线能同时传送的二进制数据的位数，或数据总线的位数，即32位、64位等总线宽度的概念。总线的位宽越宽，每秒钟数据传输率越大，总线的带宽越宽。3、系统总线的工作频率总线的工作时钟频率以MHZ为单位，工作频率越高，总线工作速度越快，总线带宽越宽。 [1]发展历程播报编辑计算机系统总线的详细发展历程，包括早期的PC总线和ISA总线、PCI/AGP总线、PCI-X总线以及主流的PCIExpress、HyperTransport高速串行总线。从PC总线到ISA、PCI总线，再由PCI进入PCIExpress和HyperTransport体系，计算机在这三次大转折中也完成三次飞跃式的提升。与这个过程相对应，计算机的处理速度、实现的功能和软件平台都在进行同样的进化，显然，没有总线技术的进步作为基础，计算机的快速发展就无从谈起。业界站在一个崭新的起点：PCIExpress和HyperTransport开创了一个近乎完美的总线架构。而业界对高速总线的渴求也是无休无止，PCIExpress4.0和HyperTransport4.0都已提上日程，它们将会再次带来效能提升。在计算机系统中，各个功能部件都是通过系统总线交换数据，总线的速度对系统性能有着极大的影响。而也正因为如此，总线被誉为是计算机系统的神经中枢。但相比CPU、显卡、内存、硬盘等功能部件，总线技术的提升步伐要缓慢得多。在PC发展的二十余年历史中，总线只进行三次更新换代，但它的每次变革都令计算机的面貌焕然一新。 [1]

总线复用：
产品介绍播报编辑如8051单片机，地址空间是16bit，数据宽度8bit，而低位地址总线8bit是与数据8bit复用的。所以总共还是16根线。数据和地址的分离是通过外部电路来完成的，一般都用273锁存器，对地址信息进行锁存，通过ALE信号来控制。然后再传输数据信息。总线复用传送播报编辑所谓复用传送就是指多个用户共享公用信道的一种机制，最常见的主要有时分多路复用、频分多路复用和码分多路复用等,优点在于:为了各子系统的信息能有效及时的被传送，并且减少总线中信号线的数量，为了不至于彼此间的信号相互干扰和避免物理空间上过于拥挤复用传送又分三种(以下为介绍)时分多路复用（TDMA）时分复用是将信道按时间加以分割成多个时间段，不同来源的信号会要求在不同的时间段内得到响应，彼此信号的传输时间在时间坐标轴上是不会重叠。频分多路复用（FDMA）频分复用就是把信道的可用频带划分成若干互不交叠的频段，每路信号经过频率调制后的频谱占用其中的一个频段，以此来实现多路不同频率的信号在同一信道中传输。而当接收端接收到信号后将采用适当的带通滤波器和频率解调器等来恢复原来的信号。码分多路复用（CDMA）码分多路复用是所被传输的信号都会有各自特定的标识码或地址码，接收端将会根据不同的标识码或地址码来区分公共信道上的传输信息，只有标识码或地址码完全一致的情况下传输信息才会被接收。技术应用播报编辑IEEE以太网标准-Ethernet以太网（Ethernet）是一种计算机局域网组网技术。IEEE制定的IEEE 802.3标准给出了以太网的技术标准。它规定了包括物理层的连线、电信号和介质访问层协议的内容。以太网是当前应用最普遍的局域网技术。以太网的标准拓扑结构为总线型拓扑，但快速以太网（100BASE-T、1000BASE-T标准）为了最大程度的减少冲突，最大程度的提高网络速度和使用效率，使用交换机（Switch hub）来进行网络连接和组织，这样，以太网的拓扑结构就成了星型，但在逻辑上，以太网仍然使用总线型拓扑和CSMA/CD（Carrier Sense Multiple Access/Collision Detect 即带冲突检测的载波监听多路访问）的总线复用技术。

数据寄存器：
定义播报编辑内存资料寄存器是指用于存放欲写入存储体中的数据，或暂存从存储体中读出的数据，准备让处理器处理的寄存器，即用来暂时存放处理器计算过程中所用到的操作数、结果和信息。数据寄存器用来暂时存放由主存储器读出的一条指令或一个数据字；反之，当向主存存入一条指令或一个数据字时，也将它们暂时存放在数据寄存器中。在单累加器结构的运算器中，数据寄存器还可兼作操作数寄存器 [1]。数据寄存器访问速度最快，完全能与 CPU 协调工作。类别播报编辑AX、BX、CX、DX可以称为数据寄存器，这4个16位寄存器又可分别分成高8位（AH、BH、CH、DH）和低8位（AL、BL、CL、DL）。因此它们既可作为4个16位数据寄存器使用，也可作为8个8位数据寄存器使用，在编程时可存放源操作数、目的操作数或运算结果。数据寄存器是存放操作数、运算结果和运算的中间结果，以减少访问存储器的次数，或者存放从存储器读取的数据以及写入存储器的数据的寄存器。AX（accumulator）累加器。作为累加器使用。是算术运算的主要寄存器。在乘、除等指令中指定用来存放操作数。以及所有的 I/O指令都使用这一寄存器与外部设备传送信息。BX（base）基址。可以作为通用寄存器使用。此外在计算机存储地址时，它经常用作基址寄存器。CX（count）计数。可以作为通用寄存器使用。常用来保存计数值，如在循环、位移和串处理指令中作隐含计数器。DX（data）数据。可以作为通用寄存器使用。一般在作双字长运算时把DX和AX组合在一起存放一个双字长数，DX用来存放高位数。对于某些I/O操作，DX可用来存放I/O的端口地址。寄存器播报编辑寄存器，是集成电路中非常重要的一种存储单元，通常由触发器组成。在集成电路设计中，寄存器可分为电路内部使用的寄存器和充当内外部接口的寄存器这两类。内部寄存器不能被外部电路或软件访问，只是为内部电路的实现存储功能或满足电路的时序要求。而接口寄存器可以同时被内部电路和外部电路或软件访问，CPU中的寄存器就是其中一种，作为软硬件的接口，为广泛的通用编程用户所熟知。CPU中至少要有六类寄存器：指令寄存器（IR）、程序计数器（PC）、地址寄存器（AR）、数据寄存器（DR）、累加寄存器（AC）、程序状态字寄存器（PSW）。这些寄存器用来暂存一个计算机字，其数目可以根据需要进行扩充。指令寄存器指令寄存器（Instruction Register，IR）用来保存当前正在执行的一条指令。当执行一条指令时，首先把该指令从主存读取到数据寄存器中，然后再传送至指令寄存器。指令包括操作码和地址码两个字段，为了执行指令，必须对操作码进行测试，识别出所要求的操作，指令译码器（Instruction Decoder，ID）就是完成这项工作的。指令译码器对指令寄存器的操作码部分进行译码，以产生指令所要求操作的控制电位，并将其送到微操作控制线路上，在时序部件定时信号的作用下，产生具体的操作控制信号。指令寄存器中操作码字段的输出就是指令译码器的输入。操作码一经译码，即可向操作控制器发出具体操作的特定信号。程序计数器程序计数器（Program Counter，PC）用来指出下一条指令在主存储器中的地址。在程序执行之前，首先必须将程序的首地址，即程序第一条指令所在主存单元的地址送入PC，因此PC的内容即是从主存提取的第一条指令的地址。当执行指令时，CPU能自动递增PC的内容，使其始终保存将要执行的下一条指令的主存地址，为取下一条指令做好准备。若为单字长指令，则(PC)+1àPC，若为双字长指令，则(PC)+2àPC，以此类推。但是，当遇到转移指令时，下一条指令的地址将由转移指令的地址码字段来指定，而不是像通常的那样通过顺序递增PC的内容来取得。因此，程序计数器的结构应当是具有寄存信息和计数两种功能的结构。地址寄存器地址寄存器（Address Register，AR）用来保存CPU当前所访问的主存单元的地址。由于在主存和CPU之间存在操作速度上的差异，所以必须使用地址寄存器来暂时保存主存的地址信息，直到主存的存取操作完成为止。当CPU和主存进行信息交换，即CPU向主存存入数据/指令或者从主存读出数据/指令时，都要使用地址寄存器和数据寄存器。如果我们把外围设备与主存单元进行统一编址，那么，当CPU和外围设备交换信息时，我们同样要使用地址寄存器和数据寄存器。累加寄存器累加寄存器通常简称累加器（Accumulator，AC），是一个通用寄存器。累加器的功能是：当运算器的算术逻辑单元ALU执行算术或逻辑运算时，为ALU提供一个工作区，可以为ALU暂时保存一个操作数或运算结果。显然，运算器中至少要有一个累加寄存器。程序状态字寄存器程序状态字（Program Status Word，PSW）用来表征当前运算的状态及程序的工作方式。程序状态字寄存器用来保存由算术/逻辑指令运行或测试的结果所建立起来的各种条件码内容，如运算结果进/借位标志（C）、运算结果溢出标志（O）、运算结果为零标志（Z）、运算结果为负标志（N）、运算结果符号标志（S）等，这些标志位通常用1位触发器来保存。除此之外，程序状态字寄存器还用来保存中断和系统工作状态等信息，以便CPU和系统及时了解机器运行状态和程序运行状态。因此，程序状态字寄存器是一个保存各种状态条件标志的寄存器。主存储器播报编辑主存储器(简称内存或主存)是计算机系统中一个主要部件， 用于保存进程运行时的程序和数据，也称可执行存储器，其容量对于当前的微机系统和大中型机，可能一般为数十 MB到数 GB，而且容量还在不断增加，而嵌入式计算机系统一般仅有几十 KB 到几 MB。CPU的控制部件只能从主存储器中取得指令和数据，数据能够从主存储器读取并将它们装入到寄存器中，或者从寄存器存入到主存储器。CPU 与外围设备交换的信息一般也依托于主存储器地址空间。由于主存储器的访问速度远低于 CPU 执行指令的速度，为缓和这一矛盾，在计算机系统中引入了寄存器和高速缓存。

数据总线：
共享与交换播报编辑数据总线(DataBus)。规范了一个大的集成应用系统中同构系统、异构系统等方面进行数据共享和交换实现方法，系统间数据交换标准。可用于微处理与内存，微处理器与输入输出接口之间传送信息。数据总线的宽度是决定计算机性能的一个重要指标。微型计算机的数据总线大多是32位或64位。1.业务实体数据交换：各个子系统在架构分层上都有业务实体层，数据交换机制在业务实体层建立了一层对所有应用系统透明的层。子系统之间，无论其实现的具体技术方案是什么，都可通过业务实体层进行共享和交互，这也就建立了可在子系统间进行持续集成和业务扩展的结构，从而实现一个可扩展的完整的一体化信息系统。 [1]2.WebService数据交换：是一种Web服务标准，Web服务提供在异构系统间共享和交换数据的方案，也可用于在产品集成中使用统一的接口标准进行数据共享和交换。交换方式播报编辑原理图1.业务实体层的数据交换，这是同构子系统系统间最直接和最高效的交换方案。在同构子系统间通过定义数据对象接口层，通过DTO进行传输，或者直接在数据库中进行数据表的连接或访问，达到同构子系统间的数据共享和交换。例如征管系统内各个子系统间的数据共享和交换、业务系统和数据挖掘间的数据共享。2.WebService数据交换，在异构子系统间，同时存在数据不集中的情况下，必须使用有效的技术手段来保证异构的数据共享和交换。WebService是基于Web的标准服务，其不受传输协议或硬件的限制，也不受子系统具体实现技术的限制。而且较先进完备的应用系统或产品都提供了基于WebService的集成接口。这就解决了异构子系统间的数据共享和交换。WebService也可以解决跨网络和行业系统的数据交换，这需要对方接口单位同样具备WebService服务。3.格式化文件数据交换，它是与外部系统文件传输，业务上的内部系统和外部信息交换需求，要求提供相应的数据共享和交换技术机制。这类问题通常使用基于文件系统的技术方案解决，例如文件报送、文件交换等。可举例说明：税、库、银三者之间就存在实时和非实时的数据交换，这种交换优化的方案就是使用文件通过Socket进行交换。此类技术实现一般采用底层技术。连接器播报编辑连接器数据总线连接器(DBconnector)是一种用于连接串行和平行电缆到数据总线的连接器。数据总线连接器命名格式是DB-x，x代表连接器内电线的数量。每条线被连接到在连接器中的一个栓上，但是在很多情况下，不是所有的栓都被分配一个功能。数据总线连接器被各种EIA/TIA标准定义。是一类用于连接序列和平行电缆到一个数据总线的连接器。DB连接器在DB-x的格式下命名，其中x代表连接器内（线）的数量。每条线与连接器中的销钉相连，但是在许多情况下，不是所有的销钉都被分配功能。DB连接器有9,15,25,37和50销钉尺寸。DB连接器定义了连接器物理结构，而不是每条线的用途。例如，DB-9有9个销钉和被用于连接一个鼠标。DB-25有25个销钉和被用于连接一台打印机。技术指标播报编辑程序总线1、总线的带宽（总线数据传输速率） 总线的带宽指的是单位时间内总线上传送的数据量，即每钞钟传送MB的最大稳态数据传输率。与总线密切相关的两个因素是总线的位宽和总线的工作频率，它们之间的关系：总线的带宽=总线的工作频率*总线的位宽/82、总线的位宽总线的位宽指的是总线能同时传送的二进制数据的位数，或数据总线的位数，即32位、64位等总线宽度的概念。总线的位宽越宽，每秒钟数据传输率越大，总线的带宽越宽。3、总线的工作频率总线的工作时钟频率以MHZ为单位，工作频率越高，总线工作速度越快，总线带宽越宽。操作播报编辑模型总线一个操作过程是完成两个模块之间传送信息，启动操作过程的是主模块，另外一个是从模块。某一时刻总线上只能有一个主模块占用总线。总线的操作步骤：主模块申请总线控制权，总线控制器进行裁决。数据传送的错误检查：主模块得到总线控制权后寻址从模块，从模块确认后进行数据传送。总线定时协议：定时协议可保证数据传输的双方操作同步，传输正确。定时协议有三种类型：同步总线定时：总线上的所有模块共用同一时钟脉冲进行操作过程的控制。各模块的所有动作的产生均在时钟周期的开始，多数动作在一个时钟周期中完成。异步总线定时：操作的发生由源或目的模块的特定信号来确定。总线上一个事件发生取决前一事件的发生，双方相互提供联络信号。总线定时协议半同步总线定时：总线上各操作的时间间隔可以不同，但必须是时钟周期的整数倍，信号的出现,采样与结束仍以公共时钟为基准。ISA总线采用此定时方法。数据传输类型：分单周方式和突发(burst)方式。单周期方式：一个总线周期只传送一个数据。突发方式：取得主线控制权后进行多个数据的传输。寻址时给出目的地首地址，访问第一个数据，数据2、3到数据n的地址在首地址基础上按一定规则自动寻址（如自动加1）。标准规范播报编辑标准系统结构总线总线是一类信号线的集合是模块间传输信息的公共通道，通过它，计算机各部件间可进行各种数据和命令的传送。为使不同供应商的产品间能够互换，给用户更多的选择，总线的技术规范要标准化。 [2]总线的标准制定要经周密考虑，要有严格的规定。总线标准（技术规范）包括以下几部分：机械结构规范：模块尺寸、总线插头、总线接插件以及安装尺寸均有统一规定。功能规范：总线每条信号线（引脚的名称）、功能以及工作过程要有统一规定。电气规范：总线每条信号线的有效电平、动态转换时间、负载能力等。

指令寄存器：
属性描述播报编辑BSDL语言中有一个重要的描述，即指令寄存器（Instruclion Register），它是由一些强制的、可选的和用户自定义的指令集合而成。关于这个指令寄存器的属性描述，必须包含5个要素：指令寄存器的长度、各种指令的名称、对应的操作码、指令寄存器的捕获操作码以及哪些指令是内部的 [4]。取指过程播报编辑取指令阶段完成的任务是将现行指令从主存中取出来并送至指令寄存器中，具体的操作如下： [5]1、将程序计数器（PC）中的内容送至存储器地址寄存器（MAR），并送地址总线（AB） [5]。2、由控制单元（CU）经控制总线（CB）向存储器发读命令 [5]。3、从主存中取出的指令通过数据总线（DB）送到存储器数据寄存器（MDR） [5]。4、将MDR的内容送至指令寄存器（R）中 [5]。5、将PC的内容递增，为取下一条指令做好准备 [5]。以上这些操作对任何一条指令来说都是必须要执行的操作，所以称为公共操作 [5]。原理播报编辑指令寄存器可以在移入一条新的指令的同时，将当前指令保持在它的输出端口。可用这个寄存器来指定所要执行的操作和选择测试数据寄存器。当TAP接收到一条指令寄存器扫描指令时，对指令寄存器进行读取。在指令寄存器工作过程中，来自TAP的控制信号选择指令寄存器的输出驱动TDO管脚 [6]。指令寄存器的功能由三部分构成：扫描移位寄存器、保持寄存器与译码逻辑。扫描移位寄存器从TDI端扫描移入当前指令代码；保持寄存器对当前指令代码进行保持；译码逻辑根据当前指令代码，产生相应的数据寄存器控制信号。三部分的运行控制信号均来自TAP控制器 [6]。概念区分播报编辑IP是指令指针寄存器（Instruction Pointer），它用来存放待要取出指令的地址偏移量。它只有与CS寄存器相结合，才能形成指向指令的真正物理地址 [7]。

操作系统：
系统简介播报编辑在计算机中，操作系统是其最基本也是最为重要的基础性系统软件。从计算机用户的角度来说，计算机操作系统体现为其提供的各项服务；从程序员的角度来说，其主要是指用户登录的界面或者接口；如果从设计人员的角度来说，就是指各式各样模块和单元之间的联系。事实上，全新操作系统的设计和改良的关键工作就是对体系结构的设计，经过几十年以来的发展，计算机操作系统已经由一开始的简单控制循环体发展成为较为复杂的分布式操作系统，再加上计算机用户需求的愈发多样化，计算机操作系统已经成为既复杂而又庞大的计算机软件系统之一。 [1]发展历史播报编辑纵观计算机之历史，操作系统与计算机硬件的发展息息相关。操作系统之本意原为提供简单的工作排序能力，后为辅助更新更复杂的硬件设施而渐渐演化。从最早的批量模式开始，分时机制也随之出现，在多处理器时代来临时，操作系统也随之添加多处理器协调功能，甚至是分布式系统的协调功能。其他方面的演变也类似于此。另一方面，个人计算机之操作系统因袭大型机的成长之路，在硬件越来越复杂、强大时，也逐步实现以往只有大型机才有的功能。从1946年诞生第一台电子计算机以来，它的每一代进化都以减少成本、缩小体积、降低功耗、增大容量和提高性能为目标，随着计算机硬件的发展，同时也加速了操作系统(简称OS)的形成和发展。 [2]最初的电脑没有操作系统，人们通过各种按钮来控制计算机，后来出现了汇编语言，操作人员通过有孔的纸带将程序输入电脑进行编译。这些将语言内置的电脑只能由制作人员自己编写程序来运行，不利于程序、设备的共用。为了解决这种问题，就出现了操作系统，这样就很好实现了程序的共用，以及对计算机硬件资源的管理。 [2]随着计算技术和大规模集成电路的发展，微型计算机迅速发展起来。从20世纪70年代中期开始出现了计算机操作系统。在美国1976年的时候就研制了DIGITAL RESEARCH软件公司出8位的CP/M操作系统。这个系统允许用户通过控制台的键盘对系统进行控制和管理，其主要功能是对文件信息进行管理，以实现其他设备文件或硬盘文件的自动存取。此后出现的一些8位操作系统多采用CP/M结构。 [2]2019年8月9日于东莞举行的华为开发者大会（HDC.2019）上正式发布的分布式操作系统。华为鸿蒙系统是一款全新的面向全场景的分布式操作系统，创造一个超级虚拟终端互联的世界，将人、设备、场景有机地联系在一起，将消费者在全场景生活中接触的多种智能终端，实现极速发现、极速连接、硬件互助、资源共享，用合适的设备提供场景体验。 [16]2023年7月5日，“开放麒麟”操作系统 1.0 版本正式发布。 [13]主要功能播报编辑计算的操作系统对于计算机可以说是十分重要的，从使用者角度来说，操作系统可以对计算机系统的各项资源板块开展调度工作，其中包括软硬件设备、数据信息等，运用计算机操作系统可以减少人工资源分配的工作强度，使用者对于计算的操作干预程度减少，计算机的智能化工作效率就可以得到很大的提升。其次在资源管理方面，如果由多个用户共同来管理一个计算机系统，那么可能就会有冲突矛盾存在于两个使用者的信息共享当中。为了更加合理的分配计算机的各个资源板块，协调计算机系统的各个组成部分，就需要充分发挥计算机操作系统的职能，对各个资源板块的使用效率和使用程度进行一个最优的调整，使得各个用户的需求都能够得到满足。最后，操作系统在计算机程序的辅助下，可以抽象处理计算系统资源提供的各项基础职能，以可视化的手段来向使用者展示操作系统功能，减低计算机的使用难度。 [3]操作系统主要包括以下几个方面的功能 ：①进程管理，其工作主要是进程调度，在单用户单任务的情况下，处理器仅为一个用户的一个任务所独占， 进程管理的工作十分简单。但在多道程序或多用户的情况 下，组织多个作业或任务时，就要解决处理器的调度、 分配和回收等问题 。②存储管理分为几种功能：存储分配、存储共享、存储保护 、存储扩张。③设备管理分有以下功能：设备分配、设备传输控制 、设备独立性。④文件管理：文件存储空间的管理、目录管理 、文件操作管理、文件保护。⑤作业管理是负责处理用户提交的任何要求。 [2]用途分类播报编辑计算机的操作系统根据不同的用途分为不同的种类，从功能角度分析，分别有实时系统、批处理系统、分时系统、网络操作系统等。 [4]实时系统主要是指系统可以快速的对外部命令进行响应，在对应的时间里处理问题，协调系统工作。 [4]批处理系统出现于20世纪60年代，批处理系统能够提高资源的利用率和系统的吞吐量。 [5]分时系统可以实现用户的人机交互需要，多个用户共同使用一个主机，很大程度上节约了资源成本。 分时系统具有多路性、独立性、交互性、及时性的优点，能够将用户-系统-终端任务实现。 [4]网络操作系统是一种能代替操作系统的软件程序，是网络的心脏和灵魂，是向网络计算机提供服务的特殊的操作系统。借由网络达到互相传递数据与各种消息，分为服务器及客户端。而服务器的主要功能是管理服务器和网络上的各种资源和网络设备的共用，加以统合并控管流量，避免有瘫痪的可能性，而客户端就是有着能接收服务器所传递的数据来运用的功能，好让客户端可以清楚的搜索所需的资源。体系结构播报编辑简单体系结构计算机操作系统诞生初期，其体系结构就属于简单体系结构，由于当时各式各样影响因素的作用，如硬件性能、平台、软件水平等方面的限制，使得当时的计算机操作系统结构呈现出一种混乱且结构模糊的状态，其操作系统的用户应用程序和其内核程序鱼龙混杂，甚至其运行的地址和空间都是一致的。这种操作系统实际上就是一系列过程和项目的简单组合，使用的模块方法也相对较为粗糙，因此导致其结构宏观上非常模糊。 [1]单体内核结构随着科学技术的不断发展和进步，硬件及其平台的水平和性能得到了很大程度的提高，其数量和种类也与日俱增，操作系统的复杂性也逐渐加深，其具备的功能以及性能越来越多，在此背景下，单体内核结构的操作系统诞生并得到了应用，例如UNIX操作系统、windows NT/XP等。一般情况下，单体内核结构的操作系统主要具备以下几种功能，分别是文件及内存管理、设备驱动、CPU调度以及网络协议处理等。由于内核的复杂性不断加深，相关的开发设计人员为了实现对其良好的控制，逐渐开始使用了一些较为成熟的模块化方法，并根据其不同的功能将其进行结构化，进而将其划分为诸多的模块，例如文件及内存管理模块、驱动模块、CPU调度模块及网络协议处理等。这些模块所使用的地址和空间与内核使用的完全一致，其以函数调用的方式构建了用于通讯的结构来实现各个模块之间的通讯。在使用模块化的方法以后，只要其通讯接口没有发生明显的变化，即使整个结构中的任何一个模块发生变化也不会对结构中的其他模块造成任何的影响，为其系统的维护和改良扩充提供了便利。虽然单体内核结构的计算机操作系统经过了模块化的处理，但是其中的全部模块仍然是在硬件之上、应用软件之下的操作系统核心中运转和工作。模块与模块之间活动的层次没有任何的差别。 [1]层次式结构层次式结构的计算机操作系统是为了减少以往操作系统中各个模块之间由于联系紧密而带来的各种问题而诞生的，其可以最大程度的减少甚至是避免循环调用现象的发生，确保调用有序，为操作系统设计目标的实现奠定了坚实的基础。在层次式结构的计算机操作系统之中，其是由诸多系统分为若干个层次的，其最底层是硬件技术，其他每一个层级均是建立在其下一层级之上的。在设计其计算机操作系统内核时，主要采用与抽象数据类型十分类似的设计方法进行的，在系统中的每一个层级均包含着多种数据和操作，且每一个的数据和操作是其他层不可见的，在每一层当中都配备了用于其他层使用的一操作接口，同时每一层发生的访问行为只能针对其下层进行，不能访问其上层的数据和服务，严格遵守了调用规则，在很大程度上避免了其他层次对某一层次的干扰和破坏。对于理想的层次式计算机系统体系结构来说，其之间的联系不仅仅是单向依赖性的，同时各个层级之间也要具备相互的独立性，且只能对低层次的模块和功能进行调用，例如THE系统。但是这种理想的全序层次式计算机操作系统在现实中建成是较为困难的，其无法完全避免模块之间循环调用现象的出现，某个层级之间仍旧存在某种循环关系，这种层次式结构又被叫作半序层次式计算机操作系统，例如SUE操作系统。 [1]微内核结构微内核计算机操作系统体系结构又可以被叫作客户机结构或者服务器结构，其实际上就是一种将系统中的代码转移到更高层次当中，尽可能地减少操作系统中的东西，仅仅保留一个小体积的内核，一般情况下其使用的主要方法就是通过用户进程来实现操作系统所具备的各项功能，具体来说就是用户进程可以将相关的请求和要求发送到服务器当中，然后由服务器完成相关的操作以后在通过某种渠道反馈到用户进程当中。在微内核结构中，操作系统的内核主要工作就是对客户端和服务器之间的通信进行处理，在系统中包括许多部分，每一个部分均具备某一方面的功能，例如文件服务、进程服务、终端服务等，这样的部分相对较小，相关的管理工作也较为便利。这种机构的服务的运行都是以用户进程的形式呈现的，既不在核心中运行，也不直接地对硬件进行访问，这样一来即使服务器发生错误或受到破坏也不会对系统造成影响，仅仅只是会造成相对应服务器的崩溃。 [1]外核结构外核结构的计算机操作系统本质上就是为了获得更高的性能和灵活性而设计出来的，在系统中，操作系统接口处于硬件层，在内核中提出全部由以往操作系统带来的抽象，并将重点和关键放在了更多硬件资源的复用方面。在操作系统的外核结构中，内核负责的主要工作仅仅为简单的申请操作以及释放和复用硬件资源，其由以往操作系统提供的抽象全部在用户空间当中运行。 [1]一般情况下，外核结构中的内核主要有三大方面的工作，分别是对资源的所有权进行跟踪、为操作系统的安全提供保护以及撤销对资源的访问行为。在核外，基本上所有的操作系统中的抽象都是以库的形式呈现出来的，而用户在访问硬件资源时也是通过库的调用来完成的。 [1]安全加固播报编辑随着计算机网络与应用技术的不断发展，信息系统安全问题越来越引起人们的关注，信息系统一旦遭受破坏，用户及单位将受到重大的损失，对信息系统进行有效的保护，是必须面对和解决的迫切课题，而操作系统安全在计算机系统整体安全中至关重要，加强操作系统安全加固和优化服务是实现信息系统安全的关键环节。当前，操作系统安全构成威胁的问题主要有系统漏洞、脆弱的登录认证方式、访问控制形同虚设、计算机病毒、特洛伊木马、隐蔽通道、系统后门恶意程序和代码感染等，加强操作系统安全加固工作是整个信息系统安全的基础。 [6]安全加固原理安全加固是指按照系统安全配置标准，结合用户信息系统实际情况，对信息系统涉及的终端主机、服务器、网络设备、数据库及应用中间件等软件系统进行安全配置加固、漏洞修复和安全设备调优。通过安全加固，可以合理加强信息系统安全性，提高其健壮性，增加攻击入侵的难度，可以使信息系统安全防范水平得到大幅提升。 [6]安全加固方法安全加固主要通过人工对系统进行漏洞扫描，针对扫描结果使用打补丁、强化账号安全、修改安全配置、优化访问控制策略、增加安全机制等方法加固系统以及堵塞系统漏洞、“后门”，完成加固工作。 [6]安全加固流程安全加固主要包含以下几个环节：（1）安全加固范围确定收集需要进行安全加固的信息系统所涉及的计算机设备、网络、数据库及应用中间件的设备情况。 [6]（2）制订安全加固方案根据信息系统的安全等级划分和具体要求，利用网络安全经验和漏洞扫描技术和工具，对加固范围内的计算机操作系统、网络设备、数据库系统及应用中间件系统进行安全评估，从内、外部对信息系统进行全面的评估，检查这些系统目前安全状况，根据现状制定相应的安全加固措施，形成安全加固方案。 [6]（3）安全加固方案实施根据制定的安全加固实施方案实施加固，完成后对加固后的系统进行全面的测试和检查，确保加固对系统业务无影响，并填写加固实施记录。 [6]（4）安全加固报告输出根据安全加固实施记录，编写最终的安全加固实施报告，对加固工作进行总结，对已加固的项目、加固效果、遗留问题进行汇总统计。 [6]系统虚拟化播报编辑操作系统虚拟化作为容器的核心技术支撑，得到了研究者的广泛关注。最近几年，无论是在以SOSP/OSDI为代表的计算机系统领域顶级学术会议上，还是以Google为代表的重要互联网企业中，都陆续出现了一批操作系统虚拟化的最新研究成果，并且成果数量呈现出逐年增加的总体趋势。 [7]操作系统虚拟化技术允许多个应用在共享同一主机操作系统 (Host OS) 内核的环境下隔离运行， 主机操作系统为应用提供一个个隔离的运行环境， 即容器实例：操作系统虚拟化技术架构可以分为容器实例层、容器管理层和内核资源层。 [7]操作系统虚拟化与传统虚拟化最本质的不同是传统虚拟化需要安装客户机操作系统 (Guest OS) 才能执行应用程序，而操作系统虚拟化通过共享的宿主机操作系统来取代 Guest OS。 [7]操作实例播报编辑嵌入式嵌入式系统使用非常广泛的系统（如VxWorks、eCos、Symbian OS及Palm OS）以及某些功能缩减版本的Linux或者其他操作系统。某些情况下，OS指称的是一个内置了固定应用软件的巨大泛用程序。在许多最简单的嵌入式系统中，所谓的OS就是指其上唯一的应用程序。iOS是由苹果公司开发的手持设备操作系统。苹果公司于2007年1月9日的Macworld 大会上公布这个系统，以Darwin为基础，属于类Unix 的商业操作系统。最初是设计给 iPhone 使用的，后来陆续套用到 iPod touch 、iPad 以及 Apple TV 等产品上。iOS与苹果的 Mac OS X 操作系统一样，属于类Unix的商业操作系统。原本这个系统名为 iPhone OS，因为 iPad，iPhone，iPod touch 都使用 iPhone OS，所以 2010 年 WWDC 大会上宣布改名为 iOS（iOS 为美国 Cisco 公司网络设备操作系统注册商标，苹果改名已获得 Cisco 公司授权）。Android是一种基于Linux的自由及开放源代码的操作系统。主要使用于移动设备，如智能手机和平板电脑，由Google公司和开放手机联盟领导及开发。尚未有统一中文名称，中国大陆地区较多人使用“安卓”。Android操作系统最初由Andy Rubin开发，主要支持手机。2005年8月由Google收购注资。2007年11月，Google与84家硬件制造商、软件开发商及电信营运商组建开放手机联盟共同研发改良Android系统。随后Google以Apache开源许可证的授权方式，发布了Android的源代码。第一部Android智能手机发布于2008年10月。Android逐渐扩展到平板电脑及其他领域上，如电视、数码相机、游戏机、智能手表等。2011年第一季度，Android在全球的市场份额首次超过塞班系统，跃居全球第一。 2013年的第四季度，Android平台手机的全球市场份额已经达到78.1%。2013年09月24日谷歌开发的操作系统Android在迎来了5岁生日，全世界采用这款系统的设备数量已经达到10亿台。 openEuler是开放原子开源基金会（OpenAtom Foundation）孵化及运营的开源项目。 [17]欧拉操作系统(openEuler，简称“欧拉”,“开源欧拉”)是面向数字基础设施的操作系统，支持服务器、云计算、边缘计算、嵌入式等应用场景，支持多样性计算，致力于提供安全、稳定、易用的操作系统。通过为应用提供确定性保障能力，支持OT领域应用及OT与ICT的融合。 [17]类 Unix主条目：类Unix所谓的类Unix家族指的是一族种类繁多的OS，此族包含了System V、BSD与Linux。由于Unix是The Open Group的注册商标，特指遵守此公司定义的行为的操作系统。而类Unix通常指的是比原先的Unix包含更多特征的OS。类Unix系统可在非常多的处理器架构下运行，在服务器系统上有很高的使用率，例如大专院校或工程应用的工作站。1991年，芬兰学生林纳斯·托瓦兹根据类Unix系统Minix编写并发布了Linux操作系统内核，其后在理查德·斯托曼的建议下以GNU通用公共许可证发布，成为自由软件Unix变种. Linux近来越来越受欢迎，它们也在个人桌面计算机市场上大有斩获，例如Ubuntu系统。某些Unix变种，例如惠普的HP-UX以及IBM的AIX仅设计用于自家的硬件产品上，而SUN的Solaris可安装于自家的硬件或x86计算机上。苹果计算机的Mac OS X是一个从NeXTSTEP、Mach以及FreeBSD共同派生出来的微内核BSD系统，此OS取代了苹果计算机早期非Unix家族的Mac OS。经历数年的披荆斩棘，自由开源的Linux系统逐渐蚕食以往专利软件的专业领域，例如以往计算机动画运算巨擘──硅谷图形公司（SGI）的IRIX系统已被Linux家族及贝尔实验室研发小组设计的九号项目与Inferno系统取代，皆用于分散表达式环境。它们并不像其他Unix系统，而是选择内置图形用户界面。九号项目原先并不普及，因为它刚推出时并非自由软件。后来改在自由及开源软件许可证Lucent Public License发布后，便开始拥有广大的用户及社群。Inferno已被售予Vita Nuova并以GPL/MIT许可证发布。当前，计算机按照计算能力排名世界500强中472台使用Linux，6台使用Windows，其余为各类BSD等Unix。Microsoft Windows主条目：Microsoft WindowsMicrosoft Windows系列操作系统是在微软给IBM机器设计的MS-DOS的基础上设计的图形操作系统。现在的Windows系统，如Windows 2000、Windows XP皆是创建于现代的Windows NT内核。NT内核是由OS/2和OpenVMS等系统上借用来的。Windows可以在32位和64位的Intel和AMD的处理器上运行，但是早期的版本也可以在DEC Alpha、MIPS与PowerPC架构上运行。虽然由于人们对于开放源代码操作系统兴趣的提升，Windows的市场占有率有所下降，但是到2004年为止，Windows操作系统在世界范围内占据了桌面操作系统90%的市场。Windows系统也被用在低级和中阶服务器上，并且支持网页服务的数据库服务等一些功能。最近微软花费了很大研究与开发的经费用于使Windows拥有能运行企业的大型程序的能力。Windows XP在2001年10月25日发布，2004年8月24日发布服务包2（Service Pack 2），2008年4月21日发布最新的服务包3（Service Pack 3）。Windows 7，是由微软公司（Microsoft）开发的操作系统，内核版本号为Windows NT 6.1。Windows 7可供家庭及商业工作环境：笔记本电脑 、多媒体中心等使用。和同为NT6成员的Windows Vista一脉相承，Windows 7继承了包括Aero风格等多项功能，并且在此基础上增添了些许功能。Windows 10是由美国微软公司开发的应用于计算机和平板电脑的操作系统，于2015年7月29日发布正式版。Windows 10操作系统在易用性和安全性方面有了极大的提升，除了针对云服务、智能移动设备、自然人机交互等新技术进行融合外，还对固态硬盘、生物识别、高分辨率屏幕等硬件进行了优化完善与支持。截至2022年5月26日，Windows 10正式版已更新至Windows 10 21H2版本。 [10]微软的操作系统Windows Vista（开发代码为Longhorn）于2007年1月30日发售。Windows Vista增加了许多功能，尤其是系统的安全性和网上管理功能，并且其拥有接口华丽的Aero Glass。但是整体而言，其在全球市场上的口碑却并不是很好。其后继者Windows 7则是于2009年10月22日发售，Windows 7改善了Windows Vista为人诟病的性能问题，相较于Windows Vista，在同样的硬件环境下，Windows 7的表现较Windows Vista为好。而Windows 10则是于2015年7月29日发售。最新的系统为Windows 11,于2021年6月25日的直播中公布并发售 [9]MacOS X主条目：MacOS和MacOS XmacOS，前称“MacOS X”或“OS X”，是一套运行于苹果Macintosh系列计算机上的操作系统。Mac OS是首个在商用领域成功的图形用户界面系统。Macintosh开发成员包括比尔·阿特金森（Bill Atkinson）、杰夫·拉斯金（Jef Raskin）和安迪·赫茨菲尔德（Andy Hertzfeld）。从OS X 10.8开始在名字中去掉Mac，仅保留OS X和版本号。2016年6月13日在WWDC2016上，苹果公司将OS X更名为macOS，现行的最新的系统版本是13.X，即macOS Ventura。 [11]Google Chrome OS主条目：Google Chrome OSGoogle Chrome OS是一项Google的轻型的、基于网络的计算机操作系统计划，其基于Google的浏览器Google Chrome的Linux内核。2021年6月，华为鸿蒙系统正式亮相，国产操作系统迈出市场化和商业化重要一步。9月30日，面向数字基础设施的开源操作系统欧拉（openEuler）全新发布，与鸿蒙实现内核技术共享。11月9日，华为携手社区全体伙伴共同将欧拉开源操作系统正式捐赠给开放原子开源基金会，以推动操作系统产业快速发展。从服务器操作系统，升级为数字基础设施的操作系统，欧拉能够支持IT、CT、OT等数字基础设施全场景，覆盖服务器、云、边、嵌入式等各种设备形态的需求，更好地满足千行百业数字化转型的需求。 [8]Harmony OS操作系统2019年8月9日华为于东莞举行的华为开发者大会（HDC.2019）上正式发布的分布式操作系统-华为鸿蒙系统（HUAWEI HarmonyOS）。国产操作系统迈出市场化和商业化重要一步。鸿蒙OS是华为公司开发的一款基于微内核、面向5G物联网 、面向全场景的分布式操作系统。鸿蒙的英文名是HarmonyOS，意为和谐。[16]这个新的操作系统将打通手机、电脑、平板、电视、工业自动化控制、无人驾驶、车机设备、智能穿戴统一成一个操作系统，并且该系统是面向下一代技术而设计的，能兼容全部安卓应用的所有Web应用。若安卓应用重新编译，在鸿蒙OS上，运行性能提升超过60%。 [18]鸿蒙OS架构中的内核会把之前的Linux内核、鸿蒙OS微内核与LiteOS合并为一个鸿蒙OS微内核。创造一个超级虚拟终端互联的世界，将人、设备、场景有机联系在一起。同时由于鸿蒙系统微内核的代码量只有Linux宏内核的千分之一，其受攻击几率也大幅降低。 [19]分布式架构首次用于终端OS，实现跨终端无缝协同体验；确定时延引擎和高性能IPC技术实现系统天生流畅； 基于微内核架构重塑终端设备可信安全； [20]对于消费者而言，HarmonyOS通过分布式技术，让8+N设备具备智慧交互的能力。在不同场景下，8+N配合华为手机提供满足人们不同需求的解决方案。对于智能硬件开发者，HarmonyOS可以实现硬件创新，并融入华为全场景的大生态。对于应用开发者，HarmonyOS让他们不用面对硬件复杂性，通过使用封装好的分布式技术APIs，以较小投入专注开发出各种全场景新体验。 [21]在万物智联时代重要机遇期，鸿蒙系统结合移动生态发展的趋势，提出了三大技术理念： 一次开发，多端部署；可分可合，自由流转；统一生态，原生智能。实时操作系统（RTOS）这类系统的最大特色是实时性，就是在接收数据、指令后尽快处理，得出处理结果后会在规定时间内执行。家庭用户购买的物联网设备都是安装的RTOS。RTOS的种类很多，一般用户常碰见的有：1、RT-Thread：一个免费、开源的系统，诞生于2006年，由一名叫熊谱翔的开发者创立，目前由国内的开源社区维护，是一个地道的国产操作系统，对硬件的要求很低。国内厂商出品的物联网设备，智能空调、共享充电宝、智能电热水器、智能手环、智能手表等等，大部分都运行着RT-Thread，是目前国内做得最成熟、也是装机量最大的物联网系统。2、FreeRTOS：也是免费、开源的系统，对硬件的要求很低，获得了亚马逊的支持。国外厂商的物联网设备大部分安装的是FreeRTOS。3、LiteOS：这是华为开发的开源系统，主要运行在支持华为智能家居服务的物联网设备上，还有华为的GT系列智能手表中。4、QNX：一个闭源的操作系统，而且授权费不低，目前归属于加拿大的黑莓公司。QNX是一个安全性、稳定性极高的操作系统，长期以来被用于核电站、风力发电站、太空飞船、战斗机、CT扫描机、核磁共振扫描机等设备上。家庭用户购买到QNX设备主要有两类：一类是汽车，由于QNX的高安全性、高稳定性，一直受到众多汽车厂商的青睐，比如宝马、保时捷、福特、小鹏汽车等等，QNX几乎占据车载系统市场60%的市场份额；另一类是扫地机器人，部分品牌的扫地机器人使用的是QNX系统，如iRobot。

应用软件：
基本概念播报编辑计算机软件分为系统软件和应用软件两大类。应用软件是为满足用户不同领域、不同问题的应用需求而提供的那部分软件。 它可以拓宽计算机系统的应用领域，放大硬件的功能。应用软件（application software）是用户可以使用的各种程序设计语言，以及用各种程序设计语言编制的应用程序的集合，分为应用软件包和用户程序。应用软件包是利用计算机解决某类问题而设计的程序的集合供多用户使用。系统软件播报编辑系统软件是指控制和协调计算机及外部设备,支持应用软件开发和运行的系统，是无需用户干预的各种程序的集合，主要功能是调度，监控和维护计算机系统；负责管理计算机系统中各种独立的硬件，使得它们可以协调工作。系统软件使得计算机使用者和其他软件将计算机当作一个整体而不需要顾及到底层每个硬件是如何工作的。用途播报编辑办公室软件文书试算表程式投影片报告数学程式创建编辑器绘图程式基础数据库档案管理系统文本编辑器。 [2]互联网软件即时通讯软件电子邮件客户端网页浏览器客户端下载工具。 [2]多媒体软件媒体播放器图像编辑软件音讯编辑软件视讯编辑软件计算机辅助设计计算机游戏桌面排版 [2]分析软件计算机代数系统统计软件数字计算计算机辅助工程设计 [2]协作软件协作产品开发 [2]商务软件会计软件企业工作流程分析客户关系管理Backoffice企业资源规划供应链管理产品生命周期管理 [2]

高级语言：
简介播报编辑计算机语言具有高级语言和低级语言之分。而高级语言又主要是相对于汇编语言而言的，它是较接近自然语言和数学公式的编程，基本脱离了机器的硬件系统，用人们更易理解的方式编写程序。编写的程序称之为源程序 [2]。高级语言并不是特指的某一种具体的语言，而是包括很多编程语言，如流行的java，c，c++，C#，pascal，python，lisp，prolog，FoxPro，易语言，中文版的C语言等等，这些语言的语法、命令格式都不相同 [3]。高级语言与计算机的硬件结构及指令系统无关，它有更强的表达能力，可方便地表示数据的运算和程序的控制结构，能更好的描述各种算法，而且容易学习掌握。但高级语言编译生成的程序代码一般比用汇编程序语言设计的程序代码要长，执行的速度也慢。所以汇编语言适合编写一些对速度和代码长度要求高的程序和直接控制硬件的程序。高级语言、汇编语言和机器语言都是用于编写计算机程序的语言 [4]。高级语言程序“看不见”机器的硬件结构，不能用于编写直接访问机器硬件资源的系统软件或设备控制软件。为此，一些高级语言提供了与汇编语言之间的调用接口。用汇编语言编写的程序，可作为高级语言的一个外部过程或函数，利用堆栈来传递参数或参数的地址 [5]。发展历程播报编辑在编程语言经历了机器语言，汇编语言等更新之后，人们发现了限制程序推广的关键因素——程序的可移植性。需要设计一个能够不依赖于计算机硬件，能够在不同机器上运行的程序。这样可以免去很多编程的重复过程，提高效率，同时这种语言又要接近于数学语言或人的自然语言。在计算机还很稀缺的50年代，诞生了第一个高级编程语言。当时计算机的造价不菲，但是每天的计算量有有限，如何有效的利用计算机有限的计算能力成为了当时人们面对的问题。同时，因为资源的稀缺，计算机的运行效率也成为了那个年代工程师追寻的目标。为了更高效的使用计算机，人们设计出了高级编程语言，来满足人们对于高效简的编程语言的追求。用高级编程语言编写的程序需要经过翻译，翻译成机器所能识别的二进制数才能由计算机去执行。虽然，高级编程语言编写的程序需要一些时间去翻译代码，从而降低了计算机的执行效率，但是实践证明，高级编程语言为工程师带来的便利远远大于降低的执行效率。 经过各软件工程师和专家的不懈努力，1954年，第一个完全意义的高级编程语言FORTRAN问世了，他完全脱离了特定机器的局限性，是第一个通用性的编程语言。从第一个编程语言问世到现今，共有几百种高级编程语言出现，很多语言成为了编程语言发展道路上的里程碑，影响很大。 比如：BASIC、JAVA、C、C++、python等。 高级编程语言也从早期的控制信号变成了现在的有结构有格式的程序编写工具，C++等语言的出现更是开启了面向对象编程语言的新章。同时伴随着软件编写效率的提高，软件开发也逐渐变成了有规模、有产业的商业项目 [6]。特点播报编辑因为明确的目标性以及理解容易，一个新手很容易去学习高级编程语言。同时高级编程语言因为发展的历史，拥有很多函数库，用户可以根据自身的需求在代码中加入头文件来调用这些函数来实现自己的功能，当然使用者也可以根据自己的喜好编写函数来在后续的代码中调用 [6]。高级编程语言作为一种通用的编程语言，它的语言结构和计算机本身的硬件以及指令系统无关，它的可阅读性更强，能够方便的表达程序的功能，更好的描述使用的算法。同时，它更 容易被初学者所掌握，很容易学习。而且容易学习掌握。但是高级编程语言因为是一种编译语言，所以他的运行速度比汇编程序要低，同时因为高级语言比较冗长，所以代码的执行速度也要慢一些 [6]。高级编程语言，作为用户层面的编程工具，用户并不需要去了解硬件的结构，而是去用逻辑的语言去实现想要的目标，但是因为高级编程语言的架构高于汇编，所以不能编写直接访问硬件资源的系统程序，因此，高级编程语言必须要调用汇编语言编写的程序来访问硬件地址 [6]。分类播报编辑1.命令式语言。这种语言的语义基础是模拟“数据存储/数据操作”的图灵机可计算模型，十分符合现代计算机体系结构的自然实现方式。其中产生操作的主要途径是依赖语句或命令产生的副作用。现代流行的大多数语言都是这一类型，比如 Fortran、Pascal、Cobol、C、C++、Basic、Ada、Java、C# 等，各种脚本语言也被看作是此种类型 [7]。2.函数式语言。这种语言的语义基础是基于数学函数概念的值映射的λ算子可计算模型。这种语言非常适合于进行人工智能等工作的计算。典型的函数式语言如 Lisp、Haskell、ML、Scheme 、F#等 [7]。3.逻辑式语言。这种语言的语义基础是基于一组已知规则的形式逻辑系统。这种语言主要用在专家系统的实现中。最著名的逻辑式语言是 Prolog [7]。4.面向对象语言。现代语言中的大多数都提供面向对象的支持，但有些语言是直接建立在面向对象基本模型上的，语言的语法形式的语义就是基本对象操作。主要的纯面向对象语言是 Smalltalk [7]。性能分析播报编辑接口分析接口主要指高级语言与汇编语言之间的联系性，Ada语言在应用的过程中可以访问汇编语言，访问情况的实现只需要程序功能，程序功能在使用的过程中破解所设定的环节，进行访问工作。对C语言而言，将汇编语言作为整体看成一个独立的部分，将独立的部分加入C的程序中，具有通讯功能。对于Macros的应用，可以应用在汇编语言中，借助编译器完成各种工作。语言的性能并没有改变，Java语言与汇编语言的 关系可以将其作为一个代码，此代码具有移植性，直接移植便可以进行操作，操作的过程方面并没有过多复杂程序 [8]。寻址分析Ada寻址情况主要借助的是SYSTEM实现，可以准确的寻址；C的寻址需要借助指针实现，可以精确的实现寻址，对于存储器寻址情况需要应用peek完成，Modula-2的寻址情况与 Ada所借助的情况一致，可以吸纳绝对的寻址 [8]。位操作分析对于不同语言具有不同的位情况，所表达的从句中可以明确指出Ada的位，会存在很多位的情况，将其组合，通过逻辑原理进行处理。C的主要功能是位操作，对于Modula-2主要借助BIYSTE，应用这样的方式可以准确进行位操作 [8]。任务支持分析不同的语言任务功能不一样，对于Ada具有较多的任务， 其支持性强，可以实现多种任务同时工作的情况。C与Ada相 比不具有这样的优势，Modula-2与Ada相比并没有其完善性，需要借助机制实现。对于这样的优势是Ada，可以独立的完成 [8]。控制程序分析系统的设定需要控制程序，对于高级语言会涵盖一定的控制结构，像Ada中具有控制能力，对于其分支可以完全掌控其运行。C中并没有完善的控制结构，主要是对分值方面使用灵活，并且简单易操作，在使用的过程中应严格按照其规定操作，避免人为原因造成问题出现。Modula-2的控制系统只是控制分支，转移需要应用FXIT，操作的过程中应严格审查操作环节，避免其操作的过程中造成出现问题，操作时应兢兢业业，因为这方面的人为操作易引起问题的形成 [8]。高级语言的工作方式播报编辑高级语言设计的程序必须经过“翻译”以后才能被机器执行。“翻译”的方法有两种，一种是解释，一种是编译。解释是把源程序翻译一句，执行一句的过程，而编译是源程序翻译成机器指令形式的目标程序的过程，再用链接程序把目标程序链接成可执行程序后才能执行 [9]。解释翻译过程。对高级语言程序进行解释并执行的程序称为解释程序（软件）。它的功能是读入源程序，按源程序动态逻辑顺序进行逐句分析、翻译，解释一句执行一句，不产生任何中间代码，最终得到程序的执行结果 [9]。

二级缓存：
产品简介播报编辑L2缓存位于CPU与内存之间的临时存储器，它的容量比内存小但交换速度快。在缓存中的数据是内存中的一小部分，但这一小部分是短时间内CPU即将访问的，当CPU调用大量数据时，就可避开内存直接从缓存中调用，从而加快读取速度。由此可见，在CPU中加入缓存是一种高效的解决方案，这样整个内存储器（缓存+内存）就变成了既有缓存的高速度，又有内存的大容量的存储系统了。缓存对CPU的性能影响很大，主要是因为CPU的数据交换顺序和CPU与缓存间的带宽引起的。L2原理播报编辑缓存的工作原理是当CPU要读取一个数据时，首先从缓存中查找，如果找到就立即读取并送给CPU处理；如果没有找到，就用相对慢的速度从内存中读取并送给CPU处理，同时把这个数据所在的数据块调入缓存中，可以使得以后对整块数据的读取都从缓存中进行，不必再调用内存。正是这样的读取机制使CPU读取缓存的命中率非常高（大多数CPU可达90%左右），也就是说CPU下一次要读取的数据90%都在缓存中，只有大约10%需要从内存读取。这大大节省了CPU直接读取内存的时间，也使CPU读取数据时基本无需等待。总的来说，CPU读取数据的顺序是先缓存后内存。 [1]L2发展历史播报编辑最早先的CPU缓存是个整体的，而且容量很低，英特尔公司从Pentium时代开始把缓存进行了分类。当时集成在CPU内核中的缓存已不足以满足CPU的需求，而制造工艺上的限制又不能大幅度提高缓存的容量。因此出现了集成在与CPU同一块电路板上或主板上的缓存，此时就把 CPU内核集成的缓存称为一级缓存，而外部的称为二级缓存。一级缓存中还分数据缓存（D-Cache）和指令缓存（I-Cache）。二者分别用来存放数据和执行这些数据的指令，而且两者可以同时被CPU访问，减少了争用Cache所造成的冲突，提高了处理器效能。英特尔公司在推出Pentium 4处理器时，还新增了一种一级追踪缓存，容量为12KB.发展过程随着CPU制造工艺的发展，二级缓存也能轻易的集成在CPU内核中，容量也在逐年提升。再用集成在CPU内部与否来定义一、二级缓存，已不确切。而且随着二级缓存被集成入CPU内核中，以往二级缓存与CPU大差距分频的情况也被改变，此时其以相同于主频的速度工作，可以为CPU提供更高的传输速度。二级缓存是CPU性能表现的关键之一，在CPU核心不变化的情况下，增加二级缓存容量能使性能大幅度提高。而同一核心的CPU高低端之分往往也是在二级缓存上有差异，由此可见二级缓存对于CPU的重要性。CPU在缓存中找到有用的数据被称为命中，当缓存中没有CPU所需的数据时（这时称为未命中），CPU才访问内存。从理论上讲，在一颗拥有二级缓存的CPU中，读取一级缓存的命中率为80%。也就是说CPU一级缓存中找到的有用数据占数据总量的80%，剩下的20%从二级缓存中读取。由于不能准确预测将要执行的数据，读取二级缓存的命中率也在80%左右（从二级缓存读到有用的数据占总数据的16%）。那么还有的数据就不得不从内存调用，但这已经是一个相当小的比例了。较高端的CPU中，还会带有三级缓存，它是为读取二级缓存后未命中的数据设计的—种缓存，在拥有三级缓存的CPU中，只有约5%的数据需要从内存中调用，这进一步提高了CPU的效率。为了保证CPU访问时有较高的命中率，缓存中的内容应该按一定的算法替换。一种较常用的算法是“最近最少使用算法”（LRU算法），它是将最近一段时间内最少被访问过的行淘汰出局。因此需要为每行设置一个计数器，LRU算法是把命中行的计数器清零，其他各行计数器加1。当需要替换时淘汰行计数器计数值最大的数据行出局。这是一种高效、科学的算法，其计数器清零过程可以把一些频繁调用后再不需要的数据淘汰出缓存，提高缓存的利用率。CPU产品中，一级缓存的容量基本在4KB到18KB之间，二级缓存的容量则分为128KB、256KB、512KB、1MB等。一级缓存容量各产品之间相差不大，而二级缓存容量则是提高CPU性能的关键。二级缓存容量的提升是由CPU制造工艺所决定的，容量增大必然导致CPU内部晶体管数的增加，要在有限的CPU面积上集成更大的缓存，对制造工艺的要求也就越高。 [2]使用方法播报编辑双核心CPU的二级缓存比较特殊，和以前的单核心CPU相比，最重要的就是两个内核的缓存所保存的数据要保持一致，否则就会出现错误，为了解决这个问题不同的CPU使用了不同的办法：Intel双核心处理器的二级缓存Intel的双核心CPU主要有Pentium D、Pentium EE、Core Duo三种，其中Pentium D、Pentium EE的二级缓存方式完全相同。Pentium D和Pentium EE的二级缓存都是CPU内部两个内核具有互相独立的二级缓存，其中，8xx系列的Smithfield核心CPU为每核心1MB，而9xx系列的Presler核心CPU为每核心2MB。这种CPU内部的两个内核之间的缓存数据同步是依靠位于主板北桥芯片上的仲裁单元通过前端总线在两个核心之间传输来实现的，所以其数据延迟问题比较严重，性能并不尽如人意。Core Duo使用的核心为Yonah，它的二级缓存则是两个核心共享2MB的二级缓存，共享式的二级缓存配合Intel的“Smart cache”共享缓存技术，实现了真正意义上的缓存数据同步，大幅度降低了数据延迟，减少了对前端总线的占用，性能表现不错，是双核心处理器上最先进的二级缓存架构。今后Intel的双核心处理器的二级缓存都会采用这种两个内核共享二级缓存的“Smart cache”共享缓存技术。AMD双核心处理器的二级缓存Athlon 64 X2 CPU的核心主要有Manchester和Toledo两种，他们的二级缓存都是CPU内部两个内核具有互相独立的二级缓存，其中，Manchester核心为每核心512KB，而Toledo核心为每核心1MB。处理器内部的两个内核之间的缓存数据同步是依靠CPU内置的System Request Interface(系统请求接口，SRI)控制，传输在CPU内部即可实现。这样一来，不但CPU资源占用很小，而且不必占用内存总线资源，数据延迟也比Intel的Smithfield核心和Presler核心大为减少，协作效率明显胜过这两种核心。不过，由于这种方式仍然是两个内核的缓存相互独立，从架构上来看也明显不如以Yonah核心为代表的Intel的共享缓存技术Smart Cache。

小型计算机：
简介播报编辑小型计算机是相对于 [1]大型计算机而言，小型计算机的软件、硬件系统规模比较小，但价格低、可靠性高、操作灵活方便，便于维护和使用。为了有效发挥计算机资源的功能和提高性能-价格比而采取的方法有四种：1、根据不同用途采用不同字长，尽可能在满足应用要求的前提下用较短的字长，以压缩计算机规模，从而降低造价。在已有的小型机中,字长为16位者较为普遍,如美国的PDP-11系列、NOVA系列, 中国的DJS100系列。2、采用微程序控制结构，结构规整，便于实现生产标准化。它又能灵活地实现各种控制功能，可根据不同应用编制相应的微程序，以获得良好的性能－价格比。3、按处理能力分档，研制小型机系列。同一系列中各档小型机的字长和指令系统往往相同，只是规模大小、处理能力不同。为各档小型机研制各种可供选择的功能部件和接口，而且使主存储器和外围设备等的配置规模也有一定的变化范围。这样，可以针对不同的应用规模选用系列中适当型号及其系统配置规模。4、研制各种软件，如实时操作系统、多用户分时操作系统、各种高级语言（包括专用语言）和各类应用程序包等，以获得解决各种应用问题的良好效果。微型化播报编辑小型机和超大规模集成电路技术的发展为微型计算机的诞生创造了条件。8位和8位以下的微型计算机、单板机和微处理器以及16位的单板机和微处理器的成本比小型机大大降低，也更便于维护和使用。在小型计算机应用领域，微型计算机与小型计算机相辅相成，得到广泛的应用。为了提高小型计算机的性能－价格比，不少厂家利用大规模集成电路技术实现小型计算机的微型化。因为体系逻辑结构是现成的，研制生产周期可以缩短，原先研制的 软件 也可以使用。超级小型计算机播报编辑为了向上扩大小型计算机的应用领域，已采用各种技术研制出 [2]超级小型计算机。这些高性能小型计算机的处理能力达到或超过了低档大型计算机的能力。因此，小型计算机和大型计算机的界线也有了一定的交错。提高性能的技术措施主要有四个方面。1、字长增加到32位，以便提高运算精度和速度，增强指令功能，扩大寻址范围，使计算机的处理能力大大提高。2、采用大型计算机中的一些技术，如采用流水线结构、通用寄存器、超高速缓冲存储器、快速总线和 通道 等来提高系统的运算速度和吞吐率。3、采用各种大规模集成电路，用快速存储器、门阵列、程序逻辑阵列、大容量存储芯片和各种接口芯片等构成计算机系统，以缩小体积和减少功耗，提高性能和可靠性。4、研制功能更强的系统软件、工具软件、通信软件、 数据库 和应用程序包，以及能支持软件核心部分的硬件系统结构、指令系统和固件，软件、硬件结合起来构成用途广泛的高性能系统。

解释器：
执行方式播报编辑Python、TCL和各种Shell程序一般而言是使用解释器执行的。微软公司的Qbasic语言也是解释方式，它不能生成可执行程序（但Quick Basic和Visual Basic可以）；运用广泛的网络编程语言java则同时有解释和编译方式。在开始之前有必要再次强调：下面介绍的解释器是一个源代码解释器。也就是说，解释器在执行时，每次读入一条语句，并且根据这条语句执行特定的操作；然后再读入下一条语句，依次类推。这与伪代码解释器是有所区别的，例如早期的Java运行时系统。两者的区别在于：源代码解释器直接对程序的源代码解释执行；而伪代码解释器先将程序的源代码转化为某种与机器无关的中间代码，然后再执行中间代码。相比之下，源代码解释器更易于创建，并且不需要一个独立的编译过程。子系统播报编辑Small BASIC解释器包括两个主要的子系统：一个是表达式解析器，负责处理数字表达式；另一个是解释器，负责程序的实际执行。对于前者，可采用本书第二章所介绍的表达式解析器。但是在这里做了某些改进，使得解析器能够解析包含在程序语句中的数字表达式，而不是只能解析孤立的表达式。解释器子系统和解析器子系统包含在同一个解释器类中，该类名为SBasic。尽管从理论上讲可以使用两个独立的类：一个包含解释器，另一个包含表达式解析器；但是将两者用同一个类来实现的代效率会更高，因为表达式解析器和解释器的代码是密不可分的。例如，两个子系统都操作保存着程序代码的同一个字符数组。如果将它们分别安排在两个类中，将会增加可观的额外开销，并导致性能上的损失和功能上的重复。此外，由于程序解释的任务繁重，而解析表达式只是其中的一部分，因此将整个解释机制包含在单个类中是很有意义的。解释器执行时，每次从程序的源代码中读入一个标识符。如果读入的是关键字，解释器就按照该关键字的要求执行规定的操作。举例来说，当解释器读入一个PRINT后，它将打印PRINT之后的字符；当读入一个GOSUB时，它就执行指定的子程序。在到达程序的结尾之前，这个过程将反复进行。可以看到，解释器只是简单地执行程序指定的动作。解释编译播报编辑解释器运行程序的方法有：1.直接运行高级编程语言 （如 Shell 自带的解释器）2.转换高级编程语言码到一些有效率的字节码 (Bytecode)，并运行这些字节码3.以解释器包含的编译器对高级语言编译，并指示处理器运行编译后的程序 (例如:JIT)Perl，Python，MATLAB，与Ruby是属于第二种方法，而UCSD Pascal则是属于第三种方式。在转译的过程中，这组高级语言所写成的程序仍然维持在源代码的格式（或某种中继语言的格式），而程序本身所指涉的动作或行为则由解释器来表现。使用解释器来运行程序会比直接运行编译过的机器码来得慢，但是相对的这个直译的行为会比编译再运行来得快。这在程序开发的雏型化阶段和只是撰写试验性的代码时尤其来得重要，因为这个“编辑-直译-除错”的循环通常比“编辑-编译-运行-除错”的循环来得省时许多。在解释器上运行程序比直接运行编译过的代码来得慢，是因为解释器每次都必须去分析并转译它所运行到的程序行，而编译过的程序就只是直接运行。这个在运行时的分析被称为"直译式的成本"。在解释器中，变量的访问也是比较慢的，因为每次要访问变量的时候它都必须找出该变量实际存储的位置，而不像编译过的程序在编译的时候就决定好了变量的位置了。在使用解释器来达到较快的开发速度和使用编译器来达到较快的运行进度之间是有许多妥协的。有些系统(例如有一些LISP)允许直译和编译的代码互相调用并共享变量。这意味着一旦一个子程序在解释器中被测试并除错过之后，它就可以被编译以获得较快的运行进度。许多解释器并不像其名称所说的那样运行原始代码，反而是把原始代码转换成更压缩的内部格式。举例来说，有些BASIC的解释器会把keywords取代成可以用来在jump table中找出相对应指令的单一byte符号。解释器也可以使用如同编译器一般的文字分析器（lexical analyzer）和语法分析器（parser）然后再转译产生出来的抽象语法树（abstract syntax tree）。可携性佳，直译式程序相较于编译式程序有较佳的可携性，可以容易的在不同软硬件平台上运行。而编译式程序经过编译后的程序则只限定于运行在开发环境平台。字节解释播报编辑考量程序运行之前所需要分析的时间，存在了一个介于直译与编译之间的可能性。例如，用Emacs Lisp所撰写的源代码会被编译成一种高度压缩且优化的另一种Lisp源代码格式，这就是一种字节码（bytecode），而它并不是机器码（因此不会被绑死在特定的硬件上）。这个"编译过的"码之后会被字节码直译器（使用C写成的）转译。在这种情况下，这个"编译过的"码可以被说成是虚拟机（不是真的硬件，而是一种字节码解释器）的机器码。这个方式被用在Open Firmware系统所使用的Forth代码中: 原始程序将会被编译成 "F code" （一种字节码），然后被一个特定平台的虚拟机直译和运行。 [1]

指令预取：
预取技术:具体方法就是在不命中时，当数据从主存储器中取出送往CPU的同时，把主存储器相邻几个单元中的数据（称为一个数据块）都取出来送入Cache中。

机器周期：
简介播报编辑时序是用定时单位来说明的。MCS-51的时序定时单位共有4个，从小到大依次是：节拍、状态、机器周期和指令周期。下面分别加以说明 [2]。节拍与状态：把振荡脉冲的周期定义为节拍（用p表示）。振荡脉冲经过二分频后定义为状态。一个状态就包含两个节拍 [2]。机器周期：MCS-51采用定时控制方式，有固定的机器周期，规定一个机器周期的宽度为6个状态，并依次表示为S1-S6。由于一个状态包括两个节拍，因此一个机器周期总共有12个节拍，分别记作S1P1、S1P2、……、S6P2。因此一个机器周期就由12个振荡脉冲周期组成 [3]。显然，当振荡脉冲频率为12MHz时，一个机器周期为1μs，当振荡脉冲频率为6MHz时，一个机器周期为2μs [3]。指令周期：指令周期是最大的时序定时单位，执行一条指令所需要的时间称为指令周期。MCS-51的指令周期根据指令的不同，可分别包含有一、二、四个机器周期 [3]。指令周期播报编辑CPU每取出一条指令并执行这条指令，都要完成一系列的操作，这一系列操作所需要的时间通常叫做一个指令周期。换言之指令周期是取出一条指令并执行这条指令的时间。由于各条指令的操作功能不同，因此各种指令的指令周期是不尽相同的。例如一条加法指令的指令周期同一条乘法指令的指令周期是不相同的 [4]。指令周期常常用若干个CPU周期数来表示，CPU周期也称为机器周期。由于CPU内部的操作速度快，而CPU访问一次内存所花的时间较长，通常用内存中读取一个指令字的最短时间来规定CPU周期。这就是说，一条指令的取出阶段（通常称为取指）需要一个CPU周期时间。而一个CPU周期时间又包含有若干个时钟周期（通常称为节拍脉冲或T周期，它是处理操作的最基本单位）。这些时钟周期的总和，决定了一个CPU周期的时间宽度。由此可知，取出和执行任何一条指令所需的最短时间为两个CPU周期。对于复杂一些的指令，则需要更多的CPU周期 [4]。总线周期播报编辑1、微处理器是在时钟信号CLK控制下按节拍工作的。8086/8088系统的时钟频率为4.77MHz，每个时钟周期约为200ns [5]。2、由于存贮器和I/O端口是挂接在总线上的，CPU对存贮器和I/O接口的访问，是通过总线实现的。我们通常把CPU通过总线对微处理器外部（存储器或I/O接口）进行一次访问所需时间，称为一个总线周期 [5]。时钟周期播报编辑时钟周期T又称为振荡周期，由单片机片内振荡电路OSC产生，常定义为时钟脉冲频率的倒数，是时序中最小的时间单位。例如，若某单片机时钟频率为1MHz，则它的时钟周期T应为1μs。由于时钟脉冲是计算机的基本工作脉冲，它控制着计算机的工作节奏，使计算机的每一步工作都统一到它的步调上来。显然，对同一种机型的计算机，时钟频率越高，计算机的工作速度就越快。但是，由于不同的计算机硬件电路和器件的不完全相同，所以它们需要的时钟周期频率范围也不一定相同 [6]。相互关系播报编辑1、指令周期由若干个机器周期组成，而机器周期又包含若干个时钟周期，基本总线周期由4个时钟周期组成 [7]。2、机器周期和总线周期的关系：机器周期指的是完成一个基本操作的时间，基本操作有时可能包含总线读/写，因而包含总线周期，但是有时可能与总线读/写无关，所以，并无明确的相互包含关系 [7]。

总线接口：
产品简介播报编辑总线接口：显示卡要插在主板上才能与主板互相交换数据。与主板连接的接口主要ISA、EISA、VESA、PCI、AGP等几种。ISA和EISA总线带宽窄、速度慢，VESA总线扩展能力差，这三种总线已经被市场淘汰。现在常见的是PCI和AGP接口。PCI接口是一种总线接口，以1/2或1/3的系统总线频率工作（通常为33MHz），如果要在处理图像数据的同时处理其它数据，那么流经PCI总线的全部数据就必须分别地进行处理，这样势必存在数据滞留现象，在数据量大时，PCI总线就显得很紧张。AGP接口是为了解决这个问题而设计的，它是一种专用的显示接口（就是说，可以在主板的PCI插槽中插上声卡、显示卡、视频捕捉卡等板卡，却不能在主板的AGP插槽中插上除了AGP显示卡以外的任何板卡），具有独占总线的特点，只有图像数据才能通过AGP端口。另外AGP使用了更高的总线频率（66MHz），这样极大地提高了数据传输率。目前的显示卡接口的发展趋势是AGP接口。要留意的是，AGP技术分AGP1×和AGP2×，后者的最大理论数据传输率是前者的2倍，将会出现支持AGP4×的显示卡（例如Savage4），它的最大理论数据传输率将达到1056MB/s。区分AGP接口和PCI接口很容易，前者的引线上下宽度错开，俗称“金手指”，后者的引线上下一般齐。总线接口种类播报编辑总线接口是成像板卡与计算机链接的接口，总线接口包括：ISA/EISA、AGP、VME、VL、PCI/PCI-X、PCMCIA、PMC、PCI EXPRESS等。在机器视觉领域中广泛使用的总线接口主要是基于PCI的各种总线：PCI、PCI-X、PCI EXPRESS。PCI总线是独立于CPU的系统总线，采用了独特的中间缓冲器设计，可将高速的外围设备直接挂在CPU总线上，打破了瓶颈，使得CPU的性能得到充分的发挥。图像采集卡利用PCI总线的优点，实现了高速图像数据传输和主控系统的对接。为了实现PCI总线数据传输速度的进一步提高，PCISIG组织在PCI标准协议的基础上指定了PCI-X、PCI EXPRESS等快速数据传输标准。同时为了适应嵌入式系统应用，PCISIG也推出了PC104、PC104 Plus、PXI、Compact PCI等标准。PCIPCI(Peripheral Component Interconnected)总线是Pentium PC机的组成部分，理论带宽可达到132MB/S,通常可达到95MB/s.PCI-XPCI是PC总线的一种扩展架构，它与PCI总线不同的是，PCI总线必须频繁地在目标设备和总线之间交换数据，而PCI-X则允许目标设备仅在单个PCI-X设备进行交换。PCI-X有三种不同的工作频率，66MHz、100MHz和133MHz，在66MHz时数据率为1GB/s.PCI-EPCI EXPRESS 也称PCI-E,是下一代PCI总线，原名为“3GIO”是由英特尔提出的，它的最大特点是传输速率快，PCI-E有X1、X4、X8、X16，PCI-X1的传输速率可达到500MB/s，X16可达到8GB/s。PC104/PC104 plus/PXI/Compact PCI以上均为专门为嵌入式控制而定义的总线

指令解码：
cpu运作原理CPU的主要运作原理，不论其外观，都是执行储存于被称为程式里的一系列指令。在此讨论的是遵循普遍的架构设计的装置。程式以一系列数字储存在电脑记忆体中。差不多所有的CPU的运作原理可分为四个阶段：提取（Fetch）、解码（Decode）、执行（Execute）和写回（Writeback）。第一阶段，提取，从程式记忆体中检索指令（为数值或一系列数值）。由程式计数器（Program Counter）指定程式记忆体的位置，程式计数器保存供识别程式位置的数值。换言之，程式计数器记录了CPU在程式里的踪迹。提取指令之后，程式计数器根据指令式长度增加记忆体单元。指令的提取常常必须从相对较慢的记忆体寻找，导致CPU等候指令的送入。这个问题主要被论及在现代处理器的快取和管线化架构（见下）。CPU根据从记忆体提取到的指令来决定其执行行为。在解码阶段，指令被拆解为有意义的片断。根据CPU的指令集架构（ISA）定义将数值解译为指令。一部分的指令数值为运算码（Opcode），其指示要进行哪些运算。其它的数值通常供给指令必要的资讯，诸如一个加法（Addition）运算的运算目标。这样的运算目标也许提供一个常数值（即立即值），或是一个空间的定址值：暂存器或记忆体位址，以定址模式决定。在旧的设计中，CPU里的指令解码部分是无法改变的硬体装置。不过在众多抽象且复杂的CPU和指令集架构中，一个微程式时常用来帮助转换指令为各种形态的讯号。这些微程式在已成品的CPU中往往可以重写，方便变更解码指令。在提取和解码阶段之后，接着进入执行阶段。该阶段中，连接到各种能够进行所需运算的CPU部件。例如，要求一个加法运算，算数逻辑单元（ALU，Arithmetic Logic Unit）将会连接到一组输入和一组输出。输入提供了要相加的数值，而且在输出将含有总和结果。ALU内含电路系统，以于输出端完成简单的普通运算和逻辑运算（比如加法和位元运算）。如果加法运算产生一个对该CPU处理而言过大的结果，在标志暂存器里，运算溢出（Arithmetic Overflow）标志可能会被设置（参见以下的数值精度探讨）。最终阶段，写回，以一定格式将执行阶段的结果简单的写回。运算结果经常被写进CPU内部的暂存器，以供随后指令快速存取。在其它案例中，运算结果可能写进速度较慢，但容量较大且较便宜的主记忆体。某些类型的指令会操作程式计数器，而不直接产生结果资料。这些一般称作“跳转”（Jumps）并在程式中带来循环行为、条件性执行（透过条件跳转）和函式。许多指令也会改变标志暂存器的状态位元。这些标志可用来影响程式行为，缘由于它们时常显出各种运算结果。例如，以一个“比较”指令判断两个值的大小，根据比较结果在标志暂存器上设置一个数值。这个标志可藉由随后的跳转指令来决定程式动向。在执行指令并写回结果资料之后，程式计数器的值会递增，反覆整个过程，下一个指令周期正常的提取下一个顺序指令。如果完成的是跳转指令，程式计数器将会修改成跳转到的指令位址，且程式继续正常执行。许多复杂的CPU可以一次提取多个指令、解码，并且同时执行。这个部分一般涉及“经典RISC管线”，那些实际上是在众多使用简单CPU的电子装置中快速普及（常称为微控制（Microcontrollers））。

CPU：
发展历史播报编辑CPU出现于大规模集成电路时代，处理器架构设计的迭代更新以及集成电路工艺的不断提升促使其不断发展完善。从最初专用于数学计算到广泛应用于通用计算，从4位到8位、16位、32位处理器，最后到64位处理器，从各厂商互不兼容到不同指令集架构规范的出现，CPU自诞生以来一直在飞速发展。 [1]CPU发展已经有50多年的历史了。我们通常将其分成六个阶段。 [3](1)第一阶段(1971年-1973年)。这是4位和8位低档微处理器时代，代表产品是Intel 4004处理器。 [3]1971年，Intel生产的4004微处理器将运算器和控制器集成在一个芯片上，标志着CPU的诞生； 1978年，8086处理器的出现奠定了X86指令集架构， 随后8086系列处理器被广泛应用于个人计算机终端、高性能服务器以及云服务器中。 [1](2)第二阶段(1974年-1977年)。这是8位中高档微处理器时代，代表产品是Intel 8080。此时指令系统已经比较完善了。 [3](3)第三阶段(1978年-1984年)。这是16位微处理器的时代，代表产品是Intel 8086。相对而言已经比较成熟了。 [3](4)第四阶段(1985年-1992年)。这是32位微处理器时代，代表产品是Intel 80386。已经可以胜任多任务、多用户的作业。 [3]1989 年发布的80486处理器实现了5级标量流水线，标志着CPU的初步成熟，也标志着传统处理器发展阶段的结束。 [1](5)第五阶段(1993年-2005年)。这是奔腾系列微处理器的时代。 [3]1995 年11 月，Intel发布了Pentium处理器，该处理器首次采用超标量指令流水结构，引入了指令的乱序执行和分支预测技术，大大提高了处理器的性能， 因此，超标量指令流水线结构一直被后续出现的现代处理器，如AMD（Advanced Micro devices）的锐龙、Intel的酷睿系列等所采用。 [1](6)第六阶段(2005年后)。处理器逐渐向更多核心，更高并行度发展。典型的代表有英特尔的酷睿系列处理器和AMD的锐龙系列处理器。 [3]为了满足操作系统的上层工作需求，现代处理器进一步引入了诸如并行化、多核化、虚拟化以及远程管理系统等功能，不断推动着上层信息系统向前发展。 [1]工作原理播报编辑冯诺依曼体系结构是现代计算机的基础。在该体系结构下，程序和数据统一存储，指令和数据需要从同一存储空间存取，经由同一总线传输，无法重叠执行。根据冯诺依曼体系，CPU的工作分为以下 5 个阶段：取指令阶段、指令译码阶段、执行指令阶段、访存取数和结果写回。 [1]取指令（IF，instruction fetch），即将一条指令从主存储器中取到指令寄存器的过程。程序计数器中的数值，用来指示当前指令在主存中的位置。当一条指令被取出后，程序计数器（PC）中的数值将根据指令字长度自动递增。 [1]指令译码阶段（ID，instruction decode），取出指令后，指令译码器按照预定的指令格式，对取回的指令进行拆分和解释，识别区分出不同的指令类别以及各种获取操作数的方法。现代CISC处理器会将拆分已提高并行率和效率。 [1]执行指令阶段（EX，execute），具体实现指令的功能。CPU的不同部分被连接起来，以执行所需的操作。访存取数阶段（MEM，memory），根据指令需要访问主存、读取操作数，CPU得到操作数在主存中的地址，并从主存中读取该操作数用于运算。部分指令不需要访问主存，则可以跳过该阶段。 [1]结果写回阶段（WB，write back），作为最后一个阶段，结果写回阶段把执行指令阶段的运行结果数据“写回”到某种存储形式。结果数据一般会被写到CPU的内部寄存器中，以便被后续的指令快速地存取；许多指令还会改变程序状态字寄存器中标志位的状态，这些标志位标识着不同的操作结果，可被用来影响程序的动作。 [1]在指令执行完毕、结果数据写回之后，若无意外事件（如结果溢出等）发生，计算机就从程序计数器中取得下一条指令地址，开始新一轮的循环，下一个指令周期将顺序取出下一条指令。 [1]许多复杂的CPU可以一次提取多个指令、解码，并且同时执行。简介播报编辑中央处理器（CPU），是电子计算机的主要设备之一，电脑中的核心配件。其功能主要是解释计算机指令以及处理计算机软件中的数据。CPU是计算机中负责读取指令，对指令译码并执行指令的核心部件。中央处理器主要包括两个部分，即控制器、运算器，其中还包括高速缓冲存储器及实现它们之间联系的数据、控制的总线。电子计算机三大核心部件就是CPU、内部存储器、输入/输出设备。中央处理器的功效主要为处理指令、执行操作、控制时间、处理数据。 [2]在计算机体系结构中，CPU是对计算机的所有硬件资源（如存储器、输入输出单元） 进行控制调配、执行通用运算的核心硬件单元。CPU是计算机的运算和控制核心。计算机系统中所有软件层的操作，最终都将通过指令集映射为CPU的操作。 [1]性能结构播报编辑性能衡量指标对于CPU而言，影响其性能的指标主要有主频、 CPU的位数、CPU的缓存指令集、CPU核心数和IPC（每周期指令数）。所谓CPU的主频，指的就是时钟频率，它直接的决定了CPU的性能，可以通过超频来提高CPU主频来获得更高性能。而CPU的位数指的就是处理器能够一次性计算的浮点数的位数，通常情况下，CPU的位数越高，CPU进行运算时候的速度就会变得越快。21世纪20年代后个人电脑使用的CPU一般均为64位，这是因为64位处理器可以处理范围更大的数据并原生支持更高的内存寻址容量，提高了人们的工作效率。而CPU的缓存指令集是存储在CPU内部的，主要指的是能够对CPU的运算进行指导以及优化的硬程序。一般来讲，CPU 的缓存可以分为一级缓存、二级缓存和三级缓存，缓存性能直接影响CPU处理性能。部分特殊职能的CPU可能会配备四级缓存。 [4]CPU结构通常来讲，CPU的结构可以大致分为运算逻辑部件、寄存器部件和控制部件等。所谓运算逻辑部件，主要能够进行相关的逻辑运算，如：可以执行移位操作以及逻辑操作，除此之外还可以执行定点或浮点算术运算操作以及地址运算和转换等命令，是一种多功能的运算单元。而寄存器部件则是用来暂存指令、数据和地址的。控制部件则是主要用来对指令进行分析并且能够发出相应的控制信号。对于中央处理器来说，可将其看作一个规模较大的集成电路，其主要任务是加工和处理各种数据。传统计算机的储存容量相对较小，其对大规模数据的处理过程中具有一定难度，且处理效果相对较低。随着我国信息技术水平的迅速发展，随之出现了高配置的处理器计算机，将高配置处理器作为控制中心，对提高计算机CPU的结构功能发挥重要作用。中央处理器中的核心部分就是控制器、运算器，其对提高计算机的整体功能起着重要作用，能够实现寄存控制、逻辑运算、信号收发等多项功能的扩散，为提升计算机的性能奠定良好基础。 [2]集成电路在计算机内起到了调控信号的作用，根据用户操作指令执行不同的指令任务。中央处理器是一块超大规模的集成电路。它由运算器、控制器、寄存器等组成，如下图，关键操作在于对各类数据的加工和处理。 [5]中央处理器结构 [5]传统计算机存储容量较小，面对大规模数据集的操作效率偏低。新一代计算机采用高配置处理器作为控制中心，CPU在结构功能方面有了很大的提升空间。中央处理器以运算器、控制器为主要装置，逐渐扩散为逻辑运算、寄存控制、程序编码、信号收发等多项功能。这些都加快了CPU调控性能的优化升级。 [5]CPU总线CPU总线是在计算机系统中最快的总线，同时也是芯片组与主板的核心。人们通常把和CPU直接相连的局部总线叫做CPU总线或者称之为内部总线，将那些和各种通用的扩展槽相接的局部总线叫做系统总线或者是外部总线。在内部结构比较单一的CPU中，往往只设置一组数据传送的总线即CPU内部总线，用来将CPU内部的寄存器和算数逻辑运算部件等连接起来，因此也可以将这一类的总线称之为ALU总线。而部件内的总线，通过使用一组总线将各个芯片连接到一起，因此可以将其称为部件内总线，一般会包含地址线以及数据线这两组线路。系统总线指的是将系统内部的各个组成部分连接在一起的线路，是将系统的整体连接到一起的基础；而系统外的总线，是将计算机和其他的设备连接到一起的基础线路。 [4]核心部分播报编辑运算器运算器是指计算机中进行各种算术和逻辑运算操作的部件， 其中算术逻辑单元是中央处理核心的部分。 [2]（1）算术逻辑单元（ALU）。算术逻辑单元是指能实现多组 算术运算与逻辑运算的组合逻辑电路，其是中央处理中的重要组成部分。算术逻辑单元的运算主要是进行二位元算术运算，如加法、减法、乘法。在运算过程中，算术逻辑单元主要是以计算机指令集中执行算术与逻辑操作，通常来说，ALU能够发挥直接读入读出的作用，具体体现在处理器控制器、内存及输入输出设备等方面，输入输出是建立在总线的基础上实施。输入指令包含一 个指令字，其中包括操作码、格式码等。 [2]（2）中间寄存器（IR）。其长度为 128 位，其通过操作数来决定实际长度。IR 在“进栈并取数”指令中发挥重要作用，在执行该指令过程中，将ACC的内容发送于IR，之后将操作数取到ACC，后将IR内容进栈。 [2]（3）运算累加器（ACC）。当前的寄存器一般都是单累加器，其长度为128位。对于ACC来说，可以将它看成可变长的累加器。在叙述指令过程中，ACC长度的表示一般都是将ACS的值作为依据，而ACS长度与 ACC 长度有着直接联系，ACS长度的加倍或减半也可以看作ACC长度加倍或减半。 [2]（4）描述字寄存器（DR）。其主要应用于存放与修改描述字中。DR的长度为64位，为了简化数据结构处理，使用描述字发挥重要作用。 [2]（5）B寄存器。其在指令的修改中发挥重要作用，B 寄存器长度为32位，在修改地址过程中能保存地址修改量，主存地址只能用描述字进行修改。指向数组中的第一个元素就是描述字， 因此，访问数组中的其它元素应当需要用修改量。对于数组成员来说，其是由大小一样的数据或者大小相同的元素组成的，且连续存储，常见的访问方式为向量描述字，因为向量描述字中的地址为字节地址，所以，在进行换算过程中，首先应当进行基本地址 的相加。对于换算工作来说，主要是由硬件自动实现，在这个过程中尤其要注意对齐，以免越出数组界限。 [2]控制器控制器是指按照预定顺序改变主电路或控制电路的接线和 改变电路中电阻值来控制电动机的启动、调速、制动与反向的主令装置。控制器由程序状态寄存器PSR，系统状态寄存器SSR， 程序计数器PC，指令寄存器等组成，其作为“决策机构”，主要任务就是发布命令，发挥着整个计算机系统操作的协调与指挥作用。 控制的分类主要包括两种，分别为组合逻辑控制器、微程序控制器，两个部分都有各自的优点与不足。其中组合逻辑控制器结构相对较复杂，但优点是速度较快；微程序控制器设计的结构简单，但在修改一条机器指令功能中，需对微程序的全部重编。 [2]品牌介绍播报编辑“龙芯”系列芯片“龙芯”系列芯片是由中国科学院中科技术有限公司设计研制的，采用MIPS体系结构，具有自主知识产权，产品现包括龙芯1号小CPU、龙芯2号中CPU和龙芯3号大CPU三个系列，此外还包括龙芯7A1000桥片。 龙芯1号系列32/64位处理器专为嵌入式领域设计，主要应用于云终端、工业控制、数据采集、手持终端、网络安全、消费电子等领域，具有低功耗、高集成度及高性价比等特点。其中龙芯lA 32位处理器和龙芯1C 64位处理器稳定工作在266～300 MHz，龙芯1B处理器是一款轻量级32位芯片。龙芯1D处理器是超声波热表、水表和气表的专用芯片。2015年，新一代北斗导航卫星搭载着我国自主研制的龙芯1E和1F芯片，这两颗芯片主要用于完成星间链路的数据处理任务一。 [6]龙芯2号系列是面向桌面和高端嵌入式应用的64位高性能低功耗处理器。龙芯2号产品包括龙芯2E、2F、2H和2K1000等芯片。龙芯2E首次实现对外生产和销售授权。龙芯2F平均性能比龙芯 2E高20%以上，可用于个人计算机、行业终端、工业控制、数据采集、网络安全等领域。龙芯2H于2012年推出正式产品，适用计算机、云终端、网络设备、消费类电子等领域需求，同时可作为HT或者 PCI-e接口的全功能套片使用。2018年，龙芯推出龙芯2K1000处理器，它主要是面向网络安全领域及移动智能领域的双核处理芯片，主频可达1 GHz，可满足工业物联网快速发展、自主可控工业安全体系的需求。 [6]龙芯3号系列是面向高性能计算机、服务器和高端桌面应用的多核处理器，具有高带宽，高性能，低功耗的特征。龙芯3A3000/3B3000处理器采用自主微结构设计，主频可达到1.5 GHz以上；计划2019年面向市场的龙芯3A4000为龙芯第三代产品的首款四核芯片，该芯片基于28nm工艺，采用新研发的GS464V 64位高性能处理器核架构，并实现 256位向量指令，同时优化片内互连和访存通路， 集成64位DDR3/4内存控制器，集成片内安全机 制，主频和性能将再次得到大幅提升。 [6]龙芯7A1000桥片是龙芯的第一款专用桥片组产品，目标是替代AMD RS780+SB710桥片组，为龙芯处理器提供南北桥功能。它于2018年2月份发布，目前搭配龙芯3A3000以及紫光4G DDR3内存应用在一款高性能网络平台上。该方案整体性能相较于3A3000+780e平台有较大提升，具有高国产率、高性能、高可靠性等特点。 [6]Intel根据Intel产品线规划，截止2021年Intel十一代消费级酷睿有六类产品：i9/i7/i5/i3/奔腾/赛扬。此外还有面向服务器的至强铂金/金牌/银牌/铜牌和面向HEDT平台的至强W系列。AMD根据AMD产品线规划，截至2021年AMD锐龙5000系列处理器有Ryzen 9/Ryzen 7/Ryzen 5/Ryzen 3四个消费级产品线。此外还有面向服务器市场的第三代霄龙EPYC处理器和面向HEDT平台的线程撕裂者系列。 [7]上海兆芯上海兆芯集成电路有限公司是成立于2013年的国资控股公司，其生产的处理器采用x86架构，产品主要有开先ZX-A、ZX-c/ZX-C+、 ZX-D、开先KX一 5000和KX一6000；开胜ZX—C+、ZX—D、KH一20000 等。其中开先KX一5000系列处理器采用28 nm工艺，提供4核或8核两种版本，整体性能较上一代产品提升高达140%，达到国际主流通用处理器性能水准，能够全面满足党政桌面办公应用，以及包括4K超高清视频观影等多种娱乐应用需求。开胜KH-20000系列处理器是兆芯面向服务器等设备推出的CPU产品。开先KX-6000系列处理器主频高达3.0 GHz，兼容全系列Windows操作系统及中科方德、中标麒麟、普华等国产自主可控操作系统，性能与Intel第七代的酷睿i5相当。 [6]上海申威申威处理器简称“Sw处理器”，出自于DEC的Alpha 21164，采用Alpha架构，具有完全自主知识产权，其产品有单核Sw-1、双核Sw-2、四核Sw-410、十六核SW-1600/SW-1610等。神威蓝光超级计算机使用了8704片SW一1600，搭载神威睿思操作系统，实现了软件和硬件全部国产化。而基于Sw-26010构建的“神威·太湖之光”超级计算机自2016 年6月发布以来，已连续四次占据世界超级计算机TOP 500榜单第一，“神威·太湖之光”上的两项千万 核心整机应用包揽了2016、2017年度世界高性能计算应用领域最高奖“戈登·贝尔”奖。 [6]分类播报编辑指令集的方式CPU的分类还可以按照指令集的方式将其分为精简指令集计算机(RISC)和复杂指令集计算机(CISC)。RISC指令长度和执行时间恒定，CISC指令长度和执行时间不一定。 RISC 指令的并行的执行程度更好，并且编译器的效率也较高。CISC指令则对不同的任务有着更好的优化，代价是电路复杂且较难提高并行度。典型的CISC指令集有x86微架构，典型的RISC指令集有ARM微架构。但在现代处理器架构中RISC和CISC指令均会在译码环节进行转换，拆分成CPU内部的类RISC指令 [4]嵌入式系统CPU传统的嵌入式领域所指范畴非常广泛，是处理器除了服务器和PC领域之外的主要应用领域。所谓“嵌入式”是指在很多芯片中，其所包含的处理器就像嵌入在里面不为人知一样。 [8]近年来随着各种新技术新领域的进一步发展，嵌入式领域本身也被发展成了几个不同的子领域而产生了分化。 [8]首先是随着智能手机(Mobile Smart Phone)和手持设备(Mobile Device)的发展，移动(Mobile)领域逐渐发展成了规模匹敌甚至超过PC领域的一个独立领域。由于Mobile领域的处理器需要加载Linux操作系统，同时涉及复杂的软件生态，因此，其具有和PC领域一样对软件生态的严重依赖。 [8]其次是实时(Real Time)嵌入式领域。该领域相对而言没有那么严重的软件依赖性，因此没有形成绝对的垄断，但是由于ARM处理器IP商业推广的成功，目前仍然以ARM的处理器架构占大多数市场份额，其他处理器架构譬如Synopsys ARC等也有不错的市场成绩。 [8]最后是深嵌入式领域。该领域更像前面所指的传统嵌入式领域。该领域的需求量非常之大，但往往注重低功耗、低成本和高能效比，无须加载像Linux这样的大型应用操作系统，软件大多是需要定制的裸机程序或者简单的实时操作系统，因此对软件生态的依赖性相对比较低。 [8]大型机CPU大型机，或者称大型主机。大型机使用专用的处理器指令集、操作系统和应用软件。大型机一词，最初是指装在非常大的带框铁盒子里的大型计算机系统，以用来同小一些的小型机和微型机有所区别。 [9]减少大型机CPU消耗是个重要工作。节约每个CPU周期，不仅可以延缓硬件升级，还可以降低基于使用规模的软件授权费。大型机体系结构主要包括以下两点：高度虚拟化，系统资源全部共享。大型机可以整合大量的负载于一体，并实现资源利用率的最大化；异步I/O操作。即当执行I/O操作时CPU将I/O指令交给I/O子系统来完成，CPU自己被释放执行其它指令。因此主机在执行繁重的I/O任务的同时，还可以同时执行其它工作。 [9]控制技术形式播报编辑中央处理器强大的数据处理功有效提升了计算机的工作效率，在数据加工操作时，并不仅仅只是一项简单的操作，中央处理器的操作是建立在计算机使用人员下达的指令任务基础上，在执行指令任务过程中，实现用户输入的控制指令与CPU的相对应。随着我国信息技术的快速发展，计算机在人们生活、工作 以及企业办公自动化中得到广泛应用，其作为一种主控设备，为促进电子商务网络的发展起着促进作用，使 CPU 控制性能的升级进程得到很大提高。指令控制、实际控制、操作控制等就是计算机CPU技术应用作用表现。 [2]（1）选择控制。集中处理模式的操作，是建立在具体程序指令的基础上实施，以此满足计算机使用者的需求，CPU 在操作过程中可以根据实际情况进行选择，满足用户的数据流程需求。 指令控制技术发挥的重要作用。根据用户的需求来拟定运算方式，使数据指令动作的有序制定得到良好维持。CPU在执行当中，程序各指令的实施是按照顺利完成，只有使其遵循一定顺序，才能保证计算机使用效果。CPU主要是展开数据集自动化处理，其 是实现集中控制的关键，其核心就是指令控制操作。 [2]（2）插入控制。CPU 对于操作控制信号的产生，主要是通过指令的功能来实现的，通过将指令发给相应部件，达到控制这些部件的目的。实现一条指令功能，主要是通过计算机中的部件执行一序列的操作来完成。较多的小控制元件是构建集中处理模式的关键，目的是为了更好的完成CPU数据处理操作。 [2]（3）时间控制。将时间定时应用于各种操作中，就是所谓的时间控制。在执行某一指令时，应当在规定的时间内完成，CPU的指令是从高速缓冲存储器或存储器中取出，之后再进行指令译码操作，主要是在指令寄存器中实施，在这个过程中，需要注意严格控制程序时间。 [2]与GPU比较播报编辑GPUGPU即图像处理器，CPU和GPU的工作流程和物理结构大致是类似的，相比于CPU而言，GPU的工作更为单一。在大多数的个人计算机中，GPU仅仅是用来绘制图像的。如果CPU想画一个二维图形，只需要发个指令给GPU，GPU就可以迅速计算出该图形的所有像素，并且在显示器上指定位置画出相应的图形。由于GPU会产生大量的热量，所以通常显卡上都会有独立的散热装置。 [3]设计结构CPU有强大的算术运算单元，可以在很少的时钟周期内完成算术计算。同时，有很大的缓存可以保存很多数据在里面。此外，还有复杂的逻辑控制单元，当程序有多个分支的时候， 通过提供分支预测的能力来降低延时。GPU是基于大的吞吐量设计，有很多的算术运算单元和很少的缓存。同时GPU支持大量的线程同时运行，如果他们需要访问同一个数据，缓存会合并这些访问，自然会带来延时的问题。尽管有延时，但是因为其算术运算单元的数量庞大，因此能够达到一个非常大的吞吐量的效果。  [3]使用场景显然，因为CPU有大量的缓存和复杂的逻辑控制单元，因此它非常擅长逻辑控制、串行的运算。相比较而言，GPU因为有大量的算术运算单元，因此可以同时执行大量的计算工作，它所擅长的是大规模的并发计算， 计算量大但是没有什么技术含量，而且要重复很多次。这样一说，我们利用GPU来提高程序运算速度的方法就显而易见了。使用CPU来做复杂的逻辑控制，用GPU来做简单但是量大的算术运算，就能够大大地提高程序的运行速度。 [3]安全问题播报编辑CPU 蓬勃发展的同时也带来了许多的安全问题。1994 年出现在Pentium处理器上的 FDIV bug（奔腾浮点除错误）会导致浮点数除法出现错误；1997年Pentium处理器上的F00F异常指令可导致CPU死机；2011年Intel处理器可信执行技术(TXT，trusted execution technology)存在缓冲区溢出问题，可被攻击者用于权限提升；2017年 Intel管理引擎(ME，management engine)组件中的漏洞可导致远程非授权的任意代码执行；2018年，Meltdown 和Spectre两个CPU漏洞几乎影响到过去20年制造的每一种计算设备，使得存储在数十亿设备上的隐私信息存在被泄露的风险。这些安全问题严重危害国家网络安全、关键基础设施安全及重要行业的信息安全，已经或者将要造成巨大损失。 [1]未来发展播报编辑通用中央处理器(CPU)芯片是信息产业的基础部件，也是武器装备的核心器件。我国缺少具有自主知识产权的CPU技术和产业，不仅造成信息 产业受制于人，而且国家安全也难以得到全面保障。 “十五”期间，国家“863计划”开始支持自主研发CPU。“十一五”期间，“核心电子器件、高端通用芯片及基础软件产品”(“核高基”)重大专项将“863计 划”中的CPU成果引入产业。从“十二五”开始，我国在多个领域进行自主研发CPU的应用和试点，在一定范围内形成了自主技术和产业体系，可满足武器装备、信息化等领域的应用需求。但国外CPU垄断已久，我国自主研发CPU产品和市场的成熟还需要一定时间。 [10]

片内总线：
例如：现用2K*8位的静态RAM芯片构成8K*8位存储器，若CPU输出的地址信号为20位。片内地址总线就是11位(2Kb=211b )。

计算机网络：
定义分类播报编辑按广义计算机网络链接示意图计算机网络也称计算机通信网。关于计算机网络的最简单定义是：一些相互连接的、以共享资源为目的的、自治的计算机的集合。若按此定义，则早期的面向终端的网络都不能算是计算机网络，而只能称为联机系统（因为那时的许多终端不能算是自治的计算机）。但随着硬件价格的下降，许多终端都具有一定的智能，因而“终端”和“自治的计算机”逐渐失去了严格的界限。若用微型计算机作为终端使用，按上述定义，则早期的那种面向终端的网络也可称为计算机网络。 [2]另外，从逻辑功能上看，计算机网络是以传输信息为基础目的，用通信线路将多个计算机连接起来的计算机系统的集合，一个计算机网络组成包括传输介质和通信设备。 [2]从用户角度看，计算机网络是这样定义的：存在着一个能为用户自动管理的网络操作系统。由它调用完成用户所调用的资源，而整个网络像一个大的计算机系统一样，对用户是透明的。 [2]一个比较通用的定义是：利用通信线路将地理上分散的、具有独立功能的计算机系统和通信设备按不同的形式连接起来，以功能完善的网络软件及协议实现资源共享和信息传递的系统。 [2]从整体上来说计算机网络就是把分布在不同地理区域的计算机与专门的外部设备用通信线路互联成一个规模大、功能强的系统，从而使众多的计算机可以方便地互相传递信息，共享硬件、软件、数据信息等资源。简单来说，计算机网络就是由通信线路互相连接的许多自主工作的计算机构成的集合体。 [2]最简单的计算机网络就只有两台计算机和连接它们的一条链路，即两个节点和一条链路。 [2]按连接计算机网络就是通过线路互连起来的、自治的计算机集合，确切的说就是将分布在不同地理位置上的具有独立工作能力的计算机、终端及其附属设备用通信设备和通信线路连接起来，并配置网络软件，以实现计算机资源共享的系统。 [2]按需求计算机网络（computer networks）就是由大量独立的、但相互连接起来的计算机共同完成计算机任务的系统。 [3]发展历程播报编辑自从计算机网络出现以后，它的发展速度与应用的广泛程度十分惊人。纵观计算机网络的发展，其大致经历了以下四个阶段： [2]诞生阶段20世纪60年代中期之前的第一代计算机网络是以单个计算机为中心的远程联机系统，典型应用是由一台计算机和全美范围内2000多个终端组成的飞机订票系统，终端是一台计算机的外围设备，包括显示器和键盘，无CPU和内存。随着远程终端的增多，在主机前增加了前端机(FEP)。当时，人们把计算机网络定义为“以传输信息为目的而连接起来，实现远程信息处理或进一步达到资源共享的系统”，这样的通信系统已具备网络的雏形。 [2]形成阶段20世纪60年代中期至70年代的第二代计算机网络是以多个主机通过通信线路互联起来，为用户提供服务，兴起于60年代后期，典型代表是美国国防部高级研究计划局协助开发的ARPANET。主机之间不是直接用线路相连，而是由接口报文处理机(IMP)转接后互联的。IMP和它们之间互联的通信线路一起负责主机间的通信任务，构成了通信子网。通信子网互联的主机负责运行程序，提供资源共享，组成资源子网。这个时期，网络概念为“以能够相互共享资源为目的互联起来的具有独立功能的计算机之集合体”，形成了计算机网络的基本概念。 [2]互联互通阶段20世纪70年代末至90年代的第三代计算机网络是具有统一的网络体系结构并遵守国际标准的开放式和标准化的网络。ARPANET兴起后，计算机网络发展迅猛，各大计算机公司相继推出自己的网络体系结构及实现这些结构的软硬件产品。由于没有统一的标准，不同厂商的产品之间互联很困难，人们迫切需要一种开放性的标准化实用网络环境，这样应运而生了两种国际通用的最重要的体系结构，即TCP/IP体系结构和国际标准化组织的OSI体系结构。 [2]高速网络技术阶段20世纪90年代至今的第四代计算机网络，由于局域网技术发展成熟，出现光纤及高速网络技术，整个网络就像一个对用户透明的大的计算机系统，发展为以因特网( Internet)为代表的互联网。 [2]组成播报编辑计算机网络的分类与一般的事物分类方法一样，可以按事物所具有的不同性质特点（即事物的属性）分类。计算机网络通俗地讲就是由多台计算机（或其它计算机网络设备）通过传输介质和软件物理（或逻辑）连接在一起组成的。总的来说计算机网络的组成基本上包括：计算机、网络操作系统、传输介质（可以是有形的，也可以是无形的，如无线网络的传输介质就是空间）以及相应的应用软件四部分。 [3]功能播报编辑数据通信数据通信是计算机网络的最主要的功能之一。数据通信是依照一定的通信协议，利用数据传输技术在两个终端之间传递数据信息的一种通信方式和通信业务。它可实现计算机和计算机、计算机和终端以及终端与终端之间的数据信息传递，是继电报、电话业务之后的第三种最大的通信业务。数据通信中传递的信息均以二进制数据形式来表现，数据通信的另一个特点是总是与远程信息处理相联系，是包括科学计算、过程控制、信息检索等内容的广义的信息处理。 [2]资源共享资源共享是人们建立计算机网络的主要目的之一。计算机资源包括硬件资源、软件资源和数据资源。硬件资源的共享可以提高设备的利用率，避免设备的重复投资，如利用计算机网络建立网络打印机；软件资源和数据资源的共享可以充分利用已有的信息资源，减少软件开发过程中的劳动，避免大型数据库的重复建设。 [2]集中管理计算机网络技术的发展和应用，已使得现代的办公手段、经营管理等发生了变化。目前，已经有了许多管理信息系统、办公自动化系统等，通过这些系统可以实现日常工作的集中管理，提高工作效率，增加经济效益。 [2]实现分布式处理网络技术的发展，使得分布式计算成为可能。对于大型的课题，可以分为许许多多小题目，由不同的计算机分别完成，然后再集中起来，解决问题。 [2]负荷均衡负荷均衡是指工作被均匀的分配给网络上的各台计算机系统。网络控制中心负责分配和检测，当某台计算机负荷过重时，系统会自动转移负荷到较轻的计算机系统去处理。 [2]由此可见，计算机网络可以大大扩展计算机系统的功能，扩大其应用范围，提高可靠性，为用户提供方便，同时也减少了费用，提高了性能价格比。 [2]分类播报编辑虽然网络类型的划分标准各种各样，但是从地理范围划分是一种大家都认可的通用网络划分标准。按这种标准可以把各种网络类型划分为局域网、城域网、广域网和互联网四种。局域网一般来说只能是一个较小区域内，城域网是不同地区的网络互联，不过在此要说明的一点就是这里的网络划分并没有严格意义上地理范围的区分，只能是一个定性的概念。下面简要介绍这几种计算机网络。 [4]局域网局域网（Local Area Network；LAN） 通常我们常见的“LAN”就是指局域网，这是我们最常见、应用最广的一种网络。局域网随着整个计算机网络技术的发展和提高得到充分的应用和普及，几乎每个单位都有自己的局域网，有的甚至家庭中都有自己的小型局域网。很明显，所谓局域网，那就是在局部地区范围内的网络，它所覆盖的地区范围较小。局域网在计算机数量配置上没有太多的限制，少的可以只有两台，多的可达几百台。一般来说在企业局域网中，工作站的数量在几十到两百台次左右。在网络所涉及的地理距离上一般来说可以是几米至10公里以内。局域网一般位于一个建筑物或一个单位内，不存在寻径问题，不包括网络层的应用。 [4]这种网络的特点就是：连接范围窄、用户数少、配置容易、连接速率高。目前局域网最快的速率要算现今的10G以太网了。IEEE的802标准委员会定义了多种主要的LAN网：以太网（Ethernet）、令牌环网（Token Ring）、光纤分布式接口网络（FDDI）、异步传输模式网（ATM）以及最新的无线局域网（WLAN）。这些都将在后面详细介绍。 [4]城域网城域网示意图（Metropolitan Area Network；MAN） 这种网络一般来说是在一个城市，但不在同一地理小区范围内的计算机互联。这种网络的连接距离可以在10￣100公里，它采用的是IEEE802.6标准。MAN与LAN相比扩展的距离更长，连接的计算机数量更多，在地理范围上可以说是LAN网络的延伸。在一个大型城市或都市地区，一个MAN网络通常连接着多个LAN网。如连接政府机构的LAN、医院的LAN、电信的LAN、公司企业的LAN等等。由于光纤连接的引入，使MAN中高速的LAN互连成为可能。 [4]城域网多采用ATM技术做骨干网。ATM是一个用于数据、语音、视频以及多媒体应用程序的高速网络传输方法。ATM包括一个接口和一个协议，该协议能够在一个常规的传输信道上，在比特率不变及变化的通信量之间进行切换。ATM也包括硬件、软件以及与ATM协议标准一致的介质。ATM提供一个可伸缩的主干基础设施，以便能够适应不同规模、速度以及寻址技术的网络。ATM的最大缺点就是成本太高，所以一般在政府城域网中应用，如邮政、银行、医院等。 [4]广域网广域示意图（Wide Area Network，WAN） 这种网络也称为远程网，所覆盖的范围比城域网（MAN）更广，它一般是在不同城市之间的LAN或者MAN网络互联，地理范围可从几百公里到几千公里。因为距离较远，信息衰减比较严重，所以这种网络一般是要租用专线，通过IMP（接口信息处理）协议和线路连接起来，构成网状结构，解决循径问题。这种城域网因为所连接的用户多，总出口带宽有限，所以用户的终端连接速率一般较低，通常为9.6Kbps-45Mbps 如：邮电部的CHINANET，CHINAPAC，和CHINADDN网。 [4]上面讲了网络的几种分类，其实在现实生活中我们真正遇得最多的还要算是局域网，因为它可大可小，无论在单位还是在家庭实现起来都比较容易，应用也是最广泛的一种网络，所以在下面我们有必要对局域网及局域网中的接入设备作一个进一步的认识。 [4]无线网无线网随着笔记本电脑（notebook computer）和个人数字助理（ Personal Digital Assistant，PDA）等便携式计算机的日益普及和发展，人们经常要在路途中接听电话、发送传真和电子邮件阅读网上信息以及登录到远程机器等。然而在汽车或飞机上是不可能通过有线介质与单位的网络相连接的，这时候可能会对无线网感兴趣了。虽然无线网与移动通信经常是联系在一起的，但这两个概念并不完全相同。例如当便携式计算机通过PCMCIA卡接入电话插口，它就变成有线网的一部分。另一方面，有些通过无线网连接起来的计算机的位置可能又是固定不变的，如在不便于通过有线电缆连接的大楼之间就可以通过无线网将两栋大楼内的计算机连接在一起。 [4]无线网特别是无线局域网有很多优点，如易于安装和使用。但无线局域网也有许多不足之处：如它的数据传输率一般比较低，远低于有线局域网；另外无线局域网的误码率也比较高，而且站点之间相互干扰比较厉害。用户无线网的实现有不同的方法。国外的某些大学在它们的校园内安装许多天线，允许学生们坐在树底下查看图书馆的资料。这种情况是通过两个计算机之间直接通过无线局域网以数字方式进行通信实现的。另一种可能的方式是利用传统的模拟调制解调器通过蜂窝电话系统进行通信。在国外的许多城市已能提供蜂窝式数字信息分组数据（ Cellular Digital Packet Data，CDPD）的业务，因而可以通过CDPD系统直接建立无线局域网。无线网络是当前国内外的研究热点，无线网络的研究是由巨大的市场需求驱动的。无线网的特点是使用户可以在任何时间、任何地点接入计算机网络，而这一特性使其具有强大的应用前景。当前已经出现了许多基于无线网络的产品，如个人通信系统（ Personal CommunicationSystem，PCS）电话、无线数据终端、便携式可视电话、个人数字助理（ PDA）等。无线网络的发展依赖于无线通信技术的支持。无线通信系统主要有：低功率的无绳电话系统、模拟蜂窝系统、数字蜂窝系统、移动卫星系统、无线LAN和无线WAN等。 [4]性能播报编辑计算机网络的性能一般是指它的几个重要的性能指标。但除了这些重要的性能指标外，还有一些非性能特征，它们对计算机网络的性能也有很大的影响。 [5]1．计算机网络的性能指标性能指标从不同的方面来度量计算机网络的性能。 [5]（1）速率计算机发送出的信号都是数字形式的。比特是计算机中数据量的单位，也是信息论中使用的信息量的单位。英文字bit来源于binary digit，意思是一个“二进制数字”，因此一个比特就是二进制数字中的一个1或0。网络技术中的速率指的是连接在计算机网络上的主机在数字信道上传送数据的速率，它也称为数据率（data rate）或比特率（bit rate）。速率是计算机网络中最重要的一个性能指标。速率的单位是bit/s（比特每秒）（即bit per second）。 [5]（2）带宽“带宽”有以下两种不同的意义。 [5]① 带宽本来是指某个信号具有的频带宽度。信号的带宽是指该信号所包含的各种不同频率成分所占据的频率范围。例如，在传统的通信线路上传送的电话信号的标准带宽是3.1kHz（从300Hz到3.4kHz，即话音的主要成分的频率范围）。这种意义的带宽的单位是赫（或千赫，兆赫，吉赫等）。 [5]② 在计算机网络中，带宽用来表示网络的通信线路所能传送数据的能力，因此网络带宽表示在单位时间内从网络中的某一点到另一点所能通过的“最高数据率”。这里一般说到的“带宽”就是指这个意思。这种意义的带宽的单位是“比特每秒”，记为bit/s。 [5]（3）吞吐量吞吐量表示在单位时间内通过某个网络（或信道、接口）的数据量。吞吐量更经常地用于对现实世界中的网络的一种测量，以便知道实际上到底有多少数据量能够通过网络。显然，吞吐量受网络的带宽或网络的额定速率的限制。例如，对于一个100Mbit/s的以太网，其额定速率是100Mbit/s，那么这个数值也是该以太网的吞吐量的绝对上限值。因此，对100Mbit/s的以太网，其典型的吞吐量可能也只有70Mbit/s。有时吞吐量还可用每秒传送的字节数或帧数来表示。 [5]（4）时延时延是指数据（一个报文或分组，甚至比特）从网络（或链路）的一端传送到另一端所需的时间。时延是个很重要的性能指标，它有时也称为延迟或迟延。网络中的时延是由以下几个不同的部分组成的。 [5]① 发送时延。发送时延是主机或路由器发送数据帧所需要的时间，也就是从发送数据帧的第一个比特算起，到该帧的最后一个比特发送完毕所需的时间。 [5]因此发送时延也叫做传输时延。发送时延的计算公式是： [5]发送时延=数据帧长度（bit/s）/信道带宽（bit/s） [5]由此可见，对于一定的网络，发送时延并非固定不变，而是与发送的帧长（单位是比特）成正比，与信道带宽成反比。 [5]② 传播时延。传播时延是电磁波在信道中传播一定的距离需要花费的时间。传播时延的计算公式是： [5]传播时延=信道长度（m）/电磁波在信道上的传播速率（m/s） [5]电磁波在自由空间的传播速率是光速，即300000km/s。电磁波在网络传输媒体中的传播速率比在自由空间要略低一些。 [5]③ 处理时延。主机或路由器在收到分组时要花费一定的时间进行处理，例如分析分组的首部，从分组中提取数据部分，进行差错检验或查找适当的路由等，这就产生了处理时延。 [5]④ 排队时延。分组在经过网络传输时，要经过许多的路由器。但分组在进入路由器后要先在输入队列中排队等待处理。在路由器确定了转发接口后，还要在输出队列中排队等待转发。这就产生了排队时延。 [5]这样，数据在网络中经历的总时延就是以上四种时延之和： [5]总时延=发送时延+传播时延+处理时延+排队时延 [5]（5）时延带宽积把以上讨论的网络性能的两个度量—传播时延和带宽相乘，就得到另一个很有用的度量：传播时延带宽积，即时延带宽积=传播时延×带宽。 [5]（6）往返时间（RTT）在计算机网络中，往返时间也是一个重要的性能指标，它表示从发送方发送数据开始，到发送方收到来自接收方的确认（接受方收到数据后便立即发送确认）总共经历的时间。 [5]当使用卫星通信时，往返时间（RTT）相对较长。 [5]（7）利用率利用率有信道利用率和网络利用率两种。信道利用率指某信道有百分之几的时间是被利用的（有数据通过），完全空闲的信道的利用率是零。网络利用率是全网络的信道利用率的加权平均值。 [5]2．计算机网络的非性能特征这些非性能特征与前面介绍的性能指标有很大的关系。 [5]（1）费用即网络的价格（包括设计和实现的费用）。网络的性能与其价格密切相关。一般说来，网络的速率越高，其价格也越高。 [5]（2）质量网络的质量取决于网络中所有构件的质量，以及这些构件是怎样组成网络的。网络的质量影响到很多方面，如网络的可靠性、网络管理的简易性，以及网络的一些性能。但网络的性能与网络的质量并不是一回事，例如，有些性能也还可以的网络，运行一段时间后就出现了故障，变得无法再继续工作，说明其质量不好。高质量的网络往往价格也较高。 [5]（3）标准化网络的硬件和软件的设计既可以按照通用的国际标准，也可以遵循特定的专用网络标准。最好采用国际标准的设计，这样可以得到更好的互操作性，更易于升级换代和维修，也更容易得到技术上的支持。 [5]（4）可靠性可靠性与网络的质量和性能都有密切关系。速率更高的网络，其可靠性不一定会更差。但速率更高的网络要可靠地运行，则往往更加困难，同时所需的费用也会较高。 [5]（5）可扩展性和可升级性网络在构造时就应当考虑到今后可能会需要扩展（即规模扩大）和升级（即性能和版本的提高）。网络的性能越高，其扩展费用往往也越高，难度也会相应增加。 [5]（6）易于管理和维护网络如果没有良好的管理和维护，就很难达到和保持所设计的性能。 [5]相关应用播报编辑21世纪人类将全面进入信息时代。信息时代的重要特征就是数字化、网络化和信息化。要实现信息化就必须依靠完善的网络，因为网络可以非常迅速地传递信息。因此网络现在已经成为信息社会的命脉和发展知识经济的重要基础。网络对社会生活的很多方面以及对社会经济的发展已经产生了不可估量的影响。 [3]这里所说的网络是指“三网”，即电信网络、有线电视网络和计算机网络。这三种网络向用户提供的服务不同。电信网络的用户可得到电话、电报以及传真等服务；有线电视网络的用户能够观看各种电视节目；计算机网络则可使用户能够迅速传送数据文件，以及从网络上查找并获取各种有用资料，包括图像和视频文件。这三种网络在信息化过程中都起到十分重要的作用，但其中发展最快的并起到核心作用的是计算机网络。随着技术的发展，电信网络和有线电视网络都逐渐融入了现代计算机网络（也称计算机通信网）的技术，这就产生了“网络融合”的概念。 [3]自从20世纪90年代以后，以因特网（Internet）为代表的计算机网络得到了飞速的发展，已从最初的教育科研网络逐步发展成为商业网络，并已成为仅次于全球电话网的世界第二大网络。因特网正在改变着我们工作和生活的各个方面，它已经给很多国家带来了巨大的好处，并加速了全球信息革命的进程。因特网是人类自印刷术发明以来在通信方面最大的变革。现在，人们的生活、工作、学习和交往都已离不开因特网了。 [3]计算机网络向用户提供的最重要的功能有两个，即连通性和共享。 [3]为什么会建立这么多的计算机网络，主要还是因为计算机网络的运用受到个人和公司的青睐。 [3]一、商业运用。1、主要是实现资源共享（resource sharing）最终打破地理位置束缚（tyranny of geography）,主要运用客户-服务器模型（client-server model）。 [3]2、提供强大的通信媒介（communication medium）。如：电子邮件（E-mail）、视频会议。 [3]3、电子商务活动。如：各种不同供应商购买子系统，然后在将这些部件组装起来。 [3]4、通过Internet与客户做各种交易。如：书店、音像在家里购买商品或者服务。 [3]二、家庭运用1、访问远程信息。如：浏览Web页面获得艺术、商务、烹饪、政府、健康、历史、爱好、娱乐、科学、运动、旅游等等信息。 [3]2、个人之间的通信。如：即时消息（instant messaging）运用<QQ、MSN、YY>、聊天室、对等通信（peer-to-communication）<通过中心数据库共享，各大网盘，但是容易造成侵犯版权>。 [3]3、交互式娱乐。如：视频点播、即时评论及参加活动<电视直播网络互动>、网络游戏。 [3]4、广义的电子商务。如：电子方式支付账单、管理银行账户、处理投资。 [3]三、移动用户以无线网络为基础。 [3]1、可移动的计算机：笔记本计算机、PDA、3G手机。 [3]2、军事：一场战争不可能靠局域网设备通信。 [3]3、运货车队、出租车、快递专车等应用。 [3]四、社会问题网络的广泛运用已经导致了新的社会、伦理和政治问题。 [3]

局域网：
局域网简介播报编辑局域网自然就是局部地区形成的一个区域网络，其特点就是分布地区范围有限，可大可小，大到一栋建筑楼 与相邻建筑之间的连接，小到可以是办公室之间的联系。局域网自身相对其他网络传输速度更快，性能更稳定，框架简易，并且是封闭性，这也是很多机构选择的原因所在。局域网自身的组成大体由计算机设备、网络连接设备、网络传输介质3大部分构成，其中，计算机设备又包括服务器与工作站，网络连接设备则包含了网卡、集线器、交换机，网络传输介质简单来说就是网线，由同轴电缆、双绞线及光缆3大原件构成。 [2]局域网是一种私有网络，一般在一座建筑物内或建筑物附近，比如家庭、办公室或工厂。局域网络被广泛用来连接个人计算机和消费类电子设备，使它们能够共享资源和交换信息。当局域网被用于公司时，它们就称为企业网络。 [3]局域网将一定区域内的各种计算机、外部设备和数据库连接起来形成计算机通信网，通过专用数据线路与其他地方的局域网或数据库连接，形成更大范围的信息处理系统。局域网通过网络传输介质将网络服务器、网络工作站、打印机等网络互联设备连接起来，实现系统管理文件，共享应用软件、办公设备，发送工作日程安排等通信服务。局域网为封闭型网络，在一定程度上能够防止信息泄露和外部网络病毒攻击，具有较高的安全性，但是 一旦发生黑客攻击等事件，极有可能导致局域网整体出现瘫痪，网络内的所有工作无法进行，甚至泄露大量公司机密，对公司事业发展造成重创。2017 年国家发布《中华人民共和国网络安全法》， 6月1日正式施行，从法律角度对网络安全和信息安全做出 了明确规定，对网络运营者、使用者都提出了相应的要求，以提高网络使用的安全性。 [4]无线局域网播报编辑无线局域网，简称WLAN，是在几千米范围内的公司楼群或是商场内的计算机互相连接所组建的计算机网络，一个无线局域网能支持几台到几千台计算机的使用。现如今无线局域网的应用已经越来越多。现在的校园、商场、公司以及高铁都在应用。无线局域网的应用为我们的生活和工作都带来很大的帮助，不仅能够快速传输人们所需要的信息，还能让人们在到联网中的联系更加快捷方便。 [5]无线局域网近来受到非常大的欢迎，尤其是家庭、旧办公楼、食堂和其他一些安装电缆太麻烦的场地。在这些系统中，每台计算机都有一个无线调制解调器和一个天线，用来与其他计算机通信。在大多数情况下，每台计算机与安装在天花板上的一个设备通信。这个设备，成为接入点、无线路由器或者基站，它主要负责中继无线计算机之间的数据包，还负责中继无线计算机和Internet之间的数据包。 [3]无线局域网的一个标准称为IEEE 802.11，俗称WIFI，已经非常广泛地使用。 [3]局域网的组建组建目标无线局域网可以传输音频、视频、文字。现在很多公司和校园都在用无线局域网。不仅能够提高办公的效率，还能快速传递信息。无线局域网的组建、维护管理都非常简单，而且很少被干扰。而且还能够节省网络费用的开支。无线局域网的组建目标主要有两个标准：第一，灵活性和独立性较强。无线局域网的部件的和相关设备的摆放不受任何空间限制。用户在连接到无线局域网以后。可以用自己是手机或笔记本等设备与系统网络进行连接，也不会影响到无线局域网的正常使用。第二，扩展性和先进性好。无线局域网的组建结构是非常简单。它会随着现在科技信息技术的发展而进行更新，提升性能和升级系统，从而使得信息传输更加快速。 [5]无线局域网应用在日常生活中的应用无线局域网的实现协议众多，当前最广泛使用的当属Wi-Fi，只需要一个路由器，即可以达到让所有具有无线功能的设备组成一个无线局域网，非常方便灵活。 目前大多数无线局域网是基于IEEE802.11标准，在这个标准下的无线局域网大多使用的是2.4GHz 或5GHz的射频。 家庭一般只需要一个路由器就可以组建小型的无线局域网络，中等规模的企业通过多个路由器以及交换机， 就能组建覆盖整个企业的中型无线局域网络，而大型企业则是需要通过一些中心化的无线控制器来组建强大的覆盖面广的大型无线局域网络。 [6]在不同行业中的应用无线局域网在医院中的应用对于医疗工作有很大的帮助，医生在查房时，需要随时查看患者的病例，然后会看患者当时的病情在下医嘱。在使用无线网络以后，医生查房时可以携带能够连接无线网络的平板电脑，随时查看患者的病例，并记录患者当时的病情。无线局域网不仅能为医生和病人提供上网服务，在医院的病人家属和访客都能享受到无线网络的快捷和便利。利用无线网络的定位服务在医院的应用也非常有帮助，医生能够及时定位到患者的位置。当患者出现紧急状况时能够得到及时抢治疗。另外，利用无线定位还可以随时知道药品的具体位置，相关人员在管理药品的库存时更加精确和方便。 [5]金融行业网点众多，建议采用 AC+瘦AP的组网架构，以便于WLAN网络统一管理。前期布网可考虑与运营商进行合作，以减少建设投资。AP的布放需要结合实际环境，如面积、楼层等做具体的适应性调整，部署范围应覆盖电子银行服务区、营业大厅、客户等候区、VIP 客户接待区、理财专区等所有客户有权到达的区域，同时将无线控制器AC部署在核心机房，无线控制器通过N*GE链路旁挂或者在线部署在运营商汇聚交换机或者核心设备 上。 [7]有线局域网播报编辑有线局域网使用了各种不同的传输技术。它们大多使用铜线作为传输介质，但也有一些使用光纤。局域网的大小受到限制，这意味着最坏情况下的传输时间也是有界的，并且事先可以知道。了解这些界限有助于网络协议的设计。通常情况下，有线局域网的运行速度在100Mbps到1Gbps之间，延迟很低（微秒或者纳秒级），而且很少发生错误。较新的局域网可以工作在高达10Gbps的速率。和无线网络相比，有线局域网在性能的所有方面都超过了它们。 [3]许多有线局域网的拓扑结构是以点到点链路为基础的。俗称以太网的IEEE 802.3是迄今为止最常见的一种有线局域网。在交换式以太网中每台计算机按照以太网协议规定的方式运行，通过一条点到点链路连接到一个盒子，这个盒子称为交换机，这就是交换式以太网名字的由来。 [3]特点及分类播报编辑局域网一般为一个部门或单位所有，建网、维护以及扩展等较容易，系统灵活性高。其主要特点是：1.覆盖的地理范围较小，只在一个相对独立的局部范围内联，如一座或集中的建筑群内。2.使用专门铺设的传输介质进行联网，数据传输速率高（10Mb/s～10Gb/s）3.通信延迟时间短，可靠性较高4.局域网可以支持多种传输介质局域网的类型很多，若按网络使用的传输介质分类，可分为有线网和无线网；若按网络拓扑结构分类，可分为总线型、星型、环型、树型、混合型等；若按传输介质所使用的访问控制方法分类，又可分为以太网、令牌环网、FDDI网和无线局域网等。其中，以太网是当前应用最普遍的局域网技术。拓扑结构播报编辑局域网通常是分布在一个有限地理范围内的网络系统，一般所涉及的地理范围只有几公里。局域网专用性非常强，具有比较稳定和规范的拓扑结构。常见的局域网拓朴结构如下：星型星型结构这种结构的网络是各工作站以星形方式连接起来的，网中的每一个节点设备都以中心节为中心，通过连接线与中心节点相连，如果一个工作站需要传输数据，它首先必须通过中心节点。由于在这种结构的网络系统中，中心节点是控制中心，任意两个节点间的通信最多只需两步，所以，能够传输速度快，并且网络构形简单、建网容易、便于控制和管理。但这种网络系统，网络可靠性低，网络共享能力差，并且一旦中心节点出现故障则导致全网瘫痪。树型树形结构网络是天然的分级结构，又被称为分级的集中式网络。其特点是网络成本低，结构比较简单。在网络中，任意两个节点之间不产生回路，每个链路都支持双向传输，并且，网络中节点扩充方便、灵活，寻查链路路径比较简单。但在这种结构网络系统中，除叶节点及其相连的链路外，任何一个工作站或链路产生故障会影响整个网络系统的正常运行。总线型总线性结构总线形结构网络是将各个节点设备和一根总线相连。网络中所有的节点工作站都是通过总线进行信息传输的。作为总线的通信连线可以是同轴电缆、双绞线，也可以是扁平电缆。在总线结构中，作为数据通信必经的总线的负载能量是有限度的，这是由通信媒体本身的物理性能决定的。所以，总线结构网络中工作站节点的个数是有限制的，如果工作站节点的个数超出总线负载能量，就需要延长总线的长度，并加入相当数量的附加转接部件，使总线负载达到容量要求。总线形结构网络简单、灵活，可扩充性能好。所以，进行节点设备的插入与拆卸非常方便。另外，总线结构网络可靠性高、网络节点间响应速度快、共享资源能力强、设备投入量少、成本低、安装使用方便，当某个工作站节点出现故障时，对整个网络系统影响小。因此，总线结构网络是最普遍使用的一种网络。但是由于所有的工作站通信均通过一条共用的总线，所以，实时性较差。环型环形结构是网络中各节点通过一条首尾相连的通信链路连接起来的一个闭合环形结构网。环形结构网络的结构也比较简单，系统中各工作站地位相等。系统中通信设备和线路比较节省。在网中信息设有固定方向单向流动，两个工作站节点之间仅有一条通路，系统中无信道选择问题；某个结点的故障将导致物理瘫痪。环网中，由于环路是封闭的，所以不便于扩充，系统响应延时长，且信息传输效率相对较低。安全问题播报编辑服务器防护能力较弱局域网相较于其他网络，其信息的传播速度较快，传递方式也相对简单，如果局域网中的某一台计算机受到了病毒的入侵，病毒会通过局域网中的信息传播散播到所有计算机当中。虽然有一些局域网中会安装一些杀毒软件，但是因为软件补丁更新不到位，或者有一些计算机没有安装杀毒软件，病毒会利用防护软件的漏洞进行网络攻击，从而导致局域网系统运行瘫痪，造成用户信息泄露、窃取用户财产等问题。 [8]网络边界接入存在风险在局域网网络边界所存在的接入风险主要包括路由的破坏、用户信息的窃听、未经授权的访问等网络设备攻击，以及某些病毒的传播等。对于局域网的运行当中，主要是拒绝服务攻击较多一些，以此造成主机死机、网络服务暂停等。而在大量的SYNFlood、ACKFlooding、UDPFlood等攻击后产生的大量垃圾数据包，使得被攻击方CPU满负荷运转或者是内存不足，造成业务服务器的关键设备业务中断或是服务质量下降。 [8]用户的安全意识薄弱局域网用户在使用网络进行数据传输时，有时会使用到外部存储设备，但是用户没有对外部设备安全检测的习惯，而是直接连接网络进行使用。导致外部数据和病毒一起进入到局域网当中，通过局域网中信息的传播，使得病毒在局域网中进行扩散，从而造成了局域网病毒入侵的情况。另外，有一些用户在进行网站浏览的过程当中，不小心点击到一些弹出的窗口或者是下载了病毒伪装的软件，也会导致计算机中毒，造成用户的信息泄露，威胁到整个局域网的安全。 [8]

并行计算：
定义播报编辑并行计算（Parallel Computing）是指同时使用多种计算资源解决计算问题的过程，是提高计算机系统计算速度和处理能力的一种有效手段。它的基本思想是用多个处理器来协同求解同一问题，即将被求解的问题分解成若干个部分，各部分均由一个独立的处理机来并行计算。并行计算系统既可以是专门设计的、含有多个处理器的超级计算机，也可以是以某种方式互连的若干台的独立计算机构成的集群。通过并行计算集群完成数据的处理，再将处理的结果返回给用户。并行计算可分为时间上的并行和空间上的并行。时间上的并行：是指流水线技术，比如说工厂生产食品的时候步骤分为：1． 清洗：将食品冲洗干净。2． 消毒：将食品进行消毒处理。3． 切割：将食品切成小块。4． 包装：将食品装入包装袋。如果不采用流水线，一个食品完成上述四个步骤后，下一个食品才进行处理，耗时且影响效率。但是采用流水线技术，就可以同时处理四个食品。这就是并行算法中的时间并行，在同一时间启动两个或两个以上的操作，大大提高计算性能。空间上的并行：是指多个处理机并发的执行计算，即通过网络将两个以上的处理机连接起来，达到同时计算同一个任务的不同部分，或者单个处理机无法解决的大型问题。比如小李准备在植树节种三棵树，如果小李1个人需要6个小时才能完成任务，植树节当天他叫来了好朋友小红、小王，三个人同时开始挖坑植树，2个小时后每个人都完成了一颗植树任务，这就是并行算法中的空间并行，将一个大任务分割成多个相同的子任务，来加快问题解决速度。特征播报编辑为利用并行计算，通常计算问题表现为以下特征：（1）将工作分离成离散部分，有助于同时解决；（2）随时并及时地执行多个程序指令；（3）多计算资源下解决问题的耗时要少于单个计算资源下的耗时。基本体系结构播报编辑并行计算科学中主要研究的是空间上的并行问题。从程序和算法设计人员的角度来看，并行计算又可分为数据并行和任务并行。一般来说，因为数据并行主要是将一个大任务化解成相同的各个子任务，比任务并行要容易处理。空间上的并行导致了两类并行机的产生，按照Flynn的说法分为：单指令流多数据流（SIMD）和多指令流多数据流（MIMD）。我们常用的串行机也叫做单指令流单数据流（SISD）。MIMD类的机器又可分为以下常见的五类：并行向量处理机（PVP）、对称多处理机（SMP）、大规模并行处理机（MPP）、工作站机群（COW）、分布式共享存储处理机（DSM）。访存模型并行计算机有以下五种访存模型：均匀访存模型（UMA）非均匀访存模型（NUMA）全高速缓存访存模型（COMA）一致性高速缓存非均匀存储访问模型（CC-NUMA）非远程存储访问模型（NORMA）。计算模型并行求解过程示意图不像串行计算机那样，全世界基本上都在使用冯·诺伊曼的计算模型；并行计算机没有一个统一的计算模型。不过，人们已经提出了几种有价值的参考模型：PRAM模型，BSP模型，LogP模型，C^3模型等。网络设置播报编辑并行计算机是靠网络将各个处理机或处理器连接起来的，一般来说有以下几种方式：处理单元间有着固定连接的一类网络，在程序执行期间，这种点到点的链接保持不变；典型的静态网络有一维线性阵列、二维网孔、树连接、超立方网络、立方环、洗牌交换网、蝶形网络等。静态连接动态连接播报编辑用交换开关构成的，可按应用程序的要求动态地改变连接组态；典型的动态网络包括总线、交叉开关和多级互连网络等。基本术语播报编辑节点度:射入或射出一个节点的边数。在单向网络中，入射和出射边之和称为节点度。网络直径:网络中任何两个节点之间的最长距离，即最大路径数。对剖宽度:对分网络各半所必须移去的最少边数。对剖带宽:每秒钟内，在最小的对剖平面上通过所有连线的最大信息位（或字节)。性能度量播报编辑基本指标执行时间工作负载存储性能加速比评测Amdahl定理Gastofson定理Sun-Ni定理可扩放性标准等效率标准等速度标准平均延迟标准并行计算与云计算云计算是在并行计算之后产生的概念，是由并行计算发展而来， 两者在很多方面有着共性。学习并行计算对于理解云计算有很大的帮助。并行计算是学习云计算必须要学习的基础课程。但并行计算不等于云计算，云计算也不等同并行计算。两者区别如下。（1）云计算萌芽于并行计算云计算的萌芽应该从计算机的并行化开始，并行机的出现是人们不满足于CPU摩尔定率的增长速度，希望把多个计算机并联起来，从而获得更快的计算速度。这是一种很简单也很朴素的实现高速计算的方法，这种方法后来被证明是相当成功的。（2）并行计算、网格计算只用于特定的科学领域，专业的用户并行计算、网格计算的提出主要是为了满足科学和技术领域的专业需要，其应用领域也基本限于科学领域。传统并行计算机的使用是一个相当专业的工作，需要使用者有较高的专业素质，多数是命令行的操作，这是很多专业人士的噩梦，更不用说普通的业余级用户了。（3）并行计算追求的高性能在并行计算的时代，人们极力追求的是高速的计算、采用昂贵的服务器，各国不惜代价在计算速度上超越他国，因此，并行计算时代的高性能机群是一个“快速消费品”，世界TOP500高性能计算机地排名不断地在刷新，一台大型机群如果在3年左右不能得到有效的利用就远远的落后了，巨额投资无法收回。（4）云计算对于单节点的计算能力要求低而云计算时代我们并不去追求使用昂贵的服务器，我们也不用去考虑TOP500的排名，云中心的计算力和存储力可随着需要逐步增加，云计算的基础架构支持这一动态增加的方式，高性能计算将在云计算时代成为“耐用消费品”。

超线程技术：
基本简介播报编辑英特尔® 超线程技术是一项硬件创新，允许在每个内核上运行多个线程。更多的线程意味着可以并行完成更多的工作。  [6]超线程技术把多线程处理器内部的两个逻辑内核模拟成两个物理芯片，让单个处理器就能使用线程级的并行计算，进而兼容多线程操作系统和软件。超线程技术充分利用空闲CPU资源，在相同时间内完成更多工作。 [2]虽然采用超线程技术能够同时执行两个线程，当两个线程同时需要某个资源时，其中一个线程必须让出资源暂时挂起，直到这些资源空闲以后才能继续。因此，超线程的性能并不等于两个CPU的性能。而且，超线程技术的CPU需要芯片组、操作系统和应用软件的支持，才能比较理想地发挥该项技术的优势。 [2]运作方式播报编辑超线程如何工作？当英特尔® 超线程技术处于激活状态时，CPU 会在每个物理内核上公开两个执行上下文。这意味着，一个物理内核现在就像两个“逻辑内核”一样，可以处理不同的软件线程。  [6]较之传统的单线程内核，两个逻辑内核可以更有效地完成任务。英特尔® 超线程 (HT) 技术充分利用了内核以前在等待其他任务完成时的空闲时间，提高了 CPU 吞吐量。  [6]每个单位时间内，一个单运行管线的CPU只能处理一个线程（操作系统：thread），以这样的单位进行，如果想要在一单位时间内处理超过一个线程是不可能的，除非是有两个CPU的实体单元。双核心技术是将两个一样的CPU放置于一个封装内（或直接将两个CPU做成一个芯片），而英特尔的多线程技术是在CPU内部仅复制必要的资源、让两个线程可同时运行；在一单位时间内处理两个线程的工作，模拟实体双核心、双线程运作。 [3]Intel自Pentium开始引入超标量、乱序运行、大量的寄存器及寄存器重命名、多指令解码器、预测运行等特性；这些特性的原理是让CPU拥有大量资源，并可以预先运行及平行运行指令，以增加指令运行效率，可是在现实中这些资源经常闲置；为了有效利用这些资源，就干脆再增加一些资源来运行第二个线程，让这些闲置资源可执行另一个线程，而且CPU只要增加少数资源就可以模拟成两个线程运作。 [3]P4处理器需多加一个Logical CPU Pointer（逻辑处理单元）。因此P4 HT的die的面积比以往的P4增大了5%。而其余部分如ALU（整数运算单元）、FPU（浮点运算单元）、L2 Cache（二级缓存）并未增加，且是共享的。 [3]超线程的需求条件播报编辑并不是所有的处理器都支持超线程，支持超线程的台式机处理器有以下几种： [4]1、Intel Pentium 4 B 3.06GHz [4]2、Intel Pentium 4 C 2.4/2.6/2.8/3.0/3.2/3.4 [4]3、Intel Pentium 4 E 2.8(800FSB)/3.0/3.2/3.4/3.6 [4]4、Intel Pentium 4 XE 3.4/3.46/3.73 [4]5、Intel Pentium 4 520/530/53l/540/541/550/551/560/561/570/57l/630/640/650/660 [4]6、Intel Pentium XE 840(双核加NT) [4]超线程除了需要CPU的支持外还需要以下几个方面的支持： [4]1、需要主板BIOS的支持。主板厂商必须在BIOS中支持超线程才可以。个别的主板需要升级BIOS才能稳定支持。 [4]2、需要操作系统支持。Windows XP即支持此功能。 [4]3、需要应用软件的支持。通常，只要支持多处理器的软件就能支持超线程技术，但是目前支持多处理器的软件并不多，当前支持超线程技术的应用软件主要有Office 2000、Office XP及Linux kernel 2.4.x以后的版本。 [4]优缺点播报编辑优点1.超线程技术的优势在于同时进行多任务批处理工作，尽管支持超线程技术的软件不多，也只有少数的软件可以享受到由超线程技术带来的性能提升，但是这符合今后软件等技术的发展方向，今后更多的软件将受益于超线程技术。 [5]2.从性能来看，部分客户可以发觉在运行某些特定软件时，超线程技术让系统有了30%的性能提升，为超线程技术优化的软件都能够享受到超线程技术的好处。 [5]3.客户同时运行两个以上的软件时候，将可以明显的感受到这两个软件的性能都得到提升相比关闭超线程技术的情况下都有很大的提升，超线程技术的效率优势只有在多任务操作时候才能得到发挥。 [5]4.支持超线程技术的Windows XP操作系统，其中的很多系统软件都已经针对超线程技术优化过，因此在使用Windows 操作系统的时候可以很好的享受到超线程技术带来好处。 [5]缺点1.因为超线程技术是对多任务处理有优势，因此当运行单线程运用软件时，超线程技术将会降低系统性能，尤其在多线程操作系统运行单线程软件时将容易出现此问题。 [5]2.在打开超线程支持后，如果处理器以双处理器模式工作，那么处理器内部缓存就会被划分成几区域，互相共享内部资源。对于不支持多处理器工作的软件在双处理器上运行时出错的概率要比单处理器上高很多。 [5]3.因为很多工作站软件为Windows 2000操作系统进行过优化，但是采用Windows 2000这样的操作系统的工作站无法完全利用超线程技术的优势，也带来不了高的工作效率 [5]4.超线程技术只能提高40%左右的性能（测评时可以看成50%，即Core i3 的执行效率为3核速率，Core i5 4核 HT与Core i7 的执行效率为6核速率） [5]与多核心区别播报编辑超线程技术与多核体系结构的区别如下：①超线程技术是通过延迟隐藏的方法，提高了处理器的性能，本质上，就是多个线程共享一个处理单元。因此，采用超线程技术所获得的性能并不是真正意义上的并行。从而采用超线程技术获得的性能提升，将会随着应用程序以及硬件平台的不同而参差不齐。②多核处理器是将两个甚至更多的独立执行单元，嵌入到一个处理器内部。每个指令序列（线程），都具有一个完整的硬件执行环境，所以各线程之间就实现了真正意义上的并行。 [2]超线程技术与多核技术相结合可以给应用程序带来更大的优化空间，进而极大地提高系统的吞吐率。 [2]

辅助存储器：
解释播报编辑PC机常见的外存储器有软盘存储器、硬盘存储器、光盘存储器等。磁盘有软磁盘和硬磁盘两种。光盘有只读型光盘CD-ROM、一次写入型光盘WORM和可重写型光盘MO三种。简介存储器的种类很多，按其用途可分为主存储器和辅助存储器，主存储器又称内存储器（简称内存），辅助存储器又称外存储器（简称外存）。内存储器最突出的特点是存取速度快，但是容量小、价格贵；外存储器的特点是容量大、价格低，但是存取速度慢。内存储器用于存放那些立即要用的程序和数据；外存储器用于存放暂时不用的程序和数据。内存储器和外存储器之间常常频繁地交换信息。 [1]外存通常是磁性介质或光盘，像硬盘，软盘，磁带，CD等，能长期保存信息，并且不依赖于电来保存信息，但是由机械部件带动，速度与CPU相比就显得慢的多。软盘：软磁盘使用柔软的聚酯材料制成原型底片，在两个表面涂有磁性材料。常用软盘直径为3.5英寸，存储容量为1.44MB.软盘通过软盘驱动器来读取数据。U盘：U盘也被称为“闪盘”，可以通过计算机的USB口存储数据。与软盘相比，由于U盘的体积小、存储量大及携带方便等诸多优点，U盘已经取代软盘的地位。硬盘：硬磁盘是由涂有磁性材料额铝合金原盘组成的，每个硬盘都由若干个磁性圆盘组成。其中固态硬盘是以闪存为存储介质的半导体存储器，其相对于机械硬盘具备读写速度快、延迟低、抗震性好等优势，在全球硬盘市场上的出货量占比不断提高。 [2]移动固态硬盘的普及，让习惯了移动机械硬盘的人们，背包重量大有减轻。而且固态和移动硬盘的结合，显然也符合移动存储产品耐碰撞、轻巧且无须等待即插即用这诸多的主要特性。 [3]磁带存储器：磁带也被称为顺序存取存储器SAM。它存储容量很大，但查找速度很慢，一般仅用作数据后备存储。计算机系统使用的磁带机有3中类型：盘式磁带机、数据流磁带机及螺旋扫描磁带机。光盘存储器：光盘指的是利用光学方式进行信息存储的圆盘。它应用了光存储技术，即使用激光在某种介质上写入信息，然后再利用激光读出信息。光盘存储器可分为:CD-ROM、CD-R、CD-RW、和DVD-ROM等。 [1]种类移动硬盘软盘、硬盘、光盘、U盘、磁带都是外部存储器。从冯.诺依曼的存储程序工作原理及计算机的组成来说，计算机分为运算器、控制器、存储器和输入/输出设备，这里的存储器就是指内存，而硬盘属于输入/输出设备。CPU运算所需要的程序代码和数据来自于内存，内存中的东西则来自于硬盘，所以硬盘并不直接与CPU打交道。硬盘相对于内存来说就是外部存储器。存储器是用来存储器数据的，内存有高速缓存和内存，计算机内部存储，外存就是类似U盘的外部存储。内存储器速度快 价格贵，容量小，断电 后内存内数据会丢失。（ROM 断电不丢失）外存储器单位价格低、容量大、速度慢、 断电后数据不会丢失。性能指标播报编辑硬盘●容量——通常所说的容量是指硬盘的总容量，一般硬盘厂商定义的单位1GB=1000MB，而系统定义的1GB=1024MB，所以会出现硬盘上的标称值大于格式化容量的情况，这算业界惯例，属于正常情况。●单碟容量就是指一张碟片所能存储的字节数，硬盘的单碟容量一般都在20GB以上。而随着硬盘单碟容量的增大，硬盘的总容量已经可以实现上百G甚至几TB了(商业购买的硬盘容量为1TB的，可能实际只有1000GB，而不是1024GB，真正意义上的1TB=1024GB)。●转速——转速是指硬盘内电机主轴的转动速度，单位是RPM(每分钟旋转次数)。转速是决定硬盘内部传输率的决定因素之一，它的快慢在很大程度上决定了硬盘的速度，同时也是区别硬盘档次的重要标目前一般的硬盘转速为5400 转和7200转，最高的转速则可达到10000转每分以上。●最高内部传输速率——这是硬盘的外圈的传输速率，它是指磁头和高速数据缓存之间的最高数据传输速率，单位为MB/s。最高内部传输速率的性能与硬盘转速以及盘片存储密度(单碟容量)有直接的关系。●平均寻道时间平均寻道时间是指硬盘磁头移动到数据所在磁道时所用的时间，单位为毫秒(ms),硬盘的平均寻道时间一般低于9毫秒。平均寻道时间越短,硬盘的读取数据能力就越高。U盘● 理论数据读取速度：18MB/s● 理论数据写入速度： 17MB/s● 无需安装驱动程序(windows 98除外)● 无需额外电源，只从usb总线取电●容量大、品种多● 带写保护开关，防止文件被抹掉，防病毒● led指示灯指示工作状态● 体积小， 重量轻：大约20g

系统软件：
基本概念播报编辑各种应用软件，虽然完成的工作各不相同，但它们都需要一些共同的基础操作，例如都要从输入设备取得数据，向输出设备送出数据，向外存写数据，从外存读数据，对数据的常规管理，等等。这些基础工作也要由一系列指令来完成。人们把这些指令集中组织在一起，形成专门的软件，用来支持应用软件的运行，这种软件称为系统软件。一般来讲，系统软件包括操作系统和一系列基本的工具（比如编译器，数据库管理，存储器格式化，文件系统管理，用户身份验证，驱动管理，网络连接等方面的工具），是支持计算机系统正常运行并实现用户操作的那部分软件。系统软件一般是在计算机系统购买时随机携带的，也可以根据需要另行安装。特点播报编辑系统软件的主要特征是：*与硬件有很强的交互性*能对资源共享进行调度管理*能解决并发操作处理中存在的协调问题*其中的数据结构复杂，外部接口多样化，便于用户反复使用主要类别播报编辑系统软件在为应用软件提供上述基本功能的同时，也进行着对硬件的管理，使在一台计算机上同时或先后运行的不同应用软件有条不紊地合用硬件设备。例如，两个应用软件都要向硬盘存入和修改数据，如果没有一个协调管理机构来为它们划定区域的话，必然形成互相破坏对方数据的局面。 [1]有代表性的系统软件有：操作系统操作系统管理计算机的硬件设备，使应用软件能方便、高效地使用这些设备。在微机上常见的有：DOS、WINDOWS、UNIX、OS/2等。操作系统(5张)在计算机软件中最重要且最基本的就是操作系统（OS）。它是最底层的软件，它控制所有计算机运行的程序并管理整个计算机的资源，是计算机裸机与应用程序及用户之间的桥梁。没有它，用户也就无法使用某种软件或程序。操作系统是计算机系统的控制和管理中心，从资源角度来看，它具有处理机、存储器管理、设备管理、文件管理等4项功能。常用的系统有DOS操作系统、WINDOWS操作系统、UNIX操作系统和Linux、Netware等操作系统。 [2]语言处理程序编译软件CPU执行每一条指令都只完成一项十分简单的操作，一个系统软件或应用软件，要由成千上万甚至上亿条指令组合而成。直接用基本指令来编写软件，是一件极其繁重而艰难的工作。计算机只能直接识别和执行机器语言，因此要计算机上运行高级语言程序就必须配备程序语言翻译程序，翻译程序本身是一组程序，不同的高级语言都有相应的翻译程序。语言处理程序如汇编语言汇编器，C语言编译、连接器等。为了提高效率，人们规定一套新的指令，称为高级语言，其中每一条指令完成一项操作，这种操作相对于软件总的功能而言是简单而基本的，而相对于CPU的一眇操作而言又是复杂的。用这种高级语言来编写程序（称为源程序）就象用预制板代替砖块来造房子，效率要高得多。但CPU并不能直接执行这些新的指令，需要编写一个软件，专门用来将源程序中的每条指令翻译成一系列CPU能接受的基本指令（也称机器语言）使源程序转化成能在计算机上运行的程序。完成这种翻译的软件称为高级语言编译软件，通常把它们归入系统软件。目前常用的高级语言有VB、C++、JAVA等，它们各有特点，分别适用于编写某一类型的程序，它们都有各自的编译软件。 [1]数据库管理数据库管理系统有组织地、动态地存贮大量数据，使人们能方便、高效地使用这些数据。数据库管理系统是一种操纵和管理数据库的大型软件，用于建立、使用和维护数据库。Foxpro，Access，Oracle，Sybase，DB2和Informix则是数据库系统。辅助程序系统辅助处理程序也称为“软件研制开发工具”、“支持软件”、“软件工具”，主要有编辑程序、调试程序、装备和连接程序、调试程序。

总线周期：
总线周期的概念1.微处理器是在时钟信号CLK控制下按节拍工作的。8086/8088系统的时钟频率为4.77MHz，每个时钟周期约为200ns。2.由于存储器和I/O端口是挂接在总线上的，CPU对存储器和I/O接口的访问，是通过总线实现的。通常把CPU通过总线对微处理器外部（存储器或I/O接口）进行一次访问所需时间称为一个总线周期。一个总线周期一般包含4个时钟周期，这4个时钟周期分别称4个状态即T1状态、T2状态、T3状态和T4状态，必要时，可在T3、T4间插入一个至数个Tw。（1）T1状态 ——输出存储器地址或I/O地址。（2）T2状态 ——输出控制信号。（3）T3和Tw状态 ——总线操作持续，并检测READY以决定是否延长时序。（4）T4状态 ——完成数据传送。

存储器地址寄存器：
公布时间播报编辑1993年，经全国科学技术名词审定委员会审定发布。 [1]出处播报编辑《电子学名词》第一版。

算术逻辑部件：
公布时间播报编辑2008年，经全国科学技术名词审定委员会审定发布。出处播报编辑《海峡两岸信息科学技术名词》。 [1]

实时系统：
基本信息播报编辑时间约束实时系统的任务具有一定的时间约束（截止时间）。根据截止时间，实时系统的实时性分为“硬实时”和“软实时”。硬实时是指应用的时间需求能够得到完全满足，否则就造成重大安全事故，甚至造成重大的生命财产损失和生态破坏，如在航空航天、军事、核工业等一些关键领域中的应用。软实时是指某些应用虽然提出时间需求，但实时任务偶尔违反这种需求对系统运行及环境不会造成严重影响，如监控系统等和信息采集系统等。可预测性可预测性是指系统能够对实时任务的执行时间进行判断，确定是否能够满足任务的时限要求。由于实时系统对时间约束要求的严格性，使可预测性成为实时系统的一项重要性能要求。除了要求硬件延迟的可预测性以外，还要求软件系统的可预测性，包括应用程序的响应时间是可预测的，即在有限的时间内完成必须的工作；以及操作系统的可预测性，即实时原语、调度函数等运行开销应是有界的，以保证应用程序执行时间的有界性。可靠性大多数实时系统要求有较高的可靠性。在一些重要的实时应用中，任何不可靠因素和计算机的一个微小故障，或某些特定强实时任务（又叫关键任务）超过时限，都可能引起难以预测的严重后果。为此，系统需要采用静态分析和保留资源的方法及冗余配置，使系统在最坏情况下都能正常工作或避免损失。可靠性已成为衡量实时系统性能不可缺少的重要指标。交互作用实时系统通常运行在一定的环境下，外部环境是实时系统不可缺少的一个组成部分。计算机子系统一般是控制系统，它必须在规定的时间内对外部请求做出反应。外部物理环境往往是被控子系统，两者互相作用构成完整的实时系统。大多数控制子系统必须连续运转以保证子系统的正常工作或准备对任何异常行为采取行动。新特点播报编辑简述早期的实时系统功能简单，包括单板机、单片机，以及简单的嵌入式实时系统等，其调度过程相对简单。随着实时系统应用范围的不断扩大，系统复杂性不断提高，实时系统具有以下新特点：多任务类在实时系统中，不但包括周期任务、偶发任务、非周期任务，还包括非实时任务。实时任务要求要满足时限，而非实时任务要求要使其响应时间尽可能的短。多种类型任务的混合，使系统的可调度性分析更加困难。复杂性任务的约束包括时间约束、资源约束、执行顺序约束和性能约束。时间约束是任何实时系统都固有的约束。资源约束是指多个实时任务共享有限的资源时，必须按照一定的资源访问控制协议进行同步，以避免死锁和高优先级任务被低优先级任务堵塞的时间（即优先级倒置时间）不可预测。执行顺序约束是指各任务的启动和执行必须满足一定的时间和顺序约束。例如，在分布式端到端（end-to-end）实时系统很重，同一任务的各子任务之间存在前驱/后驱约束关系，需要执行同步协议来管理子任务的启动和控制子任务的执行，使它们满足时间约束和系统可调度要求。性能约束是指必须满足如可靠性、可用性、可预测性、服务质量（Quality of Service,QoS）等性能指标。短暂超载在实时系统中，即使一个功能设计合理、资源充足的系统也可能由于一下原因超载：1）系统元件出现老化，外围设备错误或系统发生故障。随着系统运行时间的增长，系统元件出现老化，系统部件可能发生故障，导致系统可用资源降低，不能满足实时任务的时间约束要求。2）环境的动态变化。由于不能对未来的环境、系统状态进行正确有效地预测，因此不能从整体角度上对任务进行调度，可能导致系统超载。3）应用规模的扩大。原先满足实时任务时限要求的系统，随着应用规模的增大，可能出现不能满足任务时限要求的情况，而重新设计、重建系统在时间和经济上又不允许。调度范围播报编辑分析为了精确管理“时间”资源，已达到实时性和与预测性要求，并能够满足是实时系统的新要求，需用实时调度理论对任务进行调度和可调度性分析。任务调度技术包括调度策略和可调度性分析方法，两者是紧密结合的。任务调度技术研究的范围包括任务使用系统资源（包括处理机、内存、I/O、网络等资源）的策略和机制，以及提供判断系统性能是否可预测的方法和手段。例如，什么时候调度任务运行、在哪运行（当系统为多处理机系统或分布式系统时）、运行多长时间等等；以及判断分析用一定参数描述的实时任务能否被系统正确调度。给定一组实时任务和系统资源，确定每个任务何时何地执行的整个过程就是调度。在非实时系统中，调度的主要目的是缩短系统平均响应时间，提高系统资源利用率，或优化某一项指标；而实时系统中调度的目的则是要尽可能地保证每个任务满足他们的时间约束，及时对外部请求做出响应。实时调度技术通常有多种划分方法，常用以下两种。占式调度抢占式调度通常是优先级驱动的调度。每个任务都有优先级，任何时候具有最高优先级且已启动的任务先执行。一个正在执行的任务放弃处理器的条件为：自愿放弃处理器（等待资源或执行完毕）；有高优先级任务启动，该高优先级任务将抢占其执行。除了共享资源的临界段之外，高优先级任务一旦准备就绪，可在任何时候抢占低优先级任务的执行。抢占式调度的优点是实时性好、反应快，调度算法相对简单，可优先保证高优先级任务的时间约束，其缺点是上下文切换多。而非抢占式调度是指不允许任务在执行期间被中断，任务一旦占用处理器就必须执行完毕或自愿放弃。其优点是上下文切换少；缺点是在一般情况下，处理器有效资源利用率低，可调度性不好。驱动策略静态表驱动策略（Static Table-Driven Scheduling）是一种离线调度策略，指在系统运行前根据各任务的时间约束及关联关系，采用某种搜索策略生成一张运行时刻表。这张运行时刻表与列车运行时刻表类似，指明了各任务的起始运行时刻及运行时间。运行时刻表一旦生成就不再发生变化了。在系统运行时，调度器只需根据这张时刻表启动相应的任务即可。由于所有调度策略在离线情况下指定，因此调度器的功能被弱化，只具有分派器（Dispatcher）的功能。优先级驱动策略指按照任务优先级的高低确定任务的高低确定任务的执行顺序。优先级驱动策略又分为静态优先级调度策略。静态优先级调度是指任务的优先级分配好之后，在任务的运行过程中，优先级不会发生改变。静态优先级调度又称为固态优先级调度。动态优先级调度是指任务的优先级可以随着时间或系统状态的变化而发生变化。分类播报编辑强实时强实时系统（Hard Real-Time）：在航空航天、军事、核工业等一些关键领域中，应用时间需求应能够得到完全满足，否则就造成如飞机失事等重大地安全事故，造成重大地生命财产损失和生态破坏。因此，在这类系统的设计和实现过程中，应采用各种分析、模拟及形式化验证方法对系统进行严格的检验，以保证在各种情况下应用的时间需求和功能需求都能够得到满足。弱实时弱实时系统（Soft Real-Time）：某些应用虽然提出了时间需求，但实时任务偶尔违反这种需求对系统的运行以及环境不会造成严重影响，如视频点播（Video-On-Demand，VOD）系统、信息采集与检索系统就是典型的弱实时系统。在VOD系统中，系统只需保证绝大多数情况下视频数据能够及时传输给用户即可，偶尔的数据传输延迟对用户不会造成很大影响，也不会造成像飞机失事一样严重的后果。

存储字：
一个存储字可代表一个二进制数，也可代表一串字符，如存储字为0011011001111101，既可表示为由十六进制字符组成的367DH（ASCII码），又可代表16位的二进制数，此值对应十进制数为13 949，还可代表两个ASCII码：“6”和“”。一个存储字还可代表一条指令。 [1]

运算器：
简介播报编辑运算器由算术逻辑单元（ALU）、累加器、状态寄存器、通用寄存器组等组成。算术逻辑运算单元（ALU）的基本功能为加、减、乘、除四则运算，与、或、非、异或等逻辑操作，以及移位、求补等操作。计算机运行时，运算器的操作和操作种类由控制器决定。运算器处理的数据来自存储器；处理后的结果数据通常送回存储器，或暂时寄存在运算器中。与Control Unit共同组成了CPU的核心部分。 运算器是计算机中处理数据的功能部件。对数据处理主要包括数据的算术运算和逻辑数据的逻辑操作。因此,实现对数据的算术与逻辑运算是运算器的核心功能。 [1]基本理论播报编辑数据运算器运算器的处理对象是数据，所以数据长度和计算机数据表示方法，对运算器的性能影响极大。70年代微处理器常以1个、4个、8个、16个二进制位作为处理数据的基本单位。大多数通用计算机则以16、32、64位作为运算器处理数据的长度。能对一个数据的所有位同时进行处理的运算器称为并行运算器。如果一次只处理一位，则称为串行运算器。有的运算器一次可处理几位 （通常为6或8位），一个完整的数据分成若干段进行计算，称为串/并行运算器。运算器往往只处理一种长度的数据。有的也能处理几种不同长度的数据，如半字长运算、双倍字长运算、四倍字长运算等。有的数据长度可以在运算过程中指定，称为变字长运算。按照数据的不同表示方法，可以有二进制运算器、十进制运算器、十六进制运算器、定点整数运算器、定点小数运算器、浮点数运算器等。按照数据的性质，有地址运算器和字符运算器等。它的主要功能是进行算术运算和逻辑运算。操作运算器能执行多少种操作和操作速度，标志着运算器能力的强弱，甚至标志着计算机本身的能力。运算器最基本的操作是加法。一个数与零相加，等于简单地传送这个数。将一个数的代码求补，与另一个数相加，相当于从后一个数中减去前一个数。将两个数相减可以比较它们的大小。运算器左右移位是运算器的基本操作。在有符号的数中，符号不动而只移数据位，称为算术移位。若数据连同符号的所有位一齐移动，称为逻辑移位。若将数据的最高位与最低位链接进行逻辑移位，称为循环移位。运算器的逻辑操作可将两个数据按位进行与、或、异或，以及将一个数据的各位求非。有的运算器还能进行二值代码的16种逻辑操作。乘、除法操作较为复杂。很多计算机的运算器能直接完成这些操作。乘法操作是以加法操作为基础的，由乘数的一位或几位译码控制逐次产生部分积，部分积相加得乘积。除法则又常以乘法为基础，即选定若干因子乘以除数，使它近似为1，这些因子乘被除数则得商。没有执行乘法、除法硬件的计算机可用程序实现乘、除，但速度慢得多。有的运算器还能执行在一批数中寻求最大数，对一批数据连续执行同一种操作，求平方根等复杂操作。运算方法实现运算器的操作，特别是四则运算，必须选择合理的运算方法。它直接影响运算器的性能，也关系到运算器的结构和成本。另外，在进行数值计算时，结果的有效数位可能较长，必须截取一定的有效数位，由此而产生最低有效数位的舍入问题。选用的舍入规则也影响到计算结果的精确度。在选择计算机的数的表示方式时，应当全面考虑以下几个因素：要表示的数的类型（小数、整数、实数和复数）：决定表示方式，可能遇到的数值范围：确定存储、处理能力。数值精确度：处理能力相关；数据存储和处理所需要的硬件代价：造价高低。运算器两种常用格式：定点格式：定点格式容许的数值范围有限，但要求的处理硬件比较简单；浮点格式：容许的数值范围很大，但要求的处理硬件比较复杂。1、定点数表示法：定点指小数点的位置固定，为了处理方便，一般分为定点纯整数和纯小数。2、浮点数表示法：由于所需表示的数值取值范围相差十分悬殊，给存储和计算带来诸多不便，因此出现了浮点运算法。浮点表示法，即小数点的位置是浮动的。其思想来源于科学计数法。IEEE754的浮点数（比较特殊）浮点数的规格化：主要解决同一浮点数表示形式的不唯一性问题。规定 ，否则尾数要进行左移或右移。机器零的概念：尾数为0或是阶码值小于所能表示的最小数。3、十进制数串的表示方法：由于人们对十进制比较熟悉，因此在计算机中要增加对十进制运算的支持。两种方式：将十进制数变为二进制数运算，输出时再由二进制变为十进制。直接的十进制运算。直接运算的表示方法：字符串形式：用于非数值计算领域、压缩的十进制数串：分为定长和不定长两种。需要相应的十进制运算器和指令支持。4、自定义数据表示：标志符数据表示、描述符数据表示。区别：标志符与每个数据相连，二者合起来存放在一个存储单元，而描述符要和数据分开存放；描述符表示中，先访问描述符，后访问数据，至少增加一次访存；描述符是程序的一部分，而不是数据的一部分。原码：比较自然的表示法，最高位表示符号，0为正，1为负。优点：简单易懂。缺点：加减法运算复杂。补码：加减法运算方便，减法可以转换为加法。定点小数的补码。定点整数的补码，反码：为计算补码方便而引入。由反码求补码：符号位置1，各位取反，末位加1。移码：用于阶码的表示，两个移码容易比较大小，便于对阶。运算器ASCII码 输入码：用于汉字输入；汉字的存储；字模码：用于汉字的显示。余数处理的两种方法：恢复余数法：运算步骤不确定，控制复杂，不适合计算机运算。加减交替法：不恢复余数，运算步骤确定，适合计算机操作。逻辑数概念：不带符号的二进制数。四种逻辑运算：逻辑非、逻辑加、逻辑乘、逻辑异。多功能算术/逻辑运算单元（ALU） 并行进位，行波进位加/减法器存在的两个问题：运算时间长，行波进位加/减法器只能完成加法和减法，而不能完成逻辑操作，控制端M用来控制作算术运算还是逻辑运算，两种运算的区别在于是否对进位进行处理。M=0时，对进位无影响，为算术运算；M=1时，进位被封锁，为逻辑运算。正逻辑中，“1”用高电平表示，“0”用低电平表示，而负逻辑刚好相反。逻辑与负逻辑的关系为，正逻辑的“与”到负逻辑中变为“或”，即+·互换。内部总线，总线分类：内部总线、外部总线（系统总线）、通信总线。总线又可分为单向总线和双向总线。带锁存器的总线可实现总线的复用。运算器包括ALU、阵列乘除器件、寄存器、多路开关、三态缓冲器、数据总线等逻辑部件。运算器的设计，主要是围绕着ALU和寄存器同数据总线之间如何传送操作数和运算结果而进行的。运算器的三种结构形式：单总线结构的运算器：这种结构的主要缺点是操作进度较慢，但控制电路比较简单。双总线结构的运算器。三总线结构的运算器：三总线结构的运算器的特点是操作时间快。结构运算器运算器包括寄存器、执行部件和控制电路3个部分。在典型的运算器中有3个寄存器：接收并保存一个操作数的接收寄存器；保存另一个操作数和运算结果的累加寄存器；在进行乘、除运算时保存乘数或商数的乘商寄存器。执行部件包括一个加法器和各种类型的输入输出门电路。控制电路按照一定的时间顺序发出不同的控制信号，使数据经过相应的门电路进入寄存器或加法器，完成规定的操作。为了减少对存储器的访问，很多计算机的运算器设有较多的寄存器，存放中间计算结果，以便在后面的运算中直接用作操作数。为了提高运算速度，某些大型计算机有多个运算器。它们可以是不同类型的运算器，如定点加法器、浮点加法器、乘法器等，也可以是相同类型的运算器。运算器的组成决定于整机的设计思想和设计要求，采用不同的运算方法将导致不同的运算器组成。但由于运算器的基本功能是一样的，其算法也大致相同，因而不同机器的运算器是大同小异的。运算器主要由算术逻辑部件、通用寄存器组和状态寄存器组成。1、算术逻辑部件ALU。ALU 主要完成对二进制信息的定点算术运算、逻辑运算和各种移位操作。算术运算主要包括定点加、减、乘和除运算。逻辑运算主要有逻辑与、逻辑或、逻辑异或和逻辑非操作。移位操作主要完成逻辑左移和右移、算术左移和右移及其他一些移位操作。某些机器中，ALU 还要完成数值比较、变更数值符号、计算操作数在存储器中的地址等。可见，ALU 是一种功能较强的组合逻辑电路，有时被称为多功能发生器，它是运算器组成中的核心部件。ALU 能处理的数据位数（即字长）与机器有关。如 Z80单板机中，ALU 是 8 位；IBM PC/XT和 AT 机中，ALU 为 16 位；386 和 486微机中，ALU 是 32 位。ALU 有两个数据输入端和一个数据输出端，输入输出的数据宽度（即位数）与 ALU 处理的数据宽度相同。2、通用寄存器组设计的机器的运算器都有一组通用寄存器。它主要用来保存参加运算的操作数和运算的结果。早期的机器只设计一个寄存器，用来存放操作数、操作结果和执行移位操作运算器，由于可用于存放重复累加的数据，所以常称为累加器。通用寄存器均可以作为累加器使用。通用寄存器的数据存取速度是非常快的，一般是十几个毫微秒（μs）。如果 ALU 的两个操作数都来自寄存器，则可以极大地提高运算速度。通用寄存器同时可以兼作专用寄存器，包括用于计算操作数的地址（用来提供操作数的形式地址，据此形成有效地址再去访问主存单元）。例如，可作为变址寄存器、程序计数器（PC）、堆栈指示器（SP）等。必须注意的是，不同的机器对这组寄存器使用的情况和设置的个数是不相同的。3、状态寄存器状态寄存器用来记录算术、逻辑运算或测试操作的结果状态。程序设计中，这些状态通常用作条件转移指令的判断条件，所以又称为条件码寄存器。一般均设置如下几种状态位：（1）零标志位（Z）：当运算结果为 0 时，Z 位置“1”；非 0 时，置“0”；（2）负标志位（N）：当运算结果为负时，N 位置“1”；为正时，置“0”；（3）溢出标志位（V）：当运算结果发生溢出时，V 位置“1”；无溢出时，置“0”；（4）进位或借位标志（C）：在做加法时，如果运算结果最高有效位（对于有符号数来说，即符号位；对无符号数来说，即数值最高位）向前产生进位时，C 位置“1”；无进位时，置“0”。在做减法时，如果不够减，最高有效位向前有借位（这时向前无进位产生）时，C 位置“1”；无借位（即有进位产生）时，C 位置“0”。除上述状态外，状态寄存器还常设有保存有关中断和机器工作状态（用户态或核心态）等信息的一些标志位（应当说明，不同的机器规定的内容和标志符号不完全相同），以便及时反映机器运行程序的工作状态，所以有的机器称它为“程序状态字”或“处理机状态字”（Processor Status Word，PSW ）。性能指标播报编辑1.机器字长，机器字长是指参与运算的数据的基本位数。它决定了寄存器、运算器和数据总线的位数，因而直接影响到硬件的价格。字长标志着计算精度。为协调精度与造价，并满足多方面的要求，许多计算机允许变字长计算，例如半字长、全字长和双倍字长等。由于数和指令代码都放在主存中，因而字长与指令码长度往往有一个对应关系，字长也就影响到指令系统功能的强弱。计算机字长从 4 位、8 位、16 位、32 位到 64 位不等。机器字长可包含一个或多个字节。用于科学计算的机器，为了确保精度，需要较长的字长；用于数据处理、工业控制的机器，字长为 16 位或 32 位就能满足要求。运算器2. 运算速度，它是计算机的主要指标之一。计算机执行不同的运算和操作所需的时间可能不同，因而对运算速度存在不同的计算方法。一般常用平均速度，即在单位时间内平均能执行的指令条数来表示，如某计算机运算速度为 100 万次 /秒，就是指该机在一秒钟内能平均执行 100万条指令（即 1MIPS）。有时也采用加权平均法（即根据每种指令的执行时间以及该指令占全部操作的百分比进行计算）求得的等效速度表示。功能分类播报编辑运算器的基本功能是完成对各种数据的加工处理，例如算术四则运算，与、或、求反等逻辑运算，算术和逻辑移位操作，比较数值，变更符号，计算主存地址等。运算器中的寄存器用于临时保存参加运算的数据和运算的中间结果等。运算器中还要设置相应的部件，用来记录一次运算结果的特征情况，如是否溢出，结果的符号位，结果是否为零等。计算机所采用的运算器类型很多，从不同的角度分析，就有不同的分类方法。从小数点的表示形式可分为定点运算器和浮点运算器。定点运算器只能做定点数运算，特点是机器数所表示的范围较小，但结构较简单。浮点运算器功能较强，既能对浮点数，又能对定点数进行运算，其数的表示范围很大，但结构相当复杂。从进位制方面分为二进制运算器和十进制运算器。一般计算机都采用二进制运算器，随着计算机广泛应用于商业和数据处理，越来越多的机器都扩充十进制运算的功能，使运算器既能完成二进制的运算，也能完成十进制运算。计算机中运算器需要具有完成多种运算操作的功能，因而必须将各种算法综合起来，设计一个完整的运算部件。浮点运算器1、浮点运算器的一般结构浮点运算可用两个松散连接的定点运算部件来实现：即阶码部件和尾数部件，浮点运算器的一般结构尾数部件实质上就是一个通用的定点运算器，要求该运算器能实现加、减、乘、除四种基本算术运算。其中三个单字长寄存器用来存放操作数：AC为累加器，MQ为乘商寄存器，DR为数据寄存器。AC和MQ连起来还可组成左右移位的双字长寄存器AC－MQ。并行加法器用来完运算器成数据的加工处理，其输入来自AC和DR，而结果回送到AC。MQ寄存器在乘法时存放乘数，而除法时存放商数，所以称为乘商寄存器。DR用来存放被乘数或除数，而结果（乘积或商与余数）则存放在AC－MQ。在四则运算中，使用这些寄存器的典型方法如下：运算类别 寄存器关系加法AC+DR→AC减法AC－DR→AC乘法DR×MQ→AC－MQ除法AC÷DR→AC－MQ对阶码部件来说，只要能进行阶码相加、相减和比较操作即可。在图2-21中，操作数的阶码部分放在寄存器E1和E2，它们与并行加法器相连以便计算。浮点加法和减法所需要的阶码比较是通过E1－E2来实现的，相减的结果放入计数器E中，然后按照E的符号为决定哪一个阶码较大。在尾数相加或相减之前，需要将一个尾数进行移位，这是由计数器E来控制的，目的是使E的值按顺序减到0。E每减一次1，相应的尾数则向右移1位。一旦尾数高速完毕，它们就可按通常的定点方法进行处理。运算结果的阶码值仍放到计数器E中。2、点协处理器，80x87是美国Intel公司为处理浮点数等数据的算术运算和多种函数计算而设计生产的专用算术运算处理器。由于它们的算术运算是配合80x86CPU进行的，所以又称为协处理器。我们以80x87为例来讨论浮点运算器部件的组成。浮点协处理器的主要功能如下：（1） 可与配套的CPU芯片异步并行工作。80x87相当于386的一个I/O部件，本身有它自己的指令，但不能单独使用，它只能作为386主CPU的协处理器才能运算。因为真正的读写主存的工作不是80x87完成，而是由386执行的。如果386从主存读取的指令是80x87浮点运算指令，则它们以输出方式把该指令送到80x87，80x87接收后进行译码并执行浮点运算。在80x87进行运算期间，386可取下一条其他指令予以执行，因而实现了并行工作。如果在80x87执行浮点运算指令过程中386又取来一条80x87指令，则80x87以给出“忙”的标志信号加以拒绝，使386暂停向80x87发送命令。只有待80x87完成浮点运算而取消“忙”的标志信号以后，386才可以进行一次发送操作。（2） 高性能的80位字长的内部结构，有8个80位字长的以堆栈方式管理的寄存器组。80x87从存储器取数和向存储器写数时，均用80位的临时实数和其他6种数据类型执行自动转换。全部数据在80x87中均以80位临时实数的形式表示。因此80x87具有80位的内部结构，并有八个80位字长以 “先进后出”方式管理的寄存器组，又称寄存器堆栈。这些寄存器可以按堆栈方式工作，此时，栈顶被用作累加器；也可以按寄存器的编号直接访问任一个寄存器。（3） 浮点数的格式，完全符合IEEE制定的国际标准。（4) 能处理包括二进制浮点数、二进制整数和十进制数串三大类共7种数据。此7种数据类型在寄存器中表示如下：短整数（32位整数） S 31位 (二进制补码) 长整数（64位整数） S 63位 （二进制补码） 短实数（32位浮点数） S 指数 尾数（23位） 长实数（64位浮点数） S 指数 尾数（52位） 临时实数（80位浮点数） S 指数 尾数（64位） 十进数串（十进制18位） S －－ d17d16　…　d1d0。此处S为一位符号位，0代表正，1代表负。三种浮点数阶码的基值均为2。阶码值用移码表示，尾数用原码表示。尾数有32位、64位、80位三种。不仅仅是一个浮点运算器，还包括了执行数据运算所需要的全部控制线路，就运算部分讲，有处理浮点数指数部分的部件和处理尾数部分的部件，还有加速移位操作的移位器线路，它们通过指数总线和小数总线与八个80位字长的寄存器堆栈相连接。（5) 内部的出错管理功能为了保证操作的正确执行，80x87内部还设置了三个各为16位字长的寄存器，即特征寄存器、控制字寄存器和状态寄存器。特征寄存器用每两位表示寄存器堆栈中每个寄存器的状态，即特征值为00－11四种组合时表明相应的寄存器有正确数据、数据为0、数据非法、无数据四种情况。控制字寄存器用于控制80x87的内部操作。其中PC为精度控制位域（2位）：00为24位，01为备用，10为53位，11为64位。RC为舍入控制位域（2位）：00为就近舍入，01朝－方向舍入，10朝+方向舍入，11朝0舍入。IC为无穷大控制位：该位为0时+与－作同值处理，该位为1时+与－不作同值处理。控制寄存器的低6位作异常中断屏蔽位：IM为非法处理，DM为非法操作数，ZM为0作除数，OM为上溢，UM为下溢，PM为精度下降。状态字寄存器用于表示80x87的结果处理情况，例如当“忙”标志为1时，表示正在执行一条浮点运算指令，为0则表示80x87空闲。状态寄存器的低6位指出异常错误的6种类型，与控制寄存器低6位相。当的控制寄存器位为0（未屏蔽）而状态寄存器位为1时，因发生某种异常错误而产生中断请求。3．CPU内的浮点运算器，奔腾CPU将浮点运算器包含在芯片内。浮点运算部件采用流水线设计。指令执行过程分为8段流水线。前4段为指令预取（DF）、指令译码(D1）、地址生成（D2）、取操作数（EX），在U，V流水线中完成；后4段为执行1(X1）、执行2(X2）、结果写回寄存器堆（WF）、错误报告(ER），在浮点运算器中完成。一般情况下，由V流水线完成一条浮点操作指令。浮点部件内有浮点专用的加法器、乘法器和除法器，有8个80位寄存器组成的寄存器堆，内部的数据总线为80位宽。因此浮点部件可支持IEEE754标准的单精度和双精度格式的浮点数。另外还使用一种称为临时实数的80位浮点数。对于浮点的取数、加法、乘法等操作，采用了新的算法并用硬件来实现，其执行速度是80486的10倍多。发展播报编辑公元前5世纪，中国人发明了算盘，广泛应用于商业贸易中，算盘被认为是最早的计算机，并一直使用至今。算盘在某些方面的运算能力要超过计算机，算盘的方面体现了中国人民的智慧。运算器直到17世纪，计算设备才有了第二次重要的进步。1642年，法国人Blaise Pascal（1623-1662）发明了自动进位加法器，称为Pascalene。1694年，德国数学家Gottfried Wilhemvon Leibniz（1646-1716）改进了Pascaline，使之可以计算乘法。后来，法国人Charles Xavier Thomas de Colmar发明了可以进行四则运算的计算器。现代计算机的真正起源来自英国数学教授Charles Babbage。Charles Babbage发现通常的计算设备中有许多错误，在剑桥学习时，他认为可以利用蒸汽机进行运算。起先他设计差分机用于计算导航表，后来，他发现差分机只是专门用途的机器，于是放弃了原来的研究，开始设计包含现代计算机基本组成部分的分析机。（Analytical Engine）Babbage的蒸汽动力计算机虽然最终没有完成，以今天的标准看也是非常原始的，然而，它勾画出现代通用计算机的基本功能部分，在概念上是一个突破。在接下来的若干年中，许多工程师在另一些方面取得了重要的进步，美国人Herman Hollerith（1860-1929），根据提花织布机的原理发明了穿孔片计算机，并带入商业领域建立公司。现代计算机发展历程第一代电子管计算机(1946-1957)1946年2月15日，标志现代计算机诞生的ENIAC（Electronic Numerical Integrator and Computer）在费城公诸于世。ENIAC代表了计算机发展史上的里程碑，它通过不同部分之间的重新接线编程，还拥有并行计算能力。ENIAC由美国政府和宾夕法尼亚大学合作开发，使用了18000个电子管，70000个电阻器，有5百万个焊接点，耗电160千瓦，其运算速度为每秒5000次。第一代计算机的特点是操作指令是为特定任务而编制的，每种机器有各自不同的机器语言，功能受到限制，速度也慢。另一个明显特征是使用真空电子管和磁鼓储存数据 .第二代晶体管计算机(1957-1964)运算器1948年，晶体管发明代替了体积庞大电子管，电子设备的体积不断减小。1956年，晶体管在计算机中使用，晶体管和磁芯存储器导致了第二代计算机的产生。第二代计算机体积小、速度快、功耗低、性能更稳定。1960年，出现了一些成功地用在商业领域、大学和政府部门的第二代计算机。第二代计算机用晶体管代替电子管，还有现代计算机的一些部件：打印机、磁带、磁盘、内存、操作系统等。计算机中存储的程序使得计算机有很好的适应性，可以更有效地用于商业用途。在这一时期出现了更高级的COBOL和FORTRAN等语言，使计算机编程更容易。新的职业（程序员、分析员和计算机系统专家）和整个软件产业由此诞生。第三代集成电路计算机(1964-1972)1958年德州仪器的工程师Jack Kilby发明了集成电路（IC），将三种电子元件结合到一片小小的硅片上。更多的元件集成到单一的半导体芯片上，计算机变得更小，功耗更低，速度更快。这一时期的发展还包括使用了操作系统，使得计算机在中心程序的控制协调下可以同时运行许多不同的程序。第四代大规模集成电路计算机 (1972-至今）大规模集成电路 (LSI) 可以在一个芯片上容纳几百个元件。到了 80 年代，超大规模集成电路(VLSI) 在芯片上容纳了几十万个元件，后来的 (ULSI) 将数字扩充到百万级。可以在硬币大小的芯片上容纳如此数量的元件使得计算机的体积和价格不断下降，而功能和可靠性不断增强。70 年代中期，计算机制造商开始将计算机带给普通消费者，这时的小型机带有友好界面的软件包，供非专业人员使用的程序和最受欢迎的字处理和电子表格程序。1981 年， IBM 推出个人计算机(PC) 用于家庭、办公室和学校。80 年代个人计算机的竞争使得价格不断下跌，微机的拥有量不断增加，计算机继续缩小体积。与 IBM PC 竞争的 Apple Macintosh 系列于 1984 年推出， Macintosh 提供了友好的图形界面，用户可以用鼠标方便地操作。纪事年表播报编辑1666年，在英国Samuel Morland发明了一部可以计算加数及减数的机械计数机。1673年，Gottfried Leibniz 制造了一部踏式（stepped）圆柱形转轮的计数机，叫“Stepped Reckoner”，这部计算器可以把重复的数字相乘，并自动地加入加数器里。运算器1694年，德国数学家，Gottfried Leibniz ，把巴斯卡的Pascalene 改良，制造了一部可以计算乘数的机器，它仍然是用齿轮及刻度盘操作。1773年，Philipp-Matthaus 制造及卖出了少量精确至12位的计算机器。1775年，The third Earl of Stanhope 发明了一部与Leibniz相似的乘法计算器。1786年，J.H.Mueller 设计了一部差分机，可惜没有拨款去制造。1801年，Joseph-Marie Jacquard 的织布机是用连接按序的打孔卡控制编织的样式。1854年，George Boole 出版 "An Investigation of the Laws of Thought”，是讲述符号及逻辑理由，它后来成为计算机设计的基本概念。1858年，一条电报线第一次跨越大西洋，并且提供了几日的服务。1861年，一条跨越大陆的电报线把大西洋和太平洋沿岸连接起来。1876年，Alexander Graham Bell 发明了电话并取得专利权。1876至1878年，Baron Kelvin 制造了一部泛音分析机及潮汐预测机。1882年，William S. Burroughs 辞去在银行文员的工作，并专注于加数器的发明。1889年，Herman Hollerith 的电动制表机在比赛中有出色的表现，并被用于 1890 中的人口调查。Herman Hollerith 采用了Jacquard 织布机的概念用来计算，他用咭贮存资料，然后注入机器内编译结果。这机器使本来需要十年时间才能得到的人口调查结果，在短短六星期内做到。1893年，第一部四功能计算器被发明。1895年，Guglielmo Marconi 传送广播讯号。1896年，Hollerith 成立制表机器公司（Tabulating Machine Company）。1901年，打孔键出现，之后的半个世纪只有很少的改变。1904年，John A.Fleming 取得真空二极管的专利权，为无线电通讯建立基础。1906年，Lee de Foredt 加了一个第三活门在Felming 的二极管， 创制了三电极真空管。1907年，唱片音乐在纽约组成第一间正式的电台。1908年，英国科学家 Campbell Swinton述了电子扫描方法及预示用阴极射线管制造电视。1911年，Hollerith 的表机公司与其它两间公司合并，组成 Computer Tabulating Recording Company (C-T-R），制表及录制公司。但在1924年，改名为International Business Machine Corporation （IBM）。1911年，荷兰物理学家 Kamerlingh Onnes 在 Leiden Unversity 发现超导电。1931年，Vannever Bush 发明了一部可以解决差分程序的计数机，这机器可以解决一些令数学家，科学家头痛的复杂差分程序。1935年，IBM (International Business Machine Corporation) 引入 "IBM 601”，它是一部有算术部件及可在1秒钟内计算乘数的穿孔咭机器。它对科学及商业的计算起很大的作用。总共制造了1500 部。1937年，Alan Turing 想出了一个 "通用机器（Universal Machine）” 的概念，可以执行任何的算法，形成了一个"可计算（computability）”的基本概念。Turing 的概念比其它同类型的发明为好，因为他用了符号处理（symbol processing） 的概念。1939年11月，John Vincent Atannsoff 与 John Berry 制造了一部16位加数器。它是第一部用真空管计算的机器。1939年，Zuse 与 Schreyer 开鈶制造了"V2”[后来叫Z2]，这机器沿用 Z1的机械贮存器，加上一个用断电器逻辑（Relay Logic）的新算术部件。但当 Zuse完成草稿后，这计划被中断一年。运算器1939-40年，Schreyer 完成了用真空管的10位加数器，以及用氖气灯（霓虹灯）的存贮器。1940年1月,在 Bell Labs,Samuel Williams 及Stibitz 完成了一部可以计算复杂数字的机器，叫“复杂数字计数机（Complex Number Calculator）”，后来改称为“断电器计数机型号I （Model I Relay Calculator）”。它用电话开关部份做逻辑部件：145个断电器，10个横杠开关。数字用“Plus 3BCD”代表。在同年9月，电传打字 etype 安装在一个数学会议里，由New Hampshire 连接去纽约。1940年，Zuse 终于完成Z2，它比运作得更好，但不是太可靠。1941年夏季，Atanasoff及Berry完成了一部专为解决联立线性方程系统（system of simultaneous linear equations) 的计算器，后来叫做"ABC (Atanasoff-Berry Computer）”，它有60个50位的存贮器，以电容器（capacitories）的形式安装在2个旋转的鼓上，时钟速度是60Hz。1941年2月，Zuse 完成"V3”（后来叫Z3），是第一部操作中可编写程序的计数机。它亦是用浮点操作，有7个位的指数，14位的尾数，以及一个正负号。存贮器可以贮存64 个字，所以需要1400个断电器。它有多于1200个的算术及控制部件，而程序编写，输入，输出的与 Z1 相同。1943年1月 Howard H. Aiken完成"ASCC Mark I”（自动按序控制计算器 Mark I ，Automatic Sequence -- Controlled Calculator Mark I），亦称“Haward Mark I”。这部机器有51尺长，重5顿，由750,000部份合并而成。它有72个累加器，每一个有自己的算术部件，及23位数的寄存器。1943年12月，Tommy Flowers与他的队伍，完成第一部“Colossus”，它有2400个真空管用作逻辑部件，5 个纸带圈读取器（reader），每个可以每秒工作5000字符。1943年，由 John Brainered领导，ENIAC开始研究。而 John Mauchly 及J. Presper Eckert负责这计划的执行。1946v第一台电子数字积分计算器（ENIAC）在美国建造完成。1947年，美国计算器协会（ACM）成立。1947年，英国完成了第一个存储真空管O 1948贝尔电话公司研制成半导体。1949年，英国建造完成"延迟存储电子自动计算器"(EDSAC)1950年，"自动化"一词第一次用于汽车工业。1951年，美国麻省理工学院制成磁心1952年，第一台"储存程序计算器"诞生。1952年，第一台大型计算机系统IBM701宣布建造完成。1952年，第一台符号语言翻译机发明成功。1954年，第一台半导体计算机由贝尔电话公司研制成功。1954年，第一台通用数据处理机IBM650诞生。1955年，第一台利用磁心的大型计算机IBM705建造完成。运算器1956年，IBM公司推出科学704计算机。1957年，程序设计语言FORTRAN问世。1959年，第一台小型科学计算器IBM620研制成功。1960年，数据处理系统IBM1401研制成功。1961年，程序设计语言COBOL问世。1961年，第一台分系统计算机由麻省理工学院设计完成。1963年，BASIC语言问世。1964年，第三代计算机IBM360系列制成。1965年，美国数字设备公司推出第一台小型机PDP-8。1969年，IBM公司研制成功90列卡片机和系统--3计算机系统。1970年，IBM系统1370计算机系列制成。1971年，伊利诺大学设计完成伊利阿克IV巨型计算机。1971年，第一台微处理机4004由英特尔公司研制成功。1972年，微处理机基片开始大量生产销售。运算器1973年，第一片软磁盘由IBM公司研制成功。1975年，ATARI--8800微电脑问世。1977年，柯莫道尔公司宣称全组合微电脑PET--2001研制成功。1977年，TRS--80微电脑诞生。1977年,苹果--II型微电脑诞生。1978年，超大规模集成电路开始应用。1978年，磁泡存储器第二次用于商用计算机。1979年，夏普公司宣布制成第一台手提式微电脑。1982年，微电脑开始普及，大量进入学校和家庭。1984年，日本计算机产业着手研制"第五代计算机"---具有人工智能的计算机

控制器：
主要分类播报编辑控制器分组合逻辑控制器和微程序控制器，两种控制器各有长处和短处。组合逻辑控制器设计麻烦，结构复杂，一旦设计完成，就不能再修改或扩充，但它的速度快。微程序控制器设计方便，结构简单，修改或扩充都方便，修改一条机器指令的功能，只需重编所对应的微程序；要增加一条机器指令，只需在控制存储器中增加一段微程序，但是，它是通过执行一段微程。具体对比如下：组合逻辑控制器又称硬布线控制器，由逻辑电路构成，完全靠硬件来实现指令的功能。工作原理播报编辑电磁吸盘控制器：交流电压380V经变压器降压后，经过整流器整流变成110V直流后经控制装置进入吸盘此时吸盘被充磁，退磁时通入反向电压线路，控制器达到退磁功能。门禁控制器：门禁控制器工作在两种模式之下。一种是巡检模式，另一种是识别模式。在巡检模式下，控制器不断向读卡器发送查询代码，并接收读卡器的回复命令。这种模式会一直保持下去，直至读卡器感应到卡片。当读卡器感应到卡片后，读卡器对控制器的巡检命令产生不同的回复，在这个回复命令中，读卡器将读到的感应卡内码数据传送到门禁控制器，使门禁控制器进入到识别模式。在门禁控制器的识别模式下，门禁控制器分析感应卡内码，同设备内存储的卡片数据进行比对，并实施后续动作。门禁控制器完成接收数据的动作后，会发送命令回复读卡器，使读卡器恢复状态，同时，门禁控制器重新回到巡检模式。常见种类播报编辑组合逻辑组合逻辑控制器由时序电路、指令译码电路和组合逻辑电路三部分组成。通过指令译码器确定当前执行的指令，结合时序电路产生的节拍，共同作为组合逻辑电路的输入结果输出相应的控制信号。组合逻辑控制器是由复杂组合逻辑门电路和触发器构成，执行速度快，因此在计算机结构比如RISC中得到广泛应用。 [1]设计步骤：1、设计机器的指令系统：规定指令的种类、指令的条数以及每一条指令的格式和功能；2、初步的总体设计：如寄存器设置、总线安排、运算器设计、部件间的连接关系等；3、绘制指令流程图：标出每一条指令在什么时间、什么部件进行何种操作；4、编排操作时间表：即根据指令流程图分解各操作为微操作，按时间段列出机器应进行的微操作；5、列出微操作信号表达式，化简，电路实现。基本组成：1、指令寄存器用来存放正在执行的指令。指令分成两部分：操作码和地址码。操作码用来指示指令的操作性质，如加法、减法等；地址码给出本条指令的操作数地址或形成操作数地址的有关信息（这时通过地址形成电路来形成操作数地址）。有一种指令称为转移指令，它用来改变指令的正常执行顺序，这种指令的地址码部分给出的是要转去执行的指令的地址。2、操作码译码器：用来对指令的操作码进行译码，产生相应的控制电平，完成分析指令的功能。3、时序电路：用来产生时间标志信号。在微型计算机中，时间标志信号一般为三级：指令周期、总线周期和时钟周期。微操作命令产生电路产生完成指令规定操作的各种微操作命令。这些命令产生的主要依据是时间标志和指令的操作性质。该电路实际是各微操作控制信号表达式（如上面的A→L表达式）的电路实现，它是组合逻辑控制器中最为复杂的部分。4、指令计数器：用来形成下一条要执行的指令的地址。通常，指令是顺序执行的，而指令在存储器中是顺序存放的。所以，一般情况下下一条要执行的指令的地址可通过将现行地址加1形成，微操作命令“1”就用于这个目的。如果执行的是转移指令，则下一条要执行的指令的地址是要转移到的地址。该地址就在本转移指令的地址码字段，将其直接送往指令计数器。微程序控制器的提出是因为组合逻辑设计存在不便于设计、不灵活、不易修改和扩充等缺点。微程序微程序控制(简称微码控制)的基本思路是：用微指令产生微操作命令，一条指令的功能通过执行一系列基本操作来完成，这些基本操作称为微操作，每个微操作在相应控制信号的控制下执行，这些控制信号在微程序设计中称为微命令。微程序是一个微指令序列，对应于一条机器指令的功能，每条微指令是一个0/1序列，其中包含若干个微命令，它完成一个基本运算或传送功能，有时也将微指令字，称作控制字(controlword) [2]。微程序控制器的组成：1、控制存储器（Control Memory）用来存放各机器指令对应的微程序。译码器用来形成机器指令对应的微程序的入口地址。当将一条机器指令对应的微程序的各条微指令逐条取出，并送到微指令寄存器时，其微操作命令也就按事先的设计发出，因而也就完成了一条机器指令的功能。对每一条机器指令都是如此。2、微指令的宽度直接决定了微程序控制器的宽度。为了简化控制存储器，可采取一些措施来缩短微指令的宽度。如采用字段译码法一级分段译码。显然，微指令的控制字段将大大缩短。，一些要同时产生的微操作命令不能安排在同一个字段中。为了进一步缩短控制字段，还可以将字段译码设计成两级或多级。CPU控制器是指挥计算机的各个部件按照指令的功能要求协调工作的部件，是计算机的神经中枢和指挥中心，由指令寄存器IR（InstructionRegister）、程序计数器PC（ProgramCounter）和操作控制器OC（OperationController）三个部件组成，对协调整个电脑有序工作极为重要。指令寄存器：用以保存当前执行或即将执行的指令的一种寄存器。指令内包含有确定操作类型的操作码和指出操作数来源或去向的地址。指令长度随不同计算机而异，指令寄存器的长度也随之而异。计算机的所有操作都是通过分析存放在指令寄存器中的指令后再执行的。指令寄存器的输人端接收来自存储器的指令，指令寄存器的输出端分为两部分。操作码部分送到译码电路进行分析，指出本指令该执行何种类型的操作;地址部分送到地址加法器生成有效地址后再送到存储器，作为取数或存数的地址。存储器可以指主存、高速缓存或寄存器栈等用来保存当前正在执行的一条指令。当执行一条指令时，先把它从内存取到数据寄存器（DR）中，然后再传送至IR。指令划分为操作码和地址码字段，由二进制数字组成。为了执行任何给定的指令，必须对操作码进行测试，以便识别所要求的操作。指令译码器就是做这项工作的。指令寄存器中操作码字段的输出就是指令译码器的输入。操作码一经译码后，即可向操作控制器发出具体操作的特定信号。程序计数器：指明程序中下一次要执行的指令地址的一种计数器，又称指令计数器。它兼有指令地址寄存器和计数器的功能。当一条指令执行完毕的时候，程序计数器作为指令地址寄存器，其内容必须已经改变成下一条指令的地址，从而使程序得以持续运行。为此可采取以下两种办法：第一种办法是在指令中包含了下一条指令的地址。在指令执行过程中将这个地址送人指令地址寄存器即可达到程序持续运行的目的。这个方法适用于早期以磁鼓、延迟线等串行装置作为主存储器的计算机。根据本条指令的执行时间恰当地决定下一条指令的地址就可以缩短读取下一条指令的等待时间，从而收到提高程序运行速度的效果。第二种办法是顺序执行指令。一个程序由若干个程序段组成，每个程序段的指令可以设计成顺序地存放在存储器之中，所以只要指令地址寄存器兼有计数功能，在执行指令的过程中进行计数，自动加一个增量，就可以形成下一条指令的地址，从而达到顺序执行指令的目的。这个办法适用于以随机存储器作为主存储器的计算机。当程序的运行需要从一个程序段转向另一个程序段时，可以利用转移指令来实现。转移指令中包含了即将转去的程序段入口指令的地址。执行转移指令时将这个地址送人程序计数器(此时只作为指令地址寄存器，不计数)作为下一条指令的地址，从而达到转移程序段的目的。子程序的调用、中断和陷阱的处理等都用类似的方法。在随机存取存储器普及以后，第二种办法的整体运行效果大大地优于第一种办法，因而顺序执行指令已经成为主流计算机普遍采用的办法，程序计数器就成为中央处理器不可或缺的一个控制部件。CPU内的每个功能部件都完成一定的特定功能。信息在各部件之间传送及数据的流动控制部件的实现。通常把许多数字部件之间传送信息的通路称为“数据通路”。信息从什么地方开始，中间经过哪个寄存器或多路开关，最后传到哪个寄存器，都要加以控制。在各寄存器之间建立数据通路的任务，是由称为“操作控制器”的部件来完成的。操作控制器的功能就是根据指令操作码和时序信号，产生各种操作控制信号，以便正确地建立数据通路，从而完成取指令和执行指令的控制。有两种由于设计方法不同因而结构也不同的控制器。微操作是指不可再分解的操作，进行微操作总是需要相应的控制信号(称为微操作控制信号或微操作命令)。一台数字计算机基本上可以划分为两大部分---控制部件和执行部件。控制器就是控制部件，而运算器、存储器、外围设备相对控制器来说就是执行部件。控制部件与执行部件的一种联系就是通过控制线。控制部件通过控制线向执行部件发出各种控制命令，通常这种控制命令叫做微命令，而执行部件接受微命令后所执行的操作就叫做微操作。控制部件与执行部件之间的另一种联系就是反馈信息。执行部件通过反馈线向控制部件反映操作情况，以便使得控制部件根据执行部件的状态来下达新的微命令，这也叫做“状态测试”。微操作在执行部件中是组基本的操作。由于数据通路的结构关系，微操作可分为相容性和相斥性两种。在机器的一个CPU周期中，一组实现一定操作功能的微命令的组合，构成一条微指令。一般的微指令格式由操作控制和顺序控制两部分构成。操作控制部分用来发出管理和指挥全机工作的控制信号。其顺序控制部分用来决定产生下一个微指令的地址。事实上一条机器指令的功能是由许多条微指令组成的序列来实现的。这个微指令序列通常叫做微程序。既然微程序是有微指令组成的，那么当执行当前的一条微指令的时候。必须指出后继微指令的地址，以便当前一条微指令执行完毕以后，取下一条微指令执行。LEDLED控制器(LED controller)就是通过芯片处理控制LED灯电路中的各个位置的开关。低压型LED产品控制器：低压型LED产品一般设计电压12V-36V,每个回路LED数量3-6个串联，用电阻降压限流，每个回路电流20mA以下。一个LED产品由多个回路的 LED组成，优点是低压，结构简单，容易设计；缺点是：产品规模大时电流很大，需要配置低压开关电源。由于产品的缺点所限，低压不可能远距离输电，都是局限于体积不大的产品上，如招牌文字、小图案等。根据这个特点，控制器设计规格：12V的选用75A/30V MOS功率管控制，输出电流8A/路；24-36V选用60A/50V MOS功率管控制，输出电流5A/路。用户可以根据以上规格选定控制器的路数，跳变的可以选购NE20低压系列、渐变的选购NE10低压系列控制器即可。注意LED的必须是共阳(+)极连接法，控制器控制阴(-)极，控制器不包括低压电源高压型LED产品控制器：高压型LED产品设计电压是交流/直流220V电压，每个回路LED数量36-48个串联，每个回路电流20mA以下，限流方式有两种，一种是电阻限流，这种方式电阻功耗较大，建议使用每4个LED串接一个1/4W金属模电阻，均匀分布散热，这种接法是最稳定可靠；另一种是电阻电容串联限流，这种接法大部分电压降在电容上，电阻功耗小，只能用在稳定的长亮状态，如果闪动电容储能，反而电压加倍，LED容易损坏。凡是使用控制器的LED必须使用电阻限流方式，LED一般每个回路一米，功率5W，三色功率每米15W。常用渐变控制器NE112K控制直流1200W，NE103D交流负载4500W直流负载1500W，如果灯管闪动单元多就使用NE112K，如果只需要整体闪动就使用NE103D。如果使用渐变方式，要注意负载匹配，霓虹灯和LED的发光分布特性不一样，同一回路不能混接不同类型的负载。低压串行控制器：低压型LED产品串行控制器的特点是控制路数多，利用串行信号传输达到控制的目的，一般512单元的控制只需要4条控制连线，串行LED控制器需要在LED的光源板配有寄存器，控制器可选用型号NE040S控制器，该控制器的最大容量达到4096KBit,如果负载512单元的LED可以最大实现8192桢画面。还有就是安全行业所使用的控制器，控制探测器在各工作区间内监测气体的一种设备。门禁门禁控制器就是门禁系统的核心，对出入口通道进行管制的系统大脑，它是在传统的门锁基础上发展而来的。门控制器是读卡和控制合二为一的门禁控制产品，有独立型的也有联网型的。简单而言，门禁控制器就是集门禁控制板、读卡器于一体的机器，高档点的还包括键盘跟显示屏，只需要接上电源就可以当完整的门禁系统使用了。门控制器的分类：1、按照门控制器和管理电脑的通讯方式分为：RS485联网型门控制器、TCP/IP网络型门控制器、不联网门控制器。（1）不联网门控制器，就是一个机子管理一个门，不能用电脑软件进行控制，也不能看到记录，直接通过控制器进行控制。特点是价格便宜，安装维护简单，不能查看记录，不适合人数量多于50或者人员经常流动（指经常有人入职和离职）的地方，也不适合门数量多于5的工程。（2）485联网门控制器，就是可以和电脑进行通讯的门禁类型，直接使用软件进行管理，包括卡和事件控制。所以有管理方便、控制集中、可以查看记录、对记录进行分析处理以用于其它目的。特点是价格比较高、安装维护难道加大，但培训简单，可以进行考勤等增值服务。适合人多、流动性大、门多的工程。（3）TCP/IP网络门控制器，也叫以太网联网门禁，也是可以联网的门禁系统，但是通过网络线把电脑和控制器进行联网。除具有485门禁联网的全部优点以外，还具有速度更快，安装更简单，联网数量更大，可以跨地域或者跨城联网。但存在设备价格高，需要有电脑网络知识。适合安装在大项目、人数量多、对速度有要求、跨地域的工程中。2、按照每台控制器控制的门的数量可以分为：单门控制器、双门控制器、四门控制器及多门控制器。3、控制器根据每个门可接读卡器的数量分为：单向控制器、双向控制器。注：如果一个门，进门刷卡，出门按按钮，控制器对于每个门只能接一个读卡器，叫单向控制器。如果一个门，进门刷卡，出门也刷卡（也可以接出门按钮），每个控制器对于每个门可以接两个读卡器，一个是进门读卡器，一个是出门读卡器，叫双向控制器。电动汽车电动汽车控制器是用来控制电动车电机的启动、运行、进退、速度、停止以及电动车的其它电子器件的核心控制器件，它就象是电动车的大脑，是电动车上重要的部件。电动车主要包括电动自行车、电动二轮摩托车、电动三轮车、电动三轮摩托车、电动四轮车、电瓶车等，电动车控制器也因为不同的车型而有不同的性能和特点。超静音设计技术：独特的电流控制算法，能适用于任何一款无刷电动车电机，并且具有相当的控制效果，提高了电动车控制器的普遍适应性，使电动车电机和控制器不再需要匹配。恒流控制技术：电动车控制器堵转电流和动态运行电流完全一致，保证了电池的寿命，并且提高了电动车电机的启动转矩。自动识别电机模式系统：自动识别电动车电机的换相角度、霍尔相位和电机输出相位，只要控制器的电源线、转把线和刹车线不接错，就能自动识别电机的输入及输出模式，可以省去无刷电动车电机接线的麻烦，大大降低了电动车控制器的使用要求。随动abs系统：具有反充电/汽车EABS刹车功能，引入了汽车级的EABS防抱死技术，达到了EABS刹车静音、柔和的效果，不管在任何车速下保证刹车的舒适性和稳定性，不会出现原来的abs在低速情况下刹车刹不住的现象，完全不损伤电机，减少机械制动力和机械刹车的压力，降低刹车噪音，大大增加了整车制动的安全性；并且刹车、减速或下坡滑行时将EABS产生的能量反馈给电池，起到反充电的效果，从而对电池进行维护，延长电池寿命，增加续行里程，用户可根据自己的骑行习惯自行调整EABS刹车深度。电机锁系统：在警戒状态下，报警时控制器将电机自动锁死，控制器几乎没有电力消耗，对电机没有特殊要求，在电池欠压或其他异常情况下对电动车正常推行无任何影响。自检功能：分动态自检和静态自检，控制器只要在上电状态，就会自动检测与之相关的接口状态，如转把，刹把或其它外部开关等等，一旦出现故障，控制器自动实施保护，充分保证骑行的安全，当故障排除后控制器的保护状态会自动恢复。反充电功能：刹车、减速或下坡滑行时将EABS产生的能量反馈给电池，起到反充电的效果，从而对电池进行维护，延长电池寿命，增加续行里程。堵转保护功能：自动判断电机在过流时是处于完全堵转状态还是在运行状态或电机短路状态，如果过流时是处于运行状态，控制器将限流值设定在固定值，以保持整车的驱动能力；如电机处于纯堵转状态，则控制器2秒后将限流值控制在10A以下，起到保护电机和电池，节省电能；如电机处于短路状态，控制器则使输出电流控制在2A以下，以确保控制器及电池的安全。动静态缺相保护：指在电机运行状态时，电动车电机任意一相发生断相故障时，控制器实行保护，避免造成电机烧毁，同时保护电动车电池、延长电池寿命。功率管动态保护功能：控制器在动态运行时，实时监测功率管的工作情况，一旦出现功率管损坏的情况，控制器马上实施保护，以防止由于连锁反应损坏其他的功率管后，出现推车比较费力的现象。防飞车功能：解决了无刷电动车控制器由于转把或线路故障引起的飞车现象，提高了系统的安全性。1+1助力功能：用户可自行调整采用自向助力或反向助力，实现了在骑行中辅以动力，让骑行者感觉更轻松。巡航功能：自动/手动巡航功能一体化，用户可根据需要自行选择，8秒进入巡航，稳定行驶速度，无须手柄控制。模式切换功能：用户可切换电动模式或助力模式。防盗报警功能：超静音设计，引入汽车级的遥控防盗理念，防盗的稳定性更高，在报警状态下可锁死电机，报警喇叭音效高达125dB以上，具有极强的威慑力。并具有自学习功能，遥控距离长达150米不会有误码产生。倒车功能：控制器增加了倒车功能，当用户在正常骑行时，倒车功能失效；当用户停车时，按下倒车功能键，可进行辅助倒车，并且倒车速度最高不超过10km/h。遥控功能：采用先进的遥控技术，长达256的加密算法，灵敏度多级可调，加密性能更好，并且绝无重码现象发生，极大地提高了系统的稳定性，并具有自学习功能，遥控距离长达150米不会有误码产生。高速控制：采用最新的为马达控制设计专用的单片机，加入全新的BLDC控制算法，适用于低于6000rpm高速、中速或低速电机控制。电机相位：60度120度电机自动兼容，不管是60度电机还是120度电机，都可以兼容，不需要修改任何设置。维修方法：1、当电动车有刷控制器没有输出时（1）将万用表设置在+20发（DC）档位，先测量闸把输出信号的高、低电位。（2）如捏闸把时，闸把信号有超过4V的电位变化，则可排除闸把故障。（3）然后按照有刷控制器常用世道上脚功能表，与测量出的主控世道民逻辑芯片的电压值进行电路分析，并检查各芯片外围器件（电阻、电容、二极管）的数值是否和元件表面的标识相一致。（4）最后检查外围器件或是集成电路出现故障，可以通过更换同型号的器件来排除故障。2、当电动车无刷控制器完全没有输出时（1）参照无刷电机控制器主相位检查测量图，用万用表直流电压+50V档，检测6路MOS管栅极电压是否与转把的转动角度呈对应关系。（2）如没有对，表示控制器里的PWM电路或MOS管驱动电路有故障。（3）参照无刷控制器主相位检查图，测量芯片的输入输出引脚的电压是否与转把转动角度有对应关系，可以判断哪些芯片有故障，更换同型号芯片即可排除故障。3、当电动车有刷控制器控制部件的电源不正常时不同种类的控制器(40张)（1）电动车控制器内部电源一般采用三端稳压集成电路，一般用7805、7806、7812、7815三端 稳压集成电路，它们的输出电压分别是5V、6V、12V、15V。（2）将万用表设置在直流电压+20V(DC)档位，将万用表黑表笔与红表笔分别靠在转把的黑线和红线上，观察万用表读数是否与标称电压相符，它们的上下电压差不应超过0.2V。（3）否则说明控制器内部电源出现故障了，一般有刷控制器可以通过更换三端稳压集成电路排除故障。4、当电动车无刷控制器缺相时电动车无刷控制器电源与闸把的故障可以参考有刷控制器的故障排除方法先予排除，对无刷控制器而言，还有其特有故障现象，比如缺相。电动车无刷控制器缺相现象可以分为主相位缺相和霍耳缺相两种情况。（1）主相位缺相的检测方法可以参照电动车有刷控制器飞车故障排除法，检测MOS管是否击穿，无刷控制器MOS管击穿一般是某一个相位的上下两个一对MOS管同时击穿，更换时确保同时更换。检查测量点。（2）电动车无刷控制器的霍耳缺相表现为控制器不能识别电机霍耳信号。火灾报警火灾自动报警系统应有自动和手动两种触发装置。各种类型的火灾探测器是自动触发装置，而在防火分区疏散通道、楼梯口等处设置的手动火灾报警按钮是手动触发装置，它应具有应急情况下，人工手动通报火警的功能。火灾报警控制器是火灾自动报警系统心脏，具有下述功能：1、用来接受火灾信号并启动火灾警报装置。该设备也可用来指示着火部位和记录有关信息；2、能通过火警发送装置启动火灾报警信号或通过自动消防灭火控制装置启动自动灭火设备和消防联动控制器；3、自动地监视系统的正确运行和对特定故障给出声、光报警。火灾报警控制器种类繁多，根据不同的方法可分成不同的类别：1、按控制范围可分为：a、区域火灾报警控制器：直接连接火灾探测器，处理各种报警信息。b、集中火灾报警控制器：它一般不与火灾探测器相连，而与区域火灾报警控制器相连，处理区域级火灾报警控制器送来的报警信号，常使用在较大型系统中。c、控制中心火灾报警控制器：它兼有区域，集中两级或火灾报警控制器的特点，即可以作区域级使用，连接控制器；又可以作集中级使用，连接区域火灾报警控制器。2、按结构型式可分为：（1）壁挂式火灾报警控制器：连接的探测器回路相应少些，控制功能简单，区域报警控制器多才用这种型式；（2）台式火灾报警控制器：连接探测器回路数较多，联动控制较复杂，集中式报警器常采用这种方式；（3）框式火灾报警控制器：可实现多回路连接，具有复杂的联动控制。3、按系统布线方式分为：（1）多线制火灾报警控制器：探测器与控制器的连接采用一一对应方式；（2）总线制火灾报警控制器：控制器与探测器采用总线方式连接，探测器并联或串联在总线上。火灾报警控制器的功能：1、火灾报警：当收到探测器、手动报警开关、消火栓开关及输入模块所配接的设备所发来的火警信号时，均可在报警器中报警；2、故障报警：系统运行时控制器分时巡检，若有异常（设备故障）发出声、光报警信号，并显示故障类型及编码等；3、火警优先：在故障报警或已处理火警时，若发生火警则报火警，而当火警清除后又自动报原有的故障。pid所谓PID控制，就是在一个闭环控制系统中，使被控物理量能够迅速而准确地无限接近于控制目标的一种手段。 PID 控制功能是变频器应用技术的重要领域之一，也是变频器发挥其卓越效能的重要技术手段。变频调速产品的设计、运行、维护人员应该充分熟悉并掌握PID控制的基本理论。工业自动化水平已成为衡量各行各业现代化水平的一个重要标志。同时，控制理论的发展也经历了古典控制理论、现代控制理论和智能控制理论三个阶段。智能控制的典型实例是模糊全自动洗衣机等。自动控制系统可分为开环控制系统和闭环控制系统。一个控制系统包括控制器﹑传感器﹑变送器﹑执行机构﹑输入输出接口。控制器的输出经过输出接口﹑执行机构，加到被控系统上；控制系统的被控量，经过传感器﹐变送器﹐通过输入接口送到控制器。不同的控制系统﹐其传感器﹑变送器﹑执行机构是不一样的。比如压力控制系统要采用压力传感器。电加热控制系统的传感器是温度传感器。PID控制及其控制器或智能PID控制器（仪表）已经很多，产品已在工程实际中得到了广泛的应用，有各种各样的PID控制器产品，各大公司均开发了具有PID参数自整定功能的智能调节器（intelligent regulator），其中PID控制器参数的自动调整是通过智能化调整或自校正、自适应算法来实现。有利用PID控制实现的压力、温度、流量、液位控制器，能实现PID控制功能的可编程控制器（PLC），还有可实现PID控制的PC系统等等。可编程控制器（PLC）是利用其闭环控制模块来实现PID控制，而可编程控制器可以直接与ControlNet相连，还有可以实现PID控制功能的控制器。母联母联控制器主要用于自动控制切换带母线联络断路的两路电源的供电系统。控制模式有母联备自投，进线备自投两种。组成母联自动转换开关的有：母联控制器、三相交流过欠压断相保护器、空气断路器。适合多型号断路器，有电动操作机构就能与控制器连接。自动转换开关自动转换开关控制器是一种具有可编程，自动化测量，LCD显示，数字通讯等为一体的智能双电源切换系统。在与低压空气断路器配套后，特别适合于两路低压进线侧的自动转换和保护。自动转换开关控制器的执行部件是框架式空气断路器，两台断路器不用加装适配器，控制器直接对供应电源状态进行监测，自动控制完成常用电源与备用电源的切换。1、控制器为两路低压进线提供自动转换控制和保护；2、适合多型号的框架断路器；3、控制器的电气联锁，断路器的机械联锁，确保二台断路器不能同时合闸；4、具有手动，自动转换功能；5、控制器与断路器直接二次线连接，中间无需适配器；6、在控制器或监控中心汉显两路电源的电量参数，并能设定和更改控制器所有参数；7、供电方式可设定为一路优先，二路优先或无优先；8、具有自启动油机功能；9、具有RS-232C和RS-485通讯接口。运动运动控制器是运动控制系统的核心部件。国内的运动控制器大致可以分为3类：第1类是以单片机等微处理器作为控制核心的运动控制器。这类运动控制器速度较慢、精度不高、成本相对较低，只能在一些低速运行和对轨迹要求不高的轮廓运动控制场合应用。第2类是以专用芯片（ASIC）作为核心处理器的运动控制器，这类运动控制器结构比较简单，大多只能输出脉冲信号，工作于开环控制方式。由于这类控制器不能提供连续插补功能，也没有前馈功能，特别是对于大量的小线段连续运动的场合不能使用这类控制器。第3类是基于PC总线的以DSP或FPGA作为核心处理器的开放式运动控制器。这类开放式运动控制器以DSP芯片作为运动控制器的核心处理器，以PC机作为信息处理平台，运动控制器以插件形式嵌入PC机，即“PC+运动控制器”的模式。这样的运动控制器具有信息处理能力强，开放程度高，运动轨迹控制准确，通用性好的特点。但是这种方式存在以下缺点：运动控制卡需要插入计算机主板的PCI或者ISA插槽，因此每个具体应用都必须配置一台PC机作为上位机。这无疑对设备的体积、成本和运行环境都有一定的限制，难以独立运行和小型化。微型微控制器（MicroController）又可简称MCU或μC，也有人称为单芯片微控制器（Single Chip Microcontroller），将ROM、RAM、CPU、I/O集合在同一个芯片中,为不同的应用场合做不同组合控制。微控制器在经过这几年不断地研究、发展，历经4位、8位，到如今的16位及32位，甚至64位。产品的成熟度，以及投入厂商之多、应用范围之广，真可谓之空前。在国外大厂因开发较早、产品线广，所以技术领先，而本土厂商则以多功能为产品导向取胜。基本功能播报编辑数据缓冲：由于I/O设备的速率较低而CPU和内存的速率却很高，故在控制器中必须设置一缓冲器。在输出时，用此缓冲器暂存由主机高速传来的数据，然后才以I/O设备所具有的速率将缓冲器中的数据传送给I/O设备；在输入时，缓冲器则用于暂存从I/O设备送来的数据，待接收到一批数据后，再将缓冲器中的数据高速地传送给主机。差错控制：设备控制器还兼管对由I/O设备传送来的数据进行差错检测。若发现传送中出现了错误，通常是将差错检测码置位，并向 CPU报告，于是CPU将本次传送来的数据作废，并重新进行一次传送。这样便可保证数据输入的正确性。数据交换：这是指实现CPU与控制器之间、控制器与设备之间的数据交换。对于前者，是通过数据总线，由CPU并行地把数据写入控制器，或从控制器中并行地读出数据；对于后者，是设备将数据输入到控制器，或从控制器传送给设备。为此，在控制器中须设置数据寄存器。状态说明：标识和报告设备的状态控制器应记下设备的状态供CPU了解。例如，仅当该设备处于发送就绪状态时，CPU才能启动控制器从设备中读出数据。为此，在控制器中应设置一状态寄存器，用其中的每一位来反映设备的某一种状态。当CPU将该寄存器的内容读入后，便可了解该设备的状态。接收和识别命令：CPU可以向控制器发送多种不同的命令，设备控制器应能接收并识别这些命令。为此，在控制器中应具有相应的控制寄存器，用来存放接收的命令和参数，并对所接收的命令进行译码。例如，磁盘控制器可以接收CPU发来的Read、Write、Format等15条不同的命令，而且有些命令还带有参数；相应地，在磁盘控制器中有多个寄存器和命令译码器等。地址识别：就像内存中的每一个单元都有一个地址一样，系统中的每一个设备也都有一个地址，而设备控制器又必须能够识别它所控制的每个设备的地址。此外，为使CPU能向(或从)寄存器中写入(或读出)数据，这些寄存器都应具有唯一的地址。

PC：
释义Private club 私人俱乐部；Padding Condenser 垫整[微调]电容器；Paper chromatography 纸色谱法；Parts Catalog 零件目录；Path Control 通路控制；Peace Corps 和平队[美]；Percentage of Completion 完工百分率；Petersen Coil 消弧线圈；Pharmaceutical companies 医药公司；Phosphatidyl Choline 磷酯胆碱；磷肌酸；Photocell 光敏电阻；Photoconductor 光敏电阻，光电导体；Photo Credit 照片来源/拍摄者Pitch Circle (齿轮)节圆；Plano-convex 平凸透镜；pokemon center 《口袋妖怪》中的口袋妖怪中心（亦称神奇宝贝中心，日：ポケモン センター）；Police Constable （香港、英国）警察；Polychloroprene 聚丁二烯；Polymer Concrete 聚合物混凝土；postal code 邮政编码；Postcrossing国际明信片交流；power center 动力中心；precast concrete 混凝土预制件；Prestressed Concrete 预应力混凝土；Priced Catalogue 价目表；Prime Cost 成本，原价；printed circuit 印刷电路；Process Controller 过程控制器；product carrier 成品油轮；Production Control 生产控制；Program Controller 程序控，指希望一切事物和活动都通过代码来来实现以及控制的程序员；Program Counter 程序计数器；Programmable Controller 可编程控制器；Proportional Counter 正比计数器；Pulse Controller 脉冲控制器；Pulse Counter 脉冲计数器；Punched Card 穿孔卡片；pyrolytic carbon 热解碳；Polarization Controller 偏振控制器。perfect combo 完美连击（音游使用，也作ap，all perfect）

MAR：
剧情简介播报编辑故事讲述一个运动不行、成绩很差、严重近视的中学二年级学生虎水银太，在一次机缘巧合下被”小丑守门人“传唤而来到一直向往着的奇幻的异世界——MAR HEAVEN。在这个世界上，人们使用着名为”ÄRM“的魔法银饰。银太遇到了活着的ÄRM“巴波“并结识了一群志同道合的队友，为打倒邪恶军团”Chess兵队“、守护异世界的和平而共同努力。 [1]魔兵传奇动画制作播报编辑制作人员原作 ：安西信行原案协力：都筑伸一郎、林正人、宫坂保志监督 ：奥胁雅晴（1话 ~ 52话）、川口敬一郎（53话~102话）Supervisor： 野村敦司、沢辺伸政、斎藤裕系列构成 ：武上纯希角色设定、总作画监督：小丸敏之ÄRM设计： 虾名康哲（1话~51话）今野幸一（52话~102话）美术监督：梶原芳郎色彩设计：黑柳朋子摄影监督：ひろちけんじ编辑：小峰博美音响监督：渡边淳音乐：池田大介3DCG制作：小学馆音乐&数码娱乐制作人：笹村武史（1~10话），青木俊志（11~71话），岩田伸一、古市直彦（72~102话）线上制作人：下地志直（1〜26话）动画制作人：樱井凉介动画制作 ：SynergySP制作：东京电视台、小学馆集英社制片公司参考资料 [2]角色配音角色日本配音台湾配音虎水银太熊井统子→（代役）比嘉久美子钱欣郁巴波银河万丈曹冀鲁白雪清水爱冯嘉德桃乐丝中岛沙树冯嘉德阿尔维斯保志总一朗吴文民杰克阪口大助冯嘉德那那西小野坂昌也曹冀鲁艾伦小杉十郎太、皆川纯子（孩童）黄天佑参考资料 [3]剧集信息播报编辑话数标题（日/中）剧本分镜演出作画监督1开け! 异世界の扉!!开启吧！异世界的大门！！武上纯希奥胁雅晴小高义规长森佳容2伝说のアーム! バッボ!!传说中的ARM！巴波！！のがみかずお齐藤和也3ジャック! バトルスコップ発动!!杰克！战斗银铲发动！！神户一彦千叶大辅植田秀仁千叶大辅冈辰也4ギンタ! バッボを取り返せ!!银太！把巴波抢回来！！早川正奥村吉昭李豪世5谜の美少年、アルヴィス谜之美少年，阿尔维斯植田浩二奥胁雅晴小林哲也江森真理子6氷の中の少女、スノウ冰中的少女，白雪松园公小高义规长森佳容7目覚めよ! もう一人のエド!!觉醒吧！另一个爱德！！早川正奥胁雅晴のがみかずお久保山英一田中薰8复活のナイト・ファントム复活的骑士・魅影神户一彦小林哲也千叶大辅冈辰也9修练の门、メリロとプモル修炼之门，梅莉露与布莫路武上纯希奥村吉昭李豪世10第二次メルヘヴン大戦第二次MAR HEAVEN大战植田浩二奥胁雅晴小林哲也江森真理子11见せてやる! バッボ・バージョン（2）!!让你见识一下！巴波的第二形态！！早川正松园公福田贵之小丸敏之12ルベリアのボス、ナナシ!鲁贝利亚的首领，那那西！神户一彦奥胁雅晴おゆなむ江原康之13地底湖の戦い! ナナシVSオルコ!!地底湖之战！那那西VS欧路戈！！植田浩二植田秀仁千叶大辅冈辰也14バッボバージョン（3）! 出て来いガーゴイル!!巴波变形系列三！出来吧，石翼魔！！武上纯希奥村吉昭李豪世157人目の仲间? ジョン・ピーチ!?第七名伙伴？约翰・皮奇？！植田浩二奥胁雅晴千叶大辅こひらかずと16ウォーゲーム开始!WAR GAME开始！早川正奥村吉昭藤本义孝石川哲也171STバトル（1）! アルヴィスVSレノ!!第一回合战斗！阿尔维斯VS雷诺！！神户一彦松园公冈崎幸男江原康之登坂晋181STバトル（2）! ジャックVSパノ!!第一回合之二！杰克VS芭诺！！武上纯希小林哲也江森真理子19キャプテン・ギンタ! ガーゴイルVSガロン队长・银太！石翼魔VS盖隆神户一彦ヤマトナオミチ草刈大介20修练の门再び! ケンカのやり方教えます!!修炼之门再临！传授打架之道！！早川正奥胁雅晴福田贵之石川哲也21砂漠フィールド! 闘うお姫様!!沙漠区！奋战的公主！！神户一彦奥村吉昭曾我笃史22ナナシVSロコ! 呪いのワラ人形!!那那西VS洛可！诅咒稻草人！！武上纯希松园公冈崎幸男樱井木之实23恐るべき魔女! ドロシー!!惊人的魔女！桃乐丝！！神户一彦小林哲也江森真理子24ファントムの密やかな楽しみ魅影一个人独享的乐趣植田浩二奥胁雅晴畠山茂树草刈大介25遅れてきた男! アラン!!迟来的男人！艾伦！！武上纯希福田贵之宇中仁26男を见せるぜジャック! 魔法のキノコ!!展现男子气概的杰克！魔法蘑菇！！神户一彦奥村吉昭曾我笃史27私、负けないよ! 火山群のスノウ!!我决不会输的！火山群中的白雪！！早川正奥胁雅晴冈崎幸男樱井木之实28呪いのロウソク! ギンタVSカノッチ!!诅咒蜡烛！银太VS卡诺奇！！武上纯希植田秀仁小林哲也江森真理子29もうひとつのゾンビタトゥ! アルヴィスVSロラン!!另一个僵尸刺青！阿尔维斯VS罗兰！！植田浩二ヤマトナオミチ草刈大介30戦栗! ファントムとゾディアックのナイト!!战栗！魅影与13星座骑士！！武藏境考福田贵之槙田一章31シャドーバトル! ガーゴイルVSブラックガーゴイル!!影子对战！石翼魔VS黑影石翼魔！！武上纯希大关雅幸曾我笃史32静かなる闘志…アルヴィスの力…沉寂的斗志… 阿尔维斯的力量…神户一彦大平直树冈崎幸男岛田贤志古谷田顺久33どうなるジャック! どうするドロシー!?杰克会怎样！桃乐丝怎么办？！静谷伊佐夫おおそ独犬山口美浩氏家嘉宏野道佳代34アクアとアッコちゃんとナナシ流!亚可亚，可可与那那西的作风！高桥奈津子奥胁雅晴菅井嘉浩植田理恵35逆袭のギロム! エゴラVSガーゴイル!!奇洛姆的反攻！艾克拉VS石翼魔植田浩二奥村吉昭福田贵之まきだかずあき36ドロシーVSラプンツェル、呗え。クレイジーキルト!!桃乐丝VS拉普洁 唱吧，疯狂娃娃！！神户一彦大关雅幸曾我笃史37魔法の国、カルデアへ魔法王国，卡鲁帝亚武上纯希向中野义雄近藤优次38侵略者ファントム・ギンタ激闘の果てに…侵略者魅影・银太激战的结果植田浩二奥胁雅晴冈崎幸男岛田贤志39こども大好きナイト、アッシュ!!喜欢小孩的骑士，亚修！！静谷伊佐夫井草かほる山口美浩氏家嘉宏野道佳代40世界一ブサイク决定戦!? スノウVSエモキス!!世界第一丑选拔赛？！白雪VS爱摩奇丝！！高桥奈津子中村贤太郎小野田雄亮菅井嘉浩器田竹一41夺われた魔力! アルヴィスの危机!!魔力被夺走了！阿尔维斯的危机！！神户一彦奥田诚治福田贵之まきだかずあき42快感! 石使いのナイト、キャンディス!!快感！石使者骑士，小甜甜！！植田浩二奥胁雅晴大关雅幸曾我笃史43死の戦场! サイコスペース!!死亡战场！惊魂空间！！武上纯希向中野义雄近藤优次44运命の死闘! ナナシVSガリアン!!命中注定的决斗！那那西VS卡里安！！植田浩二奥胁雅晴冈崎ゆきお岛田贤志45雷撃×雷撃! ナナシ、よみがえる记忆!!雷击×雷击！那那西苏醒的记忆！！山口美浩氏家嘉宏野道佳代46新生ナイト、复讐のイアン!新生骑士 复仇的伊安！神户一彦井草かほる小野田雄亮岛村惠美子47伤だらけのアルヴィス遍体鳞伤的阿尔维斯静谷伊佐夫奥胁雅晴福田贵之まきだかずあき48怒りのドロシー! 砂漠の塔愤怒的桃乐丝！沙漠之塔高桥奈津子大关雅幸曾我笃史49ゾンネンズ! 狙われた修练の门!!黑星帮！被侵犯的修炼之门！！武上纯希向中野义雄近藤优次50アルヴィス×ナナシ! 禁断のラビリンス!!阿尔维斯X那那西！禁止进入的迷宫！！植田浩二奥田诚治德本善信はっとりますみ51ドロシー×スノウ! 诱惑のルージュ!!桃乐丝X白雪！魅惑唇膏！！神户一彦山口美浩氏家嘉宏野道佳代52届け! 希望のシックスセンス!!传送吧！希望的第六感！！武上纯希井草かほる小野田雄亮岛村惠美子53ファントムを倒す打倒魅影静谷伊佐夫川口敬一郎甲藤元54アランのサブイボ起鸡皮疙瘩的艾伦高桥奈津子川口敬一郎大关雅幸曾我笃史55アルヴィスが许せない阿尔维斯不能原谅植田浩二山口美浩近藤优次56ドロシーが食べられた桃乐丝被吃掉了静谷伊佐夫阿久たすく佐野英敏阿部宗孝57スノウが笑った白雪笑了高桥奈津子篠崎康行高木信一郎58イアンが怒る伊安愤怒了植田浩二小野田雄亮岛村惠美子59ギンタ东京へ银太去东京武上纯希川口敬一郎まきだかずあき60ナナシの暴走失控的那那西川口敬一郎大关雅幸曾我笃史61深窓のドロシー深宅中的桃乐丝高桥奈津子山口美浩近藤优次62アランの名探侦艾伦的名侦探静谷伊佐夫武上纯希阿久たすく阿部宗孝63アルヴィスと少女阿尔维斯和少女植田浩二みくりや恭辅佐藤道雄雨宫英雄64小雪の仮面小雪的假面具武上纯希小野田雄亮岛村惠美子65不思议の国のリリス不可思议之国的莉莉丝川口敬一郎まきだかずあき66レギンレイヴ姫の秘密雷琴列城公主的秘密猪爪慎一佐野隆史大关雅幸曾我笃史67ロコと呪いの新ARM洛可与新的诅咒ARM山口美浩近藤优次68ラストバトル始まるッ最终决战开始濑藤健嗣阿部宗孝69ジャックと炎のガーディアン杰克与炎之守护者濑藤健嗣みくりや恭辅保田康治70再戦 アルヴィス対ロラン再战 阿尔维斯VS罗兰高桥奈津子川口敬一郎小野田雄亮岛村惠美子71永远の刹那永远的刹那川口敬一郎まきだかずあき72悲しみの花嫁(キメラ)悲伤的新娘川口敬一郎大関雅幸小林一三73アランの古伤(トラウマ)艾伦的旧伤静谷伊佐夫大关雅幸山口美浩近藤优次74さらば旧友(ハロウィン)别了 老友朝日燃阿部宗孝75ルベリアの誓い鲁贝利亚的誓言植田浩二佐野隆史みくりや恭辅保田康治76真红の爪(ガーネットクロウ)真红之爪小野田雄亮岛村惠美子77ギンタ対ファントム银太VS魅影武上纯希川口敬一郎まきだかずあき78最凶猫ガーディアン最强 猫咪守护者川口敬一郎浅见松雄小林一三79决着ッ决战山口美浩近藤优次80かわいい来客可爱来客高桥奈津子濑藤健嗣朝日燃阿部宗孝81バッボ割れる巴波碎裂植田浩二佐野隆史みくりや恭辅保田康治82复活の咆哮复活的咆哮小野田雄亮岛村惠美子83イアンと花嫁伊安和新娘猪爪慎一川口敬一郎小丸敏之84スノウ夺还夺回公主植田浩二大关雅幸小林一三85爱の岚ゼピュロスブルーム爱之风暴 西风的扫帚猪爪慎一川口敬一郎山口美浩近藤优次86时间の轮舞(ロンド)时间轮舞武上纯希阿久たすく阿部宗孝87城塞都市パルトガイン要塞都市 塔鲁特盖猪爪慎一东海林真一みくりや恭辅佐藤道雄88ファントムの梦魅影的梦想高桥奈津子川口敬一郎小野田雄亮服部宪知89覚醒のゾンビタトゥ觉醒的僵尸刺青植田浩二高山功名仓智史90忘却のクラヴィーア忘却的葛拉维亚武上纯希高柳哲司大关雅幸小林一三91アルヴィスよ瞑れ阿尔维斯 安息吧猪爪慎一川口敬一郎山口美浩近藤优次92ミスティ キャッスル密西缇城堡武上纯希阿久たすく阿部宗孝93冥界のアナリーゼ冥界的风暴植田浩二山崎たかし桥口洋介藤崎贤二94蔷薇は散りてなお染まり蔷薇未被染红却已凋谢高桥奈津子川口敬一郎小野田雄亮服部宪知95スノウの真実白雪的真相猪爪慎一小高义规高山功名仓智史96伪りの平和虚伪的和平武上纯希佐野隆史大关雅幸小林一三97炎に散り 水に眠る在火焰中散去 在水中长眠猪爪慎一山崎たかし山口美浩近藤优次98さらば心优しきチェス永别了 善良的的CHESS兵队植田浩二猪爪慎一濑藤健嗣藤崎贤二99アルヴィスの光阿尔维斯之光猪爪慎一佐野隆史小野田雄亮服部宪知100哀の岚ゼピュロスブルーム哀伤的风暴 西风的扫帚高桥奈津子川口敬一郎高山功名仓智史101ギンタ対ダンナ银太VS主人武上纯希大关雅幸小林一三102ワクワクは止まらない最终回 兴奋不已川口敬一郎近藤优次参考资料 [4]动画音乐播报编辑片头曲序号集数曲名演唱作词作曲编曲OP11~26君の思い描いた梦 集メル HEAVEN（汇集你所思所想之梦的天堂）GARNET CROWAZUKI七中村由利古井弘人OP227~51晴れ时计（晴朗的指示钟）OP352~77梦・花火（梦・烟花）OP478~102风とRAINBOW（风与彩虹）片尾曲序号集数曲名演唱作词作曲编曲ED11~13I just wanna hold you tight（我只想抱紧你）小松未步小松未步小松未步小林哲ED214~26不机嫌になる私（不要追问我）岩田小百合小松未步小松未步Miss TyED327~39毎日アドベンチャー（每日冒险）Sparkling☆PointSparkling☆Point三好诚大贺好修ED440~51桜色（樱色）竹井诗织里AZUKI七桂花小林哲ED552~64MIRACLE（奇迹）爱内里菜爱内里菜大野爱果叶山武ED665~77今宵エデンの片隅で（今宵伊甸园的一角）GARNET CROWAZUKI七中村由利古井弘人ED778~90もう心摇れたりしないで（不要再让内心动摇了）北原爱子北原爱子北原爱子古井弘人ED891~101この手を伸ばせば（若是伸出这双手）GARNET CROWAZUKI七中村由利古井弘人注：第102集（最终话）片尾曲采用OP1《君の思い描いた梦 集メル HEAVEN》。参考资料 [5]关联游戏播报编辑标题游戏平台发行日期MAR HEAVEN KNOCKIN'ON HEAVEN'S DOOR（打开天堂之门）GBA2005年6月30日MAR HEAVEN ÄRM FIGHT DREAMPS22005年11月3日MAR HEAVEN 卡尔迪亚的恶魔NDS2006年3月30日MAR HEAVEN 忘却的古拉维亚2006年9月7日参考资料 [6]

定点数：
基本简介播报编辑1. 定点数表示法(fixed-point)所谓定点格式，即约定机器中所有数据的小数点位置是固定不变的。在计算机中通常采用两种简单的约定：将小数点的位置固定在数据的最高位之前，或者是固定在最低位之后。一般常称前者为定点小数，后者为定点整数。定点小数是纯小数，约定的小数点位置在符号位之后、有效数值部分最高位之前。若数据x的形式为x=x0.x1x2…xn(其中x0为符号位，x1～xn是数值的有效部分，也称为尾数，x1为最高有效位)，则在计算机中的表示形式为：一般说来，如果最末位xn= 1，前面各位都为0，则数的绝对值最小，即|x|min= 2^(-n)。如果各位均为1，则数的绝对值最大，即|x|max=1-2^(-n)。所以定点小数的表示范围是：2^(-n)≤|x|≤1 -2^(-n)表示方法播报编辑任何一个定点小数都可以被写成 ：N = NS . N-1 N-2 … N-M如果在计算机中用m+1个二进制位表示上述小数，则可以用最高(最左)一个二进制位表示符号(如用0表示正号，则1就表示负号)，而用后面的m个二进制位表示该小数的数值。小数点不用明确表示出来，因为它总是固定在符号位与最高数值位之间。定点小数的取值范围很小,对用m+1个二进制位的小数来说,其值的范围为：|N| ≤ 1-2^(-m) ，即小于1的纯小数。这对用户算题是十分不方便的，因为在算题前，必须把要用的数，通过合适的 "比例因子"化成绝对值小于1的小数，并保证运算的中间和最终结果的绝对值也都小于1，在输出真正结果时，还要把计算的结果按相应比例加以扩大。定点小数表示法，主要用在早期的计算机中，它最节省硬件。随着计算机硬件成本的大幅度降低，现代的通用计算机都被设计成能处理与计算多种类型数值的计算机。我们将主要通过定点小数讨论数值数据的不同编码方案，而且，定点小数也被用来表示浮点数的尾数部分。

浮点数：
简介播报编辑浮点计算浮点计算是指浮点数参与的运算，这种运算通常伴随着因为无法精确表示而进行的近似或舍入。一个浮点数a由两个数m和e来表示：a = m × b^e。在任意一个这样的系统中，我们选择一个基数b（记数系统的基）和精度p（即使用多少位来存储）。m（即尾数）是形如±d.ddd...ddd的p位数（每一位是一个介于0到b-1之间的整数，包括0和b-1)。如果m的第一位是非0整数,m称作规格化的。有一些描述使用一个单独的符号位(s 代表+或者-）来表示正负，这样m必须是正的。e是指数。结构由此可以看出，在计算机中表示一个浮点数，其结构如下：尾数部分（定点小数） 阶码部分（定点整数）阶符±阶码e数符±尾数m这种设计可以在某个固定长度的存储空间内表示定点数无法表示的更大范围的数。浮点加法减法运算设有两个浮点数x和y,它们分别为x = Mx*2^Exy = My*2^Ey其中Ex和Ey分别为数x和y的阶码,Mx和My为数x和y的尾数。两浮点数进行加法和减法的运算规则是设 Ex小于等于Ey，则 x±y = (Mx*2^(Ex－Ey)±My)*2^Ey,完成浮点加减运算的操作过程大体分为四步：1. 0 操作数的检查；2. 比较阶码大小并完成对阶；3. 尾数进行加或减运算；4. 结果规格化并进行舍入处理。⑴ 0 操作数检查浮点加减运算过程比定点运算过程复杂。如果判知两个操作数x或y中有一个数为0,即可得知运算结果而没有必要再进行后续的一系列操作以节省运算时间。0操作数检查步骤则用来完成这一功能。⑵ 比较阶码大小并完成对阶两浮点数进行加减，首先要看两数的阶码是否相同，即小数点位置是否对齐。若二数阶码相同，表示小数点是对齐的，就可以进行尾数的加减运算。反之，若二数阶码不同，表示小数点位置没有对齐，此时必须使二数阶码相同，这个过程叫作对阶。要对阶，首先应求出两数阶码Ex和Ey之差，即△E = Ex－Ey若△E=0,表示两数阶码相等，即Ex=Ey；若△E>0,表示Ex>Ey；若△E<0,表示Ex<Ey。当Ex≠Ey 时，要通过尾数的移动以改变Ex或Ey,使之相等。原则上，既可以通过Mx移位以改变Ex来达到Ex=Ey,也可以通过My移位以改变Ey来实现Ex=Ey。但是，由于浮点表示的数多是规格化的，尾数左移会引起最高有效位的丢失，造成很大误差。尾数右移虽引起最低有效位的丢失，但造成误差较小。因此，对阶操作规定使尾数右移，尾数右移后阶码作相应增加，其数值保持不变。显然，一个增加后的阶码与另一个阶码相等，增加的阶码的一定是小阶。因此在对阶时，总是使小阶向大阶看齐，即小阶的尾数向右移位（⑶ 尾数求和运算对阶结束后，即可进行尾数的求和运算。不论加法运算还是减法运算，都按加法进行操作，其方法与定点加减法运算完全一样。⑷ 结果规格化在浮点加减运算时，尾数求和的结果也可以得到01.ф…ф或10.ф…ф，即两符号位不等，这在定点加减法运算中称为溢出，是不允许的。但在浮点运算中，它表明尾数求和结果的绝对值大于1,向左破坏了规格化。此时将运算结果右移以实现规格化表示，称为向右规格化。规则是：尾数右移1位，阶码加1。当尾数不是1.M时需向左规格化。⑸ 舍入处理在对阶或向右规格化时，尾数要向右移位，这样，被右移的尾数的低位部分会被丢掉，从而造成一定误差，因此要进行舍入处理。简单的舍入方法有两种：一种是"0舍1入"法，即如果右移时被丢掉数位的最高位为0则舍去，为1则将尾数的末位加"1"。另一种是"恒置一"法，即只要数位被移掉，就在尾数的末尾恒置"1"。在IEEE754标准中，舍入处理提供了四种可选方法：就近舍入其实质就是通常所说的"四舍五入"。例如，尾数超出规定的23位的多余位数字是10010,多余位的值超过规定的最低有效位值的一半，故最低有效位应增1。若多余的5位　是01111,则简单的截尾即可。对多余的5位10000这种特殊情况：若最低有效位现为0,则截　尾；若最低有效位现为1,则向上进一位使其变为 0。朝0舍入 即朝数轴原点方向舍入，就是简单的截尾。无论尾数是正数还是负数，截尾都使取值的绝对值比原值的绝对值小。这种方法容易导致误差积累。朝+∞舍入 对正数来说，只要多余位不全为0则向最低有效位进1；对负数来说则是简单的截尾。朝－∞舍入 处理方法正好与 朝+∞舍入情况相反。对正数来说,只要多余位不全为0则简单截尾；对负数来说，向最低有效位进1。⑹ 溢出处理浮点数的溢出是以其阶码溢出表现出来的。在加\减运算过程中要检查是否产生了溢出：若阶码正常，加（减）运算正常结束；若阶码溢出，则要进行相应处理。另外对尾数的溢出也需要处理。阶码上溢 超过了阶码可能表示的最大值的正指数值，一般将其认为是+∞和－∞。阶码下溢 超过了阶码可能表示的最小值的负指数值，一般将其认为是0。尾数上溢 两个同符号尾数相加产生了最高位向上的进位，将尾数右移，阶码增1来重新对齐。尾数下溢 在将尾数右移时，尾数的最低有效位从尾数域右端流出，要进行舍入处理。实例播报编辑题目例如，一个指数范围为±4的4位十进制浮点数可以用来表示43210,4.321或0.0004321,但是没有足够的精度来表示432.123和43212.3（必须近似为432.1和43210)。当然，实际使用的位数通常远大于4。特别数值此外，浮点数表示法通常还包括一些特别的数值：+∞和−∞（正负无穷大）以及NaN（'Not a Number'）。无穷大用于数太大而无法表示的时候,NaN则指示非法操作或者无法定义的结果。二进制表示众所周知，计算机中的所有数据都是以二进制表示的，浮点数也不例外。然而浮点数的二进制表示法却不像定点数那么简单了。浮点数概念先澄清一个概念，浮点数并不一定等于小数，定点数也并不一定就是整数。所谓浮点数就是小数点在逻辑上是不固定的，而定点数只能表示小数点固定的数值，具用浮点数或定点数表示某哪一种数要看用户赋予了这个数的意义是什么。C++中的浮点数有6种，分别是：float：单精度,32位unsigned float：单精度无符号,32位double：双精度,64位long double：高双精度,80位下面仅以float（带符号，单精度,32位）类型的浮点数说明C++中的浮点数是如何在内存中表示的。先讲一下基础知识，纯小数的二进制表示。纯小数要想用二进制表示，必须先进行规格化，即化为 1.xxxxx * ( 2 ^ n ) 的形式（“^”代表乘方,2 ^ n表示2的n次方）。对于一个纯小数D,求n的公式如下：n = 1 + log2(D); // 纯小数求得的n必为负数再用 D / ( 2 ^ n ) 就可以得到规格化后的小数了。接下来就是十进制到二进制的转化问题，为了更好的理解，先来看一下10进制的纯小数是怎么表示的，假设有纯小数D,它小数点后的每一位数字按顺序形成一个数列：{k1,k2,k3,...,kn}那么D又可以这样表示：D = k1 / (10 ^1 ) + k2 / (10 ^ 2 ) + k3 / (10 ^ 3 ) + ... + kn / (10 ^ n )推广到二进制中，纯小数的表示法即为：D = b1 / (2 ^ 1 ) + b2 / (2 ^ 2 ) + b3 / (2 ^ 3 ) + ... + bn / (2 ^ n )现在问题就是怎样求得b1,b2,b3,……,bn。算法描述起来比较复杂，还是用数字来说话吧。声明一下,1 / ( 2 ^ n ）这个数比较特殊，我称之为位阶值。例二例如0.456,第1位,0.456小于位阶值0.5故为0；第2位,0.456大于位阶值0.25,该位为1,并将0.456减去0.25得0.206进下一位；第3位,0.206大于位阶值0.125,该位为1,并将0.206减去0.125得0.081进下一位；第4位,0.081大于0.0625,为1,并将0.081减去0.0625得0.0185进下一位；第5位0.0185小于0.03125……最后把计算得到的足够多的1和0按位顺序组合起来，就得到了一个比较精确的用二进制表示的纯小数了，同时精度问题也就由此产生，许多数都是无法在有限的n内完全精确的表示出来的，我们只能利用更大的n值来更精确的表示这个数，这就是为什么在许多领域，程序员都更喜欢用double而不是float。float的内存结构，我用一个带位域的结构体描述如下：struct MYFLOAT{bool bSign : 1; // 符号，表示正负,1位char cExponent : 8; // 指数,8位unsigned long ulMantissa : 32; // 尾数,32位};符号就不用多说了,1表示负,0表示正指数是以2为底的，范围是 -128 到 127,实际数据中的指数是原始指数加上127得到的，如果超过了127,则从-128开始计，其行为和X86架构的CPU处理加减法的溢出是一样的。比如：127 + 2 = -127；-127 - 2 = 127尾数都省去了第1位的1,所以在还原时要先在第一位加上1。它可能包含整数和纯小数两部分，也可能只包含其中一部分，视数字大小而定。对于带有整数部分的浮点数，其整数的表示法有两种，当整数大于十进制的16777215时使用的是科学计数法，如果小于或等于则直接采用一般的二进制表示法。科学计数法和小数的表示法是一样的。小数部分则是直接使用科学计数法，但形式不是X * ( 10 ^ n ），而是X * （0 000000000000000000000000000000符号位 指数位 尾数位--------------------------------------------------------------------------------例三判断两个浮点数是否相等。在这个例子中我们以C++代码来判别两个浮点数是否相等。由于浮点数在存储中无法精确表示，所以 fp1==fp2 无法准确的判断float型变量fp1与fp2是否相等。应该使用 （fp1-fl2）<0.0000001 来进行判断。示例：bool equal(float fp1,float fp2){if( abs( fp1 - fp2 ) < 0.00000001 ) return true;elsereturn false;}--------------------------------------------------------------------------------导数字分布播报编辑简介作者：concreteHAM什么是浮点数，不用我多说，这里我们要讨论的是规格化的任意进制浮点数的前导数字的概率分布。在《计算机程序设计艺术》第二卷中做了非常深入的讨论，这里我从中精炼出要点。实例例如：浮点数2.345 E 67这是一个十进制规格化浮点数，前导数字就是2。就只有一个“随机”的浮点数而言，讨论其分布式没有意义的，我们要讨论的是充分多个“随机”数进行的一系列运算后产生的浮点结果的前导数字分布。假设现在有一巨大的浮点数集，依此对数集中每个浮点数都乘以2,其中有一个十进制浮点数F,它的前导数字是1,那么它底数可能的值范围就是1.000…～1.999…，乘以一个数字2,那么它的底数就变成2.000…～3.999…，很明显乘以2以前前导数字是1的浮点个数与现在前导数字是2、3的浮点个数相同。以此我们接下来分析。对于一个b进制的浮点数，它的前导数字x范围就是0 < x < b,设f(x)是上述数集的前导数字的概率密度函数（注：是密度函数），那么它在前导数字u和v之间(0<u<v<b)的概率就是：∫[u,v]f(x)dx ⑴由前面所述的，对于一个充分小增量Δx,f(x)必须满足这样一个公式：f⑴Δx = x*f(x)Δx ⑵因为：f⑴Δx是f⑴微分段内的概率，根据前面所述,f⑴Δx概率等于f(1*x)*(x*Δx)很明显：f(x) = f⑴/x ⑶两边在[1,b]之间进行积分，等号左边必定为1,右边等于f⑴ln(b)：1 = f⑴ln(b) ⑷得：f⑴ = 1/ln(b) 带入⑶中：f(x) = 1/(x*ln(b))那么利用⑴式得：∫[u,v]1/(x*ln(b))dx= ln(v/u) / ln(b) ⑸这就是求前导数字的概率分布函数。例如b = 10进制时，前导数字为1的概率就是：= ln((1+1)/1) / ln⑽≈ 0.301前导数字为9的概率就是：= ln((9+1)/9) / ln⑽≈0.0458以下是一个测试程序（Mathematica软件）：T[n_,b_]:=Block[{res={},ran,i,a},For[i=1,i<b,i++;res=Append[res,0]];For[i=0,i<n,i++;ran=Random[]*Random[]*Random[]; 充分打乱浮点数ran=Log[b,ran];a=Floor[b^(ran-Floor[ran])]; 取出前导数字res[[a]]++ 对前导数字个数统计];Return[res]]执行T[100000,10],以10进制测试100000个浮点数，得到一个分布：{30149,18821,13317,9674,7688,6256,5306,4655,4134}和理论值相当接近。

逻辑运算：
简介播报编辑布尔运算是数字符号化的逻辑推演法，包括联合、相交、相减。在图形处理操作中引用了这种逻辑运算方法以使简单的基本图形组合产生新的形体，并由二维布尔运算发展到三维图形的布尔运算。由于布尔在符号逻辑运算中的特殊贡献，很多计算机语言中将逻辑运算称为布尔运算，将其结果称为布尔值。数学布尔运算播报编辑产生逻辑运算又称布尔运算布尔用数学方法研究逻辑问题，成功地建立了逻辑演算。他用等式表示判断，把推理看作等式的变换。这种变换的有效性不依赖人们对符号的解释，只依赖于符号的组合规律 。这一逻辑理论人们常称它为布尔代数。20世纪30年代，逻辑代数在电路系统上获得应用，随后，由于电子技术与计算机的发展，出现各种复杂的大系统，它们的变换规律也遵守布尔所揭示的规律。表示方法"∨" 表示"或""∧" 表示"与"."┐"表示"非"."=" 表示"等价".1和0表示"真"和"假"(还有一种表示,"+"表示"或", "·"表示"与"）计算机编程布尔运算播报编辑逻辑运算 (logical operators) 通常用来测试真假值。最常见到的逻辑运算就是循环的处理，用来判断是否该离开循环或继续执行循环内的指令。各种编程语言中的逻辑运算符作用CPascal等于===不等于!=<>小于<<大于>>小于等于<=<=大于等于>=>=与&&and或||or非!not异或^xor运算规则组合\结果\运算符.....And.......Or.........Xor0......0.......................0..........0............01......0.......................0..........1............10......1.......................0..........1............11......1.......................1..........1............0简单的说And:同为真时为真Or:同为假时为假Xor:相同为假三维图形布尔运算播报编辑作用Boolean（布尔运算）通过对两个以上的物体进行并集、差集、交集的运算，从而得到新的物体形态。系统提供了4种布尔运算方式：Union（并集）、Intersection（交集）和Subtraction（差集，包括A-B和B-A两种） [1]。效果物体在进行布尔运算后随时可以对两个运算对象进行修改操作，布尔运算的方式、效果也可以编辑修改，布尔运算修改的过程可以记录为动画，表现神奇的切割效果。组成部分布尔运算练习模型:骰子Boolean（布尔运算）的参数面板可分成三部分。Pick Boolean（拾取布尔运算对象）卷展栏该卷展栏用来拾取运算对象B。在布尔运算中，两个原始对象被称为运算对象，一个叫运算对象A，另一个叫运算对象B。在建立布尔运算前，首先要在视图中选择一个原始对象，这时Boolean按钮才可以使用。进入布尔运算命令面板后，单击Pick Operand B命令按钮来选择第二个运算对象。· Pick Operand B（拾取运算对象B）：单击该按钮，在场景中选择另一个物体完成布尔合成。其下的4个选项用来控制运算对象B的属性，它们要在拾取运算对象B之前确定。· Reference（参考）：将原始对象的参考复制品作为运算对象B，以后改变原始对象，也会同时改变布尔物体中的运算对象B，但改变运算对象B，不会改变原始对象。· Copy（复制）：将原始对象复制一个作为运算对象B，而不改变原始对象。当原始对象还要作其他之用时选用该方式。· Move（移动）：将原始对象直接作为运算对象B，它本身将不再存在。当原始对象无其他用途时选该用方式。该方式为默认方式。· Instance（关联）：将原始对象的关联复制品作为运算对象B，以后对两者中之一进行修改时都会同时影响另一个。Parameters（参数）卷展栏该卷展栏参数可分为三个区域。Operands（操作对象）选项组该组参数用来显示所有的运算对象的名称，并可对它们作相关的操作。Operands List（操作对象列表）：该列表框中列出所有的运算对象，供编辑操作时选择使用。Name（名称）：显示列表框中选中的操作对象的名称。可对其进行编辑。Extract Operand（提取运算对象）：它将当前指定的运算对象重新提取到场景中，作为一个新的可用对象，包括Instance（关联）和Copy（拷贝）两种属性。这样进入了布尔运算的物体仍可以被释放到场景中。只有从其上方的列表框中选择一个操作对象后才能激活该按钮。注意： 该按钮只有在修改面板中才可用。当创建面板处于激活状态时，不能从布尔物体中提取出操作对象。联想到前面所述的变形对象，在进入了变形预备物体中后，却无法再返回到场景中。不过对此还有一个可行的方法，就是利用Snapshot（快照）工具，在变形的关键帧快照克隆出一个新的造型。Operation（运算方式）选项组该组参数提供了4种运算方式可供选择。· Union（并集）：用来将两个造型合并，相交的部分将被删除，运算完成后两个物体将成为一个物体。· Intersection（交集）：用来将两个造型相交的部分保留下来，删除不相交的部分。· Subtraction（A-B）（A-B部分）：在A物体中减去与B物体重合的部分。· Subtraction（B- A）（B- A部分）：在B物体中减去与A物体重合的部分。· Cut（切除）：用B物体切除A物体，但不在A物体上添加B物体的任何部分。当Cut（切除）单选按钮被选中时，它将激活其下方的4个单选按钮让用户选择不同的切除类型。· Refine（细化）：在A物体上沿着B物体与A物体相交的面增加顶点和边数以细化A物体的表面。也就是说，根据B物体的外形将A物体的表面重新细分。· Split（劈裂）：其工作方法与Refine（细化）类似。只不过在B物体切割A物体部分的边缘多加了一排顶点。利用这种方法可以根据其他物体的外形将一个物体分成两部分。· Remove Inside（移除内部）：删除A物体中所有在B物体内部的片段面。其工作方法和Subtraction（A-B）（A-B部分）类似，只是同时也切除了B物体的表面。· Remove Outside（移除外部）：删除A物体中所有在B物体外部的片段面。其工作方法和Intersection（交集）类似，只是同时也切除了B物体的表面。Display（显示）/Update（更新）卷展栏该卷展栏参数用来控制是否在视图中显示运算结果以及每次修改后何时进行重新计算，更新视图。Display（显示）选项组该组参数用来决定是否在视图中显示布尔运算的结果，包含三个选项。· Result（结果）：显示每项布尔运算的计算结果。· Operands（操作对象）：只显示布尔合成物体而不显示运算结果。这样可以加快显示速度。· Result + Hidden Ops（结果+隐藏物体）：在实体着色的实体内以线框方式显示出隐藏的运算对象，主要用于动态布尔运算的编辑操作。Update（更新）选项组该组参数用来决定何时进行重新计算并显示布尔效果。· Always（总是）：每一次操作后都立即显示布尔结果。· When Rendering（渲染时）：只有在最后渲染时才重新计算更新效果。· Manually（手动）：选择此选项，下面的Update（更新）按钮可用，它提供手动的更新控制。· Update（更新）：需要观看更新效果时，按下此按钮，系统进行重新计算。

CISC：
产品介绍播报编辑计算机处理器包含有实现各种功能的指令或微指令，指令集越丰富，为微处理器编写程序就越容易，但是丰富的微指令集会影响其性能。复杂指令集计算机（CISC）体系结构的设计策略是使用大量的指令，包括复杂指令。与其他设计相比，在CISC中进行程序设计要比在其他设计中容易，因为每一项简单或复杂的任务都有一条对应的指令。程序设计者不需要写一大堆指令去完成一项复杂的任务。 但指令集的复杂性使得CPU和控制单元的电路非常复杂。[1]CISC包括一个丰富的微指令集，这些微指令简化了在处理器上运行的程序的创建。指令由汇编语言所组成，把一些原来由软件实现的常用的功能改用硬件的指令系统实现，编程者的工作因而减少许多，在每个指令期同时处理一些低阶的操作或运算，以提高计算机的执行速度，这种系统就被称为复杂指令系统。在CISC指令集的各种指令中，其使用频率却相差悬殊，大约有20%的指令会被反复使用，占整个程序代码的80%。而余下的80%的指令却不经常使用，在程序设计中只占20%。与RISC对比播报编辑 [1]RISC(精简指令集计算机) 设计方案，如它的名字所蕴涵的那样，有一个简化的指令集，该指令集提高处理器的效率但是需要有更复杂的外部程序。RISC结构优先选取使用频最高的简单指令，避免复杂指令；将指令长度固定，指令格式和寻地方式种类减少；以控制逻辑为主，不用或少用微码控制等措施来提高运算速度。RISC设计方案是根据John Cocke在IBM所做的工作形成的。John Cocke发现大约20%的计算机指令完成大约80%的工作。因此，基于RISC的系统通常比CISC系统速度快。它的80/20规则促进了RISC体系结构的发展。当然，和CISC架构相比较，尽管RISC架构有上述的优点，但不能认为RISC架构就可以取代CISC架构，事实上，RISC和CISC各有优势，而且界限并不那么明显。现代的CPU往往采用CISC的外围，内部加入了RISC的特性，如超长指令集CPU就是融合了RISC和CISC的优势，成为未来的CPU发展方向之一。

RISC：
释义RNA诱导沉默复合体（RNA-induced silencing complex，RISC）：一种由siRNA与Argonaute蛋白和Dicer酶复合形成的复合物。在RNAi中，利用siRNA的反义链切割靶mRNA，达到基因沉默。 [2]RISC由Dicer酶、Argonaute蛋白、siRNA等多种生物大分子装配而成。RISC 的组装是 RNAi 和 miRNA 途径的中心环节，包括 small RNA 的形成，small RNA 进入RISC 装载复合体（RISC loading complex，RLC）并进而转变为有沉默目标 mRNA 活性的 RISC  [1]。研究表明，RISC中的Dicer具有RNaseIII 结构域，在RNAi的起始阶段负责催化siRNA的产生，在RISC装配过程中起稳定RISC中间体结构和功能的作用；Argonaute蛋白是RISC中的核心蛋白，有PAZ和PIWI两个主要的结构域，前者为siRNA的传递提供结合位点,后者是RISC中的酶切割活性中心；siRNA是RISC完成特异性切割作用的向导，在成熟的RISC中虽然只包含siRNA的一条链，但siRNA在RISC形成过程中的双链结构是保证RNAi效应的决定因素 [3]。

计算机组成：
概念播报编辑计算机组成的任务是在指令集系统结构确定分配给硬件系统的功能和概念结构之后，研究各组成部分的内部构造和相互联系，以实现机器指令集的各种功能和特性。这种联系包括各功能部件的内部和相互作用。计算机组成要解决的问题是在所希望达到的性能和价格下，怎样最佳，最合理地把各个设备和部件组成成计算机，已实现所确定的ISA。计算机组成设计要确定的方面应包括：(1)数据通路宽度：数据总线上一次并行传送的信息位数。(2)专用部件的设置：是否设置乘除法、浮点运算、字符处理、地址运算等专用部件，设置的数量与机器要达到的速度、价格及专用部件的使用频度等有关。(3)各种操作对部件的共享程度：分时共享使用程度高，虽限制了速度，但价格便宜。设置部件多降低共享程度，因操作并行度提高，可提高速度，但价格也会提高。(4)功能部件的并行度：是用顺序串行，还是用重叠、流水或分布式控制和处理。(5)控制机构的组成方式：用硬联还是微程序控制，是单机处理还是多机或功能分布处理。(6)缓冲和排队技术：部件间如何设置及设置多大容量的缓冲器来协调它们的速度差；用随机、先进先出、先进后出、优先级，还是循环方式来安排事件处理的顺序。(7)预估、预判技术：为优化性能用什么原则预测未来行为。(8)可靠性技术：用什么冗余和容错技术来提高可靠性。硬件组成部分播报编辑主要分为五个部分：1. 控制器(control unit)：是整个计算机的中枢神经，其功能是对程序规定的控制信息进行解释，根据其要求进行控制，调度程序、数据、地址，协调计算机各部分工作及内存与外设的访问等。2. 运算器(arithmetic logic unit)：运算器的功能是对数据进行各种算术运算和逻辑运算，即对数据进行加工处理。3. 存储器(memory)：存储器的功能是存储程序、数据和各种信号、命令等信息，并在需要时提供这些信息。4. 输入(input)：输入设备是计算机的重要组成部分，输入设备与输出设备合称为外部设备，简称外设，输入设备的作用是将程序、原始数据、文字、字符、控制命令或现场采集的数据等信息输入到计算机。常见的输入设备有键盘、鼠标器、光电输入机、磁带机、磁盘机、光盘机等。5. 输出(output)：输出设备与输入设备同样是计算机的重要组成部分，它把计算机的中间结果或最后结果、机内的各种数据符号及文字或各种控制信号等信息输出出来。微机常用的输出设备有显示终端CRT、打印机、激光印字机、绘图仪及磁带、光盘机等。CPU=控制器+运算器   主板=I/O总线，输入输出系统   存储器=内存+硬盘 I/O设备：键盘、鼠标、扫描仪、显示器、数字化仪，读卡机、纸带等。键盘数字化仪扫描仪鼠标软件播报编辑软件概述计算机软件(computer software)是指计算机系统中的程序及其文档。程序是计算任务的处理对象和处理规则的描述；文档是为了便于了解程序所需的阐述性资料。程序必须装入机器内部才能工作，文档一般是给人看的，不一定装入机器。软件是用户与硬件之间的接口界面。用户主要是通过软件与计算机进行交流。软件是计算机系统设计的重要依据。为了方便用户，为了使计算机系统具有较高的总体效用，在设计计算机系统时，必须通盘考虑软件与硬件的结合，以及用户的要求和软件的要求。软件的正确含义应该是:(1)运行时，能够提供所要求功能和性能的指令或计算机程序集合。(2)程序能够满意地处理信息的数据结构。(3)描述程序功能需求以及程序如何操作和使用所要求的文档。软件具有与硬件不同的特点:(1)表现形式不同硬件有形，有色，有味，看得见，摸得着，闻得到。而软件无形，无色，无味，看不见，摸不着，闻不到。软件大多存在人们的脑袋里或纸面上，它的正确与否，是好是坏，一直要到程序在机器上运行才能知道。这就给设计、生产和管理带来许多困难。(2)生产方式不同软件是开发，是人的智力的高度发挥，不是传统意义上的硬件制造。尽管软件开发与硬件制造之间有许多共同点，但这两种活动是根本不同的。(3)要求不同硬件产品允许有误差，而软件产品却不允许有误差。(4)维护不同硬件是要用旧用坏的，在理论上，软件是不会用旧用坏的，但在实际上，软件也会变旧变坏。因为在软件的整个生存期中，一直处于改变维护状态。计算机软件分为系统软件和应用软件，如果把计算机比喻为一个人的话，那么硬件就表示人的身躯。而软件则表示人的思想、灵魂。一台没有安装任何软件的计算机称之为“裸机”。系统软件系统软件是指控制和协调计算机及外部设备,支持应用软件开发和运行的系统，是无需用户干预的各种程序的集合，主要功能是调度，监控和维护计算机系统；负责管理计算机系统中各种独立的硬件，使得它们可以协调工作。系统软件使得计算机使用者和其他软件将计算机当作一个整体而不需要顾及到底层每个硬件是如何工作的。（如Windows、Linux、DOS、Unix等操作系统都属于系统软件。）应用软件应用软件（application software）是用户可以使用的各种程序设计语言，以及用各种程序设计语言编制的应用程序的集合，分为应用软件包和用户程序。应用软件包是利用计算机解决某类问题而设计的程序的集合，供多用户使用。计算机软件分为系统软件和应用软件两大类。应用软件是为满足用户不同领域、不同问题的应用需求而提供的那部分软件。 它可以拓宽计算机系统的应用领域，放大硬件的功能。（如Word、Excel、QQ等都属于应用软件）

I/O设备：
功能介绍播报编辑输入/输出设备模型输入/输出（Input /Output ,简称I/O），指的是一切操作、程序或设备与计算机之间发生的数据传输过程。输入/输出系统(Input/Output System) ，指控制计算机数据流动的体制，包括程序、硬件。输入/输出设备，就是指可以与计算机进行数据传输的硬件。最常见的I/O设备有打印机、硬盘、键盘和鼠标。从严格意义上来讲，它们中有一些只能算是输入设备（比如说键盘和鼠标）；有一些只是输出设备（如打印机）。所有储存器也可以算是输入/输出设备。如硬盘、软盘、光盘等。I/O设备分类播报编辑现代计算机系统中配置了大量的外围设备，即I/O设备。依据它们的工作方式的不同，通常进行如下分类：（1）字符设备（character device），又叫做人机交互设备。用户通过这些设备实现与计算机系统的通信。它们大多是以字符为单位发送和接受数据的，数据通信的速度比较慢。例如，键盘和显示器为一体的字符终端、打印机、扫描仪、包括鼠标等，还有早期的卡片和纸带输入和输出机。含有显卡的图形显示器的速度相对较快，可以用来进行图像处理中的复杂图形的显示。（2）块设备（block device），又叫外部存储器，用户通过这些设备实现程序和数据的长期保存。与字符设备相比，它们是以块为单位进行传输的，如磁盘、磁带和光盘等。块的常见尺寸为512~32768B之间。（3）网络通信设备。这类设备主要有网卡、调制解调器等，主要用于与远程设备的通信。这类设备的传输速度比字符设备高，但比外部存储器低。这种分类的方法并不完备，有些设备并没有包括。例如，时钟既不是按块访问，也不是按字符访问，它所做的是按照预先规定好的时间间隔产生中断。但是这种分类足以使操作系统构造出处理I/O设备的软件，使它们独立于具体的设备。I/O设备故障播报编辑计算机中的I/O设备故障 [1]表现主要有以下三个方面：1.I/O设备就无法正常使用了，包括各类外接接口、笔记本的键盘打不出字、触控屏不灵等现象。2.电脑维修工具：主板诊断卡插在主板上进行跑码会显示FF代码、00代码、DD代码或无代码及反复跑C1~C5代码的现象。3.I/O设备短路等故障还会导致计算机连接外部设备的时候可能会受到静电的冲击或干扰以至于损坏其他电容、二极管等元器件。从而导致设备无法开机的严重后果。防范I/O设备故障：I/O设备设备属于精密的电子产品，使用过程中对环境要求严格，切莫在高温，潮湿的环境下使用。

寻址方式：
简介播报编辑在存储器中，操作数或指令字写入或读出的方式，有地址指定方式、相联存储方式和堆栈存取方式。几乎所有的计算机，在内存中都采用地址指定方式。当采用地址指定方式时，形成操作数或指令地址的方式称为寻址方式。寻址方式分为两类，即指令寻址方式和数据寻址方式，前者比较简单，后者比较复杂。值得注意的是，在传统方式设计的计算机中，内存中指令的寻址与数据的寻址是交替进行的。 [2]寻址模式的数量播报编辑不同的计算机体系结构在硬件中提供的寻址模式数量上有很大差异。 消除复杂寻址模式并仅使用一个或几个更简单的寻址模式有一些好处，即使它需要一些额外的指令，也许还需要一个额外的寄存器。 如果只有一些简单的寻址模式，那么设计管流水线CPU将变得更为简单。大多数RISC架构只有大约五种简单的寻址模式，而DECCAX等CISC架构有十几种寻址模式，其中一些非常复杂。 IBM System/360架构只有三种寻址模式，System/390又添加了一些。当只有少数寻址模式时，所需的特定寻址模式通常在指令代码中编码（例如IBM System/360和后继者，还有大多数RISC）。 但是当存在许多寻址模式时，通常在指令中留出特定字段来指定寻址模式。 DEC VAX允许几乎所有指令有多个存储器操作数，因此保留每个操作数说明符的前几位以指示该特定操作数的寻址模式。 保持寻址模式指定符位与操作码操作位分离产生正交指令集 。即使在具有许多寻址模式的计算机上，实际程序的测量表明下面列出的简单寻址模式占所有寻址模式的约90％或更多。 由于大多数此类测量基于编译器从高级语言生成的代码，因此这在某种程度上反映了所使用的编译器的局限性。  指令寻址播报编辑指令的寻址方式有以下两种。顺序寻址方式由于指令地址在内存中按顺序安排，当执行一段程序时，通常是一条指令接一条指令地顺序进行。也就是说，从存储器取出第1条指令，然后执行这条指令；接着从存储器取出第2条指令，再执行第二条指令；接着再取出第3条指令。 [2]这种程序顺序执行的过程，称为指令的顺序寻址方式。为此，必须使用程序计数器（又称指令计数器）PC来计数指令的顺序号，该顺序号就是指令在内存中的地址。跳跃寻址方式当程序转移执行的顺序时，指令的寻址就采取跳跃寻址方式。所谓跳跃，是指下条指令的地址码不是由程序计数器给出，而是由本条指令给出。注意，程序跳跃后，按新的指令地址开始顺序执行。因此，程序计数器的内容也必须相应改变，以便及时跟踪新的指令地址。采用指令跳跃寻址方式，可以实现程序转移或构成循环程序，从而能缩短程序长度，或将某些程序作为公共程序引用。指令系统中的各种条件转移或无条件转移指令，就是为了实现指令的跳跃寻址而设置的。 [2]注意是否跳跃可能受到状态寄存器的操作数的控制，而跳跃到的地址分为绝对地址（由标记符直接得到）和相对地址（对于当前指令地址的偏移量），跳跃的结果是当前指令修改PC程序计数器的值，所以下一条指令仍是通过程序计数器PC给出。操作数寻址播报编辑形成操作数的有效地址的方法称为操作数的寻址方式。由于大型机、小型机、微型机和单片机结构不同，从而形成了各种不同的操作数寻址方式。下面介绍一些比较典型又常用的操作数寻址方式。 [2]隐含寻址这种类型的指令，不是明显地给出操作数的地址。而是在指令中隐含着操作数的地址。例如，单地址的指令格式，就不明显地在地址字段中指出第2操作数的地址，而是规定累加寄存器AC作为第2操作数地址。指令格式明显指出的仅是第1操作数的地址D。因此，累加寄存器AC对单地址指令格式来说是隐含地址。 [2]如：DAA ；立即寻址指令的地址字段指出的不是操作数的地址，而是操作数本身，这种寻址方式称为立即寻址。立即寻址方式的特点是指令执行时间很短，因为它不需要访问内存取数，从而节省了访问内存的时间。 [2]如：MOV AX,#5678H 注意：立即数只能作为源操作数，不能作为目的操作数。直接寻址直接寻址是一种基本的寻址方法，其特点是：在指令格式的地址的字段中直接指出操作数在内存的地址。由于操作数的地址直接给出而不需要经过某种变换，所以称这种寻址方式为直接寻址方式。在指令中直接给出参与运算的操作数及运算结果所存放的主存地址，即在指令中直接给出有效地址 [2]间接寻址间接寻址是相对直接寻址而言的，在间接寻址的情况下，指令地址字段中的形式地址不是操作数的真正地址，而是操作数地址的指示器，或者说此形式地址单元的内容才是操作数的有效地址。 [2]寄存器寻址方式当操作数不放在内存中，而是放在CPU的通用寄存器中时，可采用寄存器寻址方式。显然，此时指令中给出的操作数地址不是内存的地址单元号，而是通用寄存器的编号（可以是8位也可以是16位（AX，BX，CX，DX））。指令结构中的RR型指令，就是采用寄存器寻址方式的例子。如：MOV DS，AX寄存器间接寻址方式与寄存器寻址方式的区别在于：指令格式中的寄存器内容不是操作数，而是操作数的地址，该地址指明的操作数在内存中。 [2]相对寻址方式相对寻址是把程序计数器PC的内容加上指令格式中的形式地址D而形成操作数的有效地址。程序计数器的内容就是当前指令的地址。“相对”寻址，就是相对于当前的指令地址而言。采用相对寻址方式的好处是程序员无须用指令的绝对地址编程，因而所编程序可以放在内存的任何地方。 [2]指令格式：MOV　AX，[BX+1200H]　操作数物理地址PA=(DS/SS)*16H+EA EA=(BX/BP/SI/DI)+(6/8)位偏移量Disp 对于BX，SI，DI寄存器来说段寄存器默认为DS，对于BP来说，段寄存器默认为SS [3]基址寻址方式在基址寻址方式中将CPU中的基址寄存器的内容，加上变址寄存器的内容而形成操作数的有效地址。基址寻址的优点是可以扩大寻址能力，因为与形式地址相比，基址寄存器的位数可以设置得很长，从而可以在较大的存储空间中寻址。 [2]变址寻址方式变址寻址方式与基址寻址方式计算有效地址的方法很相似，它把CPU中某个变址寄存器的内容与偏移量D相加来形成操作数有效地址。但使用变址寻址方式的目的不在于扩大寻址空间，而在于实现程序块的规律变化。为此，必须使变址寄存器的内容实现有规律的变化（如自增1、自减1、乘比例系数）而不改变指令本身，从而使有效地址按变址寄存器的内容实现有规律的变化。 [2]块寻址方式块寻址方式经常用在输入输出指令中，以实现外存储器或外围设备同内存之间的数据块传送。块寻址方式在内存中还可用于数据块移动。 [2]

多核处理器：
技术发展播报编辑256线程的CPU英特尔工程师们开发了多核芯片，使之满足“横向扩展”（而非“纵向扩充”）方法，从而提高性能。该架构实现了“分治法”战略。通过划分任务，线程应用能够充分利用多个执行内核，并可在特定的时间内执行更多任务。多核处理器是单枚芯片（也称为“硅核”），能够直接插入单一的处理器插槽中，但操作系统会利用所有相关的资源，将每个执行内核作为分立的逻辑处理器。通过在两个执行内核之间划分任务，多核处理器可在特定的时钟周期内执行更多任务。多核架构能够使软件更出色地运行，并创建一个促进未来的软件编写更趋完善的架构。尽管认真的软件厂商还在探索全新的软件并发处理模式，但是，随着向多核处理器的移植，现有软件无需被修改就可支持多核平台。操作系统专为充分利用多个处理器而设计，且无需修改就可运行。为了充分利用多核技术，应用开发人员需要在程序设计中融入更多思路，但设计流程与对称多处理 (SMP)系统的设计流程相同，并且现有的单线程应用也将继续运行。得益于线程技术的应用在多核处理器上运行时将显示出卓越的性能可扩充性。此类软件包括多媒体应用（内容创建、编辑，以及本地和数据流回放）、工程和其他技术计算应用以及诸如应用服务器和数据库等中间层与后层服务器应用。多核技术能够使服务器并行处理任务，而在以前，这可能需要使用多个处理器，多核系统更易于扩充，并且能够在更纤巧的外形中融入更强大的处理性能，这种外形所用的功耗更低、计算功耗产生的热量更少。多核技术是处理器发展的必然。推动微处理器性能不断提高的因素主要有两个：半导体工艺技术的飞速进步和体系结构的不断发展。半导体工艺技术的每一次进步都为微处理器体系结构的研究提出了新的问题，开辟了新的领域；体系结构的进展又在半导体工艺技术发展的基础上进一步提高了微处理器的性能。这两个因素是相互影响，相互促进的。一般说来，工艺和电路技术的发展使得处理器性能提高约20倍，体系结构的发展使得处理器性能提高约4倍，编译技术的发展使得处理器性能提高约1.4倍。但是今天，这种规律性的东西却很难维持。多核的出现是技术发展和应用需求的必然产物。发展历程播报编辑1971年，英特尔推出的全球第一颗通用型微处理器4004，由2300个晶体管构成。当时，公司的联合创始人之一戈登摩尔(Gordon Moore)，就提出后来被业界奉为信条的“摩尔定律”——每过18个月，芯片上可以集成的晶体管数目将增加一倍。在一块芯片上集成的晶体管数目越多，意味着运算速度即主频就更快。今天英特尔的奔腾(Pentium)四至尊版840处理器，晶体管数量已经增加至2.5亿个，相比当年的4004增加了10万倍。其主频也从最初的740kHz(每秒钟可进行74万次运算)，增长到3.9GHz(每秒钟运算39亿次)以上。当然，CPU主频的提高，或许在一定程度上也要归功于1975年进入这个领域的AMD公司的挑战。正是这样的“双雄会”，使得众多计算机用户有机会享受不断上演的“速度与激情”。一些仍不满足的发烧友甚至选择了自己超频，因为在玩很多游戏时，更快的速度可以带来额外的饕餮享受。但到了2005年，当主频接近4GHz时，英特尔和AMD发现，速度也会遇到自己的极限：那就是单纯的主频提升，已经无法明显提升系统整体性能。以英特尔发布的采用NetBurst架构的奔腾四CPU为例，它包括Willamette、Northwood和Prescott等三种采用不同核心的产品。利用冗长的运算流水线，即增加每个时钟周期同时执行的运算个数，就达到较高的主频。这三种处理器的最高频率，分别达到了2.0G、3.4G和3.8G。按照当时的预测，奔腾四在该架构下，最终可以把主频提高到10GHz。但由于流水线过长，使得单位频率效能低下，加上由于缓存的增加和漏电流控制不利造成功耗大幅度增加，3.6GHz奔腾四芯片在性能上反而还不如早些时推出的3.4GHz产品。所以，Prescott产品系列只达到3.8G，就戛然而止。英特尔上海公司一位工程师在接受记者采访时表示，Netburst微架构的好处在于方便提升频率，可以让产品的主频非常高。但性能提升并不明显，频率提高50%，性能提升可能微不足道。因为Netburst微架构的效率较低，CPU计算资源未被充分利用，就像开车时“边踩刹车边踩油门”。此外，随着功率增大，散热问题也越来越成为一个无法逾越的障碍。据测算，主频每增加1G，功耗将上升25瓦，而在芯片功耗超过150瓦后，现有的风冷散热系统将无法满足散热的需要。3.4GHz的奔腾四至尊版，晶体管达1.78亿个，最高功耗已达135瓦。实际上，在奔腾四推出后不久，就在批评家那里获得了“电炉”的美称。更有好事者用它来玩煎蛋的游戏。很显然，当晶体管数量增加导致功耗增长超过性能增长速度后，处理器的可靠性就会受到致命性的影响。就连戈登摩尔本人似乎也依稀看到了“主频为王”这条路的尽头——2005年4月，他曾公开表示，引领半导体市场接近40年的“摩尔定律”，在未来10年至20年内可能失效。多核心CPU解决方案(多核)的出现，似乎给人带来了新的希望。早在上世纪90年代末，就有众多业界人士呼吁用CMP(单芯片多处理器)技术来替代复杂性较高的单线程CPU。IBM、惠普、Sun等高端服务器厂商，更是相继推出了多核服务器CPU。不过，由于服务器价格高、应用面窄，并未引起大众广泛的注意。直到AMD抢先手推出64位处理器后，英特尔才想起利用“多核”这一武器进行“帝国反击战”。2005年4月，英特尔仓促推出简单封装双核的奔腾D和奔腾四至尊版840。AMD在之后也发布了双核皓龙(Opteron)和速龙(Athlon) 64 X2和处理器。但真正的“双核元年”，则被认为是2006年。这一年的7月23日，英特尔基于酷睿(Core)架构的处理器正式发布。2006年11月，又推出面向服务器、工作站和高端个人电脑的至强(Xeon)5300和酷睿双核和四核至尊版系列处理器。与上一代台式机处理器相比，酷睿2 双核处理器在性能方面提高40%，功耗反而降低40%。作为回应，7月24日，AMD也宣布对旗下的双核Athlon64 X2处理器进行大降价。由于功耗已成为用户在性能之外所考虑的首要因素，两大处理器巨头都在宣传多核处理器时，强调其“节能”效果。英特尔发布了功耗仅为50瓦的低电压版四核至强处理器。而AMD的“Barcelona”四核处理器的功耗没有超过95瓦。在英特尔高级副总裁帕特基辛格(Pat Gelsinger)看来，从单核到双核，再到多核的发展，证明了摩尔定律还是非常正确的，因为“从单核到双核，再到多核的发展，可能是摩尔定律问世以来，在芯片发展历史上速度最快的性能提升过程”。技术优势播报编辑从应用需求上去看，越来越多的用户在使用过程中都会涉及到多任务应用环境，日常应用中用到的非常典型的有两种应用模式。一种应用模式是一个程序采用了线程级并行编程，那么这个程序在运行时可以把并行的线程同时交付给两个核心分别处理，因而程序运行速度得到极大提高。这类程序有的是为多路工作站或服务器设计的专业程序，例如专业图像处理程序、非线视频编缉程序、动画制作程序或科学计算程序等。对于这类程序，两个物理核心和两颗处理器基本上是等价的，所以，这些程序往往可以不作任何改动就直接运行在双核电脑上。还有一些更常见的日常应用程序，例如Office、IE等，同样也是采用线程级并行编程，可以在运行时同时调用多个线程协同工作，所以在双核处理器上的运行速度也会得到较大提升。例如，打开IE浏览器上网。看似简单的一个操作，实际上浏览器进程会调用代码解析、Flash播放、多媒体播放、Java、脚本解析等一系列线程，这些线程可以并行地被双核处理器处理，因而运行速度大大加快（实际上IE浏览器的运行还涉及到许多进程级的交互通信，这里不再详述）。由此可见，对于已经采用并行编程的软件，不管是专业软件，还是日常应用软件，在多核处理器上的运行速度都会大大提高。日常应用中的另一种模式是同时运行多个程序。许多程序没有采用并行编程，例如一些文件压缩软件、部分游戏软件等等。对于这些单线程的程序，单独运行在多核处理器上与单独运行在同样参数的单核处理器上没有明显的差别。但是，由于日常使用的最最基本的程序——操作系统——是支持并行处理的，所以，当在多核处理器上同时运行多个单线程程序的时候，操作系统会把多个程序的指令分别发送给多个核心，从而使得同时完成多个程序的速度大大加快。另外，虽然单一的单线程程序无法体现出多核处理器的优势，但是多核处理器依然为程序设计者提供了一个很好的平台，使得他们可以通过对原有的单线程序进行并行设计优化，以实现更好的程序运行效果。上面介绍了多核心处理器在软件上面的应用，但游戏其实也是软件的一种，作为一种特殊的软件，对PC发展作出了较大的贡献。一些多线程游戏已经能够发挥出多核处理器的优势，对于单线程游戏，相信游戏厂商也将会改变编程策略，例如，一些游戏厂商正在对原来的一些单线程游戏进行优化，采用并行编程使得游戏运行得更快。有的游戏可以使用一个线程实现人物动画，而使用另一个线程来载入地图信息。或者使用一个线程来实现图像渲染中的矩阵运算，而使用另一个来实现更高的人工智能运算。如今，大量的支持多核心的游戏涌现出来，从而使得多核处理器的优势能得到进一步的发挥。技术瓶颈播报编辑布赖恩特直言不讳地指出，要想让多核完全发挥效力，需要硬件业和软件业更多革命性的更新。其中，可编程性是多核处理器面临的最大问题。一旦核心多过八个，就需要执行程序能够并行处理。尽管在并行计算上，人类已经探索了超过40年，但编写、调试、优化并行处理程序的能力还非常弱。易观国际分析师李也认为，“出于技术的挑战，双核甚至多核处理器被强加给了产业，而产业却并没有事先做好准备”。或许正是出于对这种失衡的担心，中国国家智能计算机中心主任孙凝辉告诉《财经》记者，“十年以后，多核这条道路可能就到头了”。在他看来，一味增加并行的处理单元是行不通的。并行计算机的发展历史表明，并行粒度超过100以后，程序就很难写，能做到128个以上的应用程序很少。CPU到了100个核以上后，并行计算机系统遇到的问题，在CPU一样会存在。“如果解决不了主流应用并行化的问题，主流CPU发展到100个核就到头了。还不知道什么样的革命性的进展能解决这些问题。”孙补充说。实际上，市场研究公司In-Stat分析师吉姆克雷格(Jim McGregor)就承认，虽然英特尔已向外界展示了80核处理器原型，但尴尬的是，还没有能够利用这一处理器的操作系统。中科院软件所并行计算实验室副主任张云泉也持类似的观点。他对《财经》记者表示，这个问题实际一直就存在，但原来在超级计算机上才会遇到，所以，讨论也多局限在学术界。所有用户都要面对这样的问题。多核心技术在应用上的优势有两个方面：为用户带来更强大的计算性能；更重要的，则是可满足用户同时进行多任务处理和多任务计算环境的要求。两大巨头都给消费者描绘出了使用多核处理器在执行多项任务时的美妙前景：同时可以检查邮件、刻录CD、修改照片、剪辑视频，并且同时可以运行杀毒软件。或者利用同一台电脑，父亲在查看财务报表，女儿在打游戏，母亲在给远方的朋友打网络电话。但并不是所有家庭只有一台电脑，也不是所有用户都要用电脑一下子做那么多事，更何况大部分应用程序还并不能自动分割成多任务，分别交给多个核心去执行。所以，对于大多数用户来说，多核所带来的实际益处，很可能并不明显。而多核所带来的挑战，或者说麻烦，却是实实在在的。美国卡内基梅隆大学计算机系教授朗道布赖恩特(Randal E Bryant)在接受《财经》记者采访时就坦称，“这给软件业制造了巨大的问题”。技术原理播报编辑多核CPU就是基板上集成有多个单核CPU，早期PD双核需要北桥来控制分配任务，核心之间存在抢二级缓存的情况，后期酷睿自己集成了任务分配系统，再搭配操作系统就能真正同时开工，2个核心同时处理2“份”任务，速度快了，万一1个核心死机，起码另一个U还可以继续处理关机、关闭软件等任务。技术关键播报编辑与单核处理器相比，多核处理器在体系结构、软件、功耗和安全性设计等方面面临着巨大的挑战，但也蕴含着巨大的潜能。多核处理器CMP和SMT一样，致力于发掘计算的粗粒度并行性。CMP可以看做是随着大规模集成电路技术的发展，在芯片容量足够大时，就可以将大规模并行处理机结构中的SMP（对称多处理机）或DSM（分布共享处理机）节点集成到同一芯片内，各个处理器并行执行不同的线程或进程。在基于SMP结构的单芯片多处理机中，处理器之间通过片外Cache或者是片外的共享存储器来进行通信。而基于DSM结构的单芯片多处理器中，处理器间通过连接分布式存储器的片内高速交叉开关网络进行通信。由于SMP和DSM已经是非常成熟的技术了，CMP结构设计比较容易，只是后端设计和芯片制造工艺的要求较高而已。正因为这样，CMP成为了最先被应用于商用CPU的“未来”高性能处理器结构。虽然多核能利用集成度提高带来的诸多好处，让芯片的性能成倍地增加，但很明显的是原来系统级的一些问题便引入到了处理器内部。核结构研究同构还是异构CMP的构成分成同构和异构两类，同构是指内部核的结构是相同的，而异构是指内部的核结构是不同的。为此，面对不同的应用研究核结构的实现对未来微处理器的性能至关重要。核本身的结构，关系到整个芯片的面积、功耗和性能。怎样继承和发展传统处理器的成果，直接影响多核的性能和实现周期。同时，根据Amdahl定理，程序的加速比决定于串行部分的性能，所以，从理论上来看似乎异构微处理器的结构具有更好的性能。核所用的指令系统对系统的实现也是很重要的，多核之间采用相同的指令系统还是不同的指令系统，能否运行操作系统等，也将是研究的内容之一。程序执行模型处理器设计的首要问题是选择程序执行模型。程序执行模型的适用性决定多核处理器能否以最低的代价提供最高的性能。程序执行模型是编译器设计人员与系统实现人员之间的接口。编译器设计人员决定如何将一种高级语言程序按一种程序执行模型转换成一种目标机器语言程序; 系统实现人员则决定该程序执行模型在具体目标机器上的有效实现。当目标机器是多核体系结构时，产生的问题是: 多核体系结构如何支持重要的程序执行模型？是否有其他的程序执行模型更适于多核的体系结构？这些程序执行模型能多大程度上满足应用的需要并为用户所接受？Cache设计多级Cache设计与一致性问题处理器和主存间的速度差距对CMP来说是个突出的矛盾，因此必须使用多级Cache来缓解。有共享一级Cache的CMP、共享二级Cache的CMP以及共享主存的CMP。通常，CMP采用共享二级Cache的CMP结构，即每个处理器核心拥有私有的一级Cache，且所有处理器核心共享二级Cache。Cache自身的体系结构设计也直接关系到系统整体性能。但是在CMP结构中，共享Cache或独有Cache孰优孰劣、需不需要在一块芯片上建立多级Cache，以及建立几级Cache等等，由于对整个芯片的尺寸、功耗、布局、性能以及运行效率等都有很大的影响，因而这些都是需要认真研究和探讨的问题。另一方面，多级Cache又引发一致性问题。采用何种Cache一致性模型和机制都将对CMP整体性能产生重要影响。在传统多处理器系统结构中广泛采用的Cache一致性模型有: 顺序一致性模型、弱一致性模型、释放一致性模型等。与之相关的Cache一致性机制主要有总线的侦听协议和基于目录的目录协议。CMP系统大多采用基于总线的侦听协议。核间通信技术CMP处理器的各CPU核心执行的程序之间有时需要进行数据共享与同步，因此其硬件结构必须支持核间通信。高效的通信机制是CMP处理器高性能的重要保障，比较主流的片上高效通信机制有两种，一种是基于总线共享的Cache结构，一种是基于片上的互连结构。总线共享Cache结构是指每个CPU内核拥有共享的二级或三级Cache，用于保存比较常用的数据，并通过连接核心的总线进行通信。这种系统的优点是结构简单，通信速度高，缺点是基于总线的结构可扩展性较差。基于片上互连的结构是指每个CPU核心具有独立的处理单元和Cache，各个CPU核心通过交叉开关或片上网络等方式连接在一起。各个CPU核心间通过消息通信。这种结构的优点是可扩展性好，数据带宽有保证; 缺点是硬件结构复杂，且软件改动较大。也许这两者的竞争结果不是互相取代而是互相合作，例如在全局范围采用片上网络而局部采用总线方式，来达到性能与复杂性的平衡。总线设计传统微处理器中，Cache不命中或访存事件都会对CPU的执行效率产生负面影响，而总线接口单元（BIU）的工作效率会决定此影响的程度。当多个CPU核心同时要求访问内存或多个CPU核心内私有Cache同时出现Cache不命中事件时，BIU对这多个访问请求的仲裁机制以及对外存储访问的转换机制的效率决定了CMP系统的整体性能。因此寻找高效的多端口总线接口单元（BIU）结构，将多核心对主存的单字访问转为更为高效的猝发（burst）访问; 同时寻找对CMP处理器整体效率最佳的一次Burst访问字的数量模型以及高效多端口BIU访问的仲裁机制将是CMP处理器研究的重要内容，Inter推出了最新的英特尔智能互连技术(QPI)技术总线，更大程度发掘了多核处理器的实力 。操作系统设计任务调度、中断处理、同步互斥对于多核CPU，优化操作系统任务调度算法是保证效率的关键。一般任务调度算法有全局队列调度和局部队列调度。前者是指操作系统维护一个全局的任务等待队列，当系统中有一个CPU核心空闲时，操作系统就从全局任务等待队列中选取就绪任务开始在此核心上执行。这种方法的优点是CPU核心利用率较高。后者是指操作系统为每个CPU内核维护一个局部的任务等待队列，当系统中有一个CPU内核空闲时，便从该核心的任务等待队列中选取恰当的任务执行，这种方法的优点是任务基本上无需在多个CPU核心间切换，有利于提高CPU核心局部Cache命中率。多数多核CPU操作系统采用的是基于全局队列的任务调度算法。多核的中断处理和单核有很大不同。多核的各处理器之间需要通过中断方式进行通信，所以多个处理器之间的本地中断控制器和负责仲裁各核之间中断分配的全局中断控制器也需要封装在芯片内部。另外,多核CPU是一个多任务系统。由于不同任务会竞争共享资源，因此需要系统提供同步与互斥机制。而传统的用于单核的解决机制并不能满足多核，需要利用硬件提供的“读－修改－写”的原子操作或其他同步互斥机制来保证。低功耗设计半导体工艺的迅速发展使微处理器的集成度越来越高，同时处理器表面温度也变得越来越高并呈指数级增长，每三年处理器的功耗密度就能翻一番。低功耗和热优化设计已经成为微处理器研究中的核心问题。CMP的多核心结构决定了其相关的功耗研究是一个至关重要的课题。低功耗设计是一个多层次问题，需要同时在操作系统级、算法级、结构级、电路级等多个层次上进行研究。每个层次的低功耗设计方法实现的效果不同——抽象层次越高，功耗和温度降低的效果越明显。当前Intel的CPU的功耗相对较低，得益于先进的英特尔构架和45纳米、32纳米制程工艺，同时Intel还专门为CPU开发了不少节能技术，比如C6深度节能技、英特尔智能功效管理 和主动管理技术 等等，Intel在移动CPU市场，更是凭借超低电压处理器（ULV）和凌动（Atom）系列处理器，遥遥领先于对手。存储器墙为了使芯片内核充分地工作，最起码的要求是芯片能提供与芯片性能相匹配的存储器带宽，虽然内部Cache的容量能解决一些问题，但随着性能的进一步提高，必须有其他一些手段来提高存储器接口的带宽，如增加单个管脚带宽的DDR、DDR2、QDR、XDR等。同样，系统也必须有能提供高带宽的存储器。所以，芯片对封装的要求也越来越高，虽然封装的管脚数每年以20%的数目提升，但还不能完全解决问题，而且还带来了成本提高的问题，为此，怎样提供一个高带宽，低延迟的接口带宽，是必须解决的一个重要问题。可靠性及安全性设计随着技术革新的发展，处理器的应用渗透到现代社会的各个层面，但是在安全性方面却存在着很大的隐患。一方面，处理器结构自身的可靠性低下，由于超微细化与时钟设计的高速化、低电源电压化，设计上的安全系数越来越难以保证，故障的发生率逐渐走高。另一方面，来自第三方的恶意攻击越来越多，手段越来越先进，已成为具有普遍性的社会问题。可靠性与安全性的提高在计算机体系结构研究领域备受注目。今后，CMP这类处理器芯片内有多个进程同时执行的结构将成为主流，再加上硬件复杂性、设计时的失误增加，使得处理器芯片内部也未必是安全的，因此，安全与可靠性设计任重而道远。技术意义播报编辑多核处理器代表了计算技术的一次创新。由于数字数据和互联网的全球化，商业和消费者开始要求多核处理器带来性能改进，这个重要创新就开始了；因为多核处理器比单核处理器具有性能和效率优势，多核处理器将会成为被广泛采用的计算模型。在驱动pc安全性和虚拟化技术的重大进程过程中，多核处理器扮演着中心作用，这些安全性和虚拟化技术的开发用于为商业计算市场提供更大的安全性、更好的资源利用率、创造更大价值。普通消费者用户也期望得到前所未有的性能，这将极大地扩展其家庭pc和数字媒体计算系统的使用。多核处理器具有不增加功耗而提高性能的好处，实现更大的性能/能耗比。在一个处理器中放入两个或多个功能强大的计算核产生了一个重大的可能性。由于多核处理器能提供比单核处理器更好的性能和效率，下一代的软件应用程序很有可能是基于多核处理器而开发的。不管这些应用是帮助专业的电影公司以更少的投入和更少的时间完成更真实的电影，还是以更彻底的方法使得pc更自然和直观，多核处理器技术将永远改变计算世界。多核处理器表达了amd了解顾客需求并且开发最能满足客户要求产品的意愿。微软多核计算的主管Dan Reed称，整个世界上很缺乏那些并行计算的研究人员，而一个间接的原因就是学院里对于并行计算的关注度不够，而这些学院正是下一代软件开发人员诞生的地方。越来越高的时钟频率导致应用程序的代码运行的越来越快，而对于当前多核处理器来讲这一规则虽然成立，但却有所不同。而这种不同可以做一个形象的比喻，那就是一部跑车和一辆学校的巴士。当跑车能够以很快的速度飞奔时，巴士虽然比较慢，但它可以载着更多的人前行。问题就是，简单地在计算机CPU上增加多个核并不能增加传统应用程序代码的运行速度，这一结果是根据一项来自于Forrester研究公司的报告得出的。换句话说，复杂的工作需要拆分来填充这辆巴士上的空座位。Forrester的报告还谈到：同时，当前四核处理器会激发更多的多处理器设计的思想，我们期待着2009年x86的服务器使用64个处理器核，并且2012年台式机也可以实现这一梦想。使得芯片的制造商以及主要的板级应用的软件厂商意识到多核编程的机遇和挑战。 [1]技术种类播报编辑单芯片多处理器(CMP)与同时多线程处理器(SimultaneousMultithreading，SMT)，这两种体系结构可以充分利用这些应用的指令级并行性和线程级并行性，从而显著提高了这些应用的性能。从体系结构的角度看，SMT比CMP对处理器资源利用率要高，在克服线延迟影响方面更具优势。CMP相对SMT的最大优势还在于其模块化设计的简洁性。复制简单设计非常容易，指令调度也更加简单。同时SMT中多个线程对共享资源的争用也会影响其性能，而CMP对共享资源的争用要少得多，因此当应用的线程级并行性较高时，CMP性能一般要优于SMT。此外在设计上，更短的芯片连线使CMP比长导线集中式设计的SMT更容易提高芯片的运行频率，从而在一定程度上起到性能优化的效果。总之，单芯片多处理器通过在一个芯片上集成多个微处理器核心来提高程序的并行性。每个微处理器核心实质上都是一个相对简单的单线程微处理器或者比较简单的多线程微处理器，这样多个微处理器核心就可以并行地执行程序代码，因而具有了较高的线程级并行性。由于CMP采用了相对简单的微处理器作为处理器核心，使得CMP具有高主频、设计和验证周期短、控制逻辑简单、扩展性好、易于实现、功耗低、通信延迟低等优点。此外，CMP还能充分利用不同应用的指令级并行和线程级并行，具有较高线程级并行性的应用如商业应用等可以很好地利用这种结构来提高性能。技术应用播报编辑并行计算技术是云计算的核心技术，也是最具挑战性的技术之一。多核处理器的出现增加了并行的层次性能使得并行程序的开发比以往更难。而当前业内并无有效的并行计算解决方案，无论是编程模型、开发语言还是开发工具，距离开发者的期望都有很大的差距。自动的并行化解决方案在过去的30年间已经被证明基本是死胡同，但传统的手工式的并行程序开发方式又难以为普通的程序员所掌握。Intel、微软、SUN、Cray等业内巨头正投入大量人力物力进行相关的研究，但真正成熟的产品在短期内很难出现。可扩展性是云计算时代并行计算的主要考量点之一，应用性能必须能随着用户的请求、系统规模的增大有效的扩展。当前大部分并行应用在超过一千个的处理器(核)上都难以获得有效的加速性能，未来的许多并行应用必须能有效扩展到成千上万个处理器上。这对开发者是巨大的挑战。 [2]产品应用播报编辑从Power、UltraSPARC T1、安腾到双核Opteron、至强Xeon，各个领域都显示出，多核处理器计算平台势必成为服务器的主流或者说是强势计算平台，但这只是上游硬件厂商的乐观预计。并不是所有的操作系统和应用软件都做好了迎接多核平台的准备，尤其是在数十年来均为单一线程开发应用的x86服务器领域。微软软件架构师HerbSutter曾指出:软件开发者对多核处理器时代的来临准备不足。他说，软件开发社区认识到处理器厂商被迫采用多核设计以应对处理器速度提升带来的发热问题，但却没有清楚地了解这样的设计为软件开发带来多少额外的工作。在过去一段长时间里，x86系统上软件的性能随着来自Intel和AMD处理器速度越来越快而不断提高，开发者只需对现有软件程序作轻微改动就能坐观其性能在随着硬件性能的上升而不断提升。不过，多核设计概念的出现迫使软件世界不得不直面并行性(将单个任务拆分成多个小块以便分别处理之后再重新组合的能力)问题。当然，为服务器设计软件的开发者已经解决了一些此类难题，因为多核处理器和多路系统在服务器市场已经存在多年(在传统的Unix领域)，一些运行在RISC架构多核多路系统上的应用程序已经被设计成多线程以利用系统的并行处理能力。但是，在x86领域，应用程序开发者多年来一直停留在单线程世界，生产所谓的“顺序软件”。情况是软件开发者必须找出新的开发软件的方法，面向对象编程的兴起增加了汇编语言的复杂性，并行编程也需要新的抽象层次。另一方面，处理器设计厂商在设计产品时也应该将软件开发者考虑在内，“处理器的首要着眼点应该是可编程性，而不是速度。”Sutter说。多核处理器要想发挥出威力，关键在于并行化软件支持，多核设计带动并行化计算的推进，而给软件带来的影响更是革命性的。Intel很早就通过超线程技术实现了逻辑上的双处理器系统，可以并行计算，但这不过是对处理器闲置资源的一种充分利用而已，并且这种充分利用只有在特定的条件下，尤其是针对流水线比较长且两种运算并不相互交叉的时候，才会有较高的效率，如编码解码、长期重复某种矩阵运算以及一些没有经过仔细编写的软件等。即使IBM的Power5架构，也需要跟最新的操作系统进行融合，加上运行在其上的软件，才有可能利用并发多线程。虚拟化技术在一定程度上能够处理一些因为多核带来的问题，可以让应用软件和操作系统在透明的环境下对处理器资源进行分配和管理。在对称多处理器方面，操作系统对资源的分配和管理并没有本质的改变，多以对称的方式进行平均分配。也就是说，在操作系统层面，当一个任务到来时，剥离成为两个并行的线程，因为线程之间需要交流以及操作系统监管，它导致的效率损失要比硬件层面大得多。并且，多数软件并没有充分考虑到双核乃至多核的运行情况，导致线程的平均分配时间以及线程之间的沟通时间都会大大增加，尤其是当线程需要反复访问内存的时候。多数操作系统还没有完全实现自由的资源分配，如IBM是通过AIX 5.3L来支持Power5上的虚拟化功能，才实现了资源的动态调配和划分的。从长远来看，需要使用虚拟化技术才可能实现操作系统对任务的具体划分，这很可能改变一些通用的编程模式。 [3]英特尔播报编辑2009年9月6日下午，英特尔在北京发布了业界首款专为多路(MP)服务器设计的四核英特尔&reg;至强&reg;7300系列服务器处理器。与英特尔前代双核产品相比，此次发布的六款全新四核至强&reg;7300系列处理器的性能和性能功耗比分别提升了两倍和三倍之多。而随着这些产品的发布，英特尔在不到15个月的时间内完成了向创新和高能效的英特尔&reg;酷睿™微体系架构的快速切换。据了解，此次推出的至强&reg;7300系列产品包括主频高达2.93GHz处理器(功耗为130瓦)，几款80瓦处理器，和一款针对四插槽刀片式服务器和高密度机架式服务器优化的50瓦版处理器(主频为1.86GHz)。具备数据流量优化(Data Traffic Optimizations)特性的英特尔&reg;7300芯片组采用平衡的平台设计，具有多项全新技术，以改善数据在处理器、内存和I/O之间的传输能力。此外，英特尔还发布了一款50瓦(每内核12.5瓦)的处理器，以推动四插槽刀片式服务器和高密度机架式服务器等高能效超密度部署产品的发展。在芯片设计方面，除内核数量增加一倍之外，相对于前代英特尔多路平台，至强&reg;7300系列处理器和英特尔&reg;7300芯片组所支持的内存容量是原来的4倍，并能支持非常高的整合比例，以减少空间、降低功耗和运营成本。预计今后将有超过50家的系统制造商发售基于英特尔&reg;至强&reg;7300系列处理器的服务器，其中包括戴尔、Egenera、富士通、富士通-西门子、日立、惠普、IBM、NEC、Sun、超微和优利等。针对需要基于全新英特尔&reg;至强&reg;7300系列处理器的完整平台的渠道客户，英特尔特别为其提供了英特尔&reg;S7000FC4UR服务器平台。该款平台可提供强劲的可扩展性能、业经验证的企业级可靠性，用于基础设施的虚拟化和整合。许多软件厂商也为基于英特尔&reg;至强&reg;7300系列处理器的平台提供了创新性的支持虚拟化和性能扩展的解决方案，如BEA、微软、甲骨文、SAP和VMware等。此外，Solaris 操作系统和其上运行的数千款应用能够充分利用英特尔&reg;至强&reg;7300 系列处理器平台的领先性能优势，为英特尔&reg;至强&reg;服务器用户提供企业级、支持关键任务的UNIX操作系统环境。这些全新四核处理器的定价根据主频、特性和客户定购数量的不同，其千枚单价从856美元至2,301美元不等。

总线事务：
名词定义：总线裁决：决定哪个主控设备使用总线。寻址阶段：主控设备送出要访问设备的地址，同时送出有关命令（读或写等），启动从设备。数据传输阶段：主、从设备间进行数据交换。结束阶段：有关信息在总线上撤销，让出总线使用权。

总线控制：
特点播报编辑控制总线CB（ControlBus）特点是：在单向、双向、双态等种形态，是总线中最复杂、最灵活、功能最强的，其数量、种类、定义随机型不同而不同。分类播报编辑控制总线就是各种信号线的集合，是计算机各部件之间传送数据、地址和控制信息的公共通道。⒈按相对于CPU与其芯片的位置来分：⑴片内总线：指在CPU内部各寄存器、算术逻辑部件ALU，控制部件以及内部高速缓冲存储器之间传输数据所用的总线，即芯片内部总线。⑵片外总线：通常所说的总线（BUS）指的外总线，是CPU与内存RAM、ROM和输入输出输入输出设备接口之间进行通讯的数据通道，CPU通过总线实现程序存取命令，内存/外设的数据交换在CPU与外设一定的情况下，总线速度是限制计算机整体性能的最大因数。⒉按总线功能分：⑴ 地址总线：（AB）用来传递地址信息。⑵数据总线：（DB）用来传递数据信息。⑶ 控制总线：（CB）用来传送各种控制信号。⒊按总线的层次结构分：⑴ CPU总线：包括CPU地址线（CAB），CPU数据线（CDB）和CPU控制线（CCB），其用来连接CPU和控制芯片。CS31通讯总线⑵ 存储器总线：包括存储器地址线（MAB）、存储器数据线（MDB）和存储器控制线（MCD），用来连接内存控制器（北桥）和内存。⑶ 系统总线：（I/O扩展总线）也称为I/O通道总线或I/O扩展总线，包括系统地址线（SAB），系统数据线（SDB）和系统控制线（SCD），用来与I/O扩展槽上的各种扩展卡相连接。⑷ 外部总线：（外围芯片总线）用来连接各种外设控制芯片，如主板上的I/O控制器（如硬盘接口控制器、软盘驱动控制器、串行/并行接口控制器等），和键盘控制器，包括外部地址线（XAB）、外部数据线（XMB）和外部控制线（XCB）。⒋系统总线（输入输出）扩展总线）又分为ISA、PCI、AGP等多种标准⑴ ISA(Industrystandardarchitecture,工业标准结构）是IBM公司为286AT电脑制定的总线工业标准，也称为AT标准。⑵ PCI(peripheralcomponentinterconnet,外部设备互连）是SIG（spelialinterestgroup）集团推出的总线结构。⑶ AGP(acceleratedgraphicsport，加速图形端口）是一种为了提高视频带宽而设计的总线规范，因为它是点对点连接，即连接控制芯片和AGP显卡，因此严格说来，AGP也是一种接口标准。ISA插槽播报编辑1、地址总线：SA0~SA19(I/O）和LA17~LA23(I/O)LⅪ测试总线技术2、数据总线：SD0~SD7(I/O）和SD8~SD15(I/O)3、控制总线：BALE(0)---USAddresslatchenable：系统地址锁存允许4、SYSCLK(0)---SYSTEMCLOCK系统时钟信号5、IR23~7,9~12,15(Z)---这是用于I/O设备通过中断控制器向CPU发送的中断请求(interruptrequest)信号6、SMEMR#和SMEMW#(0)---这是命令内存将数据送至数据总线的信号7、MEMR#和MEMW#(I/O)---内存读（MEMR）或内存写（MEMW#)信号8、DRQ0~3,5~7⑵---这是DMA请求（DMARequesc）信号9、DACK0#~3,5~7(0)---(DMAAcknowledge,DMA响应）这是对DRQ0~3,5~7的响应信号10、AEN(0)---地址允许（Addressenable）信号11、REFRESH#(I/O)---内存刷新(DRAMrefresh)信号12、SBHE(I/O)---系统总线字节允许（systembushighenable）信号13、MASTER⑵---主控信号14、MEMCS16#⑵---存储器16位片选(Memory16bitchipselect)信号15、ZOCS16#⑵---I/O16位片选(I/O16bitchipselect)信号16、OWS⑵---零等待状态(ZeroWaitState)信号技术指标播报编辑1、总线的带宽（总线数据传输速率）程序总线总线的带宽指的是单位时间内总线上传送的数据量，即每钞钟传送MB的最大稳态数据传输率。与总线密切相关的两个因素是总线的位宽和总线的工作频率，它们之间的关系：总线的带宽=总线的工作频率*总线的位宽/82、总线的位宽总线的位宽指的是总线能同时传送的二进制数据的位数，或数据总线的位数，即32位、64位等总线宽度的概念。总线的位宽越宽，每秒钟数据传输率越大，总线的带宽越宽。3、总线的工作频率总线的工作时钟频率以MHZ为单位，工作频率越高，总线工作速度越快，总线带宽越宽。操作播报编辑总线一个操作过程是完成两个模块之间传送信息，启动操作过程的是主模块，另外一个是从模块。某一时刻总线上只能有一个主模块占用总线。 [2]控制总线总线的操作步骤：主模块申请总线控制权，总线控制器进行裁决。数据传送的错误检查：主模块得到总线控制权后寻址从模块，从模块确认后进行数据传送。总线定时协议：定时协议可保证数据传输的双方操作同步，传输正确。定时协议有三种类型：同步总线定时：总线上的所有模块共用同一时钟脉冲进行操作过程的控制。各模块的所有动作的产生均在时钟周期的开始，多数动作在一个时钟周期中完成。异步总线定时：操作的发生由源或目的模块的特定信号来确定。总线上一个事件发生取决前一事件的发生，双方相互提供联络信号。控制总线模型总线定时协议 半同步总线定时：总线上各操作的时间间隔可以不同，但必须是时钟周期的整数倍，信号的出现，采样与结束仍以公共时钟为基准。ISA总线采用此定时方法。数据传输类型：分单周方式和突发（burst）方式。单周期方式：一个总线周期只传送一个数据。数据传输类型：突发方式：取得主线控制权后进行多个数据的传输。寻址时给出目的地首地址，访问第一个数据，数据2、3到数据n的地址在首地址基础上按一定规则自动寻址（如自动加1）。标准规范播报编辑总线是一类信号线的集合是模块间传输信息的公共通道，通过它，计算机各部件间可进行各种数据和命令的传送。为使不同供应商的产品间能够互换，给用户更多的选择，总线的技术规范要标准化。总线的标准制定要经周密考虑，要有严格的规定。总线标准（技术规范）包括以下几部分：机械结构规范：模块尺寸、总线插头、总线接插件以及安装尺寸均有统一规定。功能规范：总线每条信号线（引脚的名称）、功能以及工作过程要有统一规定。电气规范：总线每条信号线的有效电平、动态转换时间、负载能力等。

串行传输：
简介播报编辑串行通信技术，是指通信双方按位进行，遵守时序的一种通信方式。串行通信中，将数据按位依次传输， 每位数据占据固定的时间长度，即可使用少数几条通信线路就可以完成系统间交换信息，特别适用于计算机与计算机、计算机与外设之间的远距离通信。串行通信多用于系统间通信（多主控制系统）、设备间（主控设备与附属设备）、器件间（主控CPU与功能芯片）之间数据的串行传送，实现 数据的传输与共享。 [2]串行总线通信过程的显著特点是：通信线路少，布线简便易行，施工方便，结构灵活，系统间协商协议，自由度及灵活度较高，因此在电子电路设计、信息传递等诸多方面的应用越来越多。 [2]串行通信是指计算机主机与外设之间以及主机系统与主机系统之间数据的串行传送。使用一条数据线，将数据一位一位地依次传输，每一位数据占据一个固定的时间长度。其只需要少数几条线就可以在系统间交换信息，特别适用于计算机与计算机、计算机与外设之间的远距离通信。分类播报编辑同步通信同步通信是一种连续串行传送数据的通信方式，一次通信只传送一帧信息。这里的信息帧与异步通信中的字符帧不同，通常含有若干个数据字符。它们均由同步字符、数据字符和校验字符（CRC）组成。其中同步字符位于帧开头，用于确认数据字符的开始。数据字符在同步字符之后，个数没有限制，由所需传输的数据块长度来决定；校验字符有1到2个，用于接收端对接收到的字符序列进行正确性的校验。同步通信的缺点是要求发送时钟和接收时钟保持严格的同步。异步通信异步通信中，在异步通信中有两个比较重要的指标：字符帧格式和波特率。数据通常以字符或者字节为单位组成字符帧传送。字符帧由发送端逐帧发送，通过传输线被接收设备逐帧接收。发送端和接收端可以由各自的时钟来控制数据的发送和接收，这两个时钟源彼此独立，互不同步。接收端检测到传输线上发送过来的低电平逻辑"0"（即字符帧起始位）时，确定发送端已开始发送数据，每当接收端收到字符帧中的停止位时，就知道一帧字符已经发送完毕。特点播报编辑数据在单条一位宽的传输线上，一比特接一比特地按顺序传送的方式称为串行通信。 在并行通信中，一个字节（8位）数据是在8条并行传输线上同时由源传到目的地；而在串行通信方式中，数据是在单条1位宽的传输线上一位接一位地顺序传送。这样一个字节的数据要分8次由低位到高位按顺序一位位地传送。由此可见，串行通信的特点如下：1、节省传输线，这是显而易见的。尤其是在远程通信时，此特点尤为重要。这也是串行通信的主要优点。2、数据传送效率低。与并行通信比，这也这是显而易见的。这也是串行通信的主要缺点。例如：传送一个字节，如果并行通信所需时间为1T，则串行通信所需时间至少为8T。 由此可见，串行通信适合于远距离传送，可以从几米到数千公里。对于长距离、低速率的通信，串行通信往往是唯一的选择。并行通信适合于短距离、高速率的数据传送，通常传输距离小于30米。特别值得一提的是，现成的公共电话网是通用的长距离通信介质，它虽然是为传输声音信号设计的，但利用调制解调技术，可使现成的公共电话网系统为串行数据通信提供方便、实用的通信线路。串行通信干扰源播报编辑串行通信工作场所多处于强电/户外等复杂环境，并且通信各方间距离一般较长，因此易受干扰。 串行通信，波特率一定时，数据位的传输时间相对较短，由于串行通信的数据位采样/ 获取特点，位信息受干扰，整个字节数据就是错误信息。 [3]现实中，容易带入串行通信干扰的因素包括：（1）环境电磁干扰 在串行通信工作设备附近， 无可避免的存在强电设备、功率发射台等。 这些设备发射/感应的强电磁场感应区内，环境电磁干扰强。 串行通信设备工作在这种环境下，由于噪声(干扰)在 信号电平上的叠加，引发了通信双方数据错误。 [3]（2）系统噪声串行通信依赖于串行通信芯片。 由于芯片的设计工艺与制作水平，对输出电平的噪声控制参差不齐。 产生输出电平的噪声包括数字逻辑中供电电源和器件自身的稳定性。 通信中，供电电源的纹波无可避免的会加载到通信线路中。 纹波较大时， 容易引发串行通信的错误。 [3]（3）码率误差串行通信双方事先约定了固定的波特率作为数据传输的步调。 波特率的一致性是串行通信数据稳定可靠的基础。 由于通信双方的波特率由各自本地产生，存在误差率的波特率导致通信双方存在码率误差。 波特率误差越大，通信数据错误的几率就越大。 [3]（4）地回路与参考地电位通信双方共地应用中，由于系统间参考地信号的高低电平不一致，导致传输的信号对地电压存在一定的误差。 低电压供电应用系统中，两侧参考地电位误差过大，会引发串行通信的数据错误。 以上干扰源，在通信线屏蔽、线路隔离、校准波特率等不同的硬件优化措施下，可以减弱或消除部分干扰，但仍存在数据错误的可能性。 因此，在硬件抗干扰的保障之外，加入软件侦错机制，不可忽略，尤为必要。 [3]串行通信隔离方法播报编辑隔离的现实需要串行通信由于其工作特点（按位传输易受干扰、远距离 信息交换）、应用场合（恶劣环境的工业控制、户外等）、 器件间电平匹配（两侧器件的工作电平不一致等），需要做相应的隔离防护。通过隔离，达到以下目的。 （1）器件保护，防护隔离在电子器件高速发展的今天，低功耗、高封装的芯片应用广泛。微处理器的低电压工作条件和外围器件的高电压工作环境，其发展进程不一。当前微处理器芯片电平多以1.8V、 3.3V、5.0V等低电压器件为主，而且随着不同工作电压的数字IC的不断涌现，逻辑电平转换的必要性更加突出。例如STM32控制器的3.3V输入输出I/O与传统串行通信接口芯环境，其发展进程不一。因此，为 了实现控制器与通信接口芯片间的电平匹配，保护控制器引脚因过高或者过低的工作电压而受损，加入隔离器件尤其必要。 [2]（2）屏蔽干扰，线路隔离由于较多串行通信设备工作在工业现场的恶劣环境或配电系统的远距离传输等条件下，因此在长线通讯中线路上往往会感应出明显的干扰信号，造成通信过程的偶发性错误，进而影响整个系统的可靠运行。引入干扰信号的来源包括空间辐射、串扰、系统噪声等。例如RS － 232C通信中由于其采用单端信号传输模式，当通信双方的不同地线之间的地电位不一致时，就会引入共模干扰电压，造成通信的不稳定。 [2]串行通信中，通过通信线路屏蔽可以减少辐射干扰的影 响，通过差分方式信号传输方式可以减少共模干扰电压的影响，但为应对器件保护而进行的电平变换和为减少干扰而设计的线路隔离，仍必不可少。 [2]隔离的方法应用（1）分立器件隔离技术在隔离设计需要中，器件间电平变换隔离方法可采用单纯的分立器件完成。电平变换的最终目的就是实现工作单元两侧的电平根据各自需要而定。 分立器件隔离方法主要利用的就是电阻与晶体管的合理搭配，使得输入 / 输出间的电平实现匹配。 利用MOS管的开关作用，实现双侧电平变换，是常规有效的方法。 此种隔离方法，一般为共地隔离，仅完成电平变换，做到保护器件功能，非系统间电气隔离。 [2]（2）光电耦合器隔离技术光电耦合器，简称光耦，是一种以光为媒介来实现电信号传输的一类器件。其工作原理是把发光器（发光器件）与感光器（光敏器件）封装在芯片内部，通过外加在输入端的电信号控制发光器发光，感光器在内部光照的情况下，产生电信号，驱动输出端，实现了“电—光—电”转换。 由于光耦两侧的电信号完全隔离，内部以光为传输媒介， 因此，光耦输入/输出之间绝缘，可以完成单向信号的隔离传输，在数字电路中应用广泛。 普通光耦（TLP521）在隔离电路中的应用，受限于器件特点，其传输特性低频效果较好，高频信号传输失真严重。 实际电路测试中，115kbp的串行通信频率，通过电路器件参数匹配和电路结构优化，可基本适应。从东芝半导体公司光 耦产品系中可知，其通信速率涵盖了20kbps ～ 50Mbps， 因此在高速通信传输时，应根据设计需要选用高速光耦。 [2]（3）新型隔离技术在产品日新月异的时下，新器件层出不穷。主流芯片商德州仪器（TI）、亚诺德半导体（ADI）和芯科科技（Silicon Labs）分别研发了电容隔离、磁耦隔离、射频隔离等不同类型的数字隔离器。 [2]电容隔离电容隔离，利用了电容极板间填充材料为绝缘物质为隔离层，通过内部电场的变化来完成信号的传输。TI公司的ISO72x系列为典型电容隔离技术的应用。在电容隔离功能中，信号传输通道分为“低频通道”与 “高频通道”。低频信号通过内置振荡器产生的高频载波与PWM调制，通过差分方式进行调制传输。输出端低通滤波去除高频载波。 高频信号则不经过调制编码，差分变换后直接通过隔离层传输，输出端通过时间关系进行逻辑决策，从而控制输出多路选择器正确输出。 [2]磁耦隔离磁耦隔离，利用了变压器原理，使用变压器初级线圈与次级线圈两者之间通过磁耦合方式进行信号传递，从而实现隔离效果。ADI公司的iCoupler专利技术，就是基于芯片内空芯变压器的磁隔离技术。ADUM系列为典型磁耦隔离技术的应用。 [2]iCoupler磁隔离技术，通过芯片内部特征尺寸上实现的空芯变压器初级与次级线圈间的磁耦合实现信号隔离。信号传输采用了特定短脉冲组合方式来表示高低电平。两个连续的短脉冲表示高电平，单个短脉冲表示低电平。输出端根据检测脉冲的个数来确定输出电平状态。刷新器电路与看门狗电路提供了输入端电平状态与输出端故障安全状态方面的保障。 [2]射频隔离射频隔离，利用了无线射频传输原理。在发送端，完成基于高频信号的原始信号调制，通过发射天线发送。在接收端， 通过解调器完成已调信号的解调，恢复原始信号。通过这样的调制与解调，实现隔离的效果。Silicon Labs 公司的RF隔离即射频隔离，Si84xx系列为典型射频隔离技术的应用。 [2]RF隔离采用ISOpro型 RF射频隔离原理。芯片由半导体RF射频发射器、接收器和两者间的差动电容式隔离隔栅组成。工作中，使用基本的ON/OFF按键（OOK功能）， 输入数据为高电压时，发射器产生RF射频调制信号；输入数据为低电平时，发生器无RF射频调制信号。调制信号经过隔离隔栅送到接收器。接收器检测到同频带调制信号时，经解调器解调，输出高电平；无调制信号时，输出低电平。 [2]

IR：
关系介绍播报编辑含义具体而言，投资者关系管理(IRM：Investor Relationship Management)是指运用财经传播和营销的原理，通过管理公司同财经界和其他各界进行信息沟通的内容和渠道，以实现相关利益者价值最大化并如期获得投资者的广泛认同，规范资本市场运作、实现外部对公司经营约束的激励机制、实现股东价值最大化和保护投资者利益，以及缓解监管机构压力等。IRM还经常被通俗理解为公共关系管理(PRM：Public Relation Management)。在中国，IRM也经常被人称为财经公关。IR IRM投资者关系（IR，Investors Relation）与“投资者关系管理”（IRM, Investors Relation Management ）在西方成熟资本市场上已有三十多年的历史。然而，在发展中国家，这一概念还不广为证券市场参与各方所知，也并不是所有上市公司管理层都能充分理解IRM对公司长远发展的重要意义。 国内学者田书华曾对投资者关系管理进行过深入系统的研究。五大对象播报编辑排序值得一提的是投资者关系五大对象按重要程度排序依次是：基金经理，买方分析师，卖方分析师，财经媒体和高净资产个人（散户）。机构投资者一般都有自己的分析师团队，并有着各自不同的投资策略和偏好，对其进行有效的沟通需要专业的资本市场知识和经验，而仅仅运用公关和大众传播的思维方式效果是有限的。财经媒体的作用更多的是影响散户，所以重要程度排在买方和卖方之后。实务就实务方面而言，美国上市公司投资者关系发展较为成熟的（包括在美国上市的非美国公司），通常会委任专业的投资者关系顾问公司协助日常及项目型工作。常年的日常工作包括投资者关系策略制定，公司资本市场定位，投资亮点及故事包装，业绩发布电话会议支持，重大事件披露新闻稿的编撰润色，路演支持，投资者关系网站和投资者数据库的维护，以及接受投资者问询并对危机管理提供咨询建议。项目型服务包括详尽的股东组成结构分析，投资者反馈调查（包括常规的和特定议题如兼并收购），以及潜在机构投资者锁定及拓展计划。通常，对于新上市的公司尤其中小股本公司，为了更好的吸引资本市场的兴趣，一般会采用投资者关系顾问公司的最佳实践标准的（Best Practice），积极性的专业服务，以提高其资本市场的竞争力；对于上市多年的公司尤其是行业龙头的大型股本公司，内部投资者关系团队比较成熟，可以独立处理大部分日常工作，所以一般采用专业顾问公司的项目型服务。专业顾问公司的多年积累的数据库和成熟的研究方法可以提供更精准的调查，分析和建议。

半导体存储器：
简介播报编辑半导体集成存储器semiconductormemory每个存储单元有两个不同的表征态“ 0 ”和“1”，用以存储不同的信息 。半导体存储器是构成计算机的重要部件。同磁性存储器相比，半导体存储器具有存取速度快、存储容量大、体积小等优点，并且存储单元阵列和主要外围逻辑电路兼容，可制作在同一芯片上，使输入输出接口大为简化。因此，在计算机高速存储方面，半导体存储器已全部替代过去的磁性存储器。主要优点播报编辑这种存储器的主要优点是：①存储单元阵列和主要外围逻辑电路制作在同一个硅芯片上，输出和输入电平可以做到同片外的电路兼容和匹配。这可使计算机的运算和控制与存储两大部分之间的接口大为简化；②数据的存入和读取速度比磁性存储器约快三个数量级，可大大提高计算机运算速度；③利用大容量半导体存储器使存储体的体积和成本大大缩小和下降。因此，在计算机高速存储方面，半导体存储器已全部替代了过去的磁性存储器。用作大规模集成电路的半导体存储器，是1970年前后开始生产的1千位动态随机存储器。随着工艺技术的改进，到1984年这类产品已达到每片1兆位的存储容量。分类播报编辑按功能的不同，半导体存储器可分为随机存储器（RAM）、只读存储器（ROM）和串行存储器三大类。随着半导体集成电路工艺技术的发展，半导体存储器容量增长非常快，单片存储容量已进入兆位级水平，如16兆动态随机存储器（DRAM）已商品化，64兆、256兆DRAM在研制中。随机存储器对于任意一个地址，以相同速度高速地、随机地读出和写入数据的存储器（写入速度和读出速度可以不同）。存储单元的内部结构一般是组成二维方矩阵形式，即一位一个地址的形式(如64k×1位)。但有时也有编排成便于多位输出的形式（如8k×8位）。随机存储器主要用于组成计算机主存储器等要求快速存储的系统。按工作方式不同，随机存储器又可分为静态和动态两类。静态随机存储器的单元电路是触发器。可规定A或B两个晶体管中的一个导通时，代表“1”或代表“0”。触发器只要电源足够高，导通状态便不会改变。因此，存入每一单元的信息，如不“强迫”改写，只要有足够高的电源电压存在便不会改变，不需要任何刷新（见金属-氧化物-半导体静态随机存储器）。这种存储器的速度快，使用方便。动态随机存储器的单元由一个MOS电容和一个 MOS晶体管构成，数据以电荷形式存放在电容之中，一般以无电荷代表“0”，有电荷代表“1”，反之亦可。单元中的MOS晶体管是一个开关，它控制存储电容器中电荷的存入和取出。通常，MOS电容及与其相联接的PN结有微弱的漏电，电荷随时间而变少，直至漏完，存入的数据便会丢失。因此动态随机存储器需要每隔2～4毫秒对单元电路存储的信息重写一次，这称为刷新。这种存储器的特点是单元器件数量少，集成度高，应用最为广泛（见金属-氧化物-半导体动态随机存储器）。只读存储器用来存储长期固定的数据或信息，如各种函数表、字符和固定程序等。其单元只有一个二极管或三极管。一般规定，当器件接通时为“1”，断开时为“0”，反之亦可。若在设计只读存储器掩模版时，就将数据编写在掩模版图形中，光刻时便转移到硅芯片上。这样制备成的称为掩模只读存储器。这种存储器装成整机后，用户只能读取已存入的数据，而不能再编写数据。其优点是适合于大量生产。但是，整机在调试阶段，往往需要修改只读存储器的内容，比较费时、费事，很不灵活（见半导体只读存储器）。串行存储器它的单元排列成一维结构，犹如磁带。首尾部分的读取时间相隔很长，因为要按顺序通过整条磁带。半导体串行存储器中单元也是一维排列，数据按每列顺序读取，如移位寄存器和电荷耦合存储器等。按制造工艺技术的不同，半导体存储器可分成MOS型存储器和双极型存储器两类。70年代以来，NMOS电路(见N沟道金属-氧化物-半导体集成电路）和 CMOS电路 (见互补金属-氧化物-半导体集成电路)发展最快，用这两者都可做成极高集成度的各种半导体集成存储器。砷化镓半导体存储器如1024位静态随机存储器的读取时间已达2毫秒，预计在超高速领域将有所发展。

指令流水线：
原理播报编辑举个例子： 例如一条指令要执行要经过3个阶段：取指令、译码、执行，每个阶段都要花费一个机器周期，如果没有采用流水线技术，那么这条指令执行需要3个机器周期；如果采用了指令流水线技术，那么当这条指令完成“取指”后进入“译码”的同时，下一条指令就可以进行“取指”了，这样就提高了指令的执行效率。步骤播报编辑指令步骤的并行。常见的六级流水线将指令流的处理过程划分为取指(FI)、译码(DI)、计算操作数地址(CO)、取操作数(FO)、执行指令(EI)、写操作数(WO)等几个并行处理的过程段。这就是指令6级流水时序。在这个流水线中，处理器有六个操作部件，同时对这六条指令进行加工，加快了程序的执行速度。几乎所有的高性能计算机都采用了指令流水线。

总线结构：
总线结构优点播报编辑（1）组网费用低：从示意图可以看到这样的结构根本不需要另外的互联设备，是直接通过一条总线进行连接，所以组网费用较低；（2）这种网络因为各节点是共用总线带宽的，所以在传输速度上会随着接入网络的用户的增多而下降；（3）网络用户扩展较灵活：需要扩展用户时只需要添加一个接线器即可，但所能连接的用户数量有限；（4）维护较容易：单个节点失效不影响整个网络的正常通信。但是如果总线一断，则整个网络或者相应主干网段就断了。总线结构缺点播报编辑所有的数据都需经过总线传送，总线若出现故障则整个网络就会瘫痪。而且一次仅能一个端用户发送数据，其它端用户必须等待到获得发送权。在EAI中和星型结构相对。

总线标准：
为了使计算机产品成为全国范围内即插即用的工业化组装件，近几十年来计算机工业界制定了许多工业标准总线。优点明显：确保外设能与任一新计算机相联。定义：1、何谓标准：a、机械结构、尺寸、引脚的分布位置；b、数据线、地址线的宽度，传送规模；c、总线主设备数；d、定时控制方式，同步，异步，半同步。2、常用工业标准总线a、PC总线－－标准的总线IBMPC/-XT 20位地址线，8位数据线； IBMPC-AT总线，EISA 16位；b、IPI－－Intelligent Peripheral Interface 智能外围接口；c、Small computer system interface(SCSI)；d、PCI 总线(peripheral componet Interconnect)由标准化组织制定或行业、大公司提出，而成为行业标准。微机总线发展：ISA（Industry Standard Architecture）EISA(Extended Industry Standard Archilecture)

内存分配：
DOS内存播报编辑基本内存计算机主板上640KB以下的存储空间。DOS的系统程序和用户的应用程序都要使用这片空间。扩展内存(Extended)计算机主板上640KB以上的存储空间。这部分空间DOS不能直接管理，而是要通过扩展内存管理程序HIMEM.SYS来使用这部分内存。扩充内存(Expanded)插在计算机主板的扩充槽中的内存扩充板上的那部分存储器，它们是通过EMS.SYS程序来管理的。保留内存(Reserved)这是给计算机留做存储I/O系统数据及各种接口驱动程序使用的存储器，也称适配器内存。C++的内存分配：根据C++的语法规范，定义数组时数组长度必须用常量而不能用变量表示，此时可以使用动态内存分配解决这一问题。动态内存分配是指在程序运行时为程序中的变量分配内存空间，它完全由应用程序自己进行内存的分配和回收。程序运行时，特别要注意的是内存的分配。有以下六个地方都可以保存数据。存储区域播报编辑寄存器这是最快的保存区域，因为它位于和其他所有保存方式不同的地方：处理器内部。然而，寄存器的数量十分有限，所以寄存器是根据需要由编译器分配。我们对此没有直接的控制权，也不可能在自己的程序里找到寄存器存在的任何踪迹。 [1]堆栈驻留于常规RAM（随机访问存储器）区域，但可通过它的“堆栈指针”获得处理的直接支持。堆栈指针若向下移，会创建新的内存；若向上移，则会释放那些内存。这是一种特别快、特别有效的数据保存方式，仅次于寄存器。创建程序时，Java编译器必须准确地知道堆栈内保存的所有数据的“长度”以及“存在时间”。这是由于它必须生成相应的代码，以便向上和向下移动指针。这一限制无疑影响了程序的灵活性，所以尽管有些Java数据要保存在堆栈里——特别是对象句柄，但Java对象并不放到其中。 [1]堆一种常规用途的内存池（也在RAM区域），其中保存了Java对象。和堆栈不同，“内存堆”或“堆”（Heap）最吸引人的地方在于编译器不必知道要从堆里分配多少存储空间，也不必知道存储的数据要在堆里停留多长的时间。因此，用堆保存数据时会得到更大的灵活性。要求创建一个对象时，只需用new命令编制相关的代码即可。执行这些代码时，会在堆里自动进行数据的保存。当然，为达到这种灵活性，必然会付出一定的代价：在堆里分配存储空间时会花掉更长的时间！ [1]存储方式播报编辑静态存储这儿的“静态”（Static）是指“位于固定位置”（尽管也在RAM里）。程序运行期间，静态存储的数据将随时等候调用。可用static关键字指出一个对象的特定元素是静态的。但Java对象本身永远都不会置入静态存储空间。 [1]常数存储常数值通常直接置于程序代码内部。这样做是安全的，因为它们永远都不会改变。有的常数需要严格地保护，所以可考虑将它们置入只读存储器（ROM）。 [1]非RAM存储若数据完全独立于一个程序之外，则程序不运行时仍可存在，并在程序的控制范围之外。其中两个最主要的例子便是“流式对象”和“固定对象”。对于流式对象，对象会变成字节流，通常会发给另一台机器。而对于固定对象，对象保存在磁盘中。即使程序中止运行，它们仍可保持自己的状态不变。对于这些类型的数据存储，一个特别有用的技巧就是它们能存在于其他媒体中。一旦需要，甚至能将它们恢复成普通的、基于RAM的对象。 [1]

控制总线：
特点播报编辑控制总线CB（ControlBus）特点是：在单向、双向、双态等种形态，是总线中最复杂、最灵活、功能最强的，其数量、种类、定义随机型不同而不同。分类播报编辑控制总线就是各种信号线的集合，是计算机各部件之间传送数据、地址和控制信息的公共通道。⒈按相对于CPU与其芯片的位置来分：⑴片内总线：指在CPU内部各寄存器、算术逻辑部件ALU，控制部件以及内部高速缓冲存储器之间传输数据所用的总线，即芯片内部总线。⑵片外总线：通常所说的总线（BUS）指的外总线，是CPU与内存RAM、ROM和输入输出输入输出设备接口之间进行通讯的数据通道，CPU通过总线实现程序存取命令，内存/外设的数据交换在CPU与外设一定的情况下，总线速度是限制计算机整体性能的最大因数。⒉按总线功能分：⑴ 地址总线：（AB）用来传递地址信息。⑵数据总线：（DB）用来传递数据信息。⑶ 控制总线：（CB）用来传送各种控制信号。⒊按总线的层次结构分：⑴ CPU总线：包括CPU地址线（CAB），CPU数据线（CDB）和CPU控制线（CCB），其用来连接CPU和控制芯片。CS31通讯总线⑵ 存储器总线：包括存储器地址线（MAB）、存储器数据线（MDB）和存储器控制线（MCD），用来连接内存控制器（北桥）和内存。⑶ 系统总线：（I/O扩展总线）也称为I/O通道总线或I/O扩展总线，包括系统地址线（SAB），系统数据线（SDB）和系统控制线（SCD），用来与I/O扩展槽上的各种扩展卡相连接。⑷ 外部总线：（外围芯片总线）用来连接各种外设控制芯片，如主板上的I/O控制器（如硬盘接口控制器、软盘驱动控制器、串行/并行接口控制器等），和键盘控制器，包括外部地址线（XAB）、外部数据线（XMB）和外部控制线（XCB）。⒋系统总线（输入输出）扩展总线）又分为ISA、PCI、AGP等多种标准⑴ ISA(Industrystandardarchitecture,工业标准结构）是IBM公司为286AT电脑制定的总线工业标准，也称为AT标准。⑵ PCI(peripheralcomponentinterconnet,外部设备互连）是SIG（spelialinterestgroup）集团推出的总线结构。⑶ AGP(acceleratedgraphicsport，加速图形端口）是一种为了提高视频带宽而设计的总线规范，因为它是点对点连接，即连接控制芯片和AGP显卡，因此严格说来，AGP也是一种接口标准。ISA插槽播报编辑1、地址总线：SA0~SA19(I/O）和LA17~LA23(I/O)LⅪ测试总线技术2、数据总线：SD0~SD7(I/O）和SD8~SD15(I/O)3、控制总线：BALE(0)---USAddresslatchenable：系统地址锁存允许4、SYSCLK(0)---SYSTEMCLOCK系统时钟信号5、IR23~7,9~12,15(Z)---这是用于I/O设备通过中断控制器向CPU发送的中断请求(interruptrequest)信号6、SMEMR#和SMEMW#(0)---这是命令内存将数据送至数据总线的信号7、MEMR#和MEMW#(I/O)---内存读（MEMR）或内存写（MEMW#)信号8、DRQ0~3,5~7⑵---这是DMA请求（DMARequesc）信号9、DACK0#~3,5~7(0)---(DMAAcknowledge,DMA响应）这是对DRQ0~3,5~7的响应信号10、AEN(0)---地址允许（Addressenable）信号11、REFRESH#(I/O)---内存刷新(DRAMrefresh)信号12、SBHE(I/O)---系统总线字节允许（systembushighenable）信号13、MASTER⑵---主控信号14、MEMCS16#⑵---存储器16位片选(Memory16bitchipselect)信号15、ZOCS16#⑵---I/O16位片选(I/O16bitchipselect)信号16、OWS⑵---零等待状态(ZeroWaitState)信号技术指标播报编辑1、总线的带宽（总线数据传输速率）程序总线总线的带宽指的是单位时间内总线上传送的数据量，即每钞钟传送MB的最大稳态数据传输率。与总线密切相关的两个因素是总线的位宽和总线的工作频率，它们之间的关系：总线的带宽=总线的工作频率*总线的位宽/82、总线的位宽总线的位宽指的是总线能同时传送的二进制数据的位数，或数据总线的位数，即32位、64位等总线宽度的概念。总线的位宽越宽，每秒钟数据传输率越大，总线的带宽越宽。3、总线的工作频率总线的工作时钟频率以MHZ为单位，工作频率越高，总线工作速度越快，总线带宽越宽。操作播报编辑总线一个操作过程是完成两个模块之间传送信息，启动操作过程的是主模块，另外一个是从模块。某一时刻总线上只能有一个主模块占用总线。 [2]控制总线总线的操作步骤：主模块申请总线控制权，总线控制器进行裁决。数据传送的错误检查：主模块得到总线控制权后寻址从模块，从模块确认后进行数据传送。总线定时协议：定时协议可保证数据传输的双方操作同步，传输正确。定时协议有三种类型：同步总线定时：总线上的所有模块共用同一时钟脉冲进行操作过程的控制。各模块的所有动作的产生均在时钟周期的开始，多数动作在一个时钟周期中完成。异步总线定时：操作的发生由源或目的模块的特定信号来确定。总线上一个事件发生取决前一事件的发生，双方相互提供联络信号。控制总线模型总线定时协议 半同步总线定时：总线上各操作的时间间隔可以不同，但必须是时钟周期的整数倍，信号的出现，采样与结束仍以公共时钟为基准。ISA总线采用此定时方法。数据传输类型：分单周方式和突发（burst）方式。单周期方式：一个总线周期只传送一个数据。数据传输类型：突发方式：取得主线控制权后进行多个数据的传输。寻址时给出目的地首地址，访问第一个数据，数据2、3到数据n的地址在首地址基础上按一定规则自动寻址（如自动加1）。标准规范播报编辑总线是一类信号线的集合是模块间传输信息的公共通道，通过它，计算机各部件间可进行各种数据和命令的传送。为使不同供应商的产品间能够互换，给用户更多的选择，总线的技术规范要标准化。总线的标准制定要经周密考虑，要有严格的规定。总线标准（技术规范）包括以下几部分：机械结构规范：模块尺寸、总线插头、总线接插件以及安装尺寸均有统一规定。功能规范：总线每条信号线（引脚的名称）、功能以及工作过程要有统一规定。电气规范：总线每条信号线的有效电平、动态转换时间、负载能力等。

指令：
汉语词语播报编辑详细解释1. 指示命令。唐韩愈《魏博节度使沂国公先庙碑铭》：“号登 元和 ，大圣载营。风挥日舒，咸顺指令。” 元柳贯《浦阳十咏·昭灵仙迹》：“真仙帝遣司风雨，唤起渊龙听指令。” 陈世旭 《小镇上的将军》：“立刻就传来了上面的指令，将军的遗体就地火葬。”孔羽《睢县文史资料·袁氏陆园》：“袁家山（袁可立别业），……1949年，睢县人民政府指令于此处集存各种文物，并派有专人看守。”朱长超《月亮上的足迹》：地面站停止对他们发出指令，以免干扰他们的好梦。2. 下行公文的一种。对下级机关有所指示时用之。《新华日报》1943.9.18：“在次长的办公桌上，有电话机，来往的公事信。一切的指令、计划、方案与文件。”3. 指定电子计算机实现某种控制或运算的代码。包括操作功能和操作对象等内容。(1) ∶指导;号令 (2) ∶旧时公文的一种,是上级对下级呈请的批示(3) ∶能被计算机识别并执行的二进制代码，它规定了计算机能完成的某一操作。告诉计算机从事某一特殊运算的代码合算:指令种类：数据传送指令、算术运算指令、位运算指令、程序流程控制指令、串操作指令、处理器控制指令。4.指令，指基金管理人在管理基金资产时，向基金托管人发出的资金划拨及实物券调拨等指令。信息技术名词播报编辑基本概念告诉计算机从事某一特殊运算的代码。计算机程序发给计算机处理器的命令就是“指令（instruction）”。最低级的指令是一串0和1，表示一项实体作业操作要运行（如“Add”）。根据指令类型，某个具体的存储领域被称作“寄存器（register）”，里面包含了可用于调出指令的数据或数据存储位置。计算机的汇编语言（assembler）中，每种语言一般只响应单一的处理器指令。而高级语言的每种语言经过程序编辑后能响应多个处理器指令。在汇编语言中，宏指令（macro instruction）在汇编程序处理过程中会扩展为多个指令（以编码过的源宏定义为基础）。 [1]组成形式指令是指示计算机执行某种操作的命令。它由一串二进制数码组成。一条指令通常由两个部分组成：操作码+地址码。操作码：指明该指令要完成的操作的类型或性质，如取数、做加法或输出数据等。地址码：指明操作对象的内容或所在的存储单元地址。宏指令:宏指令是汇编语言程序中的一种伪指令它的格式为[宏指令名 ] MACRO [形式参数]……代码段……ENDM使用了“形式参数“，它们引用宏指令时被给出的一些名字或数值(实在参数)所替换。使用形式参数给宏指令带来了很大的灵活性。宏调用格式[宏指令名 ] [实际参数]实参数项将对应替换宏指令中形式参数。如果形式参数为标号时，则在宏调用中，实参也应为标号，且要求实参是唯一的。如果宏定义中有自己的标号，则在宏调用时，汇编程序自动地把标号变成唯一的标号.相关词语播报编辑伪指令:伪指令(伪操作)不像机器指令那样是在程序运行期间由计算机来执行的,它是在汇编程序对源程序汇编期间由汇编程序处理的操作.它可以完成如处理器选择,定义程序模式,定义数据,分配存储区,指示程序结束等功能.伪指令在编译的时候并不生成代码．伪指令在编译之后就不存在了搜索引擎指令播报编辑inurl命令用inurl搜索命令可以帮你搜索到在URL当中出现你搜索的关键词，很有针对性。使用格式：inurl:（+你需要搜索的内容）filetype命令在搜索引擎里面用filetype命令是可以帮助你打到相关的文档的，精确，特别是在百度推出了文库后，这个命令就更加好用啦site命令site命令用得多的一般是一些站长，他们用这个命令可以来查询某个域名被搜索引擎收录的情况，这样有利于自己对自身网站的了解和网站对搜索引擎的友好度等等使用格式：site:+域名intitle命令intitle命令顾名思义就是搜索标题的意思，在这里也对，我们有时搜索一个关键词时有上千万的内容，用这个命令就可以帮我们把跟我们搜索相关的网页排到前面。使用格式：intitle:+搜索内容 [2]

数据：
定义播报编辑数据是指对客观事件进行记录并可以鉴别的符号，是对客观事物的性质、状态以及相互关系等进行记载的物理符号或这些物理符号的组合。它是可识别的、抽象的符号。它不仅指狭义上的数字，还可以是具有一定意义的文字、字母、数字符号的组合、图形、图像、视频、音频等，也是客观事物的属性、数量、位置及其相互关系的抽象表示。例如，“0、1、2…”、“阴、雨、下降、气温”、“学生的档案记录、货物的运输情况”等都是数据。数据经过加工后就成为信息。在计算机科学中，数据是所有能输入计算机并被计算机程序处理的符号的介质的总称，是用于输入电子计算机进行处理，具有一定意义的数字、字母、符号和模拟量等的通称。计算机存储和处理的对象十分广泛，表示这些对象的数据也随之变得越来越复杂。 [1]信息信息与数据既有联系，又有区别。数据是信息的表现形式和载体，可以是符号、文字、数字、语音、图像、视频等。而信息是数据的内涵，信息是加载于数据之上，对数据作具有含义的解释。数据和信息是不可分离的，信息依赖数据来表达，数据则生动具体表达出信息。数据是符号，是物理性的，信息是对数据进行加工处理之后所得到的并对决策产生影响的数据，是逻辑性和观念性的；数据是信息的表现形式，信息是数据有意义的表示。数据是信息的表达、载体，信息是数据的内涵，是形与质的关系。数据本身没有意义，数据只有对实体行为产生影响时才成为信息。 [2]数据的语义播报编辑数据的表现形式还不能完全表达其内容，需要经过解释，数据和关于数据的解释是不可分的。例如，93是一个数据，可以是一个同学某门课的成绩，也可以是某个人的体重，还可以是计算机系2013级的学生人数。数据的解释是指对数据含义的说明，数据的含义称为数据的语义，数据与其语义是不可分的。分类播报编辑按性质分为①定位的，如各种坐标数据；②定性的，如表示事物属性的数据（居民地、河流、道路等）；③定量的，反映事物数量特征的数据，如长度、面积、体积等几何量或重量、速度等物理量；④定时的，反映事物时间特性的数据，如年、月、日、时、分、秒等。按表现形式分为①数字数据，如各种统计或量测数据。数字数据在某个区间内是离散的值 [3]；②模拟数据，由连续函数组成，是指在某个区间连续变化的物理量，又可以分为图形数据（如点、线、面）、符号数据、文字数据和图像数据等，如声音的大小和温度的变化等。按记录方式分为地图、表格、影像、磁带、纸带。按数字化方式分为矢量数据、格网数据等。在地理信息系统中，数据的选择、类型、数量、采集方法、详细程度、可信度等，取决于系统应用目标、功能、结构和数据处理、管理与分析的要求。现状与发展播报编辑大数据经济即将进入数据资本时代。数据成为与土地、劳动力、资本、技术等传统要素并列的生产要素，加快培育数据要素市场势在必行。同时，在大数据技术推动下，个人信息的应用已经由商业和经济领域，逐步扩大到政治、社会治理和公共政策等领域，并给公民的政治生活和国家的网络安全与主权等带来越来越大的影响。目前，我国已经初步建立了与国际基本接轨的个人信息保护法律体系，有些行政区域业已开始了数据要素市场的实践，意在形成系列创新安排。 [4]中国前瞻产业研究院2020年8月发布的《2020年中国数字经济发展报告》显示，自2015年提出“国家大数据战略”以来，我国的数字经济市场规模迅速扩大，截至2019年末，数字经济的总体规模达到了35.8亿元，占GDP的36.2%。 [5]2021年上海数据交易所成立，其面向全球开展大数据综合交易，这“可能是第4次工业革命的变革性事件之一”。 [4]数据知识产权播报编辑2022年11月17日，北京市、上海市、江苏省、浙江省、福建省、山东省、广东省、深圳市等8个地方被国家知识产权局办公室确定为开展数据知识产权工作的试点地方，试点工作期限为2022年11月至2023年12月 [6]。参考书籍播报编辑                 数据赋能书 名： 数据赋能作 者： 宋星出 版 社： 电子工业出版社出版时间：2021-01-01开 本：16开

GPRs：
介绍播报编辑移动通信技术从第一代的模拟通信系统发展到第二代的数字通信系统，以及之后的3G、4G、5G，正以突飞猛进的速度发展。在第二代移动通信技术中，GSM的应用最广泛。但是GSM系统只能进行电路域的数据交换，且最高传输速率为9.6kbit/s，难以满足数据业务的需求。因此，欧洲电信标准委员会（ETSI）推出了GPRS（General Packet Radio Service，通用分组无线业务）。分组交换技术是计算机网络上一项重要的数据传输技术。为了实现从传统语音业务到新兴数据业务的支持，GPRS在原GSM网络的基础上叠加了支持高速分组数据的网络，向用户提供WAP浏览（浏览因特网页面）、E-mail等功能，推动了移动数据业务的初次飞跃发展，实现了移动通信技术和数据通信技术（尤其是Internet技术）的完美结合。GPRS是介于2G和3G之间的技术，也被称为2.5G。它后面还有个弟弟EDGE，被称为2.75G。它们为实现从GSM向3G的平滑过渡奠定了基础。GPRS的功能GPRS主要是在移动用户和远端的数据网络(如支持TCP/IP、X.25等网络)之间提供一种连接，从而给移动用户提供高速无线IP和无线X.25业务，它将使得通讯速率从56 kbPs一直上升到114 kbPs，以GPRs为技术支撑，可为实现电子邮件、电子商务、移动办公、网上聊天、基于认钱P的信息浏览、互动游戏、FLASH画面、多和弦铃声、PDA终端接入、综合定位技术等，并且支持计算机和移动用户的持续连接[l]。较高的数据吞吐能力使得可以使用手持设备和笔记本电脑电脑进行电视会议和多媒体页面以及类似的应用。GPRS可以让多个用户共享某些固定的信道资源，数据速率最高可达164 kb/8。具有人类特性的PDA，边打电话边在网上冲浪; (2)GPRSB类手机如果MS能同时侦听两个系统的寻呼信息，MS可以同时附着在GSM系统和GPRS系统; (3)GPRSC类手机MS要么附着在GSM网络，要么附着在GPRS网络。它只能通过人工的方式进行切换，没有办法同时进行两种操作。 [2]GPRS的网络结构播报编辑GPRS是在GSM网络的基础上增加新的网络实体来实现分组数据业务，网络结构如图1所示。图2 GPRS网络结构图图1图2GPRS新增的网络实体：1）GSN（GPRS Support Node，GPRS支持节点）GSN是GPRS网络中最重要的网络部件，有SGSN何GGSN两种类型。SGSN（Serving GPRS Support Node，服务GPRS支持节点）SGSN的主要作用是记录MS的当前位置信息，提供移动性管理和路由选择等服务，并且在MS和GGSN之间完成移动分组数据的发送和接收。GGSN（Gateway GPRS Support Node，GPRS网关支持节点）GGSN起网关作用，把GSM网络中的分组数据包进行协议转换，之后发送到TCP/IP或X.25网络中。2）PCU（Packet Control Unit，分组控制单元）PCU位于BSS，用于处理数据业务，并将数据业务从GSM语音业务中分离出来。PCU增加了分组功能，可控制无线链路，并允许多用户占用同一无线资源。3）BG（Border Gateways，边界网关）BG用于PLMN间GPRS骨干网的互连，主要完成分属不同GPRS网络的SGSN、GGSN之间的路由功能，以及安全性管理功能，此外还可以根据运营商之间的漫游协定增加相关功能。4）CG（Charging Gateway，计费网关）CG主要完成从各GSN的话单收集、合并、预处理工作，并用作GPRS与计费中心之间的通信接口。5）DNS（Domain Name Server，域名服务器）GPRS网络中存在两种DNS。一种是GGSN同外部网络之间的DNS，主要功能是对外部网络的域名进行解析，作用等同于因特网上的普通DNS。另一种是GPRS骨干网上的DNS，主要功能是在PDP上下文激活过程中根据确定的APN（Access Point Name，接入点名称）解析出GGSN的IP地址，并且在SGSN间的路由区更新过程中，根据原路由区号码，解析出原SGSN的IP地址。 [3]GPRS的关键指标播报编辑1）容量指标①PDCH分配成功率PDCH分配成功率=（1-分配失败次数/分配尝试次数）×100%该指标反映了信道的拥塞情况，用来反映当符合信道分配条件，PCU将TCH用做PDCH的成功率。②每兆字节PDCH被清空次数每兆字节PDCH被清空次数=使用状态下的PDCH被清空次数/忙时流量该指标反映了全部信道（TCH、PDCH）的拥塞情况。③PCU资源拥塞率PCU资源拥塞率=PCU资源不足造成的信道分配失败次数/分配尝试次数×100%该指标反映了PCU的公共设备资源是否存在不足。④忙时平均激活PDCH数该指标反映了小区或BSC内PDCH数量，与TCH资源相比可以反映出PDCH占用无线资源的比例。⑤忙时数据总流量分为上行流量和下行流量，下行流量更能反映业务量的情况。⑥忙时每PDCH负荷忙时每PDCH负荷=忙时数据总流量/忙时平均激活PDCH数该指标反映了每个PDCH单位时间承载的数据量。这个指标要控制在4kbit/s以下。2）干扰指标①C/I②下行BLER③上行BLER3）移动性能指标①每兆字节小区重选次数=小区重选次数/忙时流量②短时间重选率=短时间小区重选次数/小区重选总次数×100%③乒乓重选率=乒乓重选次数/小区重选总次数×100%特点播报编辑应用上的特点手机上网还显得有些不尽人意。因此，全面的解决方法GPRS也就这样应运而生了，这项全新技术可以令您在任何时间、任何地点都能快速方便地实现连接，同时费用又很合理。简单地说：速度上去了，内容丰富了，应用增加了，而费用却更加合理。(1)高速数据传输gprs远程集中抄表系统速度10倍于GSM，还可以稳定地传送大容量的高质量音频与视频文件，可谓不一般的巨大进步。(2)永远在线由于建立新的连接几乎无需任何时间(即无需为每次数据的访问建立呼叫连接)，因而您随时都可与网络保持联系，举个例子，若无GPRS的支持，当您正在网上漫游，而此时恰有电话接入，大部分情况下您不得不断线后接通来电，通话完毕后重新拨号上网。这对大多数人来说，的确是件非常令人恼火的事。而有了GPRS，您就能轻而易举地解决这个冲突。(3)仅按数据流量计费即根据您传输的数据量(如：网上下载信息时)来计费，而不是按上网时间计费也就是说，只要不进行数据传输，哪怕您一直“在线”，也无需付费。做个“打电话”的比方，在使用GSM+WAP手机上网时，就好比电话接通便开始计费；而使用GPRS+WAP上网则要合理得多，就像电话接通并不收费，只有对话时才计算费用。总之，它真正体现了少用少付费的原则。技术上的特点数据实现分组发送和接收，按流量计费；56~115Kbps的传输速度.GPRS的应用，迟些还会配合Bluetooth(蓝牙技术)的发展。到时，数码相机加了bluetooth，就可以马上通过手机，把像片传送到遥远的地方，也不过一刻钟的时间，够酷吧，这个日子将距离我们不远了。GPRS是基本分组无线业务，采用分组交换的方式，数据速率最高可达164kbit/s，它可以给GSM用户提供移动环境下的高速数据业务，还可以提供收发Email、Internet创览等功能。 [4]在连接建立时间方面，GSM需要10-30秒，而GPRS只需要极短的时间就可以访问到相关请求；而对于费用而言，GSM是按连接时间计费的，而GPRS只需要按数据流量计费；GPRS对于网络资源的利用率而相对远远高于GSM。对应的范围1.移动办公2.移动商务3.移动信息服务4.移动互联网5.多媒体业务技术优势播报编辑（1）相对低廉的连接费用GPRS节水灌溉测控终端资源利用率高在GSM网络中，GPRS首先引入了分组交换的传输模式，使得原来采用电路交换模式的GSM传输数据方式发生了根本性的变化，这在无线资源稀缺的情况下显得尤为重要。按电路交换模式来说，在整个连接期内，用户无论是否传送数据都将独自占有无线信道。在会话期间，许多应用往往有不少的空闲时段，如上Internet浏览、收发E-mail等等。对于分组交换模式，用户只有在发送或接收数据期间才占用资源，这意味着多个用户可高效率地共享同一无线信道，从而提高了资源的利用率。GPRS用户的计费以通信的数据量为主要依据，体现了“得到多少、支付多少”的原则。实际上，GPRS用户的连接时间可能长达数小时，却只需支付相对低廉的连接费用。（2） 传输速率高GPRS可提供高达115kbps的传输速率（最高值为171.2kbps，不包括FEC）。这意味着在数年内，通过便携式电脑，GPRS用户能和ISDN用户一样快速地上网浏览，同时也使一些对传输速率敏感的移动多媒体应用成为可能。（3） 接入时间短分组交换接入时间缩短为少于1GPRS是一种新的GSM数据业务，它可以给移动用户提供无线分组数据接入股务。数据速率最高可达164kb/8.GSM空中接口的信道资源既可以被话音占用，也可以被GPRS数据业务占用。当然在信道充足的条件下，可以把一些信道定义为GPRS专用信道。要实现GPRS网络，需要在传统的GSM网络中引入新的网络接口和通信协议。GPRS网络引入GSN（GPRS Surporting Node）节点。移动台则必须是GPRS移动台或GPRS/GSM双模移动台。服务播报编辑GPRS提升GSM的数据服务性能:GPRS点到点 (P2P) 服务: 连接 (IP protocols)IP网络 and X.25网络。多播(P2MP)服务 : 一点到多点的组播和多方通话。短信服务 (SMS): 发送SMS。多媒体短信(MMS): 发送携带语音和图像信息的短消息。因特网服务提供商服务: 提供互联网内容服务。邮件服务通过POP3或者IMAP协议检查阅读发送电子邮件匿名服务: 匿名访问预定服务未来功能: 灵活加入新的功能，例如更大容量，更多用户，新的资源和无线网络。业务介绍播报编辑入门指南GPRS即“通用分组无线业务”（General Packet Radio Service的英文简称），是在现有GSM网络上开通的一种新型的分组数据传输技术。相对于原来GSM以拨号接入的电路交换数据传送方式，GPRS是分组交换技术，具有“永远在线”、“自如切换”、“高速传输”等优点，它能全面提升移动数据通信服务，使“移动梦网”服务更丰富、功能更强大，给您的生活和工作带来更多便捷与实惠。以GPRS为技术支撑，可为您实现诸如：电子邮件、电子商务、移动办公、网上聊天、基于WAP的信息浏览、互动游戏、FLASH画面、多和弦铃声、PDA终端接入、综合定位技术等，从而带您进入一个动感而彩色的移动世界，引领您尽快迈向3G时代。GPRS将是通向3G的一个重要里程碑。迄今为止,移动通信的发展还是围绕话音(话音质量和漫游范围)进行的。对非话音通信的需求仍处于初期阶段,但正稳步上升,短消息(SMS)业务量的迅速增长正说明了这一点。与此同时,国际互联网的迅猛增长,为基于网页内容的服务拓展了机遇,且很可能在下一代网络中占据支配地位。新型无线数据服务的预期需求令GSM运营商对运营环境中的重大变革充满期望。 [5]业务优势广域覆盖：GPRS在全国31个省240多个城市均有良好覆盖，基本上在手机可以打电话的地方都可以通过GPRS无线上网；永远在线：只要激活GPRS应用后，将一直保持在线，类似于无线专线网络服务。按量计费：GPRS服务虽然保持一直在线，但您不必担心费用问题；因为只有产生通信流量时才计费。高速传输：GPRS可支持53.6Kbps的峰值传输速率，理论峰值传输可达100余Kbps。业务丰富：彩信、WAP掌上资讯、金融交易、游戏百宝箱，丰富多彩的业务应用涉及人们生活的各个领域。（注：与手机支持情况和网络情况有关）设置全攻略市场上具有GPRS功能的手机品种相当丰富，而且多数GPRS手机具备“一键上网”功能，您无需进行设置，即可轻松使用，希望您在购买时选择带有GPRS“一键上网”功能的手机。如果您的GPRS手机不具备“一键上网”功能，可按照下述步骤进行设置，操作同样非常简单：APN的含义APN(Access Point Name)，即“接入点名称”，用来标识GPRS的业务种类，分为两大类：CMWAP(通过GPRS访问WAP业务)、CMNET（除了WAP以外的服务都用CMNET,比如连接因特网等）。连接配置选择GPRS方式连接：连接1：接入点名称为CMNET，用户名、密码均为空――主要用于访问因特网或java下载等；连接2：接入点名称为CMWAP，用户名、密码均为空――主要用于访问“移动梦网”WAP。其他通用配置在手机或PC中的进行其他参数配置，如：对于连接因特网需要在PC端配置、电子邮件要配置自己的POP和SMTP、WAP配置与其他WAP手机的配置一样。设置举例NOKIA6610、6100、7200、7250选择进入“服务”菜单，选择“设置”――>选择“修改服务设置”：进入修改服务设置屏幕，分别输入相关信息：“设置组名称”设为“移动梦网GPRS”“连接类型”设为“持续连接”“安全保护”设为“关”“传输方式”设为“GPRS”“GPRS接入点”设为“CMWAP”“IP地址”设为“10.0.0.172”“鉴权类型”设为“普通”“登录类型”设为“自动”“用户名”和“密码”不用设置输入完毕按返回退到待机状态在待机状态下，按“功能表”选择“服务”并进入“主页”后即可登录移动梦网主页玩转GPRSGPRS全面支持随e行业务，随时随处实现移动办公：将带有GPRS功能的SIM卡插入GPRS无线网卡中，再与移动终端（笔记本电脑或PDA）相连接，就可实现无线上网，并可以收发短信。当然也可将GPRS手机通过数据线与笔记本电脑串口相连（或经过USB、红外等端口相连接）通过GPRS访问因特网。GPRS可支持50多K的峰值传输速率，广域覆盖、随时在线等特点的发挥可使您感受随时随处的移动办公。通过GPRS使用手机WAP服务GPRS可大大提升原有WAP服务的品质，访问速度大大提升，费用更加经济，可实现新闻浏览、随身理财、移动办公、更可支持多种彩色、动感的图片、游戏等服务（需要手机支持）。收发彩信，乐趣无穷：拿出您的彩信手机通过GPRS可享受丰富多彩的彩信服务：卡通、明星、漫画、游戏等多姿多彩的彩信壁纸，还有动感十足的彩信动画任您下载。用带有摄像头的彩信手机拍下生活中的精彩画面通过GPRS发送给亲友，更是乐趣无穷！下载“移动百宝箱”中的精彩内容：使用支持JAVA功能的手机，通过GPRS方式进入“移动百宝箱”，方便地下载娱乐、生活、商务、游戏等方面的精彩内容。如下载各种棋牌游戏、电子书籍、股票证券信息、天气预报、聊天交友等。企业和行业应用：专线接入GPRS网络，实现高服务质量等级的专有服务。遥感、监测、定位等信息通过GPRS，方便的进行无线接入和传输。业务办理使用条件（1）手机支持GPRS功能。（2）全球通客户已申请开通GPRS功能。（3）在手机上正确设置GPRS参数。（4）处在GPRS网络覆盖范围内。业务办理全球通移动电话客户可以拨打客户服务热线10086或持有效身份证件到号码归属地移动各营业厅及合作厅办理开通、取消、变更GPRS业务。也可登陆号码归属地移动网站进行网上办理。漫游服务国内漫游：注：GPRS已实现北京全市覆盖，另支持国内31 省240多个市的漫游服务附录： GPRS支持的漫游城市举例北京、上海、天津、重庆、沈阳、大连、济南、青岛、石家庄、郑州、武汉、南京、杭州、宁波、温州、成都、南宁、广州、深圳、福州、厦门、海口、三亚、合肥、长沙、西安、兰州、乌鲁木齐、西宁、昆明、银川、呼和浩特、贵阳、南昌、太原、拉萨、哈尔滨、长春国际漫游GPRS已实现美国、法国、新加坡、台湾、香港、澳门等地的国际漫游。部分漫游国家和地区运营商及资费列表（资费仅供参考，以当地运营商公布最新资费为准）

页面置换算法：
常见的置换算法播报编辑最佳置换算法（OPT）这是一种理想情况下的页面置换算法，但实际上是不可能实现的。该算法的基本思想是：发生缺页时，有些页面在内存中，其中有一页将很快被访问（也包含紧接着的下一条指令的那页），而其他页面则可能要到10、100或者1000条指令后才会被访问，每个页面都可 [1]以用在该页面首次被访问前所要执行的指令数进行标记。最佳页面置换算法只是简单地规定：标记最大的页应该被置换。这个算法唯一的一个问题就是它无法实现。当缺页发生时，操作系统无法知道各个页面下一次是在什么时候被访问。虽然这个算法不可能实现，但是最佳页面置换算法可以用于对可实现算法的性能进行衡量比较。 [1]先进先出置换算法（FIFO）最简单的页面置换算法是先入先出（FIFO）法。这种算法的实质是，总是选择在主存中停留时间最长（即最老）的一页置换，即先进入内存的页，先退出内存。理由是：最早调入内存的页，其不再被使用的可能性比刚调入内存的可能性大。建立一个FIFO队列，收容所有在内存中的页。被置换页面总是在队列头上进行。当一个页面被放入内存时，就把它插在队尾上。 [1]这种算法只是在按线性顺序访问地址空间 [1]时才是理想的，否则效率不高。因为那些常被访问的页，往往在主存中也停留得最久，结果它们因变“老”而不得不被置换出去。FIFO的另一个缺点是，它有一种异常现象，即在增加存储块的情况下，反而使缺页中断率增加了。当然，导致这种异常现象的页面走向实际上是很少见的。 [1]最近最久未使用（LRU）算法FIFO算法和OPT算法之间的主要差别是，FIFO算法利用页面进入内存后的时间长短作为置换依据，而OPT算法的依据是将来使用页面的时间。如果以最近的过去作为不久将来的近似，那么就可以把过去最长一段时间里不曾被使用的页面置换掉。它的实质是，当需要置换一页时，选择在之前一段时间里最久没有使用过的页面予以置换。这种算法就称为最久未使用算法（Least Recently Used，LRU）。 [1]LRU算法是与每个页面最后使用的时间有关的。当必须置换一个页面时，LRU算法选择过去一段时间里最久未被使用的页面。 [1]LRU算法是经常采用的页面置换算法，并被认为是相当好的，但是存在如何实现它的问题。LRU算法需要实际硬件的支持。其问题是怎么确定最后使用时间的顺序，对此有两种可行的办法： [1]1.计数器。最简单的情况是使每个页表项对应一个使用时间字段，并给CPU增加一个逻辑时钟或计数器。每次存储访问，该时钟都加1。每当访问一个页面时，时钟寄存器的内容就被复制到相应页表项的使用时间字段中。这样我们就可以始终保留着每个页面最后访问的“时间”。在置换页面时，选择该时间值最小的页面。这样做， [1]不仅要查页表，而且当页表改变时（因CPU调度）要 [1]维护这个页表中的时间，还要考虑到时钟值溢出的问题。2.栈。用一个栈保留页号。每当访问一个页面时，就把它从栈中取出放在栈顶上。这样一来，栈顶总是放有目前使用最多的页，而栈底放着目前最少使用的页。由于要从栈的中间移走一项，所以要用具有头尾指针的双向链连起来。在最坏的情况下，移走一页并把它放在栈顶上需要改动6个指针。每次修改都要有开销，但需要置换哪个页面却可直接得到，用不着查找，因为尾指针指向栈底，其中有被置换页。 [1]因实现LRU算法必须有大量硬件支持，还需要一定的软件开销。所以实际实现的都是一种简单有效的LRU近似算法。 [1]一种LRU近似算法是最近未使用算法（Not Recently Used，NRU）。它在存储分块表的每一表项中增加一个引用位，操作系统定期地将它们置为0。当某一页被访问时，由硬件将该位置1。过一段时间后，通过检查这些位可以确定哪些页使用过，哪些页自上次置0后还未使用过。就可把该位是0的页淘汰出去，因为在之前最近一段时间里它未被访问过。 [1]Clock置换算法采用Clock算法时,只需为每个页面设置一位访问位，再将内存中的所有页面都通过链接指针链接成一个循环队列。当某页被访问时,就将其访问位被置为1。置换算法在选择一页淘汰时，只检查页的访问位。如果改位是0,就选择该页换出;如果是1,则重新将它置0,暂不换出，而给该页第二次驻留内存的机会,再按照FIFO算法检查下一个页面。当检查到队列中的最后一个页面时，若其访问位仍为1,则再返回到队首去检查第一个页面。由于该算法是循环地检查各页面的使用情况,故称为Clock算法。 [3]最少使用（LFU）置换算法在采用最少使用置换算法时，应为在内存中的每个页面设置一个移位寄存器，用来记录该页面被访问的频率。该置换算法选择在之前时期使用最少的页面作为淘汰页。由于存储器具有较高的访问速度，例如100 ns，在1 ms时间内可能对某页面连续访 [1]问成千上万次，因此，通常不能直接利用计数器来记录某页被访问的次数，而是采用移位寄存器方式。每次访问某页时，便将该移位寄存器的最高位置1，再每隔一定时间(例如100 ns)右移一次。这样，在最近一段时间使用最少的页面将是∑Ri最小的页。 [1] [3]LFU置换算法的页面访问图与LRU置换算法的访问图完全相同；或者说，利用这样一套硬件既可实现LRU算法，又可实现LFU算法。应该指出，LFU算法并不能真正反映出页面的使用情况，因为在每一时间间隔内，只是用寄存器的一位来记录页的使用情况，因此，访问一次和访问10 000次是等效的。 [3]6）工作集算法 [1]7）工作集时钟算法8）老化算法（非常类似LRU的有效算法） [1]9）NRU(最近未使用）算法10）第二次机会算法第二次机会算法的基本思想是与FIFO相同的，但是有所改进，避免把经常使用的页面置换出去。当选择置换页面时，检查它的访问位。如果是 [1]0，就淘汰这页；如果访问位是1，就给它第二次机会，并选择下一个FIFO页面。当一个页面得到第二次机会时，它的访问位就清为0，它的到达时间就置为当前时间。如果该页在此期间被访问过，则访问位置1。这样给了第二次机会的页面将不被淘汰，直至所有其他页面被淘汰过（或者也给了第二次机会）。因此，如果一个页面经常使用，它的访问位总保持为1，它就从来不会被淘汰出去。 [1]第二次机会算法可视为一个环形队列。用一个指针指示哪一页是下面要淘汰的。当需要一个 [1]存储块时，指针就前进，直至找到访问位是0的页。随着指针的前进，把访问位就清为0。在最坏的情况下，所有的访问位都是1，指针要通过整个队列一周，每个页都给第二次机会。这时就退化成FIFO算法了。 [1]操作系统页面置换算法代码播报编辑#include <stdio.h> [2]#include <stdlib.h>#include <unistd.h> #define TRUE 1#define FALSE 0#define INVALID -1#define NUL 0#define total_instruction 320 /*指令流长*/#define total_vp 32 /*虚页长*/#define clear_period 50 /*清零周期*/typedef struct{ /*页面结构*/int pn,pfn,counter,time;}pl_type;pl_type pl[total_vp]; /*页面结构数组*/struct pfc_struct{ /*页面控制结构*/int pn,pfn;struct pfc_struct *next;};typedef struct pfc_struct pfc_type;pfc_type pfc[total_vp],*freepf_head,*busypf_head,*busypf_tail;int diseffect,a[total_instruction];int page[total_instruction], offset[total_instruction];void initialize(int);void FIFO(int);void LRU(int);void NUR(int);int main(){int S,i;srand((int)getpid());S=(int)rand()%390;for(i=0;i<total_instruction;i+=1) /*产生指令队列*/{a[i]=S; /*任选一指令访问点*/a[i+1]=a[i]+1; /*顺序执行一条指令*/a[i+2]=(int)rand()%390; /*执行前地址指令m’*/a[i+3]=a[i+2]+1; /*执行后地址指令*/S=(int)rand()%390;}for(i=0;i<total_instruction;i++) /*将指令序列变换成页地址流*/{page[i]=a[i]/10;offset[i]=a[i]%10;}for(i=4;i<=32;i++) /*用户内存工作区从4个页面到32个页面*/{printf("%2d page frames",i);FIFO(i);LRU(i);NUR(i);printf("\n");}return 0;}void FIFO(int total_pf) /*FIFO(First in First out)ALGORITHM*//*用户进程的内存页面数*/{int i;pfc_type *p, *t;initialize(total_pf); /*初始化相关页面控制用数据结构*/busypf_head=busypf_tail=NUL; /*忙页面队列头，对列尾链接*/for(i=0;i<total_instruction;i++){if(pl[page[i]].pfn==INVALID) /*页面失效*/{diseffect+=1; /*失效次数*/if(freepf_head==NUL) /*无空33闲页面*/{p=busypf_head->next;pl[busypf_head->pn].pfn=INVALID; /*释放忙页面队列中的第一个页面*/freepf_head=busypf_head;freepf_head->next=NUL;busypf_head=p;}p=freepf_head->next; /*按方式调新页面入内存页面*/freepf_head->next=NUL;freepf_head->pn=page[i];pl[page[i]].pfn=freepf_head->pfn;if(busypf_tail==NUL)busypf_head=busypf_tail=freepf_head;else{busypf_tail->next=freepf_head;busypf_tail=freepf_head;}freepf_head=p;}}printf("FIFO:%6.4F",1-(float)diseffect/320);}void LRU(int total_pf){int min,minj,i,j,present_time;initialize(total_pf);present_time=0;for(i=0;i<total_instruction;i++){if(pl[page[i]].pfn==INVALID) /*页面失效*/{diseffect++;if(freepf_head==NUL) /*无空闲2页面*/{min=32767;for(j=0;j<total_vp;j++)if(min>pl[j].time&&pl[j].pfn!=INVALID){min=pl[j].time;minj=j;}freepf_head=&pfc[pl[minj].pfn];pl[minj].pfn=INVALID;pl[minj].time=-1;freepf_head->next=NUL;}pl[page[i]].pfn=freepf_head->pfn;pl[page[i]].time=present_time;freepf_head=freepf_head->next;}elsepl[page[i]].time=present_time;present_time++;}printf("LRU:%6.4f",1-(float)diseffect/320);}void NUR(int total_pf){int i,j,dp,cont_flag,old_dp;pfc_type *t;initialize(total_pf);dp=0;for(i=0;i<total_instruction;i++){if(pl[page[i]].pfn==INVALID) /*页面失效*/{diseffect++;if(freepf_head==NUL) /*无空闲1页面*/{cont_flag=TRUE;old_dp=dp;while(cont_flag)if(pl[dp].counter==0&&pl[dp].pfn!=INVALID)cont_flag=FALSE;else{dp++;if(dp==total_vp)dp=0;if(dp==old_dp)for(j=0;j<total_vp;j++)pl[j].counter=0;}freepf_head=&pfc[pl[dp].pfn];pl[dp].pfn=INVALID;freepf_head->next=NUL;}pl[page[i]].pfn=freepf_head->pfn;freepf_head=freepf_head->next;}elsepl[page[i]].counter=1;if(i%clear_period==0)for(j=0;j<total_vp;j++)pl[j].counter=0;}printf("NUR:%6.4f",1-(float)diseffect/320);}void initialize(int total_pf) /*初始化相关数据结构*//*用户进程的内存页面数*/{int i;diseffect=0;for(i=0;i<total_vp;i++){pl[i].pn=i;pl[i].pfn=INVALID; /*置页面控制结构中的页号，页面为空*/pl[i].counter=0;pl[i].time=-1; /*页面控制结构中的访问次数为0，时间为-1*/}for(i=1;i<total_pf;i++){pfc[i-1].next=&pfc[i];pfc[i-1].pfn=i-1;/*建立pfc[i-1]和pfc[i]之间的连接*/}pfc[total_pf-1].next=NUL;pfc[total_pf-1].pfn=total_pf-1;freepf_head=&pfc[0]; /*页面队列的头指针为pfc[0]*/}/*说明：本程序在Linux的gcc下和c-free下编译运行通过*/

地址总线：
基本信息播报编辑地址总线（Address Bus）是一种计算机总线，是CPU或有DMA能力的单元，用来沟通这些单元想要访问（读取/写入）计算机内存组件/地方的物理地址。 [1]数据总线的宽度，随可寻址的内存组件大小而变，决定有多少的内存可以被访问。举例来说：一个 16位元 宽度的位址总线 (通常在 1970年 和 1980年早期的 8位元处理器中使用) 到达 2 的 16 次方 = 65536 = 64 KB 的内存位址，而一个 32位单元位址总线 (通常在像现今 2004年 的 PC 处理器中) 可以寻址到 4,294,967,296 = 4 GB 的位址。但现在很多计算机内存已经大于4G（windows XP x32位系统最大只能识别3.29G，所以要使用4G以上大内存就要用windows x64位系统）。所以主流的计算机都是64位的处理器也就是说可以寻址到2^64=16X10^18=16EB的位址，在很长一段时间内这个数字是用不完的。在大多数的微电脑(微计算机)中，可寻址的元件都是 8 位元的 "字节" (所以 "K" 在这情况像相等于 "KB" 或 kilobyte)，有很多的电脑例子是以更大的资料区块当作他们实体上最小的可寻址元件，像是大型主机、超级电脑、以及某些工作站的CPU。 [1]地址总线AB是专门用来传送地址的，由于地址只能从CPU传向外部存储器或I/O端口，所以地址总线总是单向三态的，这与数据总线不同。地址总线的位数决定了CPU可直接寻址的内存空间大小，比如8位微机的地址总线为16位，则其最大可寻址空间为2^16=64KB，16位微型机的地址总线为20位，其可寻址空间为2^20=1MB。一般来说，若地址总线为n位，则可寻址空间为2^n位。 [1]技术指标播报编辑地址总线(图2)1、总线的带宽（总线数据传输速率） [1]总线的带宽指的是单位时间内总线上传送的数据量，即每钞钟传送MB的最大稳态数据传输率。与总线密切相关的两个因素是总线的位宽和总线的工作频率，它们之间的关系： [1]2、总线的位宽总线的位宽指的是总线能同时传送的二进制数据的位数，或数据总线的位数，即32位、64位等总线宽度的概念。总线的位宽越宽，每秒钟数据传输率越大，总线的带宽越宽。 [1]3、总线的工作频率总线的工作时钟频率以MHZ为单位，工作频率越高，总线工作速度越快，总线带宽越宽。总线带宽的计算方法：总线的带宽=总线的工作频率*总线的位宽/8。 [1]例如：对于64位、800MHz的前端总线，它的数据传输率就等于6.4GB/s=64bit×800MHz÷8(Byte)；32位、33MHz PCI总线的数据传输率就是132MB/s=32bit×33MHz÷8(Byte)，等等 [1]操作过程播报编辑第一代32位处理器地址总线一个操作过程是完成两个模块之间传送信息，启动操作过程的是主模块，另外一个是从模块。某一时刻总线上只能有一个主模块占用总线。 [1]总线的操作步骤：主模块申请总线控制权，总线控制器进行裁决。数据传送的错误检查：主模块得到总线控制权后寻址从模块，从模块确认后进行数据传送。 [1]总线定时协议：定时协议可保证数据传输的双方操作同步，传输正确。定时协议有三种类型： [1]同步总线定时：总线上的所有模块共用同一时钟脉冲进行操作过程的控制。各模块的所有动作的产生均在时钟周期的开始，多数动作在一个时钟周期中完成。 [1]异步总线定时：操作的发生由源或目的模块的特定信号来确定。总线上一个事件发生取决前一事件的发生，双方相互提供联络信号。 [1]总线定时协议半同步总线定时：总线上各操作的时间间隔可以不同，但必须是时钟周期的整数倍，信号的出现，采样与结束仍以公共时钟为基准。ISA总线采用此定时方法。 [1]数据传输类型：分单周方式和突发（burst）方式。单周期方式：一个总线周期只传送一个数据。数据传输类型：突发方式：取得主线控制权后进行多个数据的传输。寻址时给出目的地首地址，访问第一个数据，数据2、3到数据n的地址在首地址基础上按一定规则自动寻址（如自动加1）。 [1]技术规范播报编辑地址总线(图2)地址总线是一类信号线的集合是模块间传输信息的公共通道，通过它，计算机各部件间可进行各种数据和命令的传送。为使不同供应商的产品间能够互换，给用户更多的选择，总线的技术规范要标准化。 [1]总线的标准制定要经周密考虑，要有严格的规定。总线标准（技术规范）包括以下几部分：机械结构规范：模块尺寸、总线插头、总线接插件以及安装尺寸均有统一规定。功能规范：总线每条信号线（引脚的名称）、功能以及工作过程要有统一规定。电气规范：总线每条信号线的有效电平、动态转换时间、负载能力等。标准分类播报编辑ISA总线ISA技术ISA（Industrial Standard Architecture）总线标准是IBM公司1984年为推出PC/AT机而建立的系统总线标准，所以也叫AT总线。它是对XT总线的扩展，以适应8/16位数据总线要求。它在80286至80486时代应用非常广泛，以至于奔腾机中还保留有ISA总线插槽。ISA总线有98只引脚。 [1]EISA总线EISA总线是1988年由Compaq等9家公司联合推出的总线标准。它是在ISA总线的基础上使用双层插座，在原来ISA总线的98条信号线上又增加了98条信号线，也就是在两条ISA信号线之间添加一条EISA信号线。在实用中，EISA总线完全兼容ISA总线信号。 [1]VESA总线VESA（Video Electronics Standard Association）总线是1992年由60家附件卡制造商联合推出的一种局部总线，简称为VL（VESA Localbus）总线。它的推出为微机系统总线体系结构的革新奠定了基础。该总线系统考虑到CPU与主存和Cache的直接相连，通常把这部分总线称为CPU总线或主总线，其他设备通过VL总线与CPU总线相连，所以VL总线被称为局部总线。 [1]PCI总线PCI（Peripheral Component Interconnect）总线是当前最流行的总线之一，它是由Intel公司推出的一种局部总线。它定义了32位数据总线，且可扩展为64位。PCI总线主板插槽的体积比原ISA总线插槽还小，其功能比VESA、ISA有极大的改善，支持突发读写操作，最大传输速率可达132MB/s，可同时支持多组外围设备。 [1]CompactPCI“以上所列举的几种系统总线一般都用于商用PC机中，在计算机系统总线中，还有另一大类为适应工业现场环境而设计的系统总线，比如STD总线、VME总线、PC/104总线等。这里仅介绍当前工业计算机的热门总线之一：CompactPCI。” [1]发展历程播报编辑地址总线的详细发展历程，包括早期的PC总线和ISA总线、PCI/AGP总线、PCI-X总线以及主流的PCIExpress、HyperTransport高速串行总线。从PC总线到ISA、PCI总线，再由PCI进入PCIExpress和HyperTransport体系，计算机在这三次大转折中也完成三次飞跃式的提升。与这个过程相对应，计算机的处理速度、实现的功能和软件平台都在进行同样的进化，显然，没有总线技术的进步作为基础，计算机的快速发展就无从谈起。业界站在一个崭新的起点：PCIExpress和HyperTransport开创了一个近乎完美的总线架构。而业界对高速总线的渴求也是无休无止，PCIExpress2.0和HyperTransport3.0都将提上日程，它们将会再次带来效能提升。在计算机系统中，各个功能部件都是通过地址总线寻址，总线的速度对系统性能有着极大的影响。而也正因为如此，总线被誉为是计算机系统的神经中枢。但相比CPU、显卡、内存、硬盘等功能部件，总线技术的提升步伐要缓慢得多。在PC发展的二十余年历史中，总线只进行三次更新换代，但它的每次变革都令计算机的面貌焕然一新。 [1]

软件：
定义播报编辑软件，拼音为ruǎn jiàn，国标中对软件的定义为：与计算机系统操作有关的计算机程序、规程、规则，以及可能有的文件、文档及数据。其它定义：1．运行时，能够提供所要求功能和性能的指令或计算机程序集合。2．程序能够满意地处理信息的数据结构。3．描述程序功能需求以及程序如何操作和使用所要求的文档。以开发语言作为描述语言，可以认为：软件=程序+数据+文档特点播报编辑1、无形的，没有物理形态，只能通过运行状况来了解功能、特性和质量2、软件渗透了大量的脑力劳动，人的逻辑思维、智能活动和技术水平是软件产品的关键3、软件不会像硬件一样老化磨损，但存在缺陷维护和技术更新4、软件的开发和运行必须依赖于特定的计算机系统环境，对于硬件有依赖性，为了减少依赖，开发中提出了软件的可移植性5、软件具有可复用性，软件开发出来很容易被复制，从而形成多个副本分类播报编辑应用类别按应用范围划分，一般来讲软件被划分为系统软件、应用软件和介于这两者之间的中间件。系统软件系统软件为计算机使用提供最基本的功能，可分为操作系统和支撑软件，其中操作系统是最基本的软件。系统软件是负责管理计算机系统中各种独立的硬件，使得它们可以协调工作。系统软件使得计算机使用者和其他软件将计算机当作一个整体而不需要顾及到底层每个硬件是如何工作的。1．操作系统是一管理计算机硬件与软件资源的程序，同时也是计算机系统的内核与基石。操作系统身负诸如管理与配置内存、决定系统资源供需的优先次序、控制输入与输出设备、操作网络与管理文件系统等基本事务。操作系统也提供一个让使用者与系统交互的操作接口。2．支撑软件是支撑各种软件的开发与维护的软件，又称为软件开发环境（SDE）。它主要包括环境数据库、各种接口软件和工具组。著名的软件开发环境有IBM公司的Web Sphere,微软公司的等。包括一系列基本的工具（比如编译器、数据库管理、存储器格式化、文件系统管理、用户身份验证、驱动管理、网络连接等方面的工具）。应用软件系统软件并不针对某一特定应用领域，而应用软件则相反，不同的应用软件根据用户和所服务的领域提供不同的功能。应用软件是为了某种特定的用途而被开发的软件。它可以是一个特定的程序，比如一个图像浏览器。也可以是一组功能联系紧密，可以互相协作的程序的集合，比如微软的Office软件。也可以是一个由众多独立程序组成的庞大的软件系统，比如数据库管理系统。如今智能手机得到了极大的普及，运行在手机上的应用软件简称手机软件。所谓手机软件就是可以安装在手机上的软件，完善原始系统的不足与个性化。随着科技的发展，手机的功能也越来越多，越来越强大。不是像过去的那么简单死板,发展到了可以和掌上电脑相媲美。手机软件与电脑一样，下载手机软件时还要考虑你购买这一款手机所安装的系统来决定要下相对应的软件。手机主流系统有以下：Windows Phone、Symbian、iOS、Android。授权类别不同的软件一般都有对应的软件授权，软件的用户必须在同意所使用软件的许可证的情况下才能够合法的使用软件。从另一方面来讲，特定软件的许可条款也不能够与法律相违背。依据许可方式的不同，大致可将软件区分为几类：专属软件：此类授权通常不允许用户随意的复制、研究、修改或散布该软件。违反此类授权通常会有严重的法律责任。传统的商业软件公司会采用此类授权，例如微软的Windows和办公软件。专属软件的源码通常被公司视为私有财产而予以严密的保护。自由软件：此类授权正好与专属软件相反，赋予用户复制、研究、修改和散布该软件的权利，并提供源码供用户自由使用，仅给予些许的其它限制。以Linux、Firefox 和OpenOffice 可做为此类软件的代表。共享软件：通常可免费的取得并使用其试用版，但在功能或使用期间上受到限制。开发者会鼓励用户付费以取得功能完整的商业版本。根据共享软件作者的授权，用户可以从各种渠道免费得到它的拷贝，也可以自由传播它。免费软件：可免费取得和转载，但并不提供源码，也无法修改。公共软件：原作者已放弃权利，著作权过期，或作者已经不可考究的软件。使用上无任何限制。相关概念播报编辑开发流程软件开发是根据用户要求建造出软件系统或者系统中的软件部分的过程。软件开发是一项包括需求捕捉，需求分析，设计，实现和测试的系统工程。软件一般是用某种程序设计语言来实现的。通常采用软件开发工具可以进行开发。软件开发流程即Software development process。软件设计思路和方法的一般过程，包括设计软件的功能和实现的算法和方法、软件的总体结构设计和模块设计、编程和调试、程序联调和测试以及编写、提交程序。1 相关系统分析员和用户初步了解需求，然后列出要开发的系统的大功能模块，每个大功能模块有哪些小功能模块，对于有些需求比较明确相关的界面时，在这一步里面可以初步定义好少量的界面。2 系统分析员深入了解和分析需求，根据自己的经验和需求做出一份文档系统的功能需求文档。这次的文档会清楚例用系统大致的大功能模块，大功能模块有哪些小功能模块，并且还例出相关的界面和界面功能。3 系统分析员和用户再次确认需求。4 系统分析员根据确认的需求文档所例用的界面和功能需求，用迭代的方式对每个界面或功能做系统的概要设计。5 系统分析员把写好的概要设计文档给程序员，程序员根据所例出的功能一个一个的编写。6 测试编写好的系统。交给用户使用，用户使用后一个一个的确认每个功能，然后验收。软件工程师一般指从事软件开发职业的人。软件工程师10余年来一直占据高薪职业排行榜的前列，作为高科技行业的代表，技术含量很高，职位的争夺也异常激烈。软件开发是一个系统的过程，需要经过市场需求分析、软件代码编写、软件测试、软件维护等程序。软件开发工程师在整个过程中扮演着非常重要的角色，主要从事根据需求开发项目软件工作。法律保护计算机软件作为一种知识产品，其要获得法律保护，必须具备以下必要条件：（一）原创性。即软件应该是开发者独立设计、独立编制的编码组合。（二）可感知性。受保护的软件须固定在某种有形物体上，通过客观手段表达出来并为人们所知悉。（三）可再现性。即把软件转载在有形物体上的可能性。著作权归属根据《计算机软件保护条例》第10条的规定，计算机软件著作权归属软件开发者。因此，确定计算机著作权归属的一般原则是“谁开发谁享有著作权”。软件开发者指实际组织进行开发工作，提供工作条件完成软件开发，并对软件承担责任的法人或者非法人单位，以及依靠自己具有的条件完成软件开发，并对软件承担责任的公民。载体软件的载体可以是硬盘、光盘、U盘、软盘等数据存储设备。使用许可不同的软件一般都有对应的软件授权，软件的使用者必须在同意所使用软件的许可证的情况下才能够合法的使用软件。依据许可方式的不同，大致可将软件区分为几类：专属软件、自由软件、共享软件、免费软件、公共软件。生命周期播报编辑软件生命周期是指从软件定义、开发、使用、维护到报废为止的整个过程，一般包括问题定义、可行性分析、需求分析、总体设计、详细设计、编码、测试和维护。问题定义就是确定开发任务到底“要解决的问题是什么”，系统分析员通过对用户的访问调查，最后得出一份双方都满意的关于问题性质、工程目标和规模的书面报告。可行性分析就是分析上一个阶段所确定的问题到底“可行吗”，系统分析员对系统要进行更进一步的分析，更准确、更具体地确定工程规模与目标，论证在经济上和技术上是否可行，从而在理解工作范围和代价的基础上，做出软件计划。需求分析即使对用户要求进行具体分析，明确“目标系统要做什么”，把用户对软件系统的全部要求以需求说明书的形式表达出来。总体设计就是把软件的功能转化为所需要的体系结构，也就是决定系统的模块结构，并给出模块的相互调用关系、模块间传达的数据及每个模块的功能说明。详细设计就是决定模块内部的算法与数据结构，也是明确“怎么样具体实现这个系统”。编码就是选取适合的程序设计语言对每个模板进行编码，并进行模块调试。测试就是通过各种类型的测试使软件达到预定的要求。维护就是软件交付给用户使用后，对软件不断查错、纠错和修改，使系统持久地满足用户的需求。软件的生命周期也可以分为3个大的阶段，分别是计划阶段、开发阶段和维护阶段。软件过程模型软件生命周期模型也称为软件过程模型，反映软件生存周期各个阶段的工作如何组织、衔接，常用的有瀑布模型、原型模型、螺旋模型、增量模型、喷泉模型，还有建造-修补模型、MSF过程模型、快速原型模型。 [1]常见的模型瀑布模型有时也称为V模型，它是一种线型顺序模型，是项目自始至终按照一定顺序的步骤从需求分析进展到系统测试直到提交用户使用，它提供了一种结构化的、自顶向下的软件开发方法，每阶段主要工作成果从一个阶段传递到下一个阶段，必须经过严格的评审或测试，以判定是否可以开始下一阶段工作，各阶段相互独立、不重叠。瀑布模型是所有软件生命周期模型的基础。 [1]原型+瀑布模型原型模型本身是一个迭代的模型，是为了解决在产品开发的早期阶段存在的不确定性、二义性和不完整性等问题，通过建立原型使开发者进一步确定其应开发的产品，使开发者的想象更具体化，也更易于被客户所理解。原型只是真实系统的一部分或一个模型，完全可能不完成任何有用的事情，通常包括抛弃型和进化型两种，抛弃型指原型建立、分析之后要扔掉，整个系统重新分析和设计；进化型则是对需求的定义较清楚的情形，原型建立之后要保留，作为系逐渐增加的基础，采用进化型一定要重视软件设计的系统性和完整性，并且在质量要求方面没有捷径，因此，对于描述相同的功能，建立进化型原型比建立抛弃型原型所花的时间要多。原型建立确认需求之后采用瀑布模型的方式完成项目开发。 [1]增量模型与建造大厦相同，软件也是一步一步建造起来的。在增量模型中，软件被作为一系列的增量构件来设计、实现、集成和测试，每一个构件是由多种相互作用的模块所形成的提供特定功能的代码片段构成。增量模型在各个阶段并不交付一个可运行的完整产品，而是交付满足客户需求的一个子集的可运行产品。整个产品被分解成若干个构件，开发人员逐个构件地交付产品，这样做的好处是软件开发可以较好地适应变化，客户可以不断地看到所开发的软件，从而降低开发风险。一些大型系统往往需要很多年才能完成或者客户急于实现系统，各子系统往往采用增量开发的模式，先实现核心的产品，即实现基本的需求，但很多补充的特性(其中一些是已知的，另外一些是未知的)在下一期发布。增量模型强调每一个增量均发布一个可操作产品，每个增量构建仍然遵循设计-编码-测试的瀑布模型。 [1]迭代模型早在20世纪50年代末期，软件领域中就出现了迭代模型。最早的迭代过程可能被描述为“分段模型”。迭代，包括产生产品发布（稳定、可执行的产品版本）的全部开发活动和要使用该发布必需的所有其他外围元素。所以，在某种程度上，开发迭代是一次完整地经过所有工作流程的过程：（至少包括）需求工作流程、分析设计工作流程、实施工作流程和测试工作流程。实质上，它类似小型的瀑布式项目。所有的阶段（需求及其它）都可以细分为迭代。每一次的迭代都会产生一个可以发布的产品，这个产品是最终产品的一个子集。 [1]开发语言播报编辑O语言O语言是一款中文计算机语言（或称套装：O汇编语言、O中间语言、O高级语言）Java语言作为跨平台的语言，可以运行在Windows和Unix/Linux下面，长期成为用户的首选。自JDK6.0以来，整体性能得到了极大的提高，市场使用率超过20%。可能已经达到了其鼎盛时期了，不知道后面能维持多长时间。易语言（E语言）易语言是一个自主开发，适合国情，不同层次不同专业的人员易学易用的汉语编程语言。易语言降低了广大电脑用户编程的门槛，尤其是根本不懂英文或者英文了解很少的用户，可以通过使用本语言极其快速地进入Windows程序编写的大门。 [2]C/C++语言以上2个作为传统的语言，一直在效率第一的领域发挥着极大的影响力。像Java这类的语言，其核心都是用C/C++写的。在高并发和实时处理，工控等领域更是首选。习语言习语言即中文版的C语言Basic美国计算机科学家约翰·凯梅尼和托马斯·库尔茨于1959年研制的一种“初学者通用符号指令代码”，简称BASIC。由于BASIC语言易学易用，它很快就成为流行的计算机语言之一。PHP同样是跨平台的脚本语言，在网站编程上成为了大家的首选，支持PHP的主机非常便宜，PHP+Linux+MySQL+Apache的组合简单有效。Perl脚本语言的先驱，其优秀的文本处理能力，特别是正则表达式，成为了以后许多基于网站开发语言(比如PHP,Java,C#)的这方面的基础。PythonPython是一种面向对象的解释性的计算机程序设计语言，也是一种功能强大而完善的通用型语言，已经具有十多年的发展历史，成熟且稳定。Python 具有脚本语言中最丰富和强大的类库，足以支持绝大多数日常应用。这种语言具有非常简捷而清晰的语法特点，适合完成各种高层任务，几乎可以在所有的操作系统中运行。基于这种语言的相关技术正在飞速的发展，用户数量急剧扩大，相关的资源非常多。C#C#是微软公司发布的一种面向对象的、运行于NET Framework之上的高级程序设计语言，并定于在微软职业开发者论坛(PDC)上登台亮相。C#是微软公司研究员Anders Hejlsberg的最新成果。C#看起来与Java有着惊人的相似；它包括了诸如单一继承、界面，与Java几乎同样的语法，和编译成中间代码再运行的过程。但是C#与Java有着明显的不同，它借鉴了Delphi的一个特点，与COM(组件对象模型)是直接集成的，而且它是微软公司.NET Windows网络框架的主角。JavaScriptJavaScript是一种由Netscape的LiveScript发展而来的脚本语言，主要目的是为了解决服务器终端语言，比如Perl，遗留的速度问题。当时服务端需要对数据进行验证，由于网络速度相当缓慢，只有28.8kbps，验证步骤浪费的时间太多。于是Netscape的浏览器Navigator加入了Javascript，提供了数据验证的基本功能。Ruby一种为简单快捷面向对象编程（面向对象程序设计）而创的脚本语言，由日本人松本行弘（まつもとゆきひろ，英译：Yukihiro Matsumoto，外号matz）开发，遵守GPL协议和Ruby License。Ruby的作者认为Ruby > (Smalltalk + Perl) / 2，表示Ruby是一个语法像Smalltalk一样完全面向对象、脚本执行、又有Perl强大的文字处理功能的编程语言。Fortran在科学计算软件领域，Fortran曾经是最主要的编程语言。比较有代表性的有Fortran 77、Watcom Fortran、NDP Fortran等。Objective C这是一种运行在苹果公司的Mac OS X，iOS操作系统上的语言。这两种操作系统的上层图形环境，应用程序编程框架都是使用该语言实现的。随著iPhone,iPad的流行，这种语言也开始在全世界流行。PascalPascal是一种计算机通用的高级程序设计语言。Pascal的取名是为了纪念十七世纪法国著名哲学家和数学家Blaise Pascal。它由瑞士Niklaus Wirth教授于六十年代末设计并创立。Pascal语言语法严谨，层次分明，程序易写，具有很强的可读性，是第一个结构化的编程语言。SwiftSwift，苹果于2014年WWDC（苹果开发者大会）发布的新开发语言，可与Objective-C共同运行于Mac OS和iOS平台，用于搭建基于苹果平台的应用程序。统计数据播报编辑《中华人民共和国2021年国民经济和社会发展统计公报》显示：2021年，全年软件和信息技术服务业完成软件业务收入94994亿元，按可比口径计算，比上年增长17.7%。 [3]产业技术问题播报编辑2022年6月27日，在第二十四届中国科协年会闭幕式上，中国科协隆重发布10个对产业发展具有引领作用的产业技术问题，其中包括“ 如何发展自主可控的工业设计软件 ”。 [4]

奇偶校验：
基本介绍播报编辑工作方式奇偶校验是在通信过程中确保节点之间准确数据传输的过程。奇偶校验位附加到原始数据位以创建偶数或奇数位。内存中最小的单位是比特，也称为“位”，位只有两种状态分别以1和0来标示，每8个连续的比特叫做一个字节（byte）。不带奇偶校验的内存每个字节只有8位，如果其某一位存储了错误的值，就会导致其存储的相应数据发生变化，进而导致应用程序发生错误。而奇偶校验就是在每一字节（8位）之外又增加了一位作为错误检测位。在某字节中存储数据之后，在其8个位上存储的数据是固定的，因为位只能有两种状态1或0，假设存储的数据用位标示为1、1、1、0、0、1、0、1，那么把每个位相加（1+1+1+0+0+1+0+1=5），结果是奇数。对于偶校验，校验位就定义为1；对于奇校验，则相反。当CPU读取存储的数据时，它会再次把前8位中存储的数据相加，计算结果是否与校验位相一致。从而一定程度上能检测出内存错误，奇偶校验只能检测出错误而无法对其进行修正，同时虽然双位同时发生错误的概率相当低，但奇偶校验却无法检测出双位错误。优缺点奇偶校验有两种类型：奇校验和偶校验。奇偶校验位是一个表示给定位数的二进制数中1的个数是奇数或者偶数的二进制数，奇偶校验位是最简单的错误检测码。如果传输过程中包括校验位在内的奇数个数据位发生改变，那么奇偶校验位将出错表示传输过程有错误发生。因此，奇偶校验位是一种错误检测码，但是由于没有办法确定哪一位出错，所以它不能进行错误校正。发生错误时必须扔掉全部的数据，然后从头开始传输数据。在噪声很多的媒介上成功传输数据可能要花费很长的时间，甚至根本无法实现。但是奇偶校验位也有它的优点，它是使用一位数据能够达到的最好的校验码，并且它仅仅需要一些异或门就能够生成。奇偶校验被广泛应用。 [1]监督码播报编辑奇偶监督码是一种增加二进制传输系统最小距离的简单和广泛采用的方法。例如，单个的奇偶监督将使码的最小距离由一增加到二。一个二进码字，如果它的码元有奇数个1，就称为具有奇性。例如，码字“1011010111”有七个1，因此，这个码字具有奇性。同样，偶性码字具有偶数个1。注意奇性检测等效于所有码元的模二加，并能够由所有码元的异或运算来确定。对于一个n位字，奇性由式(8-1)给出：奇性=a0⊕a1⊕a2⊕…⊕an(8-1)很明显，用同样的方式，我们也能够根据每一个码字的零的个数来构成奇偶监督。单个的奇偶监督码可描述为：给每一个码字加一个监督位，用它来构成奇性或偶性监督。例如，在图8-2中，对于二进码就是这样做的。可以看出，附加码元d2，是简单地选来使每个字成为偶性的。因此，若有一个码元是错的，就可以分辨得出，因为奇偶监督将成为奇性。在一个典型系统里，在传输以前，由奇偶发生器把奇偶监督位加到每个字中。原有信息中的数字在接收机中被检测，如果没有出现正确的奇、偶性，这个信息标定为错误的，这个系统将把错误的字抛掉或者请求重发。注意，用单个的奇偶监督码仅能检出奇数个码元的错误。例如考虑图8-4里的奇性监督码。把奇、偶监督位加到一个8-4-2-1BCD码，使之能够进行奇监督（将所有监督位反过来将产生偶监督码）。可以看到，如果将任何码字里的奇数个码元反过来，那么将成为偶性码，因而，无效的字是可以分辨出来的。然而，如果有两个或四个码元反过来，那末奇偶监督将仍然是奇性码，并且这个字被认为是正确的。只当一个给定的字里同时出现两个错误的概率被忽略不计时，单个的奇偶监督才是有效的，实际上，奇监督码比偶监督码可取，因为它排除了传输全0的情况。单向校验播报编辑概述单向奇偶校验(Row Parity)由于一次只采用单个校验位，因此又称为单个位奇偶校验(Single Bit Parity)。发送器在数据祯每个字符的信号位后添一个奇偶校验位，接收器对该奇偶校验位进行检查。典型的例子是面向ASCII码的数据信号祯的传输，由于ASCII码是七位码，因此用第八个位码作为奇偶校验位。单向奇偶校验又分为奇校验(Odd Parity)和偶校验(Even Parity)，发送器通过校验位对所传输信号值的校验方法如下：奇校验保证所传输每个字符的8个位中1的总数为奇数；偶校验则保证每个字符的8个位中1的总数为偶数。显然，如果被传输字符的7个信号位中同时有奇数个(例如1、3、5、7)位出现错误，均可以被检测出来；但如果同时有偶数个(例如2、4、6)位出现错误，单向奇偶校验是检查不出来的。一般在同步传输方式中常采用奇校验，而在异步传输方式中常采用偶校验。校验方法奇校验：就是让原有数据序列中（包括你要加上的一位）1的个数为奇数1000110（0）你必须添0这样原来有3个1已经是奇数了所以你添上0之后1的个数还是奇数个。偶校验：就是让原有数据序列中（包括你要加上的一位）1的个数为偶数，偶校验实际上是循环冗余校验的一个特例，通过多项式x+ 1 得到1位CRC。1000110（1）你就必须加1了这样原来有3个1要想1的个数为偶数就只能添1了。 [1]双向校验播报编辑为了提高奇偶校验的检错能力，可采用双向奇偶校验(Row and Column Parity)，也可称为双向冗余校验(Vertical and Longitudinal Redundancy Checks)。双向奇偶校验，又称“方块校验”或“垂直水平”校验。例：1010101×1010111×1110100×0101110×1101001×0011010××××××××“×”表示 奇偶校验所采用的奇校验或偶校验的校验码。如此，对于每个数的关注就由以前的1×7次增加到了7×7次。因此，比单项校验的校验能力更强。简单的校验数据的正确性，在计算机里都是010101二进制表示，每个字节有八位二进制，最后一位为校验码，奇校验测算前七位里1的个数合的奇偶性，偶校验测算前七位里0的个数的奇偶性。当数据里其中一位变了，得到的奇偶性就变了，接收数据方就会要求发送方重新传数据。奇偶校验只可以简单判断数据的正确性，从原理上可看出当一位出错，可以准确判断，如同时两个1变成两个0就校验不出来了，只是两位或更多位及校验码在传输过程中出错的概率比较低，奇偶校验可以用的要求比较低的应用下。范例播报编辑串行数据在传输过程中，由于干扰可能引起信息的出错，例如，传输字符‘E’，其各位为：0100，0101=45HD7 D0由于干扰，可能使位变为1，（为什么不变0？）这种情况，我们称为出现了“误码”。我们把如何发现传输中的错误，叫“检错”。发现错误后，如何消除错误，叫“纠错”。最简单的检错方法是“奇偶校验”，即在传送字符的各位之外，再传送1位奇/偶校验位。奇校验：所有传送的数位（含字符的各数位和校验位）中，“1”的个数为奇数，如：1 0110，01010 0110，0101偶校验：所有传送的数位（含字符的各数位和校验位）中，“1”的个数为偶数，如：1 0100，01010 0100，0101如果传输过程中包括校验位在内的奇数个数据位发生改变，那么奇偶校验位出错将表示传输过程有错误发生。但是如果传输过程中包括校验位在内的偶数个数据位发生改变，将无法检出收到的数据是否有错误。只能让发送方重新发送。

输入设备：
基本概念播报编辑原理图“输出设备”的对称，向计算机输入数据和信息的设备。是计算机与用户或其他设备通信的桥梁。输入设备是用户和计算机系统之间进行信息交换的主要装置之一.键盘，鼠标，摄像头,扫描仪,光笔，手写输入板，游戏杆，语音输入装置等都属于输入设备输入设备（Input Device）是人或外部与计算机进行交互的一种装置，用于把原始数据和处理这些数据的程序输入到计算机中。把待输入信息转换成能为计算机处理的数据形式的设备。计算机输入的信息有数字、模拟量、文字符号、语声和图形图像等形式。对于这些信息形式，计算机往往无法直接处理，必须把它们转换成相应的数字编码后才能处理。输入信息的传输率变化也很大，它们与计算机的工作速率不相匹配。输入设备的一个作用是使这二方面协调起来，提高计算机工作效率。输入设备的种类很多，除文字及数字输入设备外，模拟信号的输入设备有数-模、模-数转换设备；图形、图像的输入设备有模式信息输入输出设备；脱机输入信息用的数据准备装置有数据准备设备等。现在的计算机能够接收各种各样的数据，既可以是数值型的数据，也可以是各种非数值型的数据，如图形、图像、声音等都可以通过不同类型的输入设备输入到计算机中，进行存储、处理和输出。计算机的输入设备按功能可分为下列几类：字符输入设备：键盘；光学阅读设备：光学标记阅读机，光学字符阅读机；图形输入设备：鼠标器、操纵杆、光笔；图像输入设备：摄像机、扫描仪、传真机；输入模型模拟输入设备：语言模数转换识别系统。输出设备：将计算机中的数据或信息输出给用户。输出设备（OutputDevice）是人与计算机交互的一种部件，用于数据的输出。它把各种计算结果数据或信息以数字、字符、图像、声音等形式表示出来。常见的有显示器、打印机、绘图仪、影像输出系统、语音输出系统、磁记录设备等。既是输入设备又是输出设备：存储设备有内存储器和外存储器，软盘、硬盘、光盘、U盘、移动硬盘等是外存储器，内存储器又分为RAM和ROM，RAM为随机存储器，ROM是只读存储器，内存条是RAM，ROM指的是主板上的存储BIOS的芯片。技术及原理播报编辑信息输入时要说明信息的具体内容、信息的形式和时间。信息输入按信息的来源（称目标系统）和处理系统之间连接的不同可分为间接连接、半直接连接和直接连接。间接连接把目标系统的信息记录在数据载体上，再通过输入设备输入处理系统。常用的载体有穿孔卡片、穿孔带、磁带、磁盘等。半直接连接利用处理系统能够处理的原始文件连接目标系统和处理系统。常用的原始文件有标记文件、磁墨水字符文件、印刷体光学字符文件和手写体光学字符文件等。直接连接通过键盘、光笔、记录设备、传感器等将信息直接输入处理系统。信息输入按照采集系统和处理系统之间控制的不同又可分为脱机输入和联机输入。脱机时输入，信息采集系统与处理系统之间通过二次数据载体相连接。联机输入时，信息采集系统将信息直接输入处理系统。信息输入还可按输入设备的智能程度分为非智能输入和智能输入。非智能输入的信息为数据，输入设备单纯地把数据转换成处理系统能够识别的代码输入。智能输入不仅能进行数据转换，还能进行运算或直接输入声音、图像、文字标记等信息。常用的输入方式有穿孔卡片、穿孔带、磁带、磁盘和字符阅读等多种方式。输入方式播报编辑穿孔卡片输入穿孔卡片输入穿孔卡片是在预定位置处穿孔的组合表示数据。最常用的卡片宽82.6毫米，长187.3毫米，厚1.78毫米。它有80个垂直的列，每列有12个穿孔位置。在一列上穿1、2或3个孔表示一个字符。每张卡片可记载一件事务、一笔帐目或一个记录。穿孔卡片在初期的电子数据处理系统中，曾被广泛地使用。卡片输入是脱机输入，先用键盘穿孔机将数据记录在卡片上，然后通过读卡机送入处理系统。读卡机把卡片上孔眼组合转换成二进制代码送入处理系统，典型的输入速率为每秒2000个字符。穿孔带输入穿孔卡片派生出的穿孔纸带穿孔带由纸、塑料或金属制成，在带上穿圆孔来记载数据。按带的宽度分为8单位带、7单位带和5单位带。在带的横向一排穿孔位置上孔眼的组合代表一个5～8位的字符代码。穿孔带输入是脱机输入，先用穿孔机将数据记录在穿孔带上，然后由穿孔带阅读机将数据送入处理系统。穿孔带也可在许多事务处理机（例如电传打字机、现金收入记录机等）上作为正常业务操作的直接副产品而得到。穿孔带阅读机把穿孔带上的孔眼组合转换成二进制代码送入处理系统。典型的输入速率为每秒1000个字符。磁带输入磁带利用表面磁层的磁化方向来记录信息。它具有密度高、经济性好、易于擦掉再用等优点，是电子数据处理系统中使用最多的输入输出载体。常用的磁带12.7毫米（1/2英寸）宽，每盘61米（2400英寸）长，盘径266.7毫米（10.5英寸）。在横向上记录一个字符，称为一帧，每帧含有7个或9个二进制位（比特）。磁带的存储密度为246帧/毫米，每秒可读写125万个字符。在电子数据处理系统中使用的磁带机能在几毫秒内启动或停止，并能保证磁带高速平稳地通过磁头（5088毫米/秒）。磁头上有一线圈，它将磁带表面记录的信息转换成电信号输入处理系统。小型电子数据处理系统常用盒式磁带机，盒式磁带使用方便、经济。典型读写速度为10～2000字符/秒。磁盘输入带孔磁盘用不导磁的圆盘作基体，表面上涂有磁性层来记载数据，安装在一根垂直的轴上，以2400～3600转/分的速度旋转。磁盘是一种最重要的数据载体。典型的磁盘组由1～12个磁盘组成，盘径为356、254、203毫米（14、10、8英寸）不等。读写磁头安装在梳状存取机构上，可在盘径方向水平移动。磁盘组的读写速率为每秒156000～885000个字符，存储容量为7.25～200兆个字符。小型电子数据处理系统广泛使用软磁盘。它的基体由塑料制成，盘径76毫米、127毫米或203毫米（3英寸、5英寸或8英寸），装在黑色的塑料封套中。磁盘旋转时，封套保持不动。软磁盘可以随时从驱动器上取下更换，使用方便，价格低廉。字符阅读它是直接联机输入，直接读出由打印机、打字机、现金收入记录机和印刷票据上的字符，将它转换成处理系统可读的代码。有光学字符识别（OCR）和磁性墨水字符识别（MICR）。光学字符识别器利用光学原理可以识别两种标准字体（OCR-A和OCR-B）。磁性墨水字符识别器利用检测磁性标记可以识别用专门的磁性墨水打印的字符，它不会因非磁性的脏物或铅笔记号等发生识别错误，比光学字符识别器更为可靠。字符阅读机还可读入条形码表示的信息。条形码是60年代开始为铁路所采用。条形码最常用在标志盒装食品或罐头的种类和价格上。它由一连串黑白相间的条子组成，一般被称为国际产品码。超级市场的收款员只要将商品贴有条形码的一面轻轻擦过条形码扫描器的“窗口”，有关商品的信息就会输入到与之相连的微型计算机中。计算机迅速算出总价格。条形码已广泛用于库存管理、医院病例、书籍定价、图书馆流通管理以及工厂装配线等方面。字符阅读机促进了人机通信的进展。人们所能认识的图形可以自动地转换成机器能够识别的图形。许多原始文件（数据清单，票据等）可直接输入计算机而无需再作介质转换，从而省掉了大量的穿孔按键等准备数据的劳动，大大提高了计算机的使用效率。现代汉字识别和手写体识别技术也取得了一定成功。CRT终端它有一个类似于打字机的输入键盘和一个阴极射线管的显示屏。信息直接从键盘输入，输入的数据首先在屏幕上显示，如果发现错误，可以立即删除或修改，然后送入处理系统。也可用光笔输入信息，光笔是形状像笔的光检测装置，由操作员掌握，用它指定屏幕上的某一点，即可对此点的信息进行修改、增添或删除。光笔在屏幕上移动可以画出相应的图形。CRT终端是使用最广泛的输入输出设备。汉字输入手写笔直接把汉字转换成处理系统能够识别的代码输入。汉字输入系统是中文信息处理系统的重要组成部分。图形输入把复杂图形如指纹、航测图、卫星遥感图片等通过摄像装置或传真机送入系统并加以识别，经过压缩、处理，转换成处理系统能够识别的代码。语音输入直接把声音、语言转换成处理系统能够识别的代码输入。模拟量输入利用模数转换器可把从传感器上收集到的连续变化的信息（模拟量）转换成处理系统能够接受的数字。把模拟量转化为数字分四步进行：取样、保持、量化、编码。输入设备类型播报编辑1.定点输入设备：（鼠标，游戏杆，触摸屏，光笔，数字转换器，数码相机，数字摄影机等）键盘输入设备键盘（Keyboard）是常用的输入设备，它是由一组开关矩阵组成，包括数字键、字母键、符号键、功能键及控制键等。每一个按键在计算机中都有它的惟一代码。当按下某个键时，键盘接口将该键的二进制代码送入计算机主机中，并将按键字符显示在显示器上。当快速大量输入字符，主机来不及处理时，先将这些字符的代码送往内存的键盘缓冲区，然后再从该缓冲区中取出进行分析处理。键盘接口电路多采用单片微处理器，由它控制整个键盘的工作，如上电时对键盘的自检、键盘扫描、按键代码的产生、发送及与主机的通讯等。鼠标器鼠标器（Mouse）是一种手持式屏幕坐标定位设备，它是适应菜单操作的软件和图形处理环境而出现的一种输入设备，特别是在现今流行的Windows图形操作系统环境下应用鼠标器方便快捷。常用的鼠标器有两种，一种是机械式的，另一种是光电式的。鼠标机械式鼠标器的底座上装有一个可以滚动的金属球，当鼠标器在桌面上移动时，金属球与桌面摩擦，发生转动。金属球与四个方向的电位器接触，可测量出上下左右四个方向的位移量，用以控制屏幕上光标的移动。光标和鼠标器的移动方向是一致的，而且移动的距离成比例。光电式鼠标器的底部装有两个平行放置的小光源。这种鼠标器在反射板上移动，光源发出的光经反射板反射后，由鼠标器接收，并转换为电移动信号送入计算机，使屏幕的光标随之移动。其他方面与机械式鼠标器一样。鼠标器上有两个键的，也有三个键的。最左边的键是拾取键，最右边的键为消除键，中间的键是菜单的选择键。由于鼠标器所配的软件系统不同，对上述三个键的定义有所不同。一般情况下，鼠标器左键可在屏幕上确定某一位置，该位置在字符输入状态下是当前输入字符的显示点；在图形状态下是绘图的参考点。在菜单选择中，左键（拾取键）可选择菜单项，也可以选择绘图工具和命令。当作出选择后系统会自动执行所选择的命令。鼠标器能够移动光标，选择各种操作和命令，并可方便地对图形进行编辑和修改，但却不能输入字符和数字。2.扫描输入设备（图像扫描仪，传真机，条形码阅读器，字符和标记识别设备等）手持扫描仪光学标记阅读机是一种用光电原理读取纸上标记的输入设备，常用的有条码读入器和计算机自动评卷记分的输入设备等。图形（图像）扫描仪是利用光电扫描将图形（图像）转换成像素数据输入到计算机中的输入设备。目前一些部门已开始把图像输入用于图像资料库的建设中。如人事档案中的照片输入，公安系统案件资料管理，数字化图书馆的建设，工程设计和管理部门的工程图管理系统，都使用了各种类型的图形（图像）扫描仪。现在人们正在研究使计算机具有人的“听觉”和“视觉”，即让计算机能听懂人说的话，看懂人写的字，从而能以人们接收信息的方式接收信息。为此，人们开辟了新的研究方向，其中包括模式识别、人工智能、信号与图像处理等，并在这些研究方向的基础上产生了语言识别、文字识别、自然语言解与机器视觉等研究方向。3.语音输入设备（麦克风，声卡和语音输入软件系统组成）

缓存：
简介播报编辑缓存的工作原理缓存是指可以进行高速数据交换的存储器，它先于内存与CPU交换数据，因此速率很快。L1 Cache（一级缓存）是CPU第一层高速缓存。内置的L1高速缓存的容量和结构对CPU的性能影响较大，不过高速缓冲存储器均由静态RAM组成，结构较复杂，在CPU管芯面积不能太大的情况下，L1级高速缓存的容量不可能做得太大。一般L1缓存的容量通常在32—256KB。L2　Cache（二级缓存）是CPU的第二层高速缓存，分内部和外部两种芯片。内部的芯片二级缓存运行速率与主频相同，而外部的二级缓存则只有主频的一半。L2高速缓存容量也会影响CPU的性能，原则是越大越好，普通台式机CPU的L2缓存一般为128KB到2MB或者更高，笔记本、服务器和工作站上用CPU的L2高速缓存最高可达1MB-3MB。由于高速缓存的速度越高价格也越贵，故有的计算机系统中设置了两级或多级高速缓存。紧靠CPU的一级高速缓存的速度最高，而容量最小，二级高速缓存的容量稍大，速度也稍低 [1]。缓存只是内存中少部分数据的复制品，所以CPU到缓存中寻找数据时，也会出现找不到的情况（因为这些数据没有从内存复制到缓存中去），这时CPU还是会到内存中去找数据，这样系统的速率就慢下来了，不过CPU会把这些数据复制到缓存中去，以便下一次不要再到内存中去取。随着时间的变化，被访问得最频繁的数据不是一成不变的，也就是说，刚才还不频繁的数据，此时已经需要被频繁的访问，刚才还是最频繁的数据，又不频繁了，所以说缓存中的数据要经常按照一定的算法来更换，这样才能保证缓存中的数据是被访问最频繁的。工作原理播报编辑缓存工作原理缓存的工作原理是当CPU要读取一个数据时，首先从CPU缓存中查找，找到就立即读取并送给CPU处理；没有找到，就从速率相对较慢的内存中读取并送给CPU处理，同时把这个数据所在的数据块调入缓存中，可以使得以后对整块数据的读取都从缓存中进行，不必再调用内存。正是这样的读取机制使CPU读取缓存的命中率非常高（大多数CPU可达90%左右），也就是说CPU下一次要读取的数据90%都在CPU缓存中，只有大约10%需要从内存读取。这大大节省了CPU直接读取内存的时间，也使CPU读取数据时基本无需等待。总的来说，CPU读取数据的顺序是先缓存后内存。RAM(Random-Access Memory)和ROM(Read-Only Memory)相对的，RAM是掉电以后，其中的信息就消失那一种，ROM在掉电以后信息也不会消失那一种。RAM又分两种，一种是静态RAM，SRAM(Static RAM)；一种是动态RAM，DRAM(Dynamic RAM)。前者的存储速率要比后者快得多，使用的内存一般都是动态RAM。为了增加系统的速率，把缓存扩大就行了，扩的越大，缓存的数据越多，系统就越快了，缓存通常都是静态RAM，速率是非常的快， 但是静态RAM集成度低（存储相同的数据，静态RAM的体积是动态RAM的6倍）， 价格高（同容量的静态RAM是动态RAM的四倍）， 由此可见，扩大静态RAM作为缓存是一个非常愚蠢的行为， 但是为了提高系统的性能和速率，必须要扩大缓存， 这样就有了一个折中的方法，不扩大原来的静态RAM缓存，而是增加一些高速动态RAM做为缓存， 这些高速动态RAM速率要比常规动态RAM快，但比原来的静态RAM缓存慢， 把原来的静态RAM缓存叫一级缓存，而把后来增加的动态RAM叫二级缓存。功能作用播报编辑硬盘的缓存主要起三种作用：预读取数据缓存当硬盘受到CPU指令控制开始读取数据时，硬盘上的控制芯片会控制磁头把正在读取的簇的下一个或者几个簇中的数据读到缓存中（由于硬盘上数据存储时是比较连续的，所以读取命中率较高），当需要读取下一个或者几个簇中的数据的时候，硬盘则不需要再次读取数据，直接把缓存中的数据传输到内存中就可以了，由于缓存的速率远远高于磁头读写的速率，所以能够达到明显改善性能的目的。写入缓存(16张)当硬盘接到写入数据的指令之后，并不会马上将数据写入到盘片上，而是先暂时存储在缓存里，然后发送一个“数据已写入”的信号给系统，这时系统就会认为数据已经写入，并继续执行下面的工作，而硬盘则在空闲（不进行读取或写入的时候）时再将缓存中的数据写入到盘片上。虽然对于写入数据的性能有一定提升，但也不可避免地带来了安全隐患——数据还在缓存里的时候突然掉电，那么这些数据就会丢失。对于这个问题，硬盘厂商们自然也有解决办法：掉电时，磁头会借助惯性将缓存中的数据写入零磁道以外的暂存区域，等到下次启动时再将这些数据写入目的地。临时存储有时候，某些数据是会经常需要访问的，像硬盘内部的缓存（暂存器的一种）会将读取比较频繁的一些数据存储在缓存中，再次读取时就可以直接从缓存中直接传输。缓存就像是一台计算机的内存一样，在硬盘读写数据时，负责数据的存储、寄放等功能。这样一来，不仅可以大大减少数据读写的时间以提高硬盘的使用效率。同时利用缓存还可以让硬盘减少频繁的读写，让硬盘更加安静，更加省电。更大的硬盘缓存，你将读取游戏时更快，拷贝文件时候更快，在系统启动中更为领先。硬盘缓存缓存容量的大小不同品牌、不同型号的产品各不相同，早期的硬盘缓存基本都很小，只有几百KB，已无法满足用户的需求。16MB和32MB缓存是现今主流硬盘所采用，而在服务器或特殊应用领域中还有缓存容量更大的产品，甚至达到了64MB、128MB等。大容量的缓存虽然可以在硬盘进行读写工作状态下，让更多的数据存储在缓存中，以提高硬盘的访问速率，但并不意味着缓存越大就越出众。缓存的应用存在一个算法的问题，即便缓存容量很大，而没有一个高效率的算法，那将导致应用中缓存数据的命中率偏低，无法有效发挥出大容量缓存的优势。算法是和缓存容量相辅相成，大容量的缓存需要更为有效率的算法，否则性能会大大折扣，从技术角度上说，高容量缓存的算法是直接影响到硬盘性能发挥的重要因素。更大容量缓存是未来硬盘发展的必然趋势。技术发展播报编辑缓存集群的配置最早先的CPU缓存是个整体的，而且容量很低，英特尔公司从Pentium时代开始把缓存进行了分类。当时集成在CPU内核中的缓存已不足以满足CPU的需求，而制造工艺上的限制又不能大幅度提高缓存的容量。因此出现了集成在与CPU同一块电路板上或主板上的缓存，此时就把 CPU内核集成的缓存称为一级缓存，而外部的称为二级缓存。一级缓存中还分数据缓存（Data Cache，D-Cache）和指令缓存（Instruction Cache，I-Cache）。二者分别用来存放数据和执行这些数据的指令，而且两者可以同时被CPU访问，减少了争用Cache所造成的冲突，提高了处理器效能。英特尔公司在推出Pentium 4处理器时，用新增的一种一级追踪缓存替代指令缓存，容量为12KμOps，表示能存储12K条微指令。随着CPU制造工艺的发展，二级缓存也能轻易的集成在CPU内核中，容量也在逐年提升。再用集成在CPU内部与否来定义一、二级缓存，已不确切。而且随着二级缓存被集成入CPU内核中，以往二级缓存与CPU大差距分频的情况也被改变，此时其以相同于主频的速率工作，可以为CPU提供更高的传输速率。二级缓存是CPU性能表现的关键之一，在CPU核心不变化的情况下，增加二级缓存容量能使性能大幅度提高。而同一核心的CPU高低端之分往往也是在二级缓存上有差异，由此可见二级缓存对于CPU的重要性。CPU在缓存中找到有用的数据被称为命中，当缓存中没有CPU所需的数据时（这时称为未命中），CPU才访问内存。从理论上讲，在一颗拥有二级缓存的CPU中，读取一级缓存的命中率为80%。也就是说CPU一级缓存中找到的有用数据占数据总量的80%，剩下的20%从二级缓存中读取。由于不能准确预测将要执行的数据，读取二级缓存的命中率也在80%左右（从二级缓存读到有用的数据占总数据的16%）。那么还有的数据就不得不从内存调用，但这已经是一个相当小的比例了。较高端的CPU中，还会带有三级缓存，它是为读取二级缓存后未命中的数据设计的—种缓存，在拥有三级缓存的CPU中，只有约5%的数据需要从内存中调用 [5]，这进一步提高了CPU的效率。为了保证CPU访问时有较高的命中率，缓存中的内容应该按一定的算法替换。一种较常用的算法是“最近最少使用算法”（LRU算法），它是将最近一段时间内最少被访问过的行淘汰出局。因此需要为每行设置一个计数器，LRU算法是把命中行的计数器清零，其他各行计数器加1。当需要替换时淘汰行计数器计数值最大的数据行出局。这是一种高效、科学的算法，其计数器清零过程可以把一些频繁调用后再不需要的数据淘汰出缓存，提高缓存的利用率。CPU产品中，一级缓存的容量基本在4KB到64KB之间，二级缓存的容量则分为128KB、256KB、512KB、1MB、2MB、4MB等。一级缓存容量各产品之间相差不大，而二级缓存容量则是提高CPU性能的关键。二级缓存容量的提升是由CPU制造工艺所决定的，容量增大必然导致CPU内部晶体管数的增加，要在有限的CPU面积上集成更大的缓存，对制造工艺的要求也就越高。主流的CPU二级缓存都在2MB左右，其中英特尔公司07年相继推出了台式机用的4MB、6MB二级缓存的高性能CPU，不过价格也是相对比较高的，对于对配置要求不是太高的朋友，一般的2MB二级缓存的双核CPU基本也可以满足日常上网需要了。2022年，新一代的奔腾处理器采用了与 12 代酷睿一样的 Intel 7 工艺，但没有大小核架构。参数方面，奔腾 G7400 为 2 核 4 线程，3.7GHz，6MB 三级缓存，46W TDP，支持 DDR4-3200 内存和 DDR5-4800 内存。核显为 UHD 710，16 EU 1.35GHz。 [4]主要意义播报编辑缓存的工作方式缓存工作的原则，就是“引用的局部性”，这可以分为时间局部性和空间局部性。空间局部性是指CPU在某一时刻需要某个数据，那么很可能下一步就需要其附近的数据；时间局部性是指当某个数据被访问过一次之后，过不了多久时间就会被再一次访问。对于应用程序而言，不管是指令流还是数据流都会出现引用的局部性现象。举个简单的例子，比如在播放DVD影片的时候，DVD数据由一系列字节组成，这个时候CPU会依次从头处理到尾地调用DVD数据，如果CPU这次读取DVD数据为1分30秒，那么下次读取的时候就会从1分31秒开始，因此这种情况下有序排列的数据都是依次被读入CPU进行处理。从数据上来看，对于Word一类的应用程序通常都有着较好的空间局部性。用户在使用中不会一次打开7、8个文档，不会在其中某一个文档中打上几个词就换另一个。大多数用户都是打开一两个文档，然后就是长时间对它们进行处理而不会做其他事情。这样在内存中的数据都会集中在一个区域中，也就可以被CPU集中处理。从程序代码上来考虑，设计者通常也会尽量避免出现程序的跳跃和分支，让CPU可以不中断地处理大块连续数据。游戏、模拟和多媒体处理程序通常都是这方面的代表，以小段代码连续处理大块数据。不过在办公运用程序中，情况就不一样了。改动字体，改变格式，保存文档，都需要程序代码不同部分起作用，而用到的指令通常都不会在一个连续的区域中。于是CPU就不得不在内存中不断跳来跳去寻找需要的代码。这也就意味着对于办公程序而言，需要较大的缓存来读入大多数经常使用的代码，把它们放在一个连续的区域中。如果缓存不够，就需要内存中的数据，而如果缓存足够大的话，所有的代码都可以放入，也就可以获得最高的效率。同理，高端的数据应用以及游戏应用则需要更高容量的缓存。CPU缓存播报编辑CPU缓存CPU缓存（Cache Memory）是位于CPU与内存之间的临时存储器，它的容量比内存小的多但是交换速率却比内存要快得多。缓存的出现主要是为了解决CPU运算速率与内存读写速率不匹配的矛盾，因为CPU运算速率要比内存读写速率快很多，这样会使CPU花费很长时间等待数据到来或把数据写入内存。在缓存中的数据是内存中的一小部分，但这一小部分是短时间内CPU即将访问的，当CPU调用大量数据时，就可避开内存直接从缓存中调用，从而加快读取速率。由此可见，在CPU中加入缓存是一种高效的解决方案，这样整个内存储器（缓存+内存）就变成了既有缓存的高速率，又有内存的大容量的存储系统了。缓存对CPU的性能影响很大，主要是因为CPU的数据交换顺序和CPU与缓存间的带宽引起的。缓存基本上都是采用SRAM存储器，SRAM是英文Static RAM的缩写，它是一种具有静态存取功能的存储器，不需要刷新电路即能保存它内部存储的数据。不像DRAM内存那样需要刷新电路，每隔一段时间，固定要对DRAM刷新充电一次，否则内部的数据即会消失，因此SRAM具有较高的性能，但是SRAM也有它的缺点，即它的集成度较低，相同容量的DRAM内存可以设计为较小的体积，但是SRAM却需要很大的体积，这也是不能将缓存容量做得太大的重要原因。它的特点归纳如下：优点是节能、速率快、不必配合内存刷新电路、可提高整体的工作效率，缺点是集成度低、相同的容量体积较大、而且价格较高，只能少量用于关键性系统以提高效率。工作原理1、读取顺序CPU要读取一个数据时，首先从Cache中查找，如果找到就立即读取并送给CPU处理；如果没有找到，就用相对慢的速度从内存中读取并送给CPU处理，同时把这个数据所在的数据块调入Cache中，可以使得以后对整块数据的读取都从Cache中进行，不必再调用内存。正是这样的读取机制使CPU读取Cache的命中率非常高（大多数CPU可达90%左右），也就是说CPU下一次要读取的数据90%都在Cache中，只有大约10%需要从内存读取。这大大节省了CPU直接读取内存的时间，也使CPU读取数据时基本无需等待。总的来说，CPU读取数据的顺序是先Cache后内存。2、缓存分类Intel从Pentium开始将Cache分开，通常分为一级高速缓存L1和二级高速缓存L2。在以往的观念中，L1 Cache是集成在CPU中的，被称为片内Cache。在L1中还分数据Cache（D-Cache）和指令Cache（I-Cache）。它们分别用来存放数据和执行这些数据的指令，而且两个Cache可以同时被CPU访问，减少了争用Cache所造成的冲突，提高了处理器效能。3、读取命中率CPU在Cache中找到有用的数据被称为命中，当Cache中没有CPU所需的数据时（这时称为未命中），CPU才访问内存。从理论上讲，在一颗拥有2级Cache的CPU中，读取L1 Cache的命中率为80%。也就是说CPU从L1 Cache中找到的有用数据占数据总量的80%，剩下的20%从L2 Cache读取。由于不能准确预测将要执行的数据，读取L2的命中率也在80%左右（从L2读到有用的数据占总数据的16%）。那么还有的数据就不得不从内存调用，但这已经是一个相当小的比例了。在一些高端领域的CPU（像Intel的Itanium）中，我们常听到L3 Cache，它是为读取L2 Cache后未命中的数据设计的—种Cache，在拥有L3 Cache的CPU中，只有约5%的数据需要从内存中调用，这进一步提高了CPU的效率。一级缓存播报编辑一级缓存（Level 1 Cache）简称L1 Cache，位于CPU内核的旁边，是与CPU结合最为紧密的CPU缓存，也是历史上最早出现的CPU缓存。由于一级缓存的技术难度和制造成本最高，提高容量所带来的技术难度增加和成本增加非常大，所带来的性能提升却不明显，性价比很低，而且现有的一级缓存的命中率已经很高，所以一级缓存是所有缓存中容量最小的，比二级缓存要小得多。一级缓存可以分为一级数据缓存（Data Cache，D-Cache）和一级指令缓存（Instruction Cache，I-Cache）。二者分别用来存放数据以及对执行这些数据的指令进行即时解码，而且两者可以同时被CPU访问，减少了争用Cache所造成的冲突，提高了处理器效能。大多数CPU的一级数据缓存和一级指令缓存具有相同的容量，例如AMD的Athlon XP就具有64KB的一级数据缓存和64KB的一级指令缓存，其一级缓存就以64KB+64KB来表示，其余的CPU的一级缓存表示方法以此类推。Intel的采用NetBurst架构的CPU（最典型的就是Pentium 4）的一级缓存有点特殊，使用了新增加的一种一级追踪缓存（Execution Trace Cache，T-Cache或ETC）来替代一级指令缓存，容量为12KμOps，表示能存储12K条即12000条解码后的微指令。一级追踪缓存与一级指令缓存的运行机制是不相同的，一级指令缓存只是对指令作即时的解码而并不会储存这些指令，而一级追踪缓存同样会将一些指令作解码，这些指令称为微指令（micro-ops），而这些微指令能储存在一级追踪缓存之内，无需每一次都作出解码的程序，因此一级追踪缓存能有效地增加在高工作频率下对指令的解码能力，而μOps就是micro-ops，也就是微型操作的意思。它以很高的速率将μops提供给处理器核心。Intel NetBurst微型架构使用执行跟踪缓存，将解码器从执行循环中分离出来。这个跟踪缓存以很高的带宽将uops提供给核心，从本质上适于充分利用软件中的指令级并行机制。Intel并没有公布一级追踪缓存的实际容量,只知道一级追踪缓存能储存12000条微指令（micro-ops）。所以，不能简单地用微指令的数目来比较指令缓存的大小。实际上，单核心的NetBurst架构CPU使用8Kμops的缓存已经基本上够用了，多出的4kμops可以大大提高缓存命中率。而要使用超线程技术的话，12KμOps就会有些不够用，这就是为什么有时候Intel处理器在使用超线程技术时会导致性能下降的重要原因。例如Northwood核心的一级缓存为8KB+12KμOps，就表示其一级数据缓存为8KB，一级追踪缓存为12KμOps；而Prescott核心的一级缓存为16KB+12KμOps，就表示其一级数据缓存为16KB，一级追踪缓存为12KμOps。在这里12KμOps绝对不等于12KB，单位都不同，一个是μOps，一个是Byte（字节），而且二者的运行机制完全不同。所以那些把Intel的CPU一级缓存简单相加，例如把Northwood核心说成是20KB一级缓存，把Prescott核心说成是28KB一级缓存，并且据此认为Intel处理器的一级缓存容量远远低于AMD处理器128KB的一级缓存容量的看法是完全错误的，二者不具有可比性。在架构有一定区别的CPU对比中，很多缓存已经难以找到对应的东西，即使类似名称的缓存在设计思路和功能定义上也有区别了，此时不能用简单的算术加法来进行对比；而在架构极为近似的CPU对比中，分别对比各种功能缓存大小才有一定的意义。二级缓存播报编辑二级缓存结构剖析二级缓存（Level2cache），它是处理器内部的一些缓冲存储器，其作用跟内存一样。上溯到上个世纪80年代，由于处理器的运行速率越来越快，慢慢地，处理器需要从内存中读取数据的速率需求就越来越高了。然而内存的速率提升速率却很缓慢，而能高速读写数据的内存价格又非常高昂，不能大量采用。从性能价格比的角度出发，英特尔等处理器设计生产公司想到一个办法，就是用少量的高速内存和大量的低速内存结合使用，共同为处理器提供数据。这样就兼顾了性能和使用成本的最优。而那些高速的内存因为是处于cpu和内存之间的位置，又是临时存放数据的地方，所以就叫做缓冲存储器了，简称“缓存”。它的作用就像仓库中临时堆放货物的地方一样，货物从运输车辆上放下时临时堆放在缓存区中，然后再搬到内部存储区中长时间存放。货物在这段区域中存放的时间很短，就是一个临时货场。 最初缓存只有一级，后来处理器速率又提升了，一级缓存不够用了，于是就添加了二级缓存。二级缓存是比一级缓存速率更慢，容量更大的内存，主要就是做一级缓存和内存之间数据临时交换的地方用。为了适应速率更快的处理器p4ee，已经出现了三级缓存了，它的容量更大，速率相对二级缓存也要慢一些，但是比内存可快多了。 缓存的出现使得cpu处理器的运行效率得到了大幅度的提升，这个区域中存放的都是cpu频繁要使用的数据，所以缓存越大处理器效率就越高，同时由于缓存的物理结构比内存复杂很多，所以其成本也很高。大量使用二级缓存带来的结果是处理器运行效率的提升和成本价格的大幅度不等比提升。举个例子，服务器上用的至强处理器和普通的p4处理器其内核基本上是一样的，就是二级缓存不同。至强的二级缓存是2mb～16mb，p4的二级缓存是512kb，于是最便宜的至强也比最贵的p4贵，原因就在二级缓存不同。即l2cache。由于l1级高速缓存容量的限制，为了再次提高cpu的运算速率，在cpu外部放置一高速存储器，即二级缓存。工作主频比较灵活，可与cpu同频，也可不同。cpu在读取数据时，先在l1中寻找，再从l2寻找，然后是内存，在后是外存储器。所以l2对系统的影响也不容忽视。最早先的cpu缓存是个整体的，而且容量很低，英特尔公司从pentium时代开始把缓存进行了分类。当时集成在cpu内核中的缓存已不足以满足cpu的需求，而制造工艺上的限制又不能大幅度提高缓存的容量。因此出现了集成在与cpu同一块电路板上或主板上的缓存，此时就把cpu内核集成的缓存称为一级缓存，而外部的称为二级缓存。随着cpu制造工艺的发展，二级缓存也能轻易的集成在cpu内核中，容量也在逐年提升。再用集成在cpu内部与否来定义一、二级缓存，已不确切。而且随着二级缓存被集成入cpu内核中，以往二级缓存与cpu大差距分频的情况也被改变，此时其以相同于主频的速率工作，可以为cpu提供更高的传输速率。三级缓存播报编辑L3 Cache(三级缓存)，分为两种，早期的是外置，逐渐都变为内置的。而它的实际作用即是，L3缓存的应用可以进一步降低内存延迟，同时提升大数据量计算时处理器的性能。降低内存延迟和提升大数据量计算能力对游戏都很有帮助。而在服务器领域增加L3缓存在性能方面仍然有显著的提升。比方具有较大L3缓存的配置利用物理内存会更有效，故它比较慢的磁盘I/O子系统可以处理更多的数据请求。具有较大L3缓存的处理器提供更有效的文件系统缓存行为及较短消息和处理器队列长度。其实最早的L3缓存被应用在AMD发布的K6-III处理器上，当时的L3缓存受限于制造工艺，并没有被集成进芯片内部，而是集成在主板上。在只能够和系统总线频率同步的L3缓存同主内存其实差不了多少。后来使用L3缓存的是英特尔为服务器市场所推出的Itanium处理器。接着就是P4EE和至强MP。Intel还打算推出一款9MB L3缓存的Itanium2处理器，和以后24MB L3缓存的双核心Itanium2处理器。但基本上L3缓存对处理器的性能提高显得不是很重要，比方配备1MB L3缓存的Xeon MP处理器却仍然不是Opteron的对手，由此可见前端总线的增加，要比缓存增加带来更有效的性能提升。超级缓存播报编辑SuperCache，也就是超级缓存，计算机的速度瓶颈主要在于机械硬盘的读写速度，SuperCache就是给硬盘的读写用高速内存来做缓存，是大内存机器的提速首选，服务器的必备利器。工作原理：对于SuperCache而言，硬盘上没有文件的概念，只是用户指定大小的一个一个小格子，例如32k，硬盘上某个小格子里面的内容被读取了，则被缓存在内存里面，下次还读这个小格子的时候，直接从内存读取，硬盘没有任何动作，从而达到了加速的目的。有两种缓存模式，1、MFU模式，每个小格子被读取的时候，做一个简单的计数，当缓存满的时候，计数值小的先被清出缓存；2、MRU模式，简单的队列，先进先出。系统缓存播报编辑缓存设计将CPU比作一个城里的家具厂，而将存储系统比作郊区的木料厂，那么实际情况就是木料厂离家具厂越来越远，即使使用更大的卡车来运送木料，家具厂也得停工来等待木料送来。在这样的情况下，一种解决方法是在市区建立一个小型仓库，在里面放置一些家具厂最常用到的木料。这个仓库实际上就是家具厂的“Cache”，家具厂就可以从仓库不停的及时运送需要的木料。当然，仓库越大，存放的木料越多，效果就越好，因为这样即使是些不常用的东西也可以在仓库里找到。需要的木料仓库里没有，就要从城外的木料厂里继续找，而家具厂就得等着了。仓库就相对于L1缓存，可以由CPU及时快速的读写，所以存储的是CPU最常用代码和数据（后面会介绍一下如何挑选“最常用”）。L1缓存的速率比系统内存快的多是因为使用的是SRAM，这种内存单晶元使用四到六个晶体管。这也使得SRAM的造价相当的高，所以不能拿来用在整个存储系统上。在大多数CPU上，L1缓存和核心一起在一块芯片上。在家具厂的例子中，就好比工厂和仓库在同一条街上。这样的设计使CPU可以从最近最快的地方得到数据，但是也使得“城外的木料厂”到“仓库”和到“家具厂”的距离差不多远。这样CPU需要的数据不在L1缓存中，也就是“Cache Miss”，从存储设备取数据就要很长时间了。处理器速率越快，两者之间的差距就越大。使用Pentium4那样的高频率处理器，从内存中取得数据就相当于“木料厂”位于另一个国家。其实，缓存是CPU的一部分，它存在于CPU中 CPU存取数据的速率非常的快，一秒钟能够存取、处理十亿条指令和数据（术语：CPU主频1G），而内存就慢很多，快的内存能够达到几十兆就不错了，可见两者的速率差异是多么的大 缓存是为了解决CPU速率和内存速率的速率差异问题 内存中被CPU访问最频繁的数据和指令被复制入CPU中的缓存，这样CPU就可以不经常到象“蜗牛”一样慢的内存中去取数据了，CPU只要到缓存中去取就行了，而缓存的速率要比内存快很多。这里要特别指出的是： 1  因为缓存只是内存中少部分数据的复制品，所以CPU到缓存中寻找数据时，也会出现找不到的情况（因为这些数据没有从内存复制到缓存中去），这时CPU还是会到内存中去找数据，这样系统的速率就慢下来了，不过CPU会把这些数据复制到缓存中去，以便下一次不要再到内存中去取。 2  因为随着时间的变化，被访问得最频繁的数据不是一成不变的，也就是说，刚才还不频繁的数据，此时已经需要被频繁的访问，刚才还是最频繁的数据，后来又不频繁了，所以说缓存中的数据要经常按照一定的算法来更换，这样才能保证缓存中的数据是被访问最频繁的。 3 关于一级缓存和二级缓存为了分清这两个概念，我们先了解一下RAM ram和ROM相对的，RAM是掉电以后，其中信息才消失的那一种，ROM是在掉电以后信息也不会消失的那一种。RAM又分两种： 一种是静态RAM、SRAM；一种是动态RAM、DRAM。磁盘缓存播报编辑磁盘缓存磁盘缓存分为读缓存和写缓存。读缓存是指，操作系统为已读取的文件数据，在内存较空闲的情况下留在内存空间中（这个内存空间被称之为“内存池”），当下次软件或用户再次读取同一文件时就不必重新从磁盘上读取，从而提高速率。写缓存实际上就是将要写入磁盘的数据先保存于系统为写缓存分配的内存空间中，当保存到内存池中的数据达到一个程度时，便将数据保存到硬盘中。这样可以减少实际的磁盘操作，有效的保护磁盘免于重复的读写操作而导致的损坏，也能减少写入所需的时间。根据写入方式的不同，有写通式和回写式两种。写通式在读硬盘数据时，系统先检查请求指令，看看所要的数据是否在缓存中，在的话就由缓存送出响应的数据，这个过程称为命中。这样系统就不必访问硬盘中的数据，由于SDRAM的速率比磁介质快很多，因此也就加快了数据传输的速率。回写式就是在写入硬盘数据时也在缓存中找，找到就由缓存就数据写入盘中，多数硬盘都是采用的回写式缓存，这样就大大提高了性能。缓存英文名为 Cache。CPU 缓存也是内存的一种，其数据交换速率快且运算频率高。磁盘缓存则是操作系统为磁盘输入输出而在普通物理内存中分配的一块内存区域。硬盘的缓冲区，硬盘的缓冲区是硬盘与外部总线交换数据的场所。硬盘的读数据的过程是将磁信号转化为电信号后，通过缓冲区一次次地填充与清空，再填充，再清空，一步步按照PCI总线的周期送出，可见，缓冲区的作用是相当重要的。它的作用也是提高性能，但是它与缓存的不同之处在于：一   它是容量固定的硬件，而不像缓存是可以由操作系统在内存中动态分配的。二   它对性能的影响大大超过磁盘缓存对性能的影响，因为没有缓冲区，就会要求每传一个字（通常是4字节）就需要读一次磁盘或写一次磁盘。缓存映射播报编辑高速缓存可以被分为直接映射缓存，组相联缓存和全相联缓存。直接映射缓存缓存的映射这种缓存中，每个组只有一行，E = 1，结构很简单，整个缓存就相当于关于组的一维数组。不命中时的行替换也很简单，就一个行嘛，哪不命中替换哪。为了适应容量小的情况，第n+1层存储器中的某个数据块，你只能被替换到上一层（也就是第n层）存储器中的某个位置的子集中。假设一个直接映射的高速缓存，（S，E，B，m) = ( 4,1,2,4 )，也就是说，地址是4位（16个），有四个组，每个组一行，每个块两个字节。由于有16个地址，表征16个字节，所以总共有8个块，但只有4个组，也就是4行。只能把多个块映射到相同的缓存组，比如0和4都映射到组1，1和5都映射到组2，等等。这下问题就来了，比如先读块0，此时块0的数据被cache到组0。然后我再读块4，因为块4也是被映射到组0的，组0又只有一行，那就只有把以前块0的数据覆盖了，要是之后我又读块0，就数据丢失了，只能到下级的存储器去找。实际的循环程序中，很容易引起这种情况，称其为抖动。这种情况的存在，自然大大影响了性能。所以，需要更好的映射方案。组相联缓存在组相联缓存里，E大于1，就是说一个组里面有多个cacheline。E等于多少，就叫有多少路，所以叫E路组相联。组相联的行匹配就要复杂一些了，因为要检查多个行的标记位和有效位。如果最终找到了，还好。当然，找不到会从下一级存储器中取出包含所需求数据的行来替换，但一个组里面这么多行，替换哪个行。如果有一个空行，自然就是替换空行，如果没有空行，那就引发了一些其他的替换策略了。除了刚才介绍过的随机策略，还有最不常使用策略，最近最少使用策略。这些策略本身是需要一定开销的，但要知道，不命中的开销是很大的，所以为了保证命中率，采取一些相对复杂的策略是值得的。全相联缓存所谓全相联，就是由一个包含所有缓存行的组组成的缓存（块可以放在高速缓存中的任意位置） [2]。由于只有一个组，所以组选择特别简单，此时地址就没有组索引了，只有标记和偏移，也就是t部分和b部分。其他的步骤，行匹配和数据选择，和组相联原理是一样的，只是规模大得多了。如果说上面关于这三种映射方法的描述非常抽象，为了能理解得更加透彻，把存储器比作一家大超市，超市里面的东西就是一个个字节或者数据。为了让好吃好玩受欢迎的东西能够容易被看到，超市可以将这些东西集中在一块放在一个专门的推荐柜台中，这个柜台就是缓存。如果仅仅是把这些货物放在柜台中即完事，那么这种就是完全关联的方式。可是如果想寻找自己想要的东西，还得在这些推荐货物中寻找，而且由于位置不定，甚至可能把整个推荐柜台寻找个遍，这样的效率无疑还是不高的。于是超市老总决定采用另一种方式，即将所有推荐货物分为许多类别，如“果酱饼干”，“巧克力饼干”，“核桃牛奶”等，柜台的每一层存放一种货物。这就是直接关联的访问原理。这样的好处是容易让顾客有的放矢，寻找更快捷，更有效。但这种方法还是有其缺点，那就是如果需要果酱饼干的顾客很多，需要巧克力饼干的顾客相对较少，显然对果酱饼干的需求量会远多于对巧克力饼干的需求量，可是放置两种饼干的空间是一样大的，于是可能出现这种情况：存放的果酱饼干的空间远不能满足市场需求的数量，而巧克力饼干的存放空间却被闲置。为了克服这个弊病，老板决定改进存货方法：还是将货物分类存放，不过分类方法有所变化，按“饼干”，“牛奶”，“果汁”等类别存货，也就是说，无论是什么饼干都能存入“ 饼干”所用空间中，这种方法显然提高了空间利用的充分性，让存储以及查找方法更有弹性。技术指标播报编辑CPU缓存CPU产品中，一级缓存的容量基本在4kb到64kb之间，二级缓存的容量则分为128kb、256kb、512kb、1mb、2mb等。一级缓存容量各产品之间相差不大，而二级缓存容量则是提高cpu性能的关键。二级缓存容量的提升是由cpu制造工艺所决定的，容量增大必然导致cpu内部晶体管数的增加，要在有限的cpu面积上集成更大的缓存，对制造工艺的要求也就越高缓存(cache)大小是CPU的重要指标之一，其结构与大小对CPU速率的影响非常大。简单地讲，缓存就是用来存储一些常用或即将用到的数据或指令，当需要这些数据或指令的时候直接从缓存中读取，这样比到内存甚至硬盘中读取要快得多，能够大幅度提升cpu的处理速率。所谓处理器缓存，通常指的是二级高速缓存，或外部高速缓存。即高速缓冲存储器，是位于CPU和主存储器dram(dynamic ram)之间的规模较小的但速率很高的存储器，通常由sram（静态随机存储器）组成。用来存放那些被cpu频繁使用的数据，以便使cpu不必依赖于速率较慢的dram（动态随机存储器）。l2高速缓存一直都属于速率极快而价格也相当昂贵的一类内存，称为sram(静态ram)，sram(static ram)是静态存储器的英文缩写。由于sram采用了与制作cpu相同的半导体工艺，因此与动态存储器dram比较，sram的存取速率快，但体积较大，价格很高。处理器缓存的基本思想是用少量的sram作为cpu与dram存储系统之间的缓冲区，即cache系统。80486以及更高档微处理器的一个显著特点是处理器芯片内集成了sram作为cache，由于这些cache装在芯片内，因此称为片内cache。486芯片内cache的容量通常为8k。高档芯片如pentium为16kb，power pc可达32kb。pentium微处理器进一步改进片内cache，采用数据和双通道cache技术，相对而言，片内cache的容量不大，但是非常灵活、方便，极大地提高了微处理器的性能。片内cache也称为一级cache。由于486，586等高档处理器的时钟频率很高，一旦出现一级cache未命中的情况，性能将明显恶化。在这种情况下采用的办法是在处理器芯片之外再加cache，称为二级cache。二级cache实际上是cpu和主存之间的真正缓冲。由于系统板上的响应时间远低于cpu的速率，没有二级cache就不可能达到486，586等高档处理器的理想速率。二级cache的容量通常应比一级cache大一个数量级以上。在系统设置中，常要求用户确定二级cache是否安装及尺寸大小等。二级cache的大小一般为128kb、256kb或512kb。在486以上档次的微机中，普遍采用256kb或512kb同步cache。所谓同步是指cache和cpu采用了相同的时钟周期，以相同的速率同步工作。相对于异步cache，性能可提高30%以上。pc及其服务器系统的发展趋势之一是cpu主频越做越高，系统架构越做越先进，而主存dram的结构和存取时间改进较慢。因此，缓存（cache）技术愈显重要，在pc系统中cache越做越大。广大用户已把cache做为评价和选购pc系统的一个重要指标。光驱缓存播报编辑光存储驱动器都带有内部缓冲器或高速缓存存储器。这些缓冲器是实际的存储芯片，安装在驱动器的电路板上，它在发送数据给PC之前可能准备或存储更大的数据段。CD/DVD典型的缓冲器大小为128KB，不过具体的驱动器可大可小（通常越多越好）。可刻录CD或DVD驱动器一般具有2MB-4MB以上的大容量缓冲器，用于防止缓存欠载（buffer underrun）错误，同时可以使刻录工作平稳、恒定的写入。一般来说，驱动器越快，就有更多的缓冲存储器，以处理更高的传输速率。CD/DVD驱动器带有缓冲或高速缓存具有很多好处。缓冲可以保证PC以固定速率接收数据。当一个应用程序从驱动器请求数据时，数据可能位于分散在光盘上不同地方。因为驱动器的访问速率相对较慢，在数据读取时会使驱动器不得不间隔性向PC发送数据。驱动器的缓冲在软件的控制下可以预先读取并准备光盘的内容目录，从而加速第一次数据请求。光驱读取数据的规律是首先在缓存里寻找，如果在缓存中没有找到才会去光盘上寻找，大容量的缓存可以预先读取的数据越多，但在实际应用中CD-ROM、DVD-ROM等读取操作时，读取重复信息的机会是相对较少的，大部分的光盘更多的时候是一次读取数量较多的文件内容，因此在CD-ROM、DVD-ROM驱动器上缓存重要性得不到体现，因此大多此类产品采用较小的缓存容量。CD-ROM一般有128KB、256KB、512KB几种；而DVD一般有128KB、256KB、512KB，只有个别的外置式DVD光驱采用了较大容量的缓存。在刻录机或COMMBO产品上，缓存就变得十分重要了。在刻录光盘时，系统会把需要刻录的数据预先读取到缓存中，然后再从缓存读取数据进行刻录，缓存就是数据和刻录盘之间的桥梁。系统在传输数据到缓存的过程中，不可避免的会发生传输的停顿，如在刻录大量小容量文件时，硬盘读取的速率很可能会跟不上刻录的速率，就会造成缓存内的数据输入输出不成比例，如果这种状态持续一段时间，就会导致缓存内的数据被全部输出，而得不到输入，此时就会造成缓存欠载错误，这样就会导致刻录光盘失败。因此刻录机和COMMBO产品都会采用较大容量的缓存容量，再配合防刻死技术，就能把刻坏盘的几率降到最低。同时缓存还能协调数据传输速率，保证数据传输的稳定性和可靠性。刻录机产品一般有2MB、4MB、8MB，COMBO产品一般有2MB、4MB、8MB的缓存容量，受制造成本的限制，缓存不可能制作到足够大，但适量的缓存容量还是选择光储需要考虑的关键之一。网络缓存播报编辑概念WWW是互联网上最受欢迎的应用之一，其快速增长造成网络拥塞和服务器超载，导致客户访问延迟增大，WWW服务质量日益显现出来。缓存技术被认为是减轻服务器负载、降低网络拥塞、增强WWW可扩展性的有效途径之一，其基本思想是利用客户访问的时间局部性（Temproral Locality）原理，将客户访问过的内容在Cache中存放一个副本，当该内容下次被访问时，不必连接到驻留网站，而是由Cache中保留的副本提供。Web内容可以缓存在客户端、代理服务器以及服务器端。研究表明，缓存技术可以显著地提高WWW性能，它可以带来以下好处：（1）减少网络流量，从而减轻拥塞。（2）降低客户访问延迟，其主要原因有：①缓存在代理服务器中的内容，客户可以直接从代理获取而不是从远程服务器获取，从而减小了传输延迟；②没有被缓存的内容由于网络拥塞及服务器负载的减轻而可以较快地被客户获取。（3）由于客户的部分请求内容可以从代理处获取，从而减轻了远程服务器负载。（4）如果由于远程服务器故障或者网络故障造成远程服务器无法响应客户的请求，客户可以从代理中获取缓存的内容副本，使得WWW服务的鲁棒性得到了加强。Web缓存系统也会带来以下问题：（1）客户通过代理获取的可能是过时的内容。（2）如果发生缓存失效，客户的访问延迟由于额外的代理处理开销而增加。因此在设计Web缓存系统时，应力求做到Cache命中率最大化和失效代价最小化。（3）代理可能成为瓶颈。因此应为一个代理设定一个服务客户数量上限及一个服务效率下限，使得一个代理系统的效率至少同客户直接和远程服务器相连的效率一样。影响Internet访问速率访问网站的过程是通过建立在TCP/IP协议之上的HTTP协议来完成的。从客户端发出一个HTTP请求开始，用户所经历的等待时间主要决定于DNS和网站的响应时间。网站域名首先必须被DNS服务器解析为IP地址，HTTP的延时则由在客户端和服务器间的若干个往返时间所决定。往返时间是指客户端等待每次请求的响应时间，平均往返时间取决于三个方面：网站服务器的延时网站服务器造成的延时在往返时间中占主要比例。当某个服务器收到多个并发HTTP请求时，会产生排队延时。由于响应一个HTTP请求，往往需要多次访问本地硬盘，所以即使是一台负载并不大的服务器，也可能产生几十或几百微秒的延时。由路由器、网关、代理服务器和防火墙引入的延时通常在客户端和服务器之间的路径上会存在多个网络设备，如路由器、网关、代理和防火墙等。它们对经过的IP包都要做存储/转发的操作，于是会引入排队延时和处理延时。在网络拥塞时，这些设备甚至会丢包，此时会寄希望于客户端和服务器通过端到端的协议来恢复通信。不同通信链路上的数据传输速率在广域网中，从一个网络设备到另一个网络设备间的数据传输速率是决定往返时间的一个重要因素。但基本带宽的作用并不是像人们想象的那么重要，一项测试表明，当网站采用T3速率接入Internet时，也仅有2%的网页或对象能以64kbps的速率提供给客户端，这显然表明，带宽在网络性能上不是最关键的因素。Internet在向世界的每一个角落延伸，用户向一个服务器发出的 请求可能会经过8000公里到1.6万公里的距离，光速带来的延时和网络设备的延时是网络如此缓慢的最根本原因。网络缓存解决根本问题既然影响网络速率的原因是由距离和光速引起，那么加速Web访问的唯一途径就是缩短客户端与网站之间的距离。通过将用户频繁访问的页面和对象存放在离用户更近的地方，才能减少光速引入的延时，同时由于减少了路由中的环节，也相应地减少了路由器、防火墙和代理等引入的延时。传统的解决办法是建立镜像服务器来达到缩短距离的目的。但这个办法存在很大的不足，对于某个站点而言，不可能在离每个用户群较近的地方都建立镜像站点，若对大多数网站都用这样的办法就更不经济，同时管理和维护镜像站点是一项非常困难的工作。网络缓存是一种降低Internet流量和提高终端用户响应时间的新兴网络技术。它的观念来自于计算机和网络的其他领域，如流行的Intel架构的CPU中就存在缓存，用于提高内存存取的速率；各种操作系统在进行磁盘存取时也会利用缓存来提高速率；分布式文件系统通常也通过缓存来提高客户机和服务器之间的速率。类型静态页面的缓存可能有2种形式：其实主要区别就是CMS是否自己负责关联内容的缓存更新管理。1 静态缓存：是在新内容发布的同时就立刻生成相应内容的静态页面，比如：2003年3月22日，管理员通过后台内容管理界面录入一篇文章后，并同步更新相关索引页上的链接。2 动态缓存：是在新内容发布以后，并不预先生成相应的静态页面，直到对相应内容发出请求时，如果前台缓存服务器找不到相应缓存，就向后台内容管理服务器发出请求，后台系统会生成相应内容的静态页面，用户第一次访问页面时可能会慢一点，但是以后就是直接访问缓存了。静态缓存的缺点：网络缓存系统结构图复杂的触发更新机制：这两种机制在内容管理系统比较简单的时候都是非常适用的。但对于一个关系比较网络缓存系统结构图复杂的网站来说，页面之间的逻辑引用关系就成为一个非常非常复杂的问题。最典型的例子就是一条新闻要同时在新闻首页和相关的3个新闻专题中出现，在静态缓存模式中，每发一篇新文章，除了这篇新闻内容本身的页面外，还需要系统通过触发器生成多个新的相关静态页面，这些相关逻辑的触发也往往就会成为内容管理系统中最复杂的部分之一。旧内容的批量更新： 通过静态缓存发布的内容，对于以前生成的静态页面的内容很难修改，这样用户访问旧页面时，新的模板根本无法生效。在动态缓存模式中，每个动态页面只需要关心，而相关的其他页面能自动更新，从而大大减少了设计相关页面更新触发器的需要。网络缓存可以在客户端，也可以在网络上，由此我们将缓存分为两类：浏览器缓存和代理缓存。几乎所有的浏览器都有一个内置的缓存，它们通常利用客户端本地的内存和硬盘来完成缓存工作，同时允许用户对缓存的内容大小作控制。浏览器缓存是网络缓存的一个极端的情况，因为缓存设在客户机本地。通常一个客户端只有一个用户或几个共享计算机用户，浏览器缓存要求的硬盘空间通常在5MB到50MB的范围内。但是浏览器缓存在用户之间难以共享，不同客户端的缓存无法实现交流，因而缓存的内容与效果相当有限。代理缓存则是一种独立的应用层网络服务,它更像E－mail、Web、DNS等服务。许多用户不仅可以共享缓存，而且可以同时访问缓存中的内容。企业级代理缓存一般需要配置高端的处理器和存储系统，采用专用的软件，要求的硬盘空间在5MB到50GB左右，内存为64MB到512MB。代理处于客户端与网站服务器之间,在某些情况下，这种连接是不允许的，如网站在防火墙内,这时客户端必须与代理建立TCP连接，然后由代理建立与网站服务器的TCP连接。代理在服务器和客户端之间起到了数据接力的作用。代理发出的HTTP请求与一般的HTTP请求有细小的不同，主要在于它包含了完整的URL，而不只是URL的路径。代理缓存的工作原理当代理缓存收到客户端的请求时，它首先检查所请求的内容是否已经被缓存。如果没有找到，缓存必须以客户端的名义转发请求，并在收到服务器发出的文件时，将它以一定的形式保存在本地硬盘，并将其发送给客户端。如果客户端请求的内容已被缓存，还存在两种可能：其一，缓存的内容已经过时，即缓存中保存的内容超过了预先设定的时限，或网站服务器的网页已经更新，这时缓存会要求原服务器验证缓存中的内容，要么更新内容，要么返回“未修改”的消息；其二，缓存的内容是新的，即与原网站的内容保持同步，此时称为缓存命中，这时缓存会立即将已保存的内容送给客户端。在客户端的请求没有命中时，反而增加了缓存存储和转发的处理时间。在这种情况下，代理缓存是否仍有意义呢？实际上，代理缓存能够同时与网站服务器建立多个并发的TCP/IP连接，并行获取网站上的内容。缓存的存在从整体上降低了对网站访问的次数，也就降低了单位时间内服务器端的排队数目，因而这时并发连接的排队延时要小得多。优秀的缓存甚至能实现对网页内相关链接内容的预取以加快连接的速率。代理缓存的策略当原服务器的文件修改或被删除后，缓存又如何知道它保存的拷贝已经作废呢？HTTP协议为缓存服务提供了基本的支持，它使缓存能向原服务器查询，某个文件是否更改,如果缓存的拷贝过时则进行有条件下载。仅当原服务器文件超过指定的日期时，才会发出新的文件。但是这些询问操作对网络服务器造成的负载几乎和获取该文件差不多,因此不可能在客户端向缓存发起请求时都执行这样的操作。HTTP协议使得服务器可以有选择地为每个文档指定生存时间,即清楚地指出某个文件的有效生命周期，生存时间很短即意味着“不要对其缓存”。拷贝的保留时间可以是固定的，也可以是通过这个文件的大小、来源、生存时间或内容计算出来的。 [3]分布缓存播报编辑分布式缓存系统是为了解决数据库服务器和web服务器之间的瓶颈。如果一个网站的流量很大，这个瓶颈将会非常明显，每次数据库查询耗费的时间将会非常可观。对于更新速度不是很快的网站，我们可以用静态化来避免过多的数据库查询。对于更新速度以秒计的网站，静态化也不会太理想，可以用缓存系统来构建。如果只是单台服务器用作缓存，问题不会太复杂，如果有多台服务器用作缓存，就要考虑缓存服务器的负载均衡。使用Memcached分布式缓存服务来达到保存用户的会话数据，而达到各个功能模块都能够跨省份、跨服务器共享本次会话中的私有数据的目的。每个省份使用一台服务器来做为Memcached服务器来存储用话的会话中的数据，当然也可以多台服务器，但必须确保每个省份的做Memcached服务器数量必须一致，这样才能够保证Memcached客户端操作的是同一份数据，保证数据的一致性。会话数据的添加、删除、修改Memcached客户端，添加、删除和、修改会话信息数据时，不仅要添加、删除、修改本省的Memcached服务器数据，而且同时要对其它省份的Memcahed服务器做同样的操作，这样用户访问其它省份的服务器的功能模块进也能读取到相同的会话数据。Memcached客户端服务器的列表使用局域网的内网IP（如：192.168.1.179）操作本省的Memcahed服务器，使用公网的IP（（如：202.183.62.210））操作其它省份的Memcahe服务器。会话数据的读取系统所有模块读取会话数据的Memcached客户端服务器列表都设为本省Memcached服务器地址的内网IP来向Memcahed服务器中读取会话数据。同一会话的确认使用Cookie来保持客户与服务端的联系。每一次会话开始就生成一个GUID作为SessionID，保存在客户端的Cookie中，作用域是顶级域名，这样二级、三级域名就可以共享到这个Cookie，系统中就使用这个SessionID来确认它是否是同一个会话。会话数据的唯一ID会话数据存储在Memcached服务器上的唯一键Key也就是会话数据数据的唯一ID定义为：SessionID_Name, SessionID就是保存在客户端Cookie中的SessionID,Name就是会话数据的名称，同一次会话中各个会话数据的Name必须是唯一的，否则新的会话数据将覆盖旧的会话数据。会话的失效时间会话的失效通过控制Cookie的有效时间来实现，会话的时间设为SessionID或Cookie中的有效时间，且每一次访问SessionID时都要重新设置一下Cookie的有效时间，这样就达到的会话的有效时间就是两次间访问Cookie中SessionID值的的最长时间，如果两次访问的间隔时间超过用效时间，保存在SessionID的Cookie将会失效，并生成新的SessionID存放在Cookie中, SessionID改变啦，会话就结束啦。Memcached服务器中会话数据的失效，每一次向Memcache服务器中添加会话数据时，都把有效时间设为一天也就是24小时，让Memcached服务使用它内部的机制去清除，不必在程序中特别做会话数据的删除操作。数据在Memcache服务器中有有效时间只是逻辑上的，就算是过了24 小时，如果分配给Memcached服务的内存还够用的话，数据还是保存在内存当中的，只是Memcache客户端读取不到而已。只有到了分配给Memcached服务的内存不够用时，它才会清理没用或者比较旧的数据，也就是懒性清除。增加缓存的方法播报编辑CPU的缓存CPU的缓存分二级：L1（一级缓存）和L2（二级缓存），当处理器要读取数据时，首先要在L1缓存中查找，其次才是L2缓存，最后才是系统内存。如果有一天你发觉自己的电脑慢了很多，进入到Windows桌面也要几分钟，这时候就要检查一下CPU的一、二级缓存有没有打开。在BIOS设置中的StandardCMOSSetup（标准CMOS设定）有两项是用来打开或关闭缓存的：CPUInternalCache设为Enable时开启CPU内部的一级缓冲区，若设置为Disabl则为关闭，这时系统性能将大大降低；ExternalCache选项是控制主板上二级缓冲区，如果主板上有二级缓存则应设成Enable。硬盘的缓存点击电脑桌面上的“开始”/“运行”，键入“Msconfig”启动“系统配置实用程序”，跟着选中“system．ini”标签下的“Vcache”项，就可以根据系统的实际情况来调节硬盘的缓存了。在该选项中一般会有三行内容：ChunkSize=1024、MaxFileCache=10240和MinFileCache=10240；其中第一行是缓冲区读写单元值，第二、三行是硬盘的最大和最小缓冲值，等号后的数值都是可以修改的，只要右键单击选中任一行就可以进行修改了。如果你的内存是128MB的话，上面这三行的取值就比较合理了，当然也可以自定。如果不知道该如何设置合适的缓冲值，请“Windows优化大师”帮忙吧，这个软件中有一个“磁盘缓存优化”项，用鼠标就可以方便地设置好缓存；又或者让“Windows优化大师”自动帮你进行优化设置。当硬盘的缓存值足够大时，硬盘就不用频繁地读写磁盘，一来可以延长硬盘的寿命，二来也可以提高数据的传输速度。另外，将硬盘的“文件系统缓存”设置为“网络服务器”，可以加快系统对硬盘的访问速度，因为文件系统缓存里存放了硬盘最近被访问过的文件名和路径，缓存越大所能储存的内容也就越多。如果点击“控制面板”/“系统”/“性能”/“文件系统”/“硬盘”，将“此计算机的主要用途”由“台式机”改为“网络服务器”，可以将原来10K左右的缓存增加至近50K左右。软驱和光驱的缓存一般来说，软驱读写数据的速度都比较慢，这是因为盘片的转速不能太高，但是，我们可以提高软驱的读写缓存，让软驱一次读写更多的数据。方法是：在桌面上的“开始”/“运行”框中键入“Regedit”运行注册表编辑器，依次进入HKEY－LOCAL－MACHINE\System\CurrentControlSet\Services\Class\FDC\0000，新建一个为ForeFifo的“DWORD值”，将其值设为“0”，这样就对软驱进行了软提速。很多人都知道右键单击桌面“我的电脑”图标，选“属性”/“性能”/“文件系统”/“CD－ROM”，将最佳的访问方式设为“四倍速或更高速”，将追加的高速缓存大小滑块拖到最大处，可以明显提高光驱的读盘速度。除了这种方式，我们还可以在注册表中设置缓冲值，方法是：进入到注册表，在HKEY－LOCAL－MACHINE\System\CurrentControlSet\Control\FileSystem\CDFS下，将CacheSize（缓存值的大小）和Prefetch（预读文件大小）两项进行手工调整，只要右键单击要选的项就可以进行修改了。 [3]

编译器：
定义播报编辑编译程序词组可以有两种认识。一、编译程序是一种动作，是根据编译原理技术，由高级程序设计语言编译器翻译成机器语言二进制代码行为。二、编译程序是动名词，特指生成编译器的软件程序。 [1]简介播报编辑编译程序编译程序compiler编译程序的实现算法较为复杂。这是因为它所翻译的语句与目标语言的指令不是一一对应关系,而是一多对应关系;同时也因为它要处理递归调用、动态存储分配、多种数据类型，以及语句间的紧密依赖关系。但是，由于高级程序设计语言书写的程序具有易读、易移植和表达能力强等特点，编译程序广泛地用于翻译规模较大、复杂性较高、且需要高效运行的高级语言书写的源程序。功能播报编辑编译程序编译程序的基本功能是把源程序（高级语言）翻译成目标程序。但是,作为一个具有实际应用价值的编译系统,除了基本功能之外，还应具备语法检查、调试措施、修改手段、覆盖处理、目标程序优化、不同语言合用以及人-机联系等重要功能。①语法检查:检查源程序是否合乎语法。如果不符合语法，编译程序要指出语法错误的部位、性质和有关信息。编译程序应使用户一次上机，能够尽可能多地查出错误。②调试措施：检查源程序是否合乎设计者的意图。为此，要求编译程序在编译出的目标程序中安置一些输出指令，以便在目标程序运行时能输出程序动态执行情况的信息，如变量值的更改、程序执行时所经历的线路等。这些信息有助于用户核实和验证源程序是否表达了算法要求。③修改手段：为用户提供简便的修改源程序的手段。编译程序通常要提供批量修改手段（用于修改数量较大或临时不易修改的错误）和现场修改手段（用于运行时修改数量较少、临时易改的错误）。④覆盖处理：主要是为处理程序长、数据量大的大型问题程序而设置的。基本思想是让一些程序段和数据公用某些存储区，其中只存放当前要用的程序或数据；其余暂时不用的程序和数据，先存放在磁盘等辅助存储器中，待需要时动态地调入。⑤目标程序优化：提高目标程序的质量,即占用的存储空间少,程序的运行时间短。依据优化目标的不同，编译程序可选择实现表达式优化、循环优化或程序全局优化。目标程序优化有的在源程序级上进行，有的在目标程序级上进行。⑥不同语言合用：其功能有助于用户利用多种程序设计语言编写应用程序或套用已有的不同语言书写的程序模块。最为常见的是高级语言和汇编语言的合用。这不但可以弥补高级语言难于表达某些非数值加工操作或直接控制、访问外围设备和硬件寄存器之不足，而且还有利于用汇编语言编写核心部分程序,以提高运行效率。⑦人-机联系：确定编译程序实现方案时达到精心设计的功能。目的是便于用户在编译和运行阶段及时了解内部工作情况，有效地监督、控制系统的运行。早期编译程序的实现方案，是把上述各项功能完全收纳在编译程序之中。然而，习惯做法是在操作系统的支持下，配置调试程序、编辑程序和连接装配程序，用以协助实现程序的调试、修改、覆盖处理，以及不同语言合用功能。但在设计编译程序时，仍须精心考虑如何与这些子系统衔接等问题。 [2]编译程序书籍特点播报编辑编译程序必须分析源程序，然后综合成目标程序。首先，检查源程序的正确性，并把它分解成若干基本成分；其次，再根据这些基本成分建立相应等价的目标程序部分。为了完成这些工作，编译程序要在分析阶段建立一些表格，改造源程序为中间语言形式，以便在分析和综合时易于引用和加工。数据结构分析和综合时所用的主要数据结构，包括符号表、常数表和中间语言程序。符号表由源程序中所用的标识符连同它们的属性组成，其中属性包括种类（如变量、数组、结构、函数、过程等）、类型（如整型、实型、字符串、复型、标号等），以及目标程序所需的其他信息。常数表由源程序中用的常数组成，其中包括常数的机内表示，以及分配给它们的目标程序地址。中间语言程序是将源程序翻译为目标程序前引入的一种中间形式的程序，其表示形式的选择取决于编译程序以后如何使用和加工它。常用的中间语言形式有波兰表示、三元组、四元组以及间接三元组等。分析部分源程序的分析是经过词法分析、语法分析和语义分析三个步骤实现的。词法分析由词法分析程序（又称为扫描程序）完成，其任务是识别单词（即标识符、常数、保留字，以及各种运算符、标点符号等）、造符号表和常数表，以及将源程序换码为编译程序易于分析和加工的内部形式。语法分析程序是编译程序的核心部分，其主要任务是根据语言的语法规则，检查源程序是否合乎语法。如不合乎语法，则输出语法出错信息；如合乎语法，则分解源程序的语法结构，构造中间语言形式的内部程序。语法分析的目的是掌握单词是怎样组成语句的，以及语句又是如何组成程序的。语义分析程序是进一步检查合法程序结构的语义正确性，其目的是保证标识符和常数的正确使用，把必要的信息收集和保存到符号表或中间语言程序中，并进行相应的语义处理。工作过程播报编辑编译程序也叫编译系统，是把用高级语言编写的面向过程的源程序翻译成目标程序的语言处理程序。编译程序把一个源程序翻译成目标程序的工作过程分为五个阶段：词法分析；语法分析；中间代码生成；代码优化；目标代码生成。主要是进行词法分析和语法分析，又称为源程序分析，分析过程中发现有语法错误，给出提示信息。（1） 词法分析词法分析的任务是对由字符组成的单词进行处理，从左至右逐个字符地对源程序进行扫描，产生一个个的单词符号，把作为字符串的源程序改造成为单词符号串的中间程序。执行词法分析的程序称为词法分析程序或扫描器。源程序中的单词符号经扫描器分析，一般产生二元式：单词种别；单词自身的值。单词种别通常用整数编码，如果一个种别只含一个单词符号，那么对这个单词符号，种别编码就完全代表它自身的值了。若一个种别含有许多个单词符号，那么，对于它的每个单词符号，除了给出种别编码以外，还应给出自身的值。词法分析器一般来说有两种方法构造：手工构造和自动生成。手工构造可使用状态图进行工作，自动生成使用确定的有限自动机来实现。（2） 语法分析编译程序的语法分析器以单词符号作为输入，分析单词符号串是否形成符合语法规则的语法单位，如表达式、赋值、循环等，最后看是否构成一个符合要求的程序，按该语言使用的语法规则分析检查每条语句是否有正确的逻辑结构，程序是最终的一个语法单位。编译程序的语法规则可用上下文无关文法来刻画。语法分析的方法分为两种：自上而下分析法和自下而上分析法。自上而下就是从文法的开始符号出发，向下推导，推出句子。而自下而上分析法采用的是移进归约法，基本思想是：用一个寄存符号的先进后出栈，把输入符号一个一个地移进栈里，当栈顶形成某个产生式的一个候选式时，即把栈顶的这一部分归约成该产生式的左邻符号。（3） 中间代码生成中间代码是源程序的一种内部表示，或称中间语言。中间代码的作用是可使编译程序的结构在逻辑上更为简单明确，特别是可使目标代码的优化比较容易实现。中间代码即为中间语言程序，中间语言的复杂性介于源程序语言和机器语言之间。中间语言有多种形式，常见的有逆波兰记号、四元式、三元式和树。（4） 代码优化代码优化是指对程序进行多种等价变换，使得从变换后的程序出发，能生成更有效的目标代码。所谓等价，是指不改变程序的运行结果。所谓有效，主要指目标代码运行时间较短，以及占用的存储空间较小。这种变换称为优化。有两类优化：一类是对语法分析后的中间代码进行优化，它不依赖于具体的计算机；另一类是在生成目标代码时进行的，它在很大程度上依赖于具体的计算机。对于前一类优化，根据它所涉及的程序范围可分为局部优化、循环优化和全局优化三个不同的级别。（5） 目标代码生成目标代码生成是编译的最后一个阶段。目标代码生成器把语法分析后或优化后的中间代码变换成目标代码。目标代码有三种形式：① 可以立即执行的机器语言代码，所有地址都重定位；② 待装配的机器语言模块，当需要执行时，由连接装入程序把它们和某些运行程序连接起来，转换成能执行的机器语言代码；③ 汇编语言代码，须经过汇编程序汇编后，成为可执行的机器语言代码。目标代码生成阶段应考虑直接影响到目标代码速度的三个问题：一是如何生成较短的目标代码；二是如何充分利用计算机中的寄存器，减少目标代码访问存储单元的次数；三是如何充分利用计算机指令系统的特点，以提高目标代码的质量。 [3]综合部分播报编辑综合阶段必须根据符号表和中间语言程序产生出目标程序，其主要工作包括代码优化、存储分配和代码生成。代码优化是通过重排和改变程序中的某些操作，以产生更加有效的目标程序。存储分配的任务是为程序和数据分配运行时的存储单元。代码生成的主要任务是产生与中间语言程序符等价的目标程序，顺序加工中间语言程序，并利用符号表和常数表中的信息生成一系列的汇编语言或机器语言指令。结构编译过程分为分析和综合两个部分，并进一步划分为词法分析、语法分析、语义分析、代码优化、存储分配和代码生成等六个相继的逻辑步骤。这六个步骤只表示编译程序各部分之间的逻辑联系，而不是时间关系。编译过程既可以按照这六个逻辑步骤顺序地执行，也可以按照平行互锁方式去执行。在确定编译程序的具体结构时，常常分若干遍实现。对于源程序或中间语言程序，从头到尾扫视一次并实现所规定的工作称作一遍。每一遍可以完成一个或相连几个逻辑步骤的工作。例如，可以把词法分析作为第一遍；语法分析和语义分析作为第二遍；代码优化和存储分配作为第三遍；代码生成作为第四遍。反之，为了适应较小的存储空间或提高目标程序质量，也可以把一个逻辑步骤的工作分为几遍去执行。例如，代码优化可划分为代码优化准备工作和实际代码优化两遍进行。一个编译程序是否分遍,以及如何分遍,根据具体情况而定。其判别标准可以是存储容量的大小、源语言的繁简、解题范围的宽窄，以及设计、编制人员的多少等。分遍的好处是各遍功能独立单纯、相互联系简单、逻辑结构清晰、优化准备工作充分。缺点是各遍之中不可避免地要有些重复的部分，而且遍和遍之间要有交接工作，因之增加了编译程序的长度和编译时间。一遍编译程序是一种极端情况，整个编译程序同时驻留在内存,彼此之间采用调用转接方式连接在一起(图2)。当语法分析程序需要新符号时，它就调用词法分析程序；当它识别出某一语法结构时，它就调用语义分析程序。语义分析程序对识别出的结构进行语义检查，并调用“存储分配”和“代码生成”程序生成相应的目标语言指令。随着程序设计语言在形式化、结构化、直观化和智能化等方面的发展，作为实现相应语言功能的编译程序，也正向自动程序设计的目标发展，以便提供理想的程序设计工具。参考书目陈火旺、钱家骅、孙永强编：《编译原理》，国防工业出版社，北京，1980。A.V.Aho, Principles of Compiler Design,Addison Wes-ley, Reading, Massachusetts, 1977.动态20世纪80年代以后，程序设计语言在形式化、结构化、直观化和智能化等方面有了长足的进步和发展，主要表现两个方面：①随着程序设计理论和方法的发展，相继推出了一系列新型程序设计语言，如结构化程序设计语言、并发程序设计语言、分布式程序设计语言、函数式程序设计语言、智能化程序设计语言、面向对象程序设计语言等；②基于语法、语义和语用方面的研究成果，从不同的角度和层次上深刻地揭示了程序设计语言的内在规律和外在表现形式。与此相应地，作为实现程序设计语言重要手段之一的编译程序，在体系结构、设计思想、实现技术和处理内容等方面均有不同程度的发展、变化和扩充。另外，编译程序已作为实现编程的重要软件工具，被纳入到软件支援环境的基本层软件工具之中。因此，规划编译程序实现方案时，应从所处的具体软件支援环境出发，既要遵循整个环境的全局性要求和规定，又要精心考虑与其他诸层软件 工具之间的相互支援、配合和衔接关系。

累加器：
简介播报编辑在中央处理器中，累加器(accumulator) 是一种寄存器，用来储存计算产生的中间结果。如果没有像累加器这样的寄存器，那么在每次计算 (加法，乘法，移位等等) 后就必须要把结果写回到内存，也许马上就得读回来。然而存取主存的速度是比从算术逻辑单元到有直接路径的累加器存取更慢。标准的例子就是把一列的数字加起来。一开始累加器设定为零，每个数字依序地被加到累加器中，当所有的数字都被加入后，结果才写回到主存中。现今的 CPU 通常有很多寄存器，所有或多数都可以被用来当作累加器。因为这个原因，"累加器" 这名词就显得有些老旧。这个名词已经几乎不在微处理器寄存器中使用，例如，运算寄存器的名称中的符号以 "A" 开头的表示是从 "accumulator" 这个历史因素得来的 (有时候认为并非 "arithmetic")。也可能混淆的是寄存器的名字前置 "A" 也表示 "address"，比如说像是Motorola 68000家族。早期的 4 位、8 位微处理器，典型具有单个累加器。8051微控制器有两个累加器：主累加器与从累加器，其中的从累加器只用于乘法（MUL AB）与除法（DIV AB）。乘法的 16 位结果放入两个 8 位累加器中。除法时，商放入主累加器，余数放入从累加器。8008的直接后继产品——8080与8086，开创了x86指令集体系结构，仍然使用两个累加器：主累加器 EAX 与从累加器 EDX 用于乘法与除法的大数运算。例如，MUL ECX 将把两个 32 位寄存器 ECX 与 EAX 相乘，64 位结果放入 EAX 与 EDX。但是 MUL 与 DIV 之外的其他算术——逻辑指令（ADD、SUB、CMP、AND、OR、XOR、TEST）可以使用 8 个寄存器：EAX、ECX、EDX、EBX、ESP、EBP、ESI、EDI 作为目的操作数（即存储结果的位置）。 [1]中央处理器播报编辑中央处理器（英语：CentralProcessingUnit，缩写：CPU），是计算机的主要设备之一，功能主要是解释计算机指令以及处理计算机软件中的数据。计算机的可编程性主要是指对中央处理器的编程。中央处理器、内部存储器和输入/输出设备是现代电脑的三大核心部件。1970年代以前，中央处理器由多个独立单元构成，后来发展出由集成电路制造的中央处理器，这些高度收缩的组件就是所谓的微处理器，其中分出的中央处理器最为复杂的电路可以做成单一微小功能强大的单元。中央处理器广义上指一系列可以执行复杂的计算机程序的逻辑机器。这个空泛的定义很容易地将在“CPU”这个名称被普遍使用之前的早期计算机也包括在内。无论如何，至少从1960年代早期开始(Weik 1961)，这个名称及其缩写已开始在电子计算机产业中得到广泛应用。尽管与早期相比，“中央处理器”在物理形态、设计制造和具体任务的执行上有了极大的发展，但是其基本的操作原理一直没有改变。早期的中央处理器通常是为大型及特定应用的计算机而定制。但是，这种昂贵的为特定应用定制CPU的方法很大程度上已经让位于开发便宜、标准化、适用于一个或多个目的的处理器类。这个标准化趋势始于由单个晶体管组成的大型机和微机年代，随着集成电路的出现而加速。IC使得更为复杂的中央处理器可以在很小的空间中设计和制造（在微米的数量级）。中央处理器的标准化和小型化都使得这一类数字设备和电子零件在现代生活中的出现频率远远超过有限应用专用的计算机。现代微处理器出现在包括从汽车到手机到儿童玩具在内的各种物品中。运算器：算术、逻辑（部件：算术逻辑单元、累加器、寄存器组、路径转换器、数据总线）；控制器：复位、使能（部件：计数器、指令寄存器、指令解码器、状态寄存器、时钟发生器、微操作信号发生器）。 [2]计算机存储器播报编辑计算机存储器（英语：Computer memory）是一种利用半导体技术制成的存储数据的电子设备。其电子电路中的数据以二进制方式存储，存储器的每一个存储单元称做记忆元。记忆体又称内存，是CPU能直接寻址的存储空间，由半导体器件制成。内存的特点是访问速率快。内存是电脑中的主要部件，它是相对于外存而言的。我们平常使用的程序，如Windows操作系统、打字软件、游戏软件等，一般都是安装在硬盘等外存上的，但仅此是不能使用其功能的，必须把它们调入内存中运行，才能真正使用其功能，我们平时输入一段文字，或玩一个游戏，其实都是在内存中进行的。就好比在一个书房里，存放书籍的书架和书柜相当于电脑的外存，而我们工作的办公桌就是内存。通常我们把要永久保存的、大量的数据存储在外存上，而把一些临时的或少量的数据和程序放在内存上，当然内存的好坏会直接影响电脑的运行速度。 [3]算术逻辑单元播报编辑算术逻辑单元（英语：Arithmetic Logic Unit, ALU）是中央处理器的执行单元，是所有中央处理器的核心组成部分，由及闸和或闸构成的算数逻辑单元，主要功能是进行二进制的算术运算，如加减乘(不包括整数除法)。基本上，在所有现代CPU体系结构中，二进制都以二补数的形式来表示。 [4]参见播报编辑虚拟内存存储器层次结构

同步总线：
定义播报编辑利用时钟信号采样数据的总线。出处播报编辑《计算机科学技术名词 》第三版。 [1]

控制单元：
定义播报编辑控制单元，英文Control Unit（CU），是CPU部件之一，有时也安装与CPU外部。其基本功能是从内存取指令、分析指令和执行指令。控制单元是实现一种或多种控制规律的控制仪表或控制部件。如：多路传输控制单元、代理设置控制单元。种类播报编辑无论哪一个种类的控制单元，原理均为通过控制单元发出的控制信号对CPU各个部分加以控制。控制单元大体可以分为以下两类。微程序式，由微程序读取和发出控制信号。通过被称为微型定序器的简单数字通路（微型电脑）对微程序加以执行。 [1]硬件型控制单元。由数字通路直接发出控制信号。由于集成电路的规模化及设计技术的进步，此种控制单元已成为可能。功能播报编辑它根据用户预先编好的程序，依次从存储器中取出各条指令，放在指令寄存器IR中，通过指令译码(分析)确定应该进行什么操作，然后通过操作控制器OC，按确定的时序，向相应的部件发出微操作控制信号。操作控制器OC中主要包括节拍脉冲发生器、控制矩阵、时钟脉冲发生器、复位电路和启停电路等控制逻辑。

微程序控制器：
定义播报编辑采用微程序控制方式的控制器称为微程序控制器。所谓微程序控制方式是指微命令不是由组合逻辑电路产生的，而是由微指令译码产生。一条机器指令往往分成几步执行，将每一步操作所需的若干位命令以代码形式编写在一条微指令中，若干条微指令组成一段微程序，对应一条机器指令。在设计CPU时，根据指令系统的需要，事先编制好各段微程序 ，且将它们存入一个专用存储器（称为控制存储器）中。微程序控制器由指令寄存器IR、程序计数器PC、程序状态字寄存器PSW、时序系统、控制存储器CM、微指令寄存器以及微地址形成电路、微地址寄存器等部件组成。执行指令时，从控制存储器中找到相应的微程序段，逐次取出微指令，送入微指令寄存器，译码后产生所需微命令，控制各步操作完成。基本概念播报编辑微命令和微操作微命令：控制部件通过控制线向执行部件发出的各种控制命令。它构成控制信号的最小单元 [1]。微操作：执行部件接受微命令后所进行的操作。它是由微命令实现的最基本操作 [1]。控制部件与执行部件通过控制线和反馈信息进行联系。微指令和微程序微指令，在机器的一个CPU周期中，一组实现一定操作功能的微命令的组合。微程序，实现一条机器指令功能的许多条微指令组成的序列。控制部件与执行部件通过控制线和反馈信息进行联系。CPU周期与微指令周期的关系在串行方式的微程序控制器中:微指令周期 = 读出微指令的时间 + 执行该条微指令的时间一个CPU周期为0.8μs，它包含四个等间隔的节拍脉冲T1—T4，每个脉冲宽度为200ns。用T4作为读取微指令的时间，用T1+T2+T3时间作为执行微指令的时间。例如，在前600ns时间内运算器进行运算，在600ns时间的末尾运算器已经运算完毕，可用T4上升沿将运算结果打入某个寄存器。与此同时可用T4间隔读取下条微指令，经200ns时间延迟，下条微指令又从只读存储器读出，并用T1上升沿打入到微指令寄存器。如忽略触发器的翻转延迟，那么下条微指令的微命令信号就从T1上升沿起就开始有效，直到下一条微指令读出后打入微指令寄存器为止。因此一条微指令的保持时间恰好是0.8μs，也就是一个CPU周期的时间。 [2]组成播报编辑微程序控制器主要由控制存储器、微指令寄存器和地址转移逻辑三大部分组成。控制存储器控制存储器用来存放实现全部指令系统的微程序，它是一种只读存储器。若指令系统中有多少条机器指令，就有多少微程序。一旦微程序固化，机器运行时则只读不写。其工作过程是：每读出一条微指令，则执行这条微指令；接着又读出下一条微指令，又执行这一条微指令……。读出一条微指令并执 行微指令的时间总和称为一个微指令周期。通常，在串行方式的微程序控制器中，微指令周期就是只读存储器的工作周期。控制存储器的字长就是微指令字的长度，其存储容量视机器指令系统而定，即取决于微程序的数量。对控制存储器的要求是速度快，读出周期要短。 [3]微指令寄存器微指令寄存器用来存放由控制存储器读出的一条微指令信息。其中微地址寄存器决定将要访问的下一条微指令的地址，而微命令寄存器则保存一条微指令的操作控制字段和判别测试字段的信息。 [3]地址转移逻辑在一般情况下，微指令由控制存储器读出后直接给出下一条微指令的地址，通常我们简称微地址，这个微地址信息就存放在微地址寄存器中。如果微程序不出现分支，那么下一条微指令的地址就直接由微地址寄存器给出。当微程序出现分支时，意味着微程序出现条件转移。在这种情况下，通过判别测试字段P和执行部件的“状态条件”反馈信息，去修改微地址寄存器的内容，并按改好的内容去读下一条微指令。地址转移逻辑就承担自动完成修改微地址的任务。 [3]控制原理播报编辑微程序控制的基本思想，就是仿照通常的解题程序的方法，把操作控制信号编成所谓的“微指令”，存放到一个只读存储器里．当机器运行时，一条又一条地读出这些微指令，从而产生全机所需要的各种操作控制信号，使相应部件执行所规定的操作 。微程序控制的基本原理是：（1）将机器指令分解为基本的微命令序列，在制造CPU时固化在控制存储器CM中，执行一条机器指令时，CPU依次从CM中取出微指令产生微命令。（2）一条微指令包含的微命令控制实现一步（一个节拍）操作，若干条微指令组成一小段微程序解释执行一条机器指令。 [1]执行过程播报编辑（1）根据计算机给出的第一条微指令的地址，从控制存储器中取出第一条微指令。（2）微指令由操作控制部分和顺序控制部分组成。操作控制部分产生微操作控制信号，控制执行部分完成规定的操作。顺序控制部分中的直接顺序控制部分放入微地址寄存器，顺序控制部分的P字段和执行部件反馈的状态条件信息决定修改微地址寄存器中的值。（3）按地址寄存器中的值从控制存储器中取出下一条微指令，继续第二步，如此循环，直到全部指令执行完毕。设计步骤播报编辑微程序控制器的设计步骤如下：（1）根据CPU的结构图描述出每条指令的微操作流程图并综合成总的流程图；（2）用混合控制法对微命令进行编码；（3）选择合适的控制和时序；（4）选用微程序的顺序控制方式为微指令安排微地址；（5）画出微程序控制器组成框图。 [4]组合逻辑控制器和微程序控制器的比较播报编辑组合逻辑控制器和微程序控制器，除了操作控制信号的形成方法和原理有差别外，其余的组成部分上没有本质的区别。最显著的差别可归纳为如下两点：实现方式微程序控制器的控制功能是在存放微程序的控制存储器和存放当前正在执行的微指令的寄存器直接控制下实现的，而组合逻辑控制器由逻辑电路实现。前者电路比较规整，各条指令控制信号的差别反映在控制存储器的内容上，因此无论是增加或修改（包括纠正设计中的错误或升级）指令，只要增加或者修改内容即可。组合逻辑控制器先用逻辑表达式列出，精简化后用逻辑门电路实现，因而显得零乱复杂，当需要增加或修改指令时很麻烦甚至不可能，因此微程序控制器得到了广泛应用，尤其是指令系统复杂的计算机，一般都采用微程序控制器。 [1]性能在同样的工艺条件下，微程序控制的速度比组合逻辑电路速度低，因为执行每条微指令都要从控制存储器（CM）中读取一次，影响了速度，而组合逻辑电路的速度主要取决于电路延迟，因而在高速或超高速计算机中，对影响速度的关键部分如CPU，往往采用组合逻辑电路。近年来，一些新的计算机系统如RISC（精简指令计算机），选用了组合逻辑控制器。 [1]

程序查询方式：
一旦某一外设被选中并启动后，主机将查询这个外设的某些状态位，看其是否准备就绪？若外设未准备就绪，主机将再次查询；若外设已准备就绪，则执行一次I/O操作。这种方式控制简单，但外设和主机不能同时工作，各外设之间也不能同时工作，系统效率很低，因此，仅适用于外设的数目不多，对I/O处理的实时要求不那么高，CPU的操作任务比较单一，并不很忙的情况。单个设备：多个设备工作流程如下程序查询方式

中断：
术语解释播报编辑指处理机处理程序运行中出现的紧急事件的整个过程.程序运行过程中，系统外部、系统内部或者现行程序本身若出现紧急事件，处理机立即中止现行程序的运行，自动转入相应的处理程序(中断服务程序)，待处理完后，再返回原来的程序运行，这整个过程称为程序中断;当处理机接受中断时，只需暂停一个或几个周期而不执行处理程序的中断，称为简单中断.中断又可分为屏蔽中断和非屏蔽中断两类.可由程序控制其屏蔽的中断称为屏蔽中断或可屏蔽中断.屏蔽时，处理机将不接受中断.反之，不能由程序控制其屏蔽，处理机一定要立即处理的中断称为非屏蔽中断或不可屏蔽中断.非屏蔽中断主要用于断电、电源故障等必须立即处理的情况.处理机响应中断时，不需执行查询程序.由被响应中断源向CPU发向量地址的中断称为向量中断，反之为非向量中断.向量中断可以提高中断响应速度。 [2]分类播报编辑硬件中断（Hardware Interrupt） [3]：可屏蔽中断（maskable interrupt）。硬件中断的一类，可通过在中断屏蔽寄存器中设定位掩码来关闭。非可屏蔽中断（non-maskable interrupt，NMI）。硬件中断的一类，无法通过在中断屏蔽寄存器中设定位掩码来关闭。典型例子是时钟中断（一个硬件时钟以恒定频率—如50Hz—发出的中断）。处理器间中断（interprocessor interrupt）。一种特殊的硬件中断。由处理器发出，被其它处理器接收。仅见于多处理器系统，以便于处理器间通信或同步。伪中断（spurious interrupt）。一类不希望被产生的硬件中断。发生的原因有很多种，如中断线路上电气信号异常，或是中断请求设备本身有问题。软件中断（Software Interrupt） [3]：软件中断。是一条CPU指令，用以自陷一个中断。由于软中断指令通常要运行一个切换CPU至内核态（Kernel Mode/Ring 0）的子例程，它常被用作实现系统调用（System call）。防止方法播报编辑要防止中断冲突，其实就是要知道什么设备容易产生中断冲突，只要知道了这点，在使用这些设备时稍微注意一下就可以了。下面我列出一些容易冲突的设备，希望对读者有用。1、声卡：一些早期的ISA型声卡，系统很有可能不认，就需要用户手动设置（一般为5）2、内置调制解调器和鼠标：一般鼠标用COM1，内置调制解调器使用COM2的中断（一般为3），这时要注意此时COM2上不应有其它设备3、网卡和鼠标：此问题一般发生在鼠标在COM1口，使用中断为3，这时要注意通常网卡的默认中断为3，两者极有可能发成冲突。4、打印机和EPP扫描仪：在安装扫描仪驱动程序时应将打印机打开，因为两个设备中串联，所以为了防止以后扫描仪驱动程序设置有误，一定要将打印机打开再安装扫描仪驱动程序。5、操作系统和BIOS：如果计算机使用了“即插即用”操作系统（例如win98），应将BIOS中PNP OS Installed设置为Yes这样可让操作系统重新设置中断。6、PS/2鼠标和BIOS：在使用PS/2鼠标时应将BIOS中PS/2 Mouse Function Control打开或设置为Auto，只有这样BIOS才能将IRQ12分配给PS/2鼠标用。功能播报编辑现代计算机中采用中断系统的主要目的是 [4]：①提高计算机系统效率。计算机系统中处理机的工作速度远高于外围设备的工作速度。通过中断可以协调它们之间的工作。当外围设备需要与处理机交换信息时，由外围设备向处理机发出中断请求，处理机及时响应并作相应处理。不交换信息时，处理机和外围设备处于各自独立的并行工作状态。②维持系统可靠正常工作。现代计算机中，程序员不能直接干预和操纵机器，必须通过中断系统向操作系统发出请求，由操作系统来实现人为干预。主存储器中往往有多道程序和各自的存储空间。在程序运行过程中，如出现越界访问，有可能引起程序混乱或相互破坏信息。为避免这类事件的发生，由存储管理部件进行监测，一旦发生越界访问，向处理机发出中断请求，处理机立即采取保护措施。③满足实时处理要求。在实时系统中，各种监测和控制装置随机地向处理机发出中断请求，处理机随时响应并进行处理。④提供故障现场处理手段。处理机中设有各种故障检测和错误诊断的部件，一旦发现故障或错误，立即发出中断请求，进行故障现场记录和隔离，为进一步处理提供必要的依据。中断优先权播报编辑在某一时刻有几个中断源同时发出中断请求时，处理器只响应其中优先权最高的中断源。当处理机正在运行某个中断服务程序期间出现另一个中断源的请求时，如果后者的优先权低于前者，处理机不予理睬，反之，处理机立即响应后者，进入所谓的“嵌套中断”。中断优先权的排序按其性质、重要性以及处理的方便性决定，由硬件的优先权仲裁逻辑或软件的顺序询问程序来实现 [4]。中断过程播报编辑按照事件发生的顺序，中断过程包括 [4]：①中断源发出中断请求;②判断当前处理机是否允许中断和该中断源是否被屏蔽;③优先权排队;④处理机执行完当前指令或当前指令无法执行完，则立即停止当前程序，保护断点地址和处理机当前状态，转入相应的中断服务程序;⑤执行中断服务程序;⑥恢复被保护的状态，执行“中断返回”指令回到被中断的程序或转入其他程序。上述过程中前四项操作是由硬件完成的，后两项是由软件完成的。向量中断播报编辑对应每个中断源设置一个向量。这些向量顺序存在主存储器的特定存储区。向量的内容是相应中断服务程序的起始地址和处理机状态字。在响应中断时，由中断系统硬件提供向量地址，处理机根据该地址取得向量，并转入相应的中断服务程序 [4]。

程序中断方式：
当主机启动外设后，无需等待查询，而是继续执行原来的程序，外设在做好输入输出准备时，向主机发出中断请求，主机接到请求后就暂时中止原来执行的程序，转去执行中断服务程序对外部请求进行处理，在中断处理完毕后返回原来的程序继续执行。显然，程序中断不仅适用于外部设备的输入输出操作，也适用于对外界发生的随机事件的处理。程序中断在信息交换方式中处理最重要的地位，它不仅允许主机和外设同时并行工作，并且允许一台主机管理多台外设，使它们同时工作。但是完成一次程序中断还需要许多辅助操作，当外设数目较多时，中断请求过分频繁，可能使CPU应接不暇；另外，对于一些高速外设，由于信息交换是成批的，如果处理不及时，可能会造成信息丢失，因此，它主要适用于中、低速外设。程序中断与调用子程序的区别子程序的执行是由程序员实现安排好的，而中断服务程序的执行则是由随机的中断事件引起的；子程序的执行受到主程序或上层子程序的控制，而中断服务程序一般与被中断的现行程序毫无关系；不存在同时调用多个子程序的情况，而有可能发生多个外设同时请求cpu为自己服务的情况。

DMA：
简介播报编辑通常会指定一个内存部分用于直接内存访问。在ISA总线标准中，高达16兆字节的内存可用于DMA。EISA和微通道架构标准允许访问全套内存地址（假设他们可以用32位寻址）。外围设备互连通过使用一个总线主控器来完成直接内存访问。直接内存访问的另一个选择是程控输入输出（PIO）接口。在程控输入输出接口中，设备之间所有的数据传输都要通过处理器。ATA/IDE接口的新协议是Ultra DMA，它提供的突发数据传输速率可达33兆字节每秒。具有Ultra DMA/33的硬盘驱动器也支持PIO模式1、3、4和多字DMA模式2（每秒16.6兆字节）。 [1]原理播报编辑外设与存储器之间以及存储器与存储器之间的数据传输，通常采用程序中断方式、程序查询方式和DMA控制方式。程序中断方式和程序查询方式都需要CPU发出输入/输出（In/Out，I/O）的指令，然后等待I/O设备完成操作之后返回，期间CPU需要等待I/O设备完成操作。DMA在传输存储器和I/O设备的数据时，无须CPU来控制数据的传输，直接通过DMA控制器（direct memory access controller，DMAC）完成外设与存储器之间以及存储器与存储器之间的数据高速传输。 [3]DMA传输原理一个完整的DMA传输包括DMA请求、DMA响应、DMA传输和DMA结束4个步骤。DMA传输原理如图1所示，图中I/O设备为源端设备，由I/O设备向目的端设备（存储器）传输数据，其DMA的基本传输过程如下：①CPU对总线控制器进行初始化，制定工作内存空间，读取DMAC中的寄存器信息，了解DMAC的传输状态[1]；②I/O设备向DMAC发送DMA请求（DMA request，DREQ），DMAC收到此信号后，向CPU发出总线保持信号（HOLD）； ③CPU当前总线周期执行结束后发出总线响应信号保持确认（hold acknowledgment，HLDA）； ④DMAC收到总线授权后，向I/O设备发送DMA响应信号DMA确认（DMA acknowledgment，DACK），表示允许I/O设备进行DMA传送；⑤开始传输时，DMAC首先从源地址读取数据并存入内部缓存中，再写入目的地址，完成总线数据从源地址到目的地址的传输[1]；⑥DMA传输完成后，DMAC向CPU发出结束信号，释放总线，使CPU重新获得总线控制权。一次DMA传输只需要执行一个DMA周期，相当于一个总线读/写周期，因而能够满足外设数据高速传输的需要。 [3]DMA是所有现代电脑的重要特色，它允许不同速度的硬件设备来沟通，而不需要依于中央处理器的大量中断负载。否则，中央处理器需要从来源把每一片段的数据复制到寄存器，然后把它们再次写回到新的地方。在这个时间中，中央处理器对于其他的工作来说就无法使用。DMA传输常使用在将一个内存区从一个设备复制到另外一个。当中央处理器初始化这个传输动作，传输动作本身是由DMA控制器来实行和完成。典型的例子就是移动一个外部内存的区块到芯片内部更快的内存去。像是这样的操作并没有让处理器工作拖延，使其可以被重新调度去处理其他的工作。DMA传输对于高性能嵌入式系统算法和网络是很重要的。 举个例子，个人电脑的ISADMA控制器拥有8个DMA通道，其中的7个通道是可以让计算机的中央处理器所利用。每一个DMA通道有一个16位地址寄存器和一个16位计数寄存器。要初始化数据传输时，设备驱动程序一起设置DMA通道的地址和计数寄存器，以及数据传输的方向，读取或写入。然后指示DMA硬件开始这个传输动作。当传输结束的时候，设备就会以中断的方式通知中央处理器。"分散-收集"（Scatter-gather）DMA允许在一次单一的DMA处理中传输数据到多个内存区域。相当于把多个简单的DMA要求串在一起。同样，这样做的目的是要减轻中央处理器的多次输出输入中断和数据复制任务。 DRQ意为DMA要求；DACK意为DMA确认。这些符号一般在有DMA功能的电脑系统硬件概要上可以看到。它们表示了介于中央处理器和DMA控制器之间的电子信号传输线路。 [1]缓存一致性问题播报编辑DMA会导致缓存一致性问题。想像中央处理器带有缓存与外部内存的情况，DMA的运作则是去访问外部内存，当中央处理器访问外部内存某个地址的时候，暂时先将新的值写入缓存中，但并未将外部内存的数据更新，若在缓存中的数据尚未更新到外部内存前发生了DMA，则DMA过程将会读取到未更新的数据。相同的，如果外部设备写入新的值到外部内存内，则中央处理器若访问缓存时则会访问到尚未更新的数据。这些问题可以用两种方法来解决：1.缓存同调系统（Cache-coherent system）：以硬件方法来完成，当外部设备写入内存时以一个信号来通知缓存控制器某内存地址的值已经过期或是应该更新数据。2.非同调系统（Non-coherent system）：以软件方法来完成，操作系统必须确认缓存读取时，DMA程序已经开始或是禁止DMA发生。第二种的方法会造成DMA的系统负担。 [2]DMA引擎播报编辑除了与硬件交互相关外，DMA也可为昂贵的内存耗费减负。比如大型的拷贝行为或scatter-gather操作，从中央处理器到专用的DMA引擎。Intel的高端服务器包含这种引擎，它被称为I/O加速技术（IOAT）。 [2]RDMA播报编辑在电脑运算领域，远程直接内存访问（英语：remote direct memory access，RDMA）是一种直接存储器访问技术，它将数据直接从一台计算机的内存传输到另一台计算机，无需双方操作系统的介入。这允许高通量、低延迟的网络通信，尤其适合在大规模并行计算机集群中使用。RDMA支持零复制网络传输，通过使网络适配器直接在应用程序内存间传输数据，不再需要在应用程序内存与操作系统缓冲区之间复制数据。这种传输不需要中央处理器、CPU缓存或上下文交换参与，并且传输可与其他系统操作并行。当应用程序执行RDMA读取或写入请求时，应用程序数据直接传输到网络，从而减少延迟并实现快速的消息传输。但是，这种策略也表现出目标节点不会收到请求完成的通知（单向通信）等相关的若干问题。 [2]

指令格式：
指令格式是描述计算机内部电路中运行高低电平的组合，这些组合用0和1在纸张上描述。不同的组合都有一定的涵义，这些高低电平的源头就是机器语言的指令格式的各个字段。指令格式包括操作码和地址码，操作数的地址，操作结果的存储地址和下一条指令的地址。

只读存储器：
基本结构播报编辑ROM基本结构图右图给出ROM的基本结构，ROM主要由地址译码器、存储体、读出线及读出放大器等部分组成。ROM是按地址寻址的存储器，由CPU给出要访问的存储单元地址ROM的地址译码器是与门的组合，输出是全部地址输入的最小项（全译码）。n位地址码经译码后2n种结果，驱动选择2n个字,即W=2n。存储体是由熔丝、二极管或晶体管等元件排成W*m的二维阵列（字位结构），共W个字，每个字m位。存储体实际上是或门的组合，ROM的输出线位数就是或门的个数。由于它工作时只是读出信息，因此可以不必设置写入电路，这使得其存储单元与读出线路也比较简单。 [2]工作过程播报编辑ROM的工作过程右图给出ROM的工作过程，CPU经地址总线送来要访问的存储单元地址，地址译码器根据输入地址码选择某条字线，然后由它驱动该字线的各位线，读出该字的各存储位元所存储的二进制代码，送入读出线输出，再经数据线送至CPU。 [1]特点播报编辑只读存储器的特点是只能读出而不能写入信息，通常在电脑主板的ROM里面固化一个基本输入/输出系统，称为BIOS（基本输入输出系统）。其主要作用是完成对系统的加电自检、系统中各功能模块的初始化、系统的基本输入/输出的驱动程序及引导操作系统。 [4]种类播报编辑ROM有多种类型，且每种只读存储器都有各自的特性和适用范围。从其制造工艺和功能上分，ROM有五种类型，即掩膜编程的只读存储器MROM（Mask-programmedROM）、可编程的只读存储器PROM（Programmable ROM）、可擦除可编程的只读存储器EPROM（Erasable Programmable ROM）、可电擦除可编程的只读存储器 EEPROM（Elecrically Erasable Programmable ROM）和快擦除读写存储器（Flash Memory）。 [2]掩膜编程的只读存储器CDROM掩膜只读存储器（Mask ROM）中存储的信息由生产厂家在掩膜工艺过程中“写入”。在制造过程中，将资料以一特制光罩（Mask）烧录于线路中，有时又称为“光罩式只读内存”（Mask ROM），此内存的制造成本较低，常用于电脑中的开机启动。其行线和列线的交点处都设置了MOS管，在制造时的最后一道掩膜工艺，按照规定的编码布局来控制MOS管是否与行线、列线相连。相连者定为1（或0），未连者为0（或1），这种存储器一旦由生产厂家制造完毕，用户就无法修改。 [1]MROM的主要优点是存储内容固定，掉电后信息仍然存在,可靠性高。缺点是信息一次写入（制造）后就不能修改，很不灵活且生产周期长，用户与生产厂家之间的依赖性大。 [2]可编程只读存储器PROM可编程只读存储器（Programmable ROM，PROM）允许用户通过专用的设备（编程器）一次性写入自己所需要的信息，其一般可编程一次，PROM存储器出厂时各个存储单元皆为1，或皆为0。用户使用时，再使用编程的方法使PROM存储所需要的数据。 [2]PROM的种类很多，需要用电和光照的方法来编写与存放的程序和信息。但仅仅只能编写一次，第一次写入的信息就被永久性地保存起来。例如，双极性PROM有两种结构：一种是熔丝烧断型，一种是PN结击穿型。它们只能进行一次性改写，一旦编程完毕，其内容便是永久性的。由于可靠性差，又是一次性编程，较少使用。PROM中的程序和数据是由用户利用专用设备自行写入，一经写入无法更改，永久保存。PROM具有一定的灵活性，适合小批量生产，常用于工业控制机或电器中。 [1]可编程可擦除只读存储器EPROM可编程可擦除只读存储器（Erasable Programmable Read Only Memory，EPROM）可多次编程，是一种以读为主的可写可读的存储器。是一种便于用户根据需要来写入，并能把已写入的内容擦去后再改写的ROM。其存储的信息可以由用户自行加电编写，也可以利用紫外线光源或脉冲电流等方法先将原存的信息擦除，然后用写入器重新写入新的信息。 EPROM比MROM和PROM更方便、灵活、经济实惠。但是EPROM采用MOS管，速度较慢。 [2]擦除远存储内容的方法可以采用以下方法：电的方法（称电可改写ROM）或用紫外线照射的方法（称光可改写ROM）。光可改写ROM可利用高电压将资料编程写入，抹除时将线路曝光于紫外线下，则资料可被清空，并且可重复使用，通常在封装外壳上会预留一个石英透明窗以方便曝光。 [2]电可擦除可编程只读存储器电可擦可编程序只读存储器（Electrically Erasable Programmable Read-Only Memory，EEPROM）是一种随时可写入而无须擦除原先内容的存储器，其写操作比读操作时间要长得多，EEPROM把不易丢失数据和修改灵活的优点组合起来，修改时只需使用普通的控制、地址和数据总线。EEPROM运作原理类似EPROM，但抹除的方式是使用高电场来完成，因此不需要透明窗。 EEPROM比 EPROM贵，集成度低，成本较高，一般用于保存系统设置的参数、IC卡上存储信息、电视机或空调中的控制器。但由于其可以在线修改，所以可靠性不如 EPROM。 [2]快擦除读写存储器快闪存储器快擦除读写存储器( Flash Memory)是英特尔公司90年代中期发明的一种高密度、非易失性的读/写半导体存储器它既有EEPROM的特点，又有RAM的特点，是一种全新的存储结构，俗称快闪存储器。它在20世纪80年代中后期首次推出，快闪存储器的价格和功能介于 EPROM和EEPROM之间。与 EEPROM一样，快闪存储器使用电可擦技术，整个快闪存储器可以在一秒钟至几秒内被擦除，速度比 EPROM快得多。另外，它能擦除存储器中的某些块，而不是整块芯片。然而快闪存储器不提供字节级的擦除，与 EPROM一样，快闪存储器每位只使用一个晶体管，因此能获得与 EPROM一样的高密度(与 EEPROM相比较)。“闪存”芯片采用单一电源（3V或者5V）供电，擦除和编程所需的特殊电压由芯片内部产生，因此可以在线系统擦除与编程。“闪存”也是典型的非易失性存储器，在正常使用情况下，其浮置栅中所存电子可保存100年而不丢失。 [3]目前，闪存已广泛用于制作各种移动存储器，如U盘及数码相机/摄像机所用的存储卡等。 [3]一次编程只读内存一次编程只读内存（One Time Programmable Read Only Memory，OTPROM）之写入原理同EPROM，但是为了节省成本，编程写入之后就不再抹除，因此不设置透明窗。 [1]使用范围播报编辑由于ROM具有断电后信息不丢失的特性，因而可用于计算机启动用的BIOS芯片。EPROM、EEPROM和Flash ROM(NOR Flash 和 NAND Flash)，性能同ROM，但可改写，一般读比写快，写需要比读高的电压，（读5V写12V）但Flash可以在相同电压下读写，且容量大成本低，如U盘MP3中使用广泛。在计算机系统里，RAM一般用作内存，ROM一般作为固件，用来存放一些硬件的驱动程序。 [3]制作原理播报编辑ROM内部结构图ROM的地址译码器是与门的组合，其输出是全部地址输入的最小项。可以把译码器表示成右图所示的与阵列，图中与阵列水平线和垂直线交叉处标的“点”表示有“与”的联系。存储单元体实际上是或门的组合，ROM的输出数即或门的个数。译码器的每个最小项都可能是或门的输入，但是，某个最小项能否成为或门的输入取决于存储信息，因此存储单元体可看成是一个或阵列。由上分析，可以从另一角度来看ROM的结构：它由两个阵列组成——“与”门阵列和“或”门阵列，其中“或”的内容是由用户设置的，因而它是可编程的，而与阵列是用来形成全部最小项的，因而是不可编程的。 [1]

全相联映射：
定义播报编辑两个不同存储器的地址空间之间的一种映射关系，一个存储器中的任意一块（页）可以映像到另外一个存储器中的任意一块（页）中。 [1]出处播报编辑《计算机科学技术名词 》第三版公布时间播报编辑2018年，经全国科学技术名词审定委员会审定发布。

存储器：
工作原理播报编辑存储器是许多存储单元的集合，按单元号顺序排列。每个单元由若干二进制位构成，以表示存储单元中存放的数值，这种结构和数组的结构非常相似，故在VHDL语言中，通常由数组描述存储器 [1]。存储器是用来存储程序和各种数据信息的记忆部件。存储器可分为主存储器（简称主存或内存）和辅助存储器（简称辅存或外存）两大类。和CPU直接交换信息的是主存。 [2]主存的工作方式是按存储单元的地址存放或读取各类信息，统称访问存储器。主存中汇集存储单元的载体称为存储体，存储体中每个单元能够存放一串二进制码表示的信息，该信息的总位数称为一个存储单元的字长。存储单元的地址与存储在其中的信息是一一对应的，单元地址只有一个，固定不变，而存储在其中的信息是可以更换的。 [2]指示每个单元的二进制编码称为地址码。寻找某个单元时，先要给出它的地址码。暂存这个地址码的寄存器叫存储器地址寄存器(MAR)。为可存放从主存的存储单元内取出的信息或准备存入某存储单元的信息，还要设置一个存储器数据寄存器(MDR)。 [2]特点播报编辑计算机的存储器可分成内存储器和外存储器。内存储器在程序执行期间被计算机频繁地使用，并且在一个指令周期期间是可直接访问的。外存储器要求计算机从一个外贮藏装置例如磁带或磁盘中读取信息。这与学生在课堂上做笔记相类似。如果学生没有看笔记就知道内容，信息就被存储在“内存储器”中。如果学生必须查阅笔记，那么信息就在“外存储器”中。 [3]内存储器有很多类型。随机存取存储器（ RAM）在计算期间被用作高速暂存记忆区。数据可以在RAM中存储、读取和用新的数据代替。当计算机在运行时RAM是可得到的。它包含了放置在计算机此刻所处理的问题处的信息。大多数RAM是“不稳定的”，这意味着当关闭计算机时信息将会丢失。只读存储器（ROM）是稳定的。它被用于存储计算机在必要时需要的指令集。存储在ROM内的信息是硬接线的”（即，它是电子元件的一个物理组成部分），且不能被计算机改变（因此称为“只读”）。可变的ROM，称为可编程只读存储器（PROM），可以将其暴露在一个外部电器设备或光学器件（如激光）中来改变。 [3]数字成像设备中的内存储器必须足够大以存放至少一幅数字图像。一幅512 x512 x8位的图像需要1/4兆字节。因此，一台处理几幅这样的图像的成像设备需要几兆字节的内存。 [3]外存储器用来储存不是实时成像任务中获取的图像，其与计算机有不同的分离层面。已经作出诊断的图像通常因为法律目的而存储多年。这些图像被称为“归档”（如磁带），它们必须在计算机上重新安装才能取回信息．硬盘驱动器中的图像被物理地安装在计算机上，且能在几毫秒内被访问。磁存储器中单个位被记录为磁畴，“北极向上”可能意味着1，“北极向下”可能意味着0。 [3]最常用的外存储器设备以两种方式之一来存储信息。磁带，以大的盘式装置的形式在20世纪70年代作为计算机存储的一大支柱，现在则以小而封闭的盒式磁带的形式成为一种相对便宜的“离线”存储选择。尽管它在加载现代录音磁带和寻找到感兴趣数据的存储位置时可能花费几秒甚至几分钟，但购买和维修这一存储媒质的长期花费是较低的。 [3]存储器各种光学存储器装置也是可得到的。在光学存储器装置中存取一串特定数据所需的时间，可能与在（磁）硬盘存取数据所需的时间一样短。在光盘某一平滑镜面上存在着微小的缺陷。在光盘表面烧一个孔洞表示二进制数1，没有烧孔洞则表示0。烧制而成的光盘是“写一次，读多次”（WORM）光盘的实例。这个特征使得它们适合于长期的档案存储，且保持较高的存取速率。直径是12 cm的盘已经成为音乐录制和常规PC使用的标准。这些磁盘被称为“高密度盘”或CD ROM。与CD ROM具有相同大小，但能存储足够的数字信息来支持几小时的高质量视频的高容量盘，被称为数字视频盘（DVD）。DVD正变得流行。有时候根据要求利用机械装置从一大批光盘中提取和安装盘。这些装置被称为是“自动唱片点唱机”。 [3]存储器(17张)分类播报编辑构成存储器的存储介质主要采用半导体器件和磁性材料。存储器中最小的存储单位就是一个双稳态半导体电路或一个CMOS晶体管或磁性材料的存储元，它可存储一个二进制代码。由若干个存储元组成一个存储单元，然后再由许多存储单元组成一个存储器。 [4]根据存储材料的性能及使用方法的不同，存储器有几种不同的分类方法。 [4]1．按存储介质分类半导体存储器：用半导体器件组成的存储器。 [4]磁表面存储器：用磁性材料做成的存储器。 [4]2．按存储方式分类随机存储器：任何存储单元的内容都能被随机存取，且存取时间和存储单元的物理位置无关。 [4]顺序存储器：只能按某种顺序来存取，存取时间与存储单元的物理位置有关。 [4]3．按存储器的读写功能分类只读存储器（ROM）：存储的内容是固定不变的，只能读出而不能写入的半导体存储器。 [4]随机读写存储器（RAM）：既能读出又能写入的半导体存储器。 [4]4．按信息的可保存性分类非永久记忆的存储器：断电后信息即消失的存储器。 [4]永久记忆性存储器：断电后仍能保存信息的存储器。 [4]5．按在计算机系统中的作用分类主存储器（内存）：用于存放活动的程序和数据，其速度高、容量较小、每位价位高。 [4]辅助存储器（外存储器）：主要用于存放当前不活跃的程序和数据，其速度慢、容量大、每位价位低。 [4]缓冲存储器：主要在两个不同工作速度的部件起缓冲作用。 [4]存储系统的分级结构结构播报编辑存储器结构在MCS - 51系列单片机中，程序存储器和数据存储器互相独立，物理结构也不相同。程序存储器为只读存储器，数据存储器为随机存取存储器。从物理地址空间看，共有4个存储地址空间，即片内程序存储器、片外程序存储器、片内数据存储器和片外数据存储器，I/O接口与外部数据存储器统一编址。 [5]存储系统的层次结构播报编辑为提高存储器的性能，通常把各种不同存储容量、存取速度和价格的存储器按层次结构组成多层存储器，并通过管理软件和辅助硬件有机组合成统一的整体，使所存放的程序和数据按层次分布在各存储器中。 [6]主要采用三级层次结构来构成存储系统，由高速缓冲存储器Cache、主存储器和辅助存储器组成。图中自上向下容量逐渐增大，速度逐级降低，成本则逐次减少。 [6]整个结构可看成主存一辅存和Cache-主存两个层次。在辅助硬件和计算机操作系统的管理下，可把主存一辅存作为一个存储整体，形成的可寻址存储空间比主存储器空间大得多。由于辅存容量大，价格低，使得存储系统的整体平均价格降低。Cache-主存层次可以缩小主存和CPU之间的速度差距，从整体上提高存储器系统的存取速度。 [6]一个较大的存储系统由各种不同类型的存储设备构成，形成具有多级层次结构的存储系统。该系统既有与CPU相近的速度，又有极大的容量，而价格又是较低的。可见，采用多级层次结构的存储器系统可有效地解决存储器的速度、容量和价格之间的矛盾。 [6]存储器储存器的扩展播报编辑任何存储芯片的存储容量都是有限的。要构成一定容量的内存，单个芯片往往不能满足字长或存储单元个数的要求，甚至字长和存储单元数都不能满足要求。这时，就需要用多个存储芯片进行组合，以满足对存储容量的需求，这种组合就称为存储器的扩展。存储器扩展时要解决的问题主要包括位扩展、字扩展和字位扩展。 [7]异步SRAM的接口是一种非常典型的半导体存储芯片接口，掌握了它的接口设计方法就意味着掌握了一系列半导体存储芯片接口的设计方法（包括 NoR Flash、E2PROM等），同时也为学习其他半导体存储芯片的接口设计打下了基础。本节以异步SRAM的接口为例，介绍半导体存储芯片接口设计的基本方法与原则。 [7]存储器组织播报编辑Flash存储控制器功能包括存储器组织、启动选择、IAP、ISP、片上Flash编程及校验和计算。在存储器组织中介绍了Flash存储控制器映射和系统存储器映射。Flash存储控制器包含片上 Flash和 Boot loader片上存储器是可编程的，包括APRON、 LDROM、数据 Flash和用户配置区。地址映射包括 Flash存储映射和5个地址映射：支持IAP功能的 LDROM，不支持IAP功能的 LDROM，支持IAP功能的APRON，不支持IAP功能的 APROM，以及支持IAP功能的 Boot loader。 [8]存储器芯片存储管理的目的播报编辑存储管理要实现的目的是为用户提供方便、安全和充分大的存储空间。 [9]方便是指将逻辑地址和物理地址分开，用户只在各自的逻辑地址空间编写程序，不必过问物理空间和物理地址的细节，地址的转换由操作系统自动完成；安全是指同时驻留在内存的多个用户进程相互之间不会发生干扰，也不会访问操作系统所占有的空间；充分大的存储空间是指利用虚拟存储技术，从逻辑上对内存空间进行扩充，从而可以使用户在较小的内存里运行较大的程序。 [9]存储器层次框图存储器阵列播报编辑如何增加磁盘的存取速度，如何防止数据因磁盘的故障而丢失及如何有效地利用磁盘空间，一直是电脑专业人员和用户的困扰；而大容量磁盘的价格非常昂贵，对用户形成很大的负担。磁盘阵列技术的产生一举解决了这些问题。 [10]过去十多年来，CPU的处理速度几乎是呈几何级数的跃升，内存（memory）的存取速度亦大幅增加，而数据储存装置主要是在与磁盘（hard disk）的存取速度相较之下，较为缓慢。整个I/O吞吐量不能和系统匹配，形成电脑系统的瓶颈，拉低了电脑系统的整体性能。若不能有效地提升磁盘的存取速度，CPU、内存及磁盘问的不平衡将使CPU及内存的改进形成浪费。 [10]目前改进磁盘存取速度的方式主要有两种。 [10]一是磁盘快取控制（disk cache controller），它将从磁盘读取的数据存在快取内存（cachememory）中以减少磁盘存取的次数，数据的读写都在快取内存中进行，大幅增加存取的速度，如要读取的数据不在快取内存中，或要写数据到磁盘时，才做磁盘的存取动作。这种方式在单工期环境（single-tasking environment)如DOS之下，对大量数据的存取有很好的性能（量小且频繁的存取则不然），但在多工（multi-tasking）环境之下（因为要不停地做数据交换的动作）或数据库的存取（因每一记录都很小）就不能显示其性能。这种方式没有任何安全保障。 [10]二是使用磁盘阵列的技术。磁盘阵列是把多个磁盘组成一个阵列，当作单一磁盘使用，它将数据以分段(striping)的方式储存在不同的磁盘中，存取数据时，阵列中的相关磁盘一起动作，大幅减低数据的存取时间，同时有更佳的空间利用率。磁盘阵列所利用的不同的技术，称为RAID level，不同的level针对不同的系统及应用，以解决数据安全的问题。 [10]一般高性能的磁盘阵列都是以硬件的形式来达成，进一步把磁盘快取控制及磁盘阵列结合在一个控制器（RAID controller）或控制卡上，针对不同的用户解决人们对磁盘输出/输入系统的四大要求：(1)增加存取速度；(2)容错（fault tolerance），即安全性；(3)有效利用磁盘空间；(4)尽量平衡CPU、内存及磁盘的性能差异，提高电脑的整体工作性能。 [10]关于磁盘阵列技术的阵列原理，1987年，加州伯克利大学的一位人员发表了名为“磁盘阵列研究”的论文，正式提到了RAID也就是磁盘阵列，论文提出廉价的5.25〞及3.5〞的硬盘也能如大机器上的8”盘一样能提供大容量、高性能和数据的一致性，并详述了RAID的技术。 [10]磁盘阵列针对不同的应用，使用不同技术，称为RAID level。RAID是RedundantArray of Inexpensive Disks的缩写，每- level代表一种技术。 目前业界公认的标准是RAIDO- RAID5。这个level并不代表技术的高低，leve15并不高于leve13，levell也不低于level4，至于要选择哪一种RAID level的产品，视用户的操作环境（operating environment）及应用（application）而定，与level的高低没有必然的关系。RAIDO没有安全的保障，但其快速，所以适合高速I/O的系统；RAID1适用于需安全性又要兼顾速度的系统，RAID2及RAID3适用于大型电脑及影像、CAD/CAM等处理；RAID5多用于OLTP，因有金融机构及大型数据处理中心的迫切需要，故使用较多而较有名气，但也因此形成很多人对磁盘阵列的误解，以为磁盘阵列非要RAID5不可。RAID4较少使用，和RAID5有其共同之处，但RAID4适合大量数据的存取。其他如RAID6、RAID7，乃至RAID10、RAID50、RAID100等，都是厂商各做各的，并无一致的标准，在此不作说明。 [10]外部EPROM扩展原理(7张)总而言之，RAIDO及RAID1最适合PC服务器及图形工作站的用户，提供最佳的性能及最便宜的价格，以低成本符合市场的需求。RAID2及RAID3适用于大档案且输入/输出需求不频繁的应用如影像处理及CAD/CAM等；而RAID5则适用于银行、金融、股市、数据库等大型数据处理中心的OLTP应用；RAID4与RAID5有相同的特性及应用方式，但其较适用于大型文件的读取。 [10]未来趋势播报编辑存储器是计算机中数据存放的主要介质。随着近年来的发展, 存储器的变化日新月异, 各种新型存储器进入市场, 普及针对新型存储器的维护方法已经迫在眉睫。 [11]从PCRAM和MRAM到RRAM等更多技术，一系列全新的存储技术正不断涌向晶圆厂。而推动这一进程的正是游戏和移动产品领域的技术进步， 以及云计算的发展。这些应用都非常重要，它们正在不断扩展当今主流存储技术的能力。例如，游戏应用需要速度极快的主存储器和高容量的辅助（存储类）存储器，从而在用户浑然不觉的情况下处理数据, 快速管理海量的图形数据。毕竟，没人希望在游戏玩到关键时刻，突然遇到意外的卡顿。对于云计算，其最大的优势在于能够通过网络访问海量数据，而无需将这些数据直接存储在我们的个人设备上。同样，速度也至关重要，因为除非必要，没人愿意多等待哪怕一纳秒。 [12]半导体存储器随着数据存储技术的迅猛发展，用户对存储性价比的要求也越来越高，而云存储技术无需硬件设备的支持，这就大大增加了存储的安全性能，用户也无需对硬件设施进行维护，减少了投入成本，提升存储效率。 [13]

扩展总线：
扩展总线简介播报编辑总线就是在模块与模块之间或设备与设备之间的一组进行互连和传输信息的信号线，信息包括指令、数据和地址。计算机的总线都是具有一定的含义的。对于连接到总线上的多个设备而言，任何一个设备发出的信号可以被连接到总线上的所有其他设备接收。如果两个以上的设备同时在总线上发出自己的信号，则会发生信号混乱。因此，在同一时间内，连接到总线是哪个的多个设备中只能有一个设备主动进行信号的传输，其他设备只能处于被动接收的状态。在20世纪70年代后期，以Apple ΙΙ为代表的个人计算机逐步风靡全球。各种计算机的主板都有两个问题要解决，一是如何与外围高速交换数据，二是如何扩展计算机功能，解决问题的方法是采用各种类型的扩展总线。个人计算机的发展是与扩展总线的不断改进、不断更新分不开的。总线标准的制定，以及各种总线对市场的争夺，市场主流的不断更迭，始终是和个人计算机不断发展的步伐紧密配合的。扩展总线类型播报编辑PC/XT总线系统80年代初期，IBM PC/XT的出现，它所用的8位扩展总线代表了当时的一种新型总线标准。这种机型很快就使Apple ΙΙ相形见绌。随着外部设备性能、主存储器速度和16位中央处理器性能的提高，8位总线已不能适应新的技术，IBM使用Intel公司推出了全新的16位微处理器80286开发PC/AT个人计算机，采用了全新的16位扩展总线，由PC/XT扩展总线增加地址信号、数据信号线及控制信号线而成，这两种总线在同一块底板上并存了较长一段时间。ISA总线系统PC/AT的扩展总线系统也就是后来在市场上使用了很长时间的工业标准体系结构总线（ISA）。这种16位的扩展总线系统在相当长时间内一直是市场上主板制造商使用的主流，有人试图另立标准，但都被市场所淘汰（这种情况一直延续到32位微处理器芯片出现之后）。 [1]

SRAM：
基本简介播报编辑SRAM不需要刷新电路即能保存它内部存储的数据。而DRAM（Dynamic Random Access Memory）每隔一段时间，要刷新充电一次，否则内部的数据即会消失，因此SRAM具有较高的性能，但是SRAM也有它的缺点，即它的集成度较低，功耗较DRAM大 [1]，相同容量的DRAM内存可以设计为较小的体积，但是SRAM却需要很大的体积。同样面积的硅片可以做出更大容量的DRAM，因此SRAM显得更贵。 [2]主要规格播报编辑一种是置于cpu与主存间的高速缓存，它有两种规格：一种是固定在主板上的高速缓存（Cache Memory）；另一种是插在卡槽上的COAST（Cache On A Stick）扩充用的高速缓存，另外在CMOS芯片1468l8的电路里，它的内部也有较小容量的128字节SRAM，存储所设置的配置数据。还有为了加速CPU内部数据的传送，自80486CPU起，在CPU的内部也设计有高速缓存，故在Pentium CPU就有所谓的L1 Cache（一级高速缓存）和L2Cache（二级高速缓存）的名词，一般L1 Cache是建在CPU的内部，L2 Cache是设计在CPU的外部，但是Pentium Pro把L1和L2 Cache同时设计在CPU的内部，故Pentium Pro的体积较大。Pentium Ⅱ又把L2 Cache移至CPU内核之外的黑盒子里。SRAM显然速度快，不需要刷新操作，但是也有另外的缺点，就是价格高，体积大，所以在主板上还不能作为用量较大的主存。 [2]主要用途播报编辑图1 SRAMSRAM主要用于二级高速缓存（Level2 Cache）。它利用晶体管来存储数据。与DRAM相比，SRAM的速度快，但在相同面积中SRAM的容量要比其他类型的内存小。SRAM的速度快但昂贵，一般用小容量的SRAM作为更高速CPU和较低速DRAM 之间的缓存（cache）。SRAM也有许多种，如AsyncSRAM （Asynchronous SRAM，异步SRAM）、Sync SRAM （Synchronous SRAM，同步SRAM）、PBSRAM （Pipelined Burst SRAM，流水式突发SRAM），还有INTEL没有公布细节的CSRAM等。基本的SRAM的架构如图1所示，SRAM一般可分为五大部分：存储单元阵列（core cells array），行/列地址译码器（decode），灵敏放大器（Sense Amplifier），控制电路（control circuit），缓冲/驱动电路（FFIO）。SRAM是静态存储方式，以双稳态电路作为存储单元，SRAM不像DRAM一样需要不断刷新，而且工作速度较快，但由于存储单元器件较多，集成度不太高，功耗也较大。 [2]工作原理播报编辑图2 六管单元电路图SRAM的工作原理：假设准备往图2的6T存储单元写入“1”，先将某一组地址值输入到行、列译码器中，选中特定的单元，然后使写使能信号WE有效，将要写入的数据“1”通过写入电路变成“1”和“0”后分别加到选中单元的两条位线BL,BLB上，此时选中单元的WL=1，晶体管N0,N5打开，把BL,BLB上的信号分别送到Q,QB点，从而使Q=1，QB=0，这样数据“1”就被锁存在晶体管P2,P3,N3,N4构成的锁存器中。写入数据“0”的过程类似。SRAM的读过程以读“1”为例，通过译码器选中某列位线对BL,BLB进行预充电到电源电压VDD，预充电结束后，再通过行译码器选中某行，则某一存储单元被选中，由于其中存放的是“1”，则WL=1、Q=1、QB=0。晶体管N4、N5导通，有电流经N4、N5到地，从而使BLB电位下降，BL、BLB间电位产生电压差，当电压差达到一定值后打开灵敏度放大器，对电压进行放大，再送到输出电路，读出数据。 [2]类型播报编辑1.非挥发性SRAM非挥发性SRAM（Non-volatile SRAM，nvSRAM）具有SRAM的标准功能，但在失去电源供电时可以保住其数据。非挥发性SRAM用于网络、航天、医疗等需要关键场合—保住数据是关键的而且不可能用上电池。2.异步SRAM异步SRAM（Asynchronous SRAM）的容量从4 Kb到64 Mb。SRAM的快速访问使得异步SRAM适用于小型的cache很小的嵌入式处理器的主内存，这种处理器广泛用于工业电子设备、测量设备、硬盘、网络设备等等。根据晶体管类型分类双极性结型晶体管（用于TTL与ECL）—非常快速但是功耗巨大MOSFET（用于CMOS）—本文详细介绍的类型，低功耗，应用广泛。根据功能分类异步—独立的时钟频率，读写受控于地址线与控制使能信号。同步—所有工作是时钟脉冲边沿开始，地址线、数据线、控制线均与时钟脉冲配合。根据特性分类零总线翻转（Zero bus turnaround，ZBT）—SRAM总线从写到读以及从读到写所需要的时钟周期是0同步突发SRAM（synchronous-burst SRAM，syncBurst SRAM）—DDR SRAM—同步、单口读/写，双数据率I/OQDR SRAM（Quad Data Rate (QDR) SRAM）—同步，分开的读/写口，同时读写4个字（word）。根据触发类型二进制SRAM三进制计算机SRAM [2]结构原理播报编辑SRAM （Static RAM），即静态RAM，它也由晶体管组成。接通代表1，断开表示0，并且状态会保持到接收了一个改变信号为止。这些晶体管不需要刷新，但停机或断电时，它们同DRAM一样，会丢掉信息。SRAM的速度非常快，通常能以20ns或更快的速度工作。一个DRAM存储单元仅需一个晶体管和一个小电容。而每个SRAM单元需要四到六个晶体管和其他零件。所以，除了价格较贵外，SRAM芯片在外形上也较大，与DRAM相比要占用更多的空间。由于外形和电气上的差别，SRAM和DRAM是不能互换的。SRAM的高速和静态特性使它们通常被用来作为Cache存储器。计算机的主板上都有Cache插座。图3 SRAM如图3所示的是一个SRAM的结构框图。由图3看出SRAM一般由五大部分组成，即存储单元阵列、地址译码器（包括行译码器和列译码器）、灵敏放大器、控制电路和缓冲/驱动电路。在图3中，A0-Am-1为地址输入端，CSB， WEB和OEB为控制端，控制读写操作，为低电平有效，1100-11ON-1为数据输入输出端。存储阵列中的每个存储单元都与其它单元在行和列上共享电学连接，其中水平方向的连线称为“字线”，而垂直方向的数据流入和流出存储单元的连线称为“位线”。通过输入的地址可选择特定的字线和位线，字线和位线的交叉处就是被选中的存储单元，每一个存储单元都是按这种方法被唯一选中，然后再对其进行读写操作。有的存储器设计成多位数据如4位或8位等同时输入和输出，这样的话，就会同时有4个或8个存储单元按上述方法被选中进行读写操作。在SRAM 中，排成矩阵形式的存储单元阵列的周围是译码器和与外部信号的接口电路。存储单元阵列通常采用正方形或矩阵的形式，以减少整个芯片面积并有利于数据的存取。以一个存储容量为4K位的SRAM为例，共需12条地址线来保证每一个存储单元都能被选中（=4096）。如果存储单元阵列被排列成只包含一列的长条形，则需要一个12/4K位的译码器，但如果排列成包含64行和64列的正方形，这时则只需一个6/64位的行译码器和一个6/64位的列译码器，行、列译码器可分别排列在存储单元阵列的两边，64行和64列共有4096个交叉点，每一个点就对应一个存储位。因此，将存储单元排列成正方形比排列成一列的长条形要大大地减少整个芯片地面积。存储单元排列成长条形除了形状奇异和面积大以外，还有一个缺点，那就是单排在列的上部的存储单元与数据输入/输出端的连线就会变得很长，特别是对于容量比较大得存储器来说，情况就更为严重，而连线的延迟至少是与它的长度成线性关系，连线越长，线上的延迟就越大，所以就会导致读写速度的降低和不同存储元连线延迟的不一致性，这些都是在设计中需要避免的。 [3]应用与使用播报编辑特性SRAM是比DRAM更为昂贵，但更为快速、低功耗（仅空闲状态）。因此SRAM首选用于带宽要求高。SRAM比起DRAM更为容易控制，也更是随机访问。由于复杂的内部结构，SRAM比DRAM的占用面积更大，因而不适合用于更高储存密度低成本的应用，如PC内存。时钟频率与功耗SRAM功耗取决于它的访问频率。如果用高频率访问SRAM，其功耗比DRAM大得多。有的SRAM在全带宽时功耗达到几个瓦特量级。另一方面，SRAM如果用于温和的时钟频率的微处理器，其功耗将非常小，在空闲状态时功耗可以忽略不计—几个微瓦特级别。SRAM用于：通用的产品asynchronous界面，例如28针32Kx8的chip（通常命名为XXC256），以及类似的产品最多16 Mbit每片synchronous界面，通常用做高速缓存（cache）以及其它要求突发传输的应用，最多18 Mbit（256Kx72）每片集成于芯片内作为微控制器的RAM或者cache（通常从32 bytes到128kilobytes）作为强大的微处理器的主caches，如x86系列与许多其它CPU（从8kiB到几百万字节的量级）作为寄存器（参见寄存器堆）用于特定的ICs或ASIC（通常在几千字节量级）用于FPGA与CPLD嵌入式应用工业与科学用的很多子系统，汽车电子等等都用到了SRAM。现代设备中很多都嵌入了几千字节的SRAM。实际上几乎所有实现了电子用户界面的现代设备都可能用上了SRAM，如玩具。数码相机、手机、音响合成器等往往用了几兆字节的SRAM。 实时信号处理电路往往使用双口（dual-ported）的SRAM。用于计算机SRAM用于PC、工作站、路由器以及外设：内部的CPU高速缓存，外部的突发模式使用的SRAM缓存，硬盘缓冲区，路由器缓冲区，等等。LCD显示器或者打印机也通常用SRAM来缓存数据。SRAM做的小型缓冲区也常见于CDROM与CDRW的驱动器中，通常为256 KiB或者更多，用来缓冲音轨数据。线缆调制解调器及类似的连接于计算机的设备也使用了SRAM。爱好者搭建自己的处理器的业余爱好者更愿意选用SRAM，这是由于其易用性的工作界面。没有DRAM所需的刷新周期；地址总线与数据总线直接访问而不是像DRAM那样多工分别访问。SRAM通常只需3个控制信号：Chip Enable (CE), Write Enable (WE)与Output Enable（OE）。对于同步SRAM,还需要时钟信号（Clock，CLK）。 [4]参见播报编辑DRAM,包括PSRAM (pseudo-static RAM)闪存晶体管

总线：
工作原理播报编辑如果说主板（Mother Board）是一座城市，那么总线就像是城市里的公共汽车（bus），能按照固定行车路线，传输来回不停运作的比特（bit）。一条线路在同一时间内都仅能负责传输一个比特。因此，必须同时采用多条线路才能传送更多数据，而总线可同时传输的数据数就称为宽度（width），以比特为单位，总线宽度愈大，传输性能就愈佳。总线的带宽（即单位时间内可以传输的总数据数）为：总线带宽 = 频率 x 宽度/8（Bytes/sec） [3]。当总线空闲（其他器件都以高阻态形式连接在总线上）且一个器件要与目的器件通信时，发起通信的器件驱动总线，发出地址和数据。其他以高阻态形式连接在总线上的器件如果收到（或能够收到）与自己相符的地址信息后，即接收总线上的数据。发送器件完成通信，将总线让出（输出变为高阻态）。在计算机中用于连接各种功能部件并在它们之间传送数据的公用线路或通路。在计算机系统中按其所连接的对象，总线可分为： 片总线，又称器件级总线，它是中央处理器芯片内部的总线。内总线，又称系统总线或板级总线，它是计算机各功能部件之间的传输通路，微型计算机总线通常称为内总线。外总线，又称通信总线，它是计算机系统之间，或者是计算机主机与外围设备之间的传输通路 [1]。总线是一种共享型的数据传送设备。虽然总线上可联接多个设备，但任一时刻通常只能有一对设备参与数据传输。按信息传输的形式，总线可分为并行总线和串行总线两种。并行总线对n位二进制信息用n条传输线同时传送，其特点是传输速度快，但系统结构较复杂，它用于计算机系统内的各部件之间的连接；串行总线对多位二进制信息共用一条传输线，多位二进制信息按时间先后顺序通过总线，它的特点是结构简单，但其传输速度较慢。总线必须有明确的规范： 总线定时协议，即在总线上传送信息时必须遵守一定的定时规则，例如同步总线定时，异步总线定时，半同步总线定时等。总线的物理特性，包括信号、电源、地址的电气特性，以及连线、接插件的机械特性。总线带宽，它是总线所能达到的最高传输率，其单位是MB/S。总线特性播报编辑由于总线是连接各个部件的一组信号线。通过信号线上的信号表示信息，通过约定不同信号的先后次序即可约定操作如何实现。总线的特性如下（1）物理特性：物理特性又称为机械特性，指总线上部件在物理连接时表现出的一些特性，如插头与插座的几何尺寸、形状、引脚个数及排列顺序等。（2）功能特性：　　功能特性是指每一根信号线的功能，如地址总线用来表示地址码。数据总线用来表示传输的数据，控制总线表示总线上操作的命令、状态等。（3）电气特性：　　电气特性是指每一根信号线上的信号方向及表示信号有效的电平范围，通常，由主设备（如CPU）发出的信号称为输出信号（OUT），送入主设备的信号称为输入信号（IN）。通常数据信号和地址信号定义高电平为逻辑1、低电平为逻辑0，控制信号则没有俗成的约定，如WE表示低电平有效、Ready表示高电平有效。不同总线高电平、低电平的电平范围也无统一的规定，通常与TTL是相符的。（4）时间特性：　　时间特性又称为逻辑特性，指在总线操作过程中每一根信号线上信号什么时候有效，通过这种信号有效的时序关系约定，确保了总线操作的正确进行。　　为了提高计算机的可拓展性，以及部件及设备的通用性，除了片内总线外，各个部件或设备都采用标准化的形式连接到总线上，并按标准化的方式实现总线上的信息传输。而总线的这些标准化的连接形式及操作方式，统称为总线标准。如ISA、PCI、USB总线标准等，相应的，采用这些标准的总线为ISA总线、PCI总线、USB总线等。总线分类播报编辑总线按功能和规范可分为五大类型:数据总线（Data Bus）：在CPU与RAM之间来回传送需要处理或是需要储存的数据。地址总线（Address Bus）：用来指定在RAM（Random Access Memory）之中储存的数据的地址。控制总线（Control Bus）：将微处理器控制单元（Control Unit）的信号，传送到周边设备。扩展总线（Expansion Bus）：外部设备和计算机主机进行数据通信的总线，例如ISA总线，PCI总线。局部总线（Local Bus）：取代更高速数据传输的扩展总线。三类总线在微机系统中的地位和关系其中的数据总线DB（Data Bus）、地址总线AB（Address Bus）和控制总线CB（Control Bus），也统称为系统总线，即通常意义上所说的总线。有的系统中，数据总线和地址总线是复用的，即总线在某些时刻出现的信号表示数据而另一些时刻表示地址；而有的系统是分开的。51系列单片机的地址总线和数据总线是复用的，而一般PC中的总线则是分开的。“数据总线DB”用于传送数据信息。数据总线是双向三态形式的总线，即他既可以把CPU的数据传送到存储器或I/O接口等其它部件，也可以将其它部件的数据传送到CPU。数据总线的位数是微型计算机的一个重要指标，通常与微处理的字长相一致。例如Intel 8086微处理器字长16位，其数据总线宽度也是16位。需要指出的是，数据的含义是广义的，它可以是真正的数据，也可以是指令代码或状态信息，有时甚至是一个控制信息，因此，在实际工作中，数据总线上传送的并不一定仅仅是真正意义上的数据。常见的数据总线为ISA（ISA总线）、EISA、VESA、PCI等。“地址总线AB”是专门用来传送地址的，由于地址只能从CPU传向外部存储器或I/O端口，所以地址总线总是单向三态的，这与数据总线不同。地址总线的位数决定了CPU可直接寻址的内存空间大小，比如8位微机的地址总线为16位，则其最大可寻址空间为2^16=64KB，16位微型机（x位处理器指一个时钟周期内微处理器能处理的位数[1 、0]多少，即字长大小）的地址总线为20位，其可寻址空间为2^20=1MB。一般来说，若地址总线为n位，则可寻址空间为2^n字节。“控制总线CB”用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和I/O接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的，比如：中断申请信号、复位信号、总线请求信号、设备就绪信号等。因此，控制总线的传送方向由具体控制信号而定，(信息)一般是双向的，控制总线的位数要根据系统的实际控制需要而定。实际上控制总线的具体情况主要取决于CPU。按照传输数据的方式划分，可以分为串行总线和并行总线。串行总线中，二进制数据逐位通过一根数据线发送到目的器件；并行总线的数据线通常超过2根。常见的串行总线有SPI、I2C、USB及RS232等。按照时钟信号是否独立，可以分为同步总线和异步总线。同步总线的时钟信号独立于数据，而异步总线的时钟信号是从数据中提取出来的。SPI、I2C是同步串行总线，RS232采用异步串行总线。内部总线播报编辑并发CAMAC，用于仪表检测系统工业标准架构总线（ISA）扩展ISA（EISA）Low Pin Count（LPC）微通道（MCA）MBus多总线（Multibus），用于工业生产系统NuBus，或称IEEE 1196OPTi本地总线，用于早期Intel 80486主板外围部件互联总线（PCI）S-100总线（S-100 bus），或称IEEE 696，用于Altair或类似微处理器SBus或称IEEE 1496VESA本地总线（VLB，VL-bus）VERSAmodule Eurocard bus（VME总线）STD总线（STD bus），用于八位或十六位微处理器系统UnibusQ-BusPC/104PC/104 PlusPC/104 ExpressPCI-104PCIe-104串行1-WireHyperTransportI²C串行PCI（PCIe）串行外围接口总线（SPI总线）火线i.Link（IEEE 1394）外部总线播报编辑外部总线指缆线和连接器系统，用来传输I/O路径技术指定的数据和控制信号，另外还包括一个总线终结电阻或电路，这个终结电阻用来减弱电缆上的信号反射干扰。并发ATA：磁盘/磁带周边附件总线，也称 PATA、IDE、EIDE、ATAPI 等等。　　(the original ATA is parallel, but see also the recentserial ATA)HIPPI（HIgh Performance Parallel Interface）：高速平行接口。IEEE-488：也称 GPIB（General-Purpose Instrumentation Bus）或 HPIB（Hewlett-Packard Instrumentation Bus）。PC card：前身为知名的PCMCIA，常用于笔记本电脑和其它便携式设备，但自从引入USB以及嵌入式网络后，这个总线就慢慢不再使用了。SCSI（Small Computer System Interface）：小型电脑系统接口，磁盘/磁带周边附件总线。串行USB Universal Serial Bus, 大量外部设备均采用此总线Serial Attached SCSIand otherserial SCSIbusesSerial ATAController Area Network("CAN总线")EIA-485FireWireThunderbolt计算机总线播报编辑计算机总线是一组能为多个部件分时共享的信息传送线，用来连接多个部件并为之提供信息交换通路。总线不仅是一组信号线，从广义上讲，总线是一组传送线路及相关的总线协议。a.主板的总线在计算机科学技术中，人们常常以MHz表示的速度来描述总线频率。计算机总线的种类很多，前端总线的英文名字是Front Side Bus，通常用FSB表示，是将CPU连接到北桥芯片的总线。计算机的前端总线频率是由CPU和北桥芯片共同决定的。b.硬盘的总线总线一般有SCSI、ATA、SATA等几种。SATA是串行ATA的缩写，为什么要使用串行ATA就要从PATA——并行ATA的缺点说起。我们知道ATA或者说普通IDE硬盘的数据线最初就是40根的排线，这40根线里面有数据线、时钟线、控制线、地线，其中32根数据线是并行传输的（一个时钟周期可以同时传输4个字节的数据），因此对同步性的要求很高。这就是为什么从PATA-66（就是常说的DMA66）接口开始必须使用80根的硬盘数据线，其实增加的这40根全是屏蔽用的地线，而且只在主板一边接地（千万不要接反了，反了的话屏蔽作用大大降低），有了良好的屏蔽硬盘的传输速度才能达到66MB/s、100MB/s和最高的133MB/s。但是在PATA-133之后，并行传输速度已经到了极限，而且PATA的三大缺点暴露无遗：信号线长度无法延长、信号同步性难以保持、5V信号线耗电较大。那为什么SCSI-320接口的数据线能达到320MB/s的高速、而且线缆可以很长呢？你有没有注意到SCSI的高速数据线是“花线”？这可不是为了好看，那“花”的部分实际上就是一组组的差分信号线两两扭合而成，这成本可不是普通电脑系统愿意承担的。c.其他的总线计算机中其他的总线还有：通用串行总线USB（Universal Serial Bus）、IEEE1394、PCI等等。技术指标播报编辑1、总线的带宽（总线数据传输速率）总线的带宽指的是单位时间内总线上传送的数据量，即每秒钟传送MB的最大稳态数据传输率 [2]。与总线密切相关的两个因素是总线的位宽和总线的工作频率，它们之间的关系：总线的带宽=总线的工作频率*总线的位宽/8或者 总线的带宽=（总线的位宽/8 ）/总线周期2、总线的位宽总线的位宽指的是总线能同时传送的二进制数据的位数，或数据总线的位数，即32位、64位等总线宽度的概念。总线的位宽越宽，每秒钟数据传输率越大，总线的带宽越宽。3、总线的工作频率总线的工作时钟频率以MHZ为单位，工作频率越高，总线工作速度越快，总线带宽越宽。合理搭配播报编辑总线主板北桥芯片负责联系内存、显卡等数据吞吐量最大的部件，并和南桥芯片连接。CPU就是通过前端总线（FSB）连接到北桥芯片，进而通过北桥芯片和内存、显卡交换数据。前端总线是CPU和外界交换数据的最主要通道，因此前端总线的数据传输能力对计算机整体性能作用很大，如果没足够快的前端总线，再强的CPU也不能明显提高计算机整体速度。数据传输最大带宽取决于所有同时传输的数据的宽度和传输频率，即数据带宽=（总线频率×数据位宽）÷8。PC机上所能达到的前端总线频率有266MHz、333MHz、400MHz、533MHz、800MHz几种，前端总线频率越大，代表着CPU与北桥芯片之间的数据传输能力越大，更能充分发挥出CPU的功能。CPU技术发展很快，运算速度提高很快，而足够大的前端总线可以保障有足够的数据供给给CPU，较低的前端总线将无法供给足够的数据给CPU，这样就限制了CPU性能得发挥，成为系统瓶颈。总线操作播报编辑总线一个操作过程是完成两个模块之间传送信息，启动操作过程的是主模块，另外一个是从模块。某一时刻总线上只能有一个主模块占用总线。总线的操作步骤：主模块申请总线控制权，总线控制器进行裁决。总线的操作步骤：主模块得到总线控制权后寻址从模块，从模块确认后进行数据传送。数据传送的错误检查。总线定时协议：定时协议可保证数据传输的双方操作同步，传输正确。定时协议有三种类型：同步总线定时：总线上的所有模块共用同一时钟脉冲进行操作过程的控制。各模块的所有动作的产生均在时钟周期的开始，多数动作在一个时钟周期中完成。异步总线定时：操作的发生由源或目的模块的特定信号来确定。总线上一个事件发生取决前一事件的发生，双方相互提供联络信号。总线定时协议半同步总线定时：总线上各操作的时间间隔可以不同，但必须是时钟周期的整数倍，信号的出现,采样与结束仍以公共时钟为基准。ISA总线采用此定时方法。数据传输类型：分单周期方式和突发(burst)方式。单周期方式：一个总线周期只传送一个数据。突发方式：取得主线控制权后进行多个数据的传输。寻址时给出目的地首地址，访问第一个数据，数据2、3到数据n的地址在首地址基础上按一定规则自动寻址（如自动加1）。总线标准播报编辑为什么要制定总线标准?便于机器的扩充和新设备的添加，有了总线标准，不同厂商可以按照同样的标准和规范生产各种不同功能的芯片、模块和整机，用户可以根据功能需求去选择不同厂家生产的、基于同种总线标准的模块和设备，甚至可以按照标准，自行设计功能特殊的专用模块和设备，以组成自己所需的应用系统。这样可使芯片级、模块级、设备级等各级别的产品都具有兼容性和互换性，以使整个计算机系统的可维护性和可扩充性得到充分保证。总线标准的技术规范？机械结构规范：模块尺寸、总线插头、总线接插件以及安装尺寸均有统一规定。功能规范：总线每条信号线（引脚的名称）、功能以及工作过程要有统一规定。电气规范：总线每条信号线的有效电平、动态转换时间、负载能力等。哪种总线是标准的？主板上的处理器-主存总线经常是特定的专用总线，而用于连接各种I/O模块的I/O总线和底板式总线则通常可在不同计算机中互用。实际上，底板式总线和I/O总线通常是标准总线，可被许多由不同公司制造的不同计算机使用。总线标准-ISAISA（IndustrialStandardArchitecture）总线是IBM公司1984年为推出PC/AT机而建立的系统总线标准。所以也叫AT总线。主要特点:(1)支持64KI/O地址空间、16M主存地址空间的寻址，支持15级硬中断、7级DMA通道。(2)是一种简单的多主控总线。除了CPU外，DMA控制器、DRAM刷新控制器和带处理器的智能接口控制卡都可成为总线主控设备。(3)支持8种总线事务类型：存储器读、存储器写、I/O读、I/O写、中断响应、DMA响应、存储器刷新、总线仲裁。它的时钟频率为8MHz，共有98根信号线。数据线和地址线分离，数据线宽度为16位，可以进行8位或16位数据的传送，所以最大数据传输率为16MB/s。总线标准-EISAEISA(ExtendedIndustrialStanderdArchitecture)总线 是一种在ISA总线基础上扩充的开放总线标准。 支持多总线主控和突发传输方式。时钟频率为8.33MHz。共有198根信号线，在原ISA总线的98根线的基础上扩充了100根线，与原ISA总线完全兼容。具有分立的数据线和地址线。数据线宽度为32位，具有8位、16位、32位数据传输能力，所以最大数据传输率为33MB/s。地址线的宽度为32位，所以寻址能力达232。即：CPU或DMA控制器等这些主控设备能够对4G范围的主存地址空间进行访问。总线标准-PCIPCI（PeripheralComponentInterconnect）总线是一种高性能的32位局部总线。它由Intel公司于1991年底提出，后来又联合IBM、DEC等100多家PC业界主要厂家，于1992年成立PCI集团，称为PCISIG，进行统筹和推广PCI标准的工作。用于高速外设的I/O接口和主机相连。采用自身33MHz的总线频率，数据线宽度为32位，可扩充到64位，所以数据传输率可达132MB/s～264MB/s。速度快、支持无限突发传输方式 、支持并发工作（PCI桥提供数据缓冲,并使总线独立于CPU） ，可在主板上和其他系统总线（如：ISA、EISA或MCA）相连接，系统中的高速设备挂接在PCI总线上，而低速设备仍然通过ISA、EISA等这些低速I/O总线支持。支持基于微处理器的配置，可用在单处理器系统中，也可用于多处理器系统。优点与缺点播报编辑采用总线结构的主要优点1、面向存储器的双总线结构信息传送效率较高，这是它的主要优点。但CPU与I/O接口都要访问存储器时，仍会产生冲突。2、CPU与高速的局部存储器和局部I/O接口通过高传输速率的局部总线连接，速度较慢的全局存储器和全局I/O接口与较慢的全局总线连接，从而兼顾了高速设备和慢速设备，使它们之间不互相牵扯。3、简化了硬件的设计。便于采用模块化结构设计方法，面向总线的微型计算机设计只要按照这些规定制作cpu插件、存储器插件以及I/O插件等，将它们连入总线就可工作，而不必考虑总线的详细操作。4、简化了系统结构。整个系统结构清晰。连线少，底板连线可以印制化。5、系统扩充性好。一是规模扩充，规模扩充仅仅需要多插一些同类型的插件。二是功能扩充，功能扩充仅仅需要按照总线标准设计新插件，插件插入机器的位置往往没有严格的限制。6、系统更新性能好。因为cpu、存储器、I/O接口等都是按总线规约挂到总线上的，因而只要总线设计恰当，可以随时随着处理器的芯片以及其他有关芯片的进展设计新的插件，新的插件插到底板上对系统进行更新，其他插件和底板连线一般不需要改。7、便于故障诊断和维修。用主板测试卡可以很方便找到出现故障的部位，以及总线类型。采用总线结构的缺点由于在CPU与主存储器之间、CPU与I/O设备之间分别设置了总线，从而提高了微机系统信息传送的速率和效率。但是由于外部设备与主存储器之间没有直接的通路，它们之间的信息交换必须通过CPU才能进行中转，从而降低了CPU的工作效率（或增加了CPU的占用率。一般来说，外设工作时要求CPU干预越少越好。CPU干预越少，这个设备的CPU占用率就越低，说明设备的智能化程度越高），这是面向CPU的双总线结构的主要缺点。同时还包括：1、利用总线传送具有分时性。当有多个主设备同时申请总线的使用是必须进行总线的仲裁。2、总线的带宽有限，如果连接到总线上的某个硬件设备没有资源调控机制容易造成信息的延时（这在某些即时性强的地方是致命的）。3、连到总线上的设备必须有信息的筛选机制，要判断该信息是否是传给自己的。相关信息播报编辑任何一个微处理器都要与一定数量的部件和外围设备连接，但如果将各部件和每一种外围设备都分别用一组线路与CPU直接连接，那么连线将会错综复杂，甚至难以实现。为了简化硬件电路设计、简化系统结构，常用一组线路，配置以适当的接口电路，与各部件和外围设备连接，这组共用的连接线路被称为总线。采用总线结构便于部件和设备的扩充，尤其制定了统一的总线标准则容易使不同设备间实现互连。微机中总线一般有内部总线、系统总线和外部总线。内部总线是微机内部各外围芯片与处理器之间的总线，用于芯片一级的互连；而系统总线是微机中各插件板与系统板之间的总线，用于插件板一级的互连；外部总线则是微机和外部设备之间的总线，微机作为一种设备，通过该总线和其他设备进行信息与数据交换，它用于设备一级的互连。另外，从广义上说，计算机通信方式可以分为并行通信和串行通信，相应的通信总线被称为并行总线和串行总线。并行通信速度快、实时性好，但由于占用的口线多，不适于小型化产品；而串行通信速率虽低，但在数据通信吞吐量不是很大的微处理电路中则显得更加简易、方便、灵活。串行通信一般可分为异步模式和同步模式。---随着微电子技术和计算机技术的发展，总线技术也在不断地发展和完善，而使计算机总线技术种类繁多，各具特色。总线的发展史播报编辑ISA总线（Industry Standard Architecture）最早的PC总线是IBM公司1981年在PC/XT电脑采用的系统总线，它基于8bit的8088 处理器，被称为PC总线或者PC/XT总线。1984年，IBM 推出基于16-bit Intel 80286处理器的PC/AT 电脑，系统总线也相应地扩展为16bit，并被称呼为PC/AT 总线。而为了开发与IBM PC 兼容的外围设备，行业内便逐渐确立了以IBM PC 总线规范为基础的ISA（工业标准架构：Industry Standard Architecture ）总线。PCI总线（Peripheral Component Interconnect）由于ISA/EISA总线速度缓慢，一度出现CPU 的速度甚至还高过总线的速度，造成硬盘、显示卡还有其它的外围设备只能通过慢速并且狭窄的瓶颈来发送和接受数据，使得整机的性能受到严重的影响。为了解决这个问题，1992年Intel 在发布486处理器的时候，也同时提出了32-bit 的PCI（周边组件互连）总线。AGP 总线（Accelerated Graphics Port）PCI 总线是独立于CPU 的系统总线，可将显示卡、声卡、网卡、硬盘控制器等高速的外围设备直接挂在CPU 总线上，打破了瓶颈，使得CPU 的性能得到充分的发挥。可惜的是，由于PCI 总线只有133MB/s 的带宽，对付声卡、网卡、视频卡等绝大多数输入/输出设备也许显得绰绰有余，但对于胃口越来越大的3D 显卡却力不从心，并成为了制约显示子系统和整机性能的瓶颈。因此，PCI 总线的补充——AGP 总线就应运而生了。PCI-Express在经历了长达10年的修修补补，PCI 总线已经无法满足电脑性能提升的要求，必须由带宽更大、适应性更广、发展潜力更深的新一代总线取而代之，这就是PCI-Express 总线。相对于PCI总线来讲，PCI-Express总线能够提供极高的带宽，来满足系统的需求。PCI Express总线2.0标准的带宽如下表所示：PCI-E 2.0标准带宽经历着这么三代半（AGP总线只是一种增强型的PCI总线）的发展，PC的外部总线终于发展到PCI-E 4.0，提供了比以往总线大得多的带宽。至于今后总线发展的方向，相信会随着人们对带宽需要的不断增加，而很快来出现。专业术语播报编辑1.intermediate distribution bus中间分布总线2.VESA local bus (VL-bus) VESA局域总线3.analysis, bus bounce总线跳动分析4.analog summing bus模拟加法总线5.architecture, micro-channel bus (MCA)微通道总线（体系）结构6.arbitration bus判优总线7.arbiter, bus总线判优器8.backplane bus基架总线9.back-off, bus总线退出10.base bus基底总线11.bus-timing emulation总线时序仿真12.bus-intensive总线密集13.bus-control unit总线控制单元14.bus, utility公用程序总线15.bus, summing加法总线16.bus, realtime system integration (RTSIBus)即时系统综合总线17.bus, peripheral interface外设接口总线18.bus, multisystem extension interface (MXIbus)多系统延伸接口总线19.bus, multidrop parallel分支平行总线20.bus, micro-channel微通道总线

总线带宽：
概念简介播报编辑从电子电路角度出发，带宽（Bandwidth）本意指的是电子电路中存在一个固有通频带，各类复杂的电子电路无一例外都存在电感、电容或相当功能的储能元件，即使没有采用现成的电感线圈或电容，导线自身就是一个电感，而导线与导线之间、导线与地之间便可以组成电容——这就是通常所说的杂散电容或分布电容；不管是哪种类型的电容、电感，都会对信号起着阻滞作用从而消耗信号能量，严重的话会影响信号品质。这种效应与交流电信号的频率成正比关系，当频率高到一定程度、令信号难以保持稳定时，整个电子电路自然就无法正常工作。为此，电子学上就提出了带宽的概念，它指的是电路可以保持稳定工作的频率范围。而属于该体系的有显示器带宽、通讯/网络中的带宽等等。而第二种带宽的概念指的其实是数据传输率，譬如内存带宽、总线带宽、网络带宽等等，都是以字节/秒为单位。对于电子电路中的带宽，决定因素在于电路设计。它主要是由高频放大部分元件的特性决定，而高频电路的设计是比较困难的部分，成本也比普通电路要高很多。这部分内容涉及到电路设计的知识，对此我们就不做深入的分析。而对于总线、内存中的带宽，决定其数值的主要因素在于工作频率和位宽，在这两个领域，带宽等于工作频率与位宽的乘积，因此带宽和工作频率、位宽两个指标成正比。不过工作频率或位宽并不能无限制提高，它们受到很多因素的制约 [1]。总线带宽简介播报编辑在计算机系统中，总线的作用就好比是人体中的神经系统，它承担的是所有数据传输的职责，而各个子系统间都必须藉由总线才能通讯，例如，CPU和北桥间有前端总线、北桥与显卡间为AGP总线、芯片组间有南北桥总线，各类扩展设备通过PCI、PCI-X总线与系统连接；主机与外部设备的连接也是通过总线进行，如流行的USB 2.0、IEEE1394总线等等，一句话，在一部计算机系统内，所有数据交换的需求都必须通过总线来实现！按照工作模式不同，总线可分为两种类型，一种是并行总线，它在同一时刻可以传输多位数据，好比是一条允许多辆车并排开的宽敞道路，而且它还有双向单向之分；另一种为串行总线，它在同一时刻只能传输一个数据，好比只容许一辆车行走的狭窄道路，数据必须一个接一个传输、看起来仿佛一个长长的数据串，故称为“串行”。并行总线和串行总线的描述参数存在一定差别。对并行总线来说，描述的性能参数有以下三个：总线宽度、时钟频率、数据传输频率。其中，总线宽度就是该总线可同时传输数据的位数，好比是车道容许并排行走的车辆的数量；例如，16位总线在同一时刻传输的数据为16位，也就是2个字节；而32位总线可同时传输4个字节，64位总线可以同时传输8个字节......显然，总线的宽度越大，它在同一时刻就能够传输更多的数据。不过总线的位宽无法无限制增加。总线的带宽指的是这条总线在单位时间内可以传输的数据总量，它等于总线位宽与工作频率的乘积。例如，对于64位、800MHz的前端总线，它的数据传输率就等于64bit×800MHz÷8(Byte)=6.4GB/s；32位、33MHz PCI总线的数据传输率就是32bit×33MHz÷8=132MB/s，等等，这项法则可以用于所有并行总线上面——看到这里，读者应该明白我们所说的总线带宽指的就是它的数据传输率。对串行总线来说，带宽和工作频率的概念与并行总线完全相同，只是它改变了传统意义上的总线位宽的概念。在频率相同的情况下，并行总线比串行总线快得多，那么，为什么各类并行总线反而要被串行总线接替呢？原因在于并行总线虽然一次可以传输多位数据，但它存在并行传输信号间的干扰现象，频率越高、位宽越大，干扰就越严重，因此要大幅提高现有并行总线的带宽是非常困难的；而串行总线不存在这个问题，总线频率可以大幅向上提升，这样串行总线就可以凭借高频率的优势获得高带宽。而为了弥补一次只能传送一位数据的不足，串行总线常常采用多条管线（或通道）的做法实现更高的速度——管线之间各自独立，多条管线组成一条总线系统，从表面看来它和并行总线很类似，但在内部它是以串行原理运作的。对这类总线，带宽的计算公式就等于“总线频率×管线数”，这方面的例子有PCI Express和HyperTransport，前者有×1、×2、×4、×8、×16和×32多个版本，在第一代PCI Express技术当中，单通道的单向信号频率可达2.5GHz，我们以×16举例，这里的16就代表16对双向总线，一共64条线路，每4条线路组成一个通道，二条接收，二条发送。这样可以换算出其总线的带宽为2.5GHz×16/10=4GB/s（单向）。除10是因为每字节采用10位编码。内存带宽播报编辑除总线之外，内存也存在类似的带宽概念。其实所谓的内存带宽，指的也就是内存总线所能提供的数据传输能力，但它决定于内存芯片和内存模组而非纯粹的总线设计，加上地位重要，往往作为单独的对象讨论。SDRAM、DDR和DDRⅡ的总线位宽为64位，RDRAM的位宽为16位。而这两者在结构上有很大区别：SDRAM、DDR和DDRⅡ的64位总线必须由多枚芯片共同实现，计算方法如下：内存模组位宽=内存芯片位宽×单面芯片数量（假定为单面单物理BANK）；如果内存芯片的位宽为8位，那么模组中必须、也只能有8颗芯片，多一枚、少一枚都是不允许的；如果芯片的位宽为4位，模组就必须有16颗芯片才行，显然，为实现更高的模组容量，采用高位宽的芯片是一个好办法。而对RDRAM来说就不是如此，它的内存总线为串联架构，总线位宽就等于内存芯片的位宽。和并行总线一样，内存的带宽等于位宽与数据传输频率的乘积，例如，DDR400内存的数据传输频率为400MHz，那么单条模组就拥有64bit×400MHz÷8(Byte)=3.2GB/s的带宽；PC 800标准RDRAM的频率达到800MHz，单条模组带宽为16bit×800MHz÷ 8=1.6GB/s。为了实现更高的带宽，在内存控制器中使用双通道技术是一个理想的办法，所谓双通道就是让两组内存并行运作，内存的总位宽提高一倍，带宽也随之提高了一倍！带宽可以说是内存性能最主要的标志，业界也以内存带宽作为主要的分类标准，但它并非决定性能的要素，在实际应用中，内存延迟的影响并不亚于带宽。如果延迟时间太长的话相当不利，此时即便带宽再高也无济于事 [2]。带宽匹配播报编辑计算机系统中存在形形色色的总线，这不可避免带来总线速度匹配问题，其中最常出问题的地方在于前端总线和内存、南北桥总线和PCI总线。前端总线与内存匹配与否对整套系统影响最大，最理想的情况是前端总线带宽与内存带宽相等，而且内存延迟要尽可能低。在Pentium4刚推出的时候，Intel采用RDRAM内存以达到同前端总线匹配，但RDRAM成本昂贵，严重影响推广工作，Intel曾推出搭配PC133 SDRAM的845芯片组，但SDRAM仅能提供1.06GB/s的带宽，仅相当于400MHz前端总线带宽的1/3，严重不匹配导致系统性能大幅度下降；后来，Intel推出支持DDR266的845D才勉强好转，但仍未实现与前端总线匹配；接着，Intel将P4前端总线提升到533MHz、带宽增长至4.26GB/s，虽然配套芯片组可支持DDR333内存，可也仅能满足2/3而已；P4的前端总线提升到800MHz，而配套的865/875P芯片组可支持双通道DDR400——这个时候才实现匹配的理想状态，当然，这个时候继续提高内存带宽意义就不是特别大，因为它超出了前端总线的接收能力。南北桥总线带宽曾是一个尖锐的问题，早期的芯片组都是通过PCI总线来连接南北桥，而它所能提供的带宽仅仅只有133MB/s，若南桥连接两个ATA-100硬盘、100M网络、IEEE1394接口......区区133MB/s带宽势必形成严重的瓶颈，为此，各芯片组厂商都发展出不同的南北桥总线方案，如Intel的Hub-Link、VIA的V-Link、SiS 的MuTIOL，还有AMD的 HyperTransport等等，它们的带宽都大大超过了133MB/s，最高纪录已超过1GB/s，瓶颈效应已不复存在。PCI总线带宽不足还是比较大的矛盾，PC上使用的PCI总线均为32位、33MHz类型，带宽133MB/s，而这区区133MB/s必须满足网络、硬盘控制卡（如果有的话）之类的扩展需要，一旦使用千兆网络，瓶颈马上出现，业界打算自2004年开始以PCI Express总线来全面取代PCI总线，届时PCI带宽不足的问题将成为历史。显示器播报编辑以上我们所说的“带宽”指的都是速度概念，但对CRT显示器来说，它所指的带宽则是频率概念、属于电路范畴，更符合“带宽”本来的含义。要了解显示器带宽的真正含义，必须简单介绍一下CRT显示器的工作原理——由灯丝、阴极、控制栅组成的电子枪，向外发射电子流，这些电子流被拥有高电压的加速器加速后获得很高的速度，接着这些高速电子流经过透镜聚焦成极细的电子束打在屏幕的荧光粉层上，而被电子束击中的地方就会产生一个光点；光点的位置由偏转线圈产生的磁场控制，而通过控制电子束的强弱和通断状态就可以在屏幕上形成不同颜色、不同灰度的光点——在某一个特定的时刻，整个屏幕上其实只有一个点可以被电子束击中并发光。为了实现满屏幕显示，这些电子束必须从左到右、从上到下一个一个象素点进行扫描，若要完成800×600分辨率的画面显示，电子枪必须完成800×600=480000个点的顺序扫描。由于荧光粉受到电子束击打后发光的时间很短，电子束在扫描完一个屏幕后必须立刻再从头开始——这个过程其实十分短暂，在一秒钟时间电子束往往都能完成超过85个完整画面的扫描、屏幕画面更新85次，人眼无法感知到如此小的时间差异会“误以为”屏幕处于始终发亮的状态。而每秒钟屏幕画面刷新的次数就叫场频，或称为屏幕的垂直扫描频率、以Hz（赫兹）为单位，也就是我们俗称的“刷新率”。以800×600分辨率、85Hz刷新率计算，电子枪在一秒钟至少要扫描800×600×85=40800000个点的显示；如果将分辨率提高到1024×768，将刷新率提高到100Hz，电子枪要扫描的点数将大幅提高。按照业界公认的计算方法，显示器带宽指的就是显示器的电子枪在一秒钟内可扫描的最高点数总和，它等于“水平分辨率×垂直分辨率×场频（画面刷新次数）”，单位为MHz(兆赫)；由于显像管电子束的扫描过程是非线性的，为避免信号在扫描边缘出现衰减影响效果、保证图像的清晰度，总是将边缘扫描部分忽略掉，但在电路中它们依然是存在的。因此，我们在计算显示器带宽的时候还应该除一个取值为0.6~0.8 的“有效扫描系数”，故得出带宽计算公式如下：“带宽=水平像素（行数）×垂直像素（列数）×场频（刷新频率）÷扫描系数”。扫描系数一般取为0.744。例如，要获得分辨率1024×768、刷新率85Hz的画面，所需要的带宽应该等于：1024×768×85÷0.744，结果大约是90MHz。不过，这个定义并不符合带宽的原意，称之为“像素扫描频率”似乎更为贴切。带宽的 最初概念确实也是电路中的问题——简单点说就是：在“带宽”这个频率宽度之内，放大器可以处于良好的工作状态，如果超出带宽范围，信号会很快出现衰减失真现象。从本质上说，显示器的带宽描述的也是控制电路的频率范围，带宽高低直接决定显示器所能达到的性能等级。由于前文描述的“像素扫描频率”与控制电路的“带宽”基本是成正比关系，显示器厂商就干脆把它当作显示器的“带宽”——这种做法当然没有什么错，只是容易让人产生认识上的误区。当然，从用户的角度考虑没必要追究这么多，毕竟以“像素扫描频率”作为“带宽”是很合乎人们习惯的，大家可方便使用公式计算出达到某种显示状态需要的最低带宽数值。但是反过来说，“带宽数值完全决定着屏幕的显示状态”是否也成立呢？答案是不完全成立，因为屏幕的显示状态除了与带宽有关系之外，还与一个重要的概念相关——它就是“行频”。行频又称为“水平扫描频率”，它指的是电子枪每秒在荧光屏上扫描过的水平线数量，计算公式为：“行频=垂直分辨率×场频（画面刷新率）×1.07”，其中1.07为校正参数，因为显示屏上下方都存在我们看不到的区域。可见，行频是一个综合分辨率和刷新率的参数，行频越大，显示器就可以提供越高的分辨率或者刷新率。例如，1台17寸显示器要在1600×1200分辨率下达到75Hz的刷新率，那么带宽值至少需要221MHz，行频则需要96KHz，两项条件缺一不可；要达到这么高的带宽相对容易，而要达到如此高的行频就相当困难，后者成为主要的制约因素，而出于商业因素考虑，显示器厂商会突出带宽而忽略行频，这种宣传其实是一种误导。通讯带宽播报编辑在通讯和网络领域，带宽的含义又与上述定义存在差异，它指的是网络信号可使用的最高频率与最低频率之差、或者说是“频带的宽度”，也就是所谓的“Bandwidth”、“信道带宽”——这也是最严谨的技术定义。在100M以太网之类的铜介质布线系统中，双绞线的信道带宽通常用MHz为单位，它指的是信噪比恒定的情况下允许的信道频率范围，不过，网络的信道带宽与它的数据传输能力（单位Byte/s）存在一个稳定的基本关系。我们也可以用高速公路来作比喻：在高速路上，它所能承受的最大交通流量就相当于网络的数据运输能力，而这条高速路允许形成的宽度就相当于网络的带宽。显然，带宽越高、数据传输可利用的资源就越多，因而能达到越高的速度；除此之外，我们还可以通过改善信号质量和消除瓶颈效应实现更高的传输速度。网络带宽与数据传输能力的正比关系最早是由贝尔实验室的工程师Claude Shannon所发现，因此这一规律也被称为Shannon定律。而通俗起见普遍也将网络的数据传输能力与“网络带宽”完全等同起来，这样“网络带宽”表面上看与“总线带宽”形成概念上的统一，但这两者本质上就不是一个意思、相差甚远。总结播报编辑对总线和内存来说，带宽高低对系统性能有着举足轻重的影响——倘若总线、内存的带宽不够高的话，处理器的工作频率再高也无济于事，因此带宽可谓是与频率并立的两大性能决定要素。而对CRT显示器而言，带宽越高，往往可以获得更高的分辨率、显示精度越高，不过CRT显示器的带宽都能够满足标准分辨率下85Hz刷新率或以上的显示需要（相信没有太多的朋友喜欢用非常高的分辨率去运行程序或者游戏），这样带宽高低就不是一个太敏感的参数了，当然，如果你追求高显示品质那是另一回事了 [3]。

通用寄存器：
简介播报编辑通用寄存器可用于传送和暂存数据，也可参与算术逻辑运算，并保存运算结果。除此之外，它们还各自具有一些特殊功能。通用寄存器的长度取决于机器字长，汇编语言程序员必须熟悉每个寄存器的一般用途和特殊用途，只有这样，才能在程序中做到正确、合理地使用它们。16位cpu通用寄存器共有　8　个：AX,BX,CX,DX,BP,SP,SI,DI.八个寄存器都可以作为普通的数据寄存器使用。但有的有特殊的用途：AX为累加器，CX为计数器，BX，BP为基址寄存器，SI,DI为变址寄存器，BP还可以是基指针，SP为堆栈指针。32位cpu通用寄存器共有　8　个： EAX,EBX,ECX,EDX,EBP,ESP,ESI,EDI功能和上面差不多分类播报编辑数据寄存器数据寄存器主要用来保存操作数和运算结果等信息，从而节省读取操作数所需占用总线和访问存储器的时间。 [1]32位CPU有4个32位的通用寄存器EAX、EBX、ECX和EDX。对低16位数据的存取，不会影响高16位的数据。这些低16位寄存器分别命名为：AX、BX、CX和DX，它和先前的CPU中的寄存器相一致。  [1]4个16位寄存器又可分割成8个独立的8位寄存器(AX：AH-AL、BX：BH-BL、CX：CH-CL、DX：DH-DL)，每个寄存器都有自己的名称，可独立存取。程序员可利用数据寄存器的这种“可分可合”的特性，灵活地处理字/字节的信息。  [1]寄存器AX和AL通常称为累加器(Accumulator)，用累加器进行的操作可能需要更少时间。累加器可用于乘、除、输入/输出等操作，它们的使用频率很高； 寄存器BX称为基地址寄存器(Base Register)。它可作为存储器指针来使用； 寄存器CX称为计数寄存器(Count Register)。在循环和字符串操作时，要用它来控制循环次数；在位操作中，当移多位时，要用CL来指明移位的位数； 寄存器DX称为数据寄存器(Data Register)。在进行乘、除运算时，它可作为默认的操作数参与运算，也可用于存放I/O的端口地址。  [1]在16位CPU中，AX、BX、CX和DX不能作为基址和变址寄存器来存放存储单元的地址，但在32位CPU中，其32位寄存器EAX、EBX、ECX和EDX不仅可传送数据、暂存数据保存算术逻辑运算结果，而且也可作为指针寄存器，所以，这些32位寄存器更具有通用性。详细内容请见第3.8节——32位地址的寻址方式。 [1]变址寄存器32位CPU有2个32位通用寄存器ESI和EDI。其低16位对应先前CPU中的SI和DI，对低16位数据的存取，不影响高16位的数据。  [1]寄存器ESI、EDI、SI和DI称为变址寄存器(Index Register)，它们主要用于存放存储单元在段内的偏移量，用它们可实现多种存储器操作数的寻址方式(在第3章有详细介绍)，为以不同的地址形式访问存储单元提供方便。 变址寄存器不可分割成8位寄存器。作为通用寄存器，也可存储算术逻辑运算的操作数和运算结果。  [1]它们可作一般的存储器指针使用。在字符串操作指令的执行过程中，对它们有特定的要求，而且还具有特殊的功能。 [1]指针寄存器32位CPU有2个32位通用寄存器EBP和ESP。其低16位对应先前CPU中的SBP和SP，对低16位数据的存取，不影响高16位的数据。 寄存器EBP、ESP、BP和SP称为指针寄存器(Pointer Register)，主要用于存放堆栈内存储单元的偏移量，用它们可实现多种存储器操作数的寻址方式(在第3章有详细介绍)，为以不同的地址形式访问存储单元提供方便。指针寄存器不可分割成8位寄存器。作为通用寄存器，也可存储算术逻辑运算的操作数和运算结果。 [1]段寄存器段寄存器是根据内存分段的管理模式而设置的。内存单元的物理地址由段寄存器的值和一个偏移量组合而成的，这样可用两个较少位数的值组合成一个可访问较大物理空间的内存地址。 [1]指令指针寄存器32位CPU把指令指针扩展到32位，并记作EIP，EIP的低16位与先前CPU中的IP作用相同。 指令指针EIP、IP(Instruction Pointer)是存放下次将要执行的指令在代码段的偏移量。在具有预取指令功能的系统中，下次要执行的指令通常已被预取到指令队列中，除非发生转移情况。所以，在理解它们的功能时，不考虑存在指令队列的情况。 在实方式下，由于每个段的最大范围为64K，所以，EIP中的高16位肯定都为0，此时，相当于只用其低16位的IP来反映程序中指令的执行次序。 [1]主要用途播报编辑通用寄存器数据寄存器AX乘、除运算，字的输入输出，中间结果的缓存AL字节的乘、除运算，字节的输入输出，十进制算术运算AH字节的乘、除运算，存放中断的功能号BX存储器指针CX串操作、循环控制的计数器CL移位操作的计数器DX字的乘、除运算，间接的输入输出变址寄存器SI存储器指针、串指令中的源操作数指针DI存储器指针、串指令中的目的操作数指针分类示意图变址 寄存器BP存储器指针、存取堆栈的指针SP堆栈的栈顶指针指令指针IP/EIP标志位寄存器Flag/EFlag32位CPU的段寄存器16位CPU的段寄存器ES 附加段寄存器CS 代码段寄存器SS 堆栈段寄存器DS 数据段寄存器新增加的段寄存器FS 附加段寄存器GS 附加段寄存器相关信息播报编辑运算器结构寄存器是CPU内部重要的数据存储资源，用来暂存数据和地址，是汇编程序员能直接使用的硬件资源之一。由于寄存器的存取速度比内存快，所以，在用汇编语言编写程序时，要尽可能充分利用寄存器的存储功能。寄存器一般用来保存程序的中间结果，为随后的指令快速提供操作数，从而避免把中间结果存入内存，再读取内存的操作。在高级语言(如：C/C++语言)中，也有定义变量为寄存器类型的，这就是提高寄存器利用率的一种可行的方法。另外，由于寄存器的个数和容量都有限，不可能把所有中间结果都存储在寄存器中，所以，要对寄存器进行适当的调度。根据指令的要求，如何安排适当的寄存器，避免操作数过多的传送操作是一项细致而又周密的工作。

字长：
概念播报编辑计算机采用二进制编码方式表示数、字符、指令和其它控制信息。 [1]计算机在存储、传送或操作时，作为一个单元的一组二进制码称为字，一个字中的二进制位的位数称为字长。 [2]通常称处理字长为8位数据的CPU叫8位CPU，32位CPU就是在同一时间内处理字长为32位的二进制数据。二进制的每一个0或1是组成二进制的最小单位，称为位（bit）。常用的字长为8位、16位、32位和64位。字长为8位的编码称为字节，是计算机中的基本编码单位。 [3]字长与计算机的功能和用途有很大的关系，是计算机的一个重要技术指标。字长直接反映了一台计算机的计算精度，为适应不同的要求及协调运算精度和硬件造价间的关系，大多数计算机均支持变字长运算，即机内可实现半字长、全字长（或单字长）和双倍字长运算。在其他指标相同时，字长越大计算机的处理数据的速度就越快。早期的微机字长一般是8位和16位，386以及更高的处理器大多是32位。市面上的计算机的处理器大部分已达到64位。字长由微处理器对外数据通路的数据总线条数决定。 [3]通俗含义播报编辑字长是CPU的主要技术指标之一，指的是CPU一次能并行处理的二进制位数，通常PC机的字长为16位（早期），32位，64位。 [4]PC机可以通过编程的方法来处理任意大小的数字，但数字越大，PC机就要花越长的时间来计算。PC机在一次操作中能处理的最大数字是由PC机的字长确定的。 [5]我们先来看一下人脑是如何进行计算的，例如5×6则立即可以得到答案是30，但对于55×66，就不可能立即得到正确的答案，这就是说55或66已走出了人脑的“字长”，这是为了得出结果，就必须把复杂的问题（如55×66）分解成易于处理的问题（如55×66可分解为50×60，50×6，5×60，5×6），然后再综合起来，得出结果。 [5]字长同样PC机也是这样处理问题的，一台16位字长的PC机，可以直接处理2的16次方（65536）之内的数字，对于超过65536的数字就需要分解的方法来处理。32位pc机比16位机优越的原因就在于它在一次操作中能处理的数字大，32位字长的PC机能直接处理的数字高达40亿（2的32次方），能处理的的数字越大，则操作的次数就越少，从而系统的效率也就越高。 [5]CPU大多是64位的，但大多都以32位字长运行，都没能展示它的字长的优越性，因为它必须与64位软件（如64位的操作系统等）相辅相成，也就是说，字长受软件系统的制约，例如，在32位软件系统中64位字长的CPU只能当32位用。 [4]固定字长与可变字长播报编辑每一个储存位置都可以由其地址找到。但是每一储存位置的长度( length)尚未指定。 [6]在某些计算机中，每一储存位置是由固定的位数所组成的。每当计算机涉及到某一个储存位置时，即表示它要引用此一固定长度的位置，亦称为一个“字” ( word ) 。像此种型态的组织，我们称之为固定字长( fixed word length)或可定址字( word-address-able)。例如典型的迷你计算机，一个字长为16个位。 [6]可定址位元组另一些计算机，它的每个地址所引用是一个位元组或一个字。这种计算机，我们称之为可定址字( character-ddressable )或可定址位元组( byte-addresable )。右图所示即为此种储存体，因为这10个位元组的每一个位元组，皆可个别设定一个位址。 [6]至于可定址字元的计算机，经常被称为可变字长( variable word length )的机器。 [6]我们之所以称之“可变字长” ，乃是因为只要利用一个计算机已有的指令(如" add"或"move " )，它就可以去处理字数目为可变的字。但对固定字长的计算机而言，它所处理的字数目是由指合本身所指定的。 [6]可定址字元右图所示为固定字长与可变字长储存体组织的比较。图a所示为每字可存4个字元的固定字长组织。注意此种组织中，虽然是每4个字元形成一组，且可赋予一个地址，但是每个个别的字元却不能赋予位址。在图b的可定址字元或可变字长的组织中，计算机可将其中每一个字赋予一个位址。 [6]在图b中，假设要取出其中前5个字(即字母SANTA)时，需要分别引用5个位址。但实际上，并不需要如此。因为有一种可变字长指令，可让你一次就取出一组的字。在指令中，你只要第一个字元的位址，然后再指定一共要取出几个字元即可。例如，在图b ，一个读取字母SANTA的指令，只要指定第一个字元的位址( 001 )及所要读取的字数(5) ，则此5个字元即可被读出。 [6]可变字长组织其主要优点为储存体的使用效率高；即，只需使用与字数一样的位置即可储存该组字(注：如果想储存SANTA这一组字，只需使用5个位置即可)。然而，固定字长的组织可能会有浪费内存空间的现象。例如图a中的第三个字(位址为003 )仅被利用一半，而其另一半则未被使用。 [6]双倍字长播报编辑双倍字长是指计算机内部参与运算的数的位数。它决定着计算机内部寄存器、ALU和数据总线的位数，直接影响着机器的硬件规模和造价。双倍字长直接反映了一台计算机的计算精度，为适应不同的要求及协调运算精度和硬件造价间的关系，大多数计算机均支持变字长运算，即机内可实现半字长、全字长（或单字长）和双倍字长运算。 [7]微型机的字长通常为4位、8位、16位和32位，64位字长的高性能微型计算机也已推出。 [7]双倍字长对计算机计算精度的影响： [7]4位字长：2^4=16；16位字长：2^16=65536=64K32位字长：2^32=4,294,967,296=4G；64位字长：2^64≈1.8445×10^19数据总线DB用于传送数据信息。数据总线是双向三态形式的总线，即他既可以把CPU的数据传送到存储器或I/O接口等其它部件，也可以将其它部件的数据传送到CPU。数据总线的位数是微型计算机的一个重要指标，通常与微处理的字长相一致。例如Intel8086微处理器字长16位，其数据总线宽度也是16位。需要指出的是，数据的含义是广义的，它可以是真正的数据，也可以指令代码或状态信息，有时甚至是一个控制信息，因此，在实际工作中，数据总线上传送的并不一定仅仅是真正意义上的数据。 [7]地址总线AB是专门用来传送地址的，由于地址只能从CPU传向外部存储器或I/O端口，所以地址总线总是单向三态的，这与数据总线不同。地址总线的位数决定了CPU可直接寻址的内存空间大小，比如8位微机的地址总线为16位，则其最大可寻址空间为2^16=64KB，16位微型机的地址总线为20位，其可寻址空间为2^20=1MB。一般来说，若地址总线为n位，则可寻址空间为2^（n-10）千字节。 [7]控制总线CB用来传送控制信号和时序信号。控制信号中，有的是微处理器送往存储器和I/O接口电路的，如读/写信号，片选信号、中断响应信号等；也有是其它部件反馈给CPU的，比如：中断申请信号、复位信号、总线请求信号、限备就绪信号等。因此，控制总线的传送方向由具体控制信号而定，一般是双向的，控制总线的位数要根据系统的实际控制需要而定。实际上控制总线的具体情况主要取决于CPU。 [7]字长的选择播报编辑在设计计算机时，字长的选择是非常重要的。设计上的考虑倾向于为特定的用途（如地址）设定特定的位长。然而，出于经济的考虑，又应该仅使用一种尺寸，或者很少的几种与基本尺寸成倍数或分数（约数）关系的尺寸。这个首选的基本尺寸就成为该构架的字长。 [8]字符的尺寸对于字长的选择也有影响。20世纪60年代中期以前，字符大部分以6位存储；这样最多允许64个字符，因此不能又大写字符。由于将字长定义成字符尺寸的倍数在处理时间和存储空间上都比较划算，所以这个时期字长也就被定义为6位（在二进制机器上）的倍数。通常的选择是36位字长，这也是适合于浮点数格式的一个长度。 [8]随着IBM360系统的引入——该系统使用8位字符，并支持大小写字母——标准的字符（确切地说：字节）尺寸也转变成为8位。从那以后，字长也自然变成了8的倍数，16、32、64位字长被广泛使用。 [8]各种字长的架构早期的计算机设计中包括所谓的“可变字长”设计。（原文：Early machine designs included some that used what is often termed avariable word length.——译者）。在这类设计中，数字操作数没有固定的长度，它们通过检查某个特殊字符来判断是否结束。这样的机器使用BCD编码表示数字，例如IBM 702、IBM 705、IBM 7080、IBM 7010、UNIVAC 1050、IBM 1401和IBM 1620。 [8]大部分这样的机器一次处理一个存储单元，因为每条指令和数据占用的数个单元，所以指令将使用数个周期来读取存储器。这类机器经常因为这个原因变得非常慢。例如，在IBM 1620 Model I上，取指令需要8个周期，只是为了读取12个数字（Model II降低到6个周期，不过如果指令不需要取其中的一个1个地址域的话，可以只需要4个周期；如果两个都不需要，则只需要1个周期）。 [8]字和字节编址字长对计算机构架的存储器模式有很大的影响。特别是：通常选择字作为存储器的编址方案，所谓存储器编址方案就是地址码能够指定的最小存储单位。编号相邻的存储器字组，其地址编号相差一。在计算机中这样很自然，因为它通常总是要处理以字为单位的数据（或者是以字的倍数）。并且具有让指令可以使用最小的长度来指定一个地址的优点，这样，就可以减少指令长度或者可以定义更多的指令条数。 [8]当计算机很大的工作量是用来处理字节时，通常定义字节作为地址编址单位要比字更好。这样做字符串中的单个字符可以通过地址直接指定。当然，一个字仍然可以被地址访问，但是比起字编址方案，它的地址将使用更多的位数。在这种组织结构中，字长需要被定义为字符长度的整数倍。这种编址方案在IBM 360中被使用，此后即变成计算机设计中最普遍的方案。 [8]2的幂数据常常要占用不同大小的存储空间，例如，有些数值比其他的数值要求有更高的精度。通常使用的长度是编址单位（以字为单位编址或以字节为单位编址）的倍数，这个倍数常常是的2的幂。这样做是比较便利的，因为这样的话，将一个处理对象在数组中的索引值转化为这个处理对象的地址只需要进行一个移位操作（这在硬件上只需要进行布线的变化）而不需要进行乘操作。某些时候这样的做法还可以避免除操作。因此，一些现代计算机设计使用的字长（或者其他的操作数）是2的幂乘以字节尺寸。 [8]字长表播报编辑年份计算机架构字长整数长度浮点数长度长度指令编址单位字符长度1941Zuse Z322 b–w8 bw–1942ABC50 bw––––1944Harvard Mark I23 dw–24 b––1946（1948）{1953}ENIAC（w/Panel #16）{w/Panel #26}10 dw, 2w（w）{w}––（2d, 4d, 6d, 8d）––{w}–1951UNIVAC I12 dw–½ww1 d1952IAS machine40 bw–½ww5 b1952IBM 70136 b½w,w–½w½w,w6 b1952UNIVAC 60nd1d, ... 10d–––2d, 3d1953IBM 702nd0d, ... 511d–5dd1 d1953UNIVAC 120nd1d, ... 10d–––2d, 3d1954（1955）IBM 650（w/IBM 653）10 dw–（w）ww2 d1954IBM 70436 bwwww6 b1954IBM 705nd0d, ... 255d–5dd1 d1954IBM NORC16 dww, 2www–1956IBM 305nd1d, ... 100d–10dd1 d1958UNIVAC II12 dw–½ww1 d1958SAGE32 b½w–ww6 b1958Autonetics Recomp II40 bw, 79 b, 8d, 15d2w½w½w,w5 b1959IBM 1401nd1d, ...–d, 2d, 4d, 5d, 7d, 8dd1 d1959（TBD）IBM 1620nd2d, ...–（4d, ... 102d）12dd2 d1960LARC12 dw, 2ww, 2www2 d1960IBM 1410nd1d, ...–d, 2d, 6d, 7d, 11d, 12dd1 d1960IBM 707010 dwwww,d2 d1960PDP-118 bw–ww6 b1961IBM 7030（Stretch）64 b1b, ... 64b,1d, ... 16dw½w,wb, ½w,w1 b, ... 8 b1961IBM 7080nd0d, ... 255d–5dd1 d1962UNIVAC III25 b, 6 dw, 2w, 3w, 4w–ww6 b1962UNIVAC 110736 b/6w, ⅓w, ½w,wwww6 b1962IBM 7010nd1d, ...–d, 2d, 6d, 7d, 11d, 12dd1 d1962IBM 709436 bww, 2www6 b1963Gemini Guidance Computer39 b26 b–13 b13 b, 26 b–1963（1966）Apollo Guidance Computer15 bw–w, 2ww–1964CDC 660060 bww¼w, ½ww6 b1965IBM 36032 b½w,w,1d, ... 16dw, 2w½w,w, 1½w8 b8 b1965UNIVAC 110836 b/6w, ¼w, ⅓w, ½w,w, 2ww, 2www6 b, 9 b1965PDP-812 bw–ww8 b1970PDP-1116 bw2w, 4ww, 2w, 3w8 b8 b1971Intel 40044 bw,d–2w, 4ww–1972Intel 80088 bw, 2d–w, 2w, 3ww8 b1972Calcomp 9009 bw–w, 2ww8 b1974Intel 80808 bw, 2w, 2d–w, 2w, 3ww8 b1975Cray-164 b24 b,ww¼w, ½ww8 b1975Motorola 68008 bw, 2d–w, 2w, 3ww8 b1975MOS Tech. 6501MOS Tech. 65028 bw, 2d–w, 2w, 3ww8 b1976Zilog Z808 bw, 2w, 2d–w, 2w, 3w, 4w, 5ww8 b1978（1980）Intel 8086（w/Intel 8087）16 b½w,w, 2d（w, 2w, 4w）–（2w, 4w, 5w, 17d）½w,w, ... 7w8 b8 b1978VAX-11/78032 b¼w, ½w,w, 1d, ... 31d, 1b, ... 32bw, 2w¼w, ... 14¼w8 b8 b1979Motorola 6800032 b¼w, ½w,w, 2d–½w,w, ... 7½w8 b8 b1982（1983）Motorola 68020（w/Motorola 68881）32 b¼w, ½w,w, 2d–（w, 2w, 2½w）½w,w, ... 7½w8 b8 b1985ARM132 bw–w8 b8 b1985MIPS32 b¼w, ½w,ww, 2ww8 b8 b1989Intel 8048616 b½w,w, 2dw, 2w, 4w2w, 4w, 5w, 17d½w,w, ... 7w8 b8 b1989Motorola 6804032 b¼w, ½w,w, 2dw, 2w, 2½w½w,w, ... 7½w8 b8 b1991PowerPC32 b¼w, ½w,ww, 2ww8 b8 b2000IA-6464 b8 b, ¼w, ½w,w½w,w41 b8 b8 b2002XScale32 bww, 2w½w,w8 b8 b说明：b:位, d: 10进制数,w:该构架的字长,n:变量长度（variable size）有关术语播报编辑字在计算机中，一串数码作为一个整体来处理或运算的，称为一个计算机字，简称字，字反映计算机一次并行处理的一组二进制数。字通常分为若干个字节(每个字节一般是8位)。在存储器中，通常每个单元存储一个字，因此每个字都是可以寻址的。字的长度用位数来表示。 [9]在计算机的运算器、控制器中，通常都是以字为单位进行传送的。字在不同的地址出现，其含义是不相同。例如，送往控制器去的字是指令，而送往运算器去的字就是一个数。 [9]字节字节是指一小组相邻的二进制数码。通常是8位作为一个字节。它是构成信息的一个小单位，并作为一个整体来参加操作，比字小，是构成字的单位。在微型计算机中，通常用多少字节来表示存储器的存储容量。 [9]

RAM：
简介播报编辑存储器是数字系统中用以存储大量信息的设备或部件，是计算机和数字设备中的重要组成部分。存储器可分为随机存取存储器（RAM）和只读存储器（ROM）两大类。随机存取存储器（RAM）既可向指定单元存入信息又可从指定单元读出信息。任何RAM中存储的信息在断电后均会丢失，所以RAM是易失性存储器。ROM为只读存储器，除了固定存储数据、表格、固化程序外，在组合逻辑电路中也有着广泛用途。 [2]特点播报编辑随机存取静态随机存取存储器所谓“随机存取”，指的是当存储器中的数据被读取或写入时，所需要的时间与这段信息所在的位置或所写入的位置无关。相对的，读取或写入顺序访问（Sequential Access）存储设备中的信息时，其所需要的时间与位置就会有关系。它主要用来存放操作系统、各种应用程序、数据等。 [3]当RAM处于正常工作时，可以从RAM中读出数据，也可以往RAM中写入数据。与ROM相比较，RAM的优点是读/写方便、使用灵活，特别适用于经常快速更换数据的场合。 [4]易失性当电源关闭时，RAM不能保留数据。如果需要保存数据，就必须把它们写入一个长期的存储设备中（例如硬盘）。 [3]RAM的工作特点是通电后，随时可在任意位置单元存取数据信息，断电后内部信息也随之消失。 [5]对静电敏感正如其他精细的集成电路，随机存取存储器对环境的静电荷非常敏感。静电会干扰存储器内电容器的电荷，引致数据流失，甚至烧坏电路。故此触碰随机存取存储器前，应先用手触摸金属接地。 [3]访问速度现代的随机存取存储器几乎是所有访问设备中写入和读取速度最快的，存取延迟和其他涉及机械运作的存储设备相比，也显得微不足道。 [3]需要刷新现代的随机存取存储器依赖电容器存储数据。电容器充满电后代表1（二进制），未充电的代表0。由于电容器或多或少有漏电的情形，若不作特别处理，数据会渐渐随时间流失。刷新是指定期读取电容器的状态，然后按照原来的状态重新为电容器充电，弥补流失了的电荷。需要刷新正好解释了随机存取存储器的易失性。 [3]组成播报编辑RAM工作原理RAM由存储矩阵、地址译码器、读/写控制器、输入/输出、片选控制等几部分组成。 [6]（1）存储矩阵。如图所示，RAM的核心部分是一个寄存器矩阵，用来存储信息，称为存储矩阵。 [6]（2）地址译码器。地址译码器的作用是将寄存器地址所对应的二进制数译成有效的行选信号和列选信号，从而选中该存储单元。 [6]（3）读/写控制器。访问RAM时，对被选中的寄存器进行读操作还是进行写操作，是通过读写信号来进行控制的。读操作时，被选中单元的数据经数据线、输入/输出线传送给CPU（中央处理单元）；写操作时，CPU将数据经输入/输岀线、数据线存入被选中单元。 [6]（4）输入/输出。RAM通过输入/输岀端与计算机的CPU交换数据，读出时它是输岀端，写入时它是输入端，一线两用。由读/写控制线控制。输入/输出端数据线的条数，与一个地址中所对应的寄存器位数相同，也有的RAM芯片的输入/输出端是分开的。通常RAM的输出端都具有集电极开路或三态输出结构。 [6]（5）片选控制。由于受RAM的集成度限制。一台计算机的存储器系统往往由许多RAM组合而成。CPU访问存储器时，一次只能访问RAM中的某一片（或几片），即存储器中只有一片（或几片）RAM中的一个地址接受CPU访问，与其交换信息，而其他片RAM与CPU不发生联系，片选就是用来实现这种控制的。通常一片RAM有一根或几根片选线，当某一片的片选线接入有效电平时，该片被选中，地址译码器的输出信号控制该片某个地址的寄存器与CPU接通；当片选线接入无效电平时，则该片与CPU之间处于断开状态。 [6]类别播报编辑根据存储单元的工作原理不同， RAM分为静态RAM和动态RAM。 [7]静态随机存储器静态存储单元是在静态触发器的基础上附加门控管而构成的。因此，它是靠触发器的自保功能存储数据的。SRAM存放的信息在不停电的情况下能长时间保留，状态稳定，不需外加刷新电路，从而简化了外部电路设计。但由于SRAM的基本存储电路中所含晶体管较多，故集成度较低，且功耗较大。 [7]SRAM特点如下：●存储原理：由触发器存储数据。 [8]●单元结构：六管NMOS或OS构成。 [8]●优点：速度快、使用简单、不需刷新、静态功耗极低；常用作Cache。 [8]●缺点：元件数多、集成度低、运行功耗大。 [8]●常用的SRAM集成芯片：6116（2K×8位），6264（8K×8位），62256（32K×8位），2114（1K×4位）。 [8]动态随机存储器DRAM利用电容存储电荷的原理保存信息，电路简单，集成度高。由于任何电容都存在漏电，因此，当电容存储有电荷时，过一段时间由于电容放电会导致电荷流失，使保存信息丢失。解决的办法是每隔一定时间（一般为2ms）须对DRAM进行读出和再写入，使原处于逻辑电平“l”的电容上所泄放的电荷又得到补充，原处于电平“0”的电容仍保持“0”，这个过程叫DRAM的刷新。 [7]DRAM的刷新操作不同于存储器读/写操作，主要表现在以下几点：（1）刷新地址由刷新地址计数器产生，不是由地址总线提供。 [7]（2）DRAM基本存储电路可按行同时刷新，所以刷新只需要行地址，不需要列地址。 [7]（3）刷新操作时存储器芯片的数据线呈高阻状态，即片内数据线与外部数据线完全隔离。 [7]DRAM与SRAM相比具有集成度高、功耗低、价格便宜等优点，所以在大容量存储器中普遍采用。DRAM的缺点是需要刷新逻辑电路，且刷新操作时不能进行正常读，写操作。 [7]DRAM特点如下：●存储原理：利用MOS管栅极电容可以存储电荷的原理，需刷新（早期：三管基本单元；之后：单管基本单元）。 [8]●刷新（再生）：为及时补充漏掉的电荷以避免存储的信息丢失，必须定时给栅极电容补充电荷的操作。 [8]●刷新时间：定期进行刷新操作的时间。该时间必须小于栅极电容自然保持信息的时间（小于2ms）。 [8]●优点： 集成度远高于SRAM、功耗低，价格也低。 [8]●缺点：因需刷新而使外围电路复杂；刷新也使存取速度较SRAM慢，所以在计算机中，DRAM常用于作主存储器。 [8]尽管如此，由于DRAM存储单元的结构简单，所用元件少，集成度高，功耗低，所以已成为大容量RAM的主流产品。 [8]相关概念播报编辑与只读存储器区别动态随机存取存储器在计算机中，RAM 、ROM都是数据存储器。RAM 是随机存取存储器，它的特点是易挥发性，即掉电失忆。ROM 通常指固化存储器（一次写入，反复读取），它的特点与RAM 相反。举个例子来说也就是，如果突然停电或者没有保存就关闭了文件，那么ROM可以随机保存之前没有储存的文件但是RAM会使之前没有保存的文件消失。 [8]与内存之间的关系笔记本电脑内存在计算机的组成结构中，有一个很重要的部分，就是存储器。存储器是用来存储程序和数据的部件，对于计算机来说，有了存储器，才有记忆功能，才能保证正常工作。存储器的种类很多，按其用途可分为主存储器和辅助存储器，主存储器又称内存储器（简称内存），辅助存储器又称外存储器（简称外存）。外存通常是磁性介质或光盘，像硬盘，软盘，磁带，CD等，能长期保存信息，并且不依赖于电来保存信息，但是由机械部件带动，速度与CPU相比就显得慢的多。内存指的就是主板上的存储部件，是CPU直接与之沟通，并用其存储数据的部件，存放当前正在使用（即执行中）的数据和程序，它的物理实质就是一组或多组具备数据输入输出和数据存储功能的集成电路，内存只用于暂时存放程序和数据，一旦关闭电源或发生断电，其中的程序和数据就会丢失。 [8]快速周期随机存取存储器从一有计算机开始，就有内存。内存发展到今天也经历了很多次的技术改进，从最早的DRAM一直到FPMDRAM、EDODRAM、SDRAM等，内存的速度一直在提高且容量也在不断的增加。今天，服务器主要使用的是什么样的内存？IA架构的服务器普遍使用的是Registered ECC SDRAM。 [8]既然内存是用来存放当前正在使用的（即执行中）的数据和程序，那么它是怎么工作的？我们平常所提到的计算机的内存指的是动态内存（即DRAM），动态内存中所谓的“动态”，指的是当我们将数据写入DRAM后，经过一段时间，数据会丢失，因此需要额外设一个电路进行内存刷新操作。 [8]

指令周期：
基本概念播报编辑指令周期，读取－执行周期（fetch-and-execute cycle）是指CPU要执行指令经过的步骤。计算机之所以能自动地工作，是因为CPU能从存放程序的内存里取出一条指令并执行这条指令；紧接着又是取指令，执行指令，如此周而复始，构成了一个封闭的循环。除非遇到停机指令，否则这个循环将一直继续下去。指令周期 :CPU从内存取出一条指令并执行这条指令的时间总和。CPU周期 :又称机器周期，CPU访问一次内存所花的时间较长，因此用从内存读取一条指令字的最短时间来定义。时钟周期: 通常称为节拍脉冲或T周期。一个CPU周期包含若干个时钟周期。 [1]类别播报编辑非访内指令CLA是一条非访内指令，它需要两个CPU 周期，其中取指令阶段需要一个CPU周期，执行指令阶段需要一个CPU周期。1、取指令阶段(1)程序计数器PC的内容20(八进制)被装入地址寄存器AR；(2)程序计数器内容加1，变成21，为取下一条指令做好准备；(3)地址寄存器的内容被放到地址总线上；(4)所选存储器单元20的内容经过数据总线，传送到数据缓冲寄存器DR；(5)缓冲寄存器的内容传送到指令寄存器IR；(6)指令寄存器中的操作码被译码或测试；(7)CPU识别出是指令CLA，至此，取指令阶段即告结束。2、执行指令阶段(1)操作控制器送一控制信号给算术逻辑运算单元ALU；(2)ALU响应该控制信号，将累加寄存器AC的内容全部清零，从而执行了CLA指令。 [1]取数指令1.送操作数地址第二个CPU周期主要完成送操作数地址。在此阶段，CPU的动作只有一个，那就是把指令寄存器中的地址码部分(30)装入地址寄存器，其中30为内存中存放操作数的地址。2.两操作数相加第三个CPU周期主要完成取操作数并执行加法操作中。在此阶段，CPU完成如下动作：(1)把地址寄存器中的操作数的地址发送到地址总线上。(2)由存储器单元30中读出操作数，并经过数据总线传送到缓冲寄存器。(3)执行加操作：由数据缓冲寄存器来的操作数可送往ALU 的一个输入端，已等候在累加器内的另 一个操作数(因为CLA指令执行结束后累加器内容为零)送往ALU的另一输入端，于是ALU将两数相加，产生运算结果为0+6=6。这个结果放回累加器，替换了累加器中原先的数0 。 [1]存数指令STA指令的指令周期由三个CPU周期组成。1.送操作数地址在执行阶段的第一个CPU周期中，CPU完成的动作是把指令寄存器中地址码部分的形式地址40装到地址寄存器。其中数字40是操作数地址。2.存储和数执行阶段的第二个CPU周期中，累加寄存器的内容传送到缓冲寄存器，然后再存入到所选定的存储单元(40)中。CPU完成如下动作：(1)累加器的内容被传送到数据缓冲寄存器DR；(2)把地址寄存器的内容发送到地址总线上，即为将要存入的数据6的内存单元号；(3)把缓冲寄存器的内容发送到数据总线上；(4)数据总线上的数写入到所选中的存储器单元中，即将数6写入到存储器40号单元中。注意 在这个操作之后，累加器中仍然保留和数6，而存储器40号单元中原先的内容被冲掉 。 [1]空操作指令第四条指令即“NOP”指令，这是一条空操作指令。其中第一个CPU周期中取指令，CPU把23号单元的“NOP”指令取出放到指令寄存器，第二个CPU周期中执行该指令。因译码器译出是“NOP”指令，第二个CPU周期中操作控制器不发出任何控制信号。NOP指令可用来调机之用。1.第一个CPU周期（取指令阶段）CPU把24号单元的“JMP 21”指令取出放至指令寄存器，同时程序计数器内容加1，变为25，从而取下一条指令做好准备。2.第二个CPU周期（执行阶段）CPU把指令寄存器中地址码部分21送到程序计数器，从而用新内容21代替PC原先的内容25。这样，下一条指令将不从25单元读出，而是从内存21单元开始读出并执行，从而改变了程序原先的执行顺序。注意 执行“JMP 21”指令时，我们此处所给的四条指令组成的程序进入了死循环，除非人为停机，否则这个程序将无休止地运行下去，因而内存单元40中的和数将一直不断地发生变化。当然，我们此处所举的转移地址21是随意的，仅仅用来说明转移指令能够改变程序的执行顺序而已。 [1]特点介绍播报编辑指令不同，所需的机器周期数也不同。对于一些简单的的单字节指令，在取指令周期中，指令取出到指令寄存器后，立即译码执行，不再需要其它的机器周期。对于一些比较复杂的指令，例如转移指令、乘法指令，则需要两个或者两个以上的机器周期。 [2]从指令的执行速度看，单字节和双字节指令一般为单机器周期和双机器周期，三字节指令都是双机器周期，只有乘、除指令占用4个机器周期。因此在进行编程时，在完成相同工作的情况下，选用占用机器周期少的命令会提高程序的执行速率，尤其是在编写大型程序程序的时候，其效果更加明显。 [2]

位宽：
简介播报编辑显存位宽是显存在一个时钟周期内所能传送数据的位数，位数越大则瞬间所能传输的数据量越大，这是显存的重要参数之一。显存带宽=显存频率X显存位宽/8，那么在显存频率相当的情况下，显存位宽将决定显存带宽的大小。同样显存频率为500MHz的128位和256位显存，那么它俩的显存带宽将分别为：128位=500MHz*128∕8=8GiB/s，而256位=500MHz*256∕8=16GiB/s，是128位的2倍，可见显存位宽在显存数据中的重要性。显卡的显存是由一块块的显存芯片构成的，显存总位宽同样也是由显存颗粒的位宽组成，显存位宽=显存颗粒位宽×显存颗粒数。显存颗粒上都带有相关厂家的内存编号，可以去网上查找其编号，就能了解其位宽，再乘以显存颗粒数，就能得到显卡的位宽。 [1]分类播报编辑市场上的常见显存位宽有128位、192位、256位、384位、512位和1024位六种，人们习惯上叫的128位、256位显卡、384位显卡、512位显卡和1024位显卡就是指其相应的显存位宽。显存位宽越高，性能越好价格也就越高，因此中高位宽的显存更多应用于高端显卡，而普通显卡基本都采用128位显宽，而1024位显卡属于顶级了。鉴别播报编辑有一个较为简便的方法，但只适应于一般情况，存在一些特殊情况，在大部分情况下能适用。显存的封装形式主要有TSOP和BGA两种，一般情况下BGA封装的显存是32位/颗的，而TSOP封装的颗粒是16位/颗的。如果显卡采用了四颗BGA封装的显存，那么它的位宽是128位的，而如果是八颗TSOP封装颗粒，那么位宽也是128位的，但如果显卡只采用了四颗TSOP封装颗粒，那么显存位宽就只有64位。这只是一个一般情况下的技巧，不一定符合所有的情况，要做到最为准确的判断，还是查看显存编号！

SDRAM：
演变播报编辑SDRAM从发展到现在已经经历了五代，分别是：第一代SDR SDRAM，第二代DDR SDRAM，第三代DDR2 SDRAM，第四代DDR3 SDRAM，第五代，DDR4 SDRAM。第一代SDRAM采用单端（Single-Ended）时钟信号,第二代、第三代与第四代由于工作频率比较快，所以采用可降低干扰的差分时钟信号作为同步时钟。SDR SDRAM的时钟频率就是数据存储的频率，第一代内存用时钟频率命名，如pc100，pc133则表明时钟信号为100或133MHz，数据读写速率也为100或133MHz。之后的第二，三，四代DDR（Double Data Rate）内存则采用数据读写速率作为命名标准，并且在前面加上表示其DDR代数的符号，PC-即DDR，PC2=DDR2，PC3=DDR3。如PC2700是DDR333，其工作频率是333/2=166MHz，2700表示带宽为2.7G。DDR的读写频率从DDR200到DDR400，DDR2从DDR2-400到DDR2-800，DDR3从DDR3-800到DDR3-1600。很多人将SDRAM错误的理解为第一代也就是 SDR SDRAM，并且作为名词解释，皆属误导。SDR不等于SDRAM。Pin:模组或芯片与外部电路连接用的金属引脚，而模组的pin就是常说的“金手指”。SIMM：Single In-line Memory Module,单列内存模组。内存模组就是我们常说的内存条，所谓单列是指模组电路板与主板插槽的接口只有一列引脚（虽然两侧都有金手指）。DIMM：Double In-line Memory Module，双列内存模组。是我们常见的模组类型，所谓双列是指模组电路板与主板插槽的接口有两列引脚，模组电路板两侧的金手指对应一列引脚。RIMM：registered DIMM，带寄存器的双线内存模块，这种内存槽只能插DDR或Rambus内存。SO-DIMM:笔记本常用的内存模组。工作电压：SDR：3.3VDDR：2.5VDDR2：1.8VDDR3：1.5VDDR4：1.2VSDRSDRAM内存条的金手指通常是168线，而DDR SDRAM内存条的金手指通常是184线的。几代产品金手指的缺口数及缺口位置也不同有效防止反插与错插，SDR SDRAM有两个缺口，DDR只有一个缺口。关系播报编辑结构、时序与性能的关系一、影响性能的主要时序参数所谓的影响性能是并不是指SDRAM的带宽，频率与位宽固定后，带宽也就不可更改了。但这是理想的情况，在内存的工作周期内，不可能总处于数据传输的状态，因为要有命令、寻址等必要的过程。但这些操作占用的时间越短，内存工作的效率越高，性能也就越好。非数据传输时间的主要组成部分就是各种延迟与潜伏期。通过上文的讲述，大家应该很明显看出有三个参数对内存的性能影响至关重要，它们是tRCD、CL和tRP。每条正规的内存模组都会在标识上注明这三个参数值，可见它们对性能的敏感性。以内存最主要的操作——读取为例。tRCD决定了行寻址（有效）至列寻址（读/写命令）之间的间隔，CL决定了列寻址到数据进行真正被读取所花费的时间，tRP则决定了相同L-Bank中不同工作行转换的速度。现在可以想象一下读取时可能遇到的几种情况（分析写入操作时不用考虑CL即可）：1．要寻址的行与L-Bank是空闲的。也就是说该L-Bank的所有行是关闭的，此时可直接发送行有效命令，数据读取前的总耗时为tRCD+CL，这种情况我们称之为页命中（PH，Page Hit）。2．要寻址的行正好是前一个操作的工作行，也就是说要寻址的行已经处于选通有效状态，此时可直接发送列寻址命令，数据读取前的总耗时仅为CL，这就是所谓的背靠背（Back to Back）寻址，我们称之为页快速命中（PFH，Page Fast Hit）或页直接命中（PDH，Page Direct Hit）。3．要寻址的行所在的L-Bank中已经有一个行处于活动状态（未关闭），这种现象就被称作寻址冲突，此时就必须要进行预充电来关闭工作行，再对新行发送行有效命令。结果，总耗时就是tRP+tRCD+CL，这种情况我们称之为页错失（PM，Page Miss）。显然，PFH是最理想的寻址情况，PM则是最糟糕的寻址情况。上述三种情况发生的机率各自简称为PHR——PH Rate、PFHR——PFH Rate、PMR——PM Rate。因此，系统设计人员（包括内存与北桥芯片）都尽量想提高PHR与PFHR，同时减少PMR，以达到提高内存工作效率的目的。二、增加PHR的方法显然，这与预充电管理策略有着直接的关系，目前有两种方法来尽量提高PHR。自动预充电技术就是其中之一，它自动的在每次行操作之后进行预充电，从而减少了日后对同一L-Bank不同行寻址时发生冲突的可能性。但是，如果要在当前行工作完成后马上打开同一L-Bank的另一行工作时，仍然存在tRP的延迟。怎么办？ 此时就需要L-Bank交错预充电了。VIA的4路交错式内存控制就是在一个L-Bank工作时，对下一个要工作的L-Bank进行预充电。这样，预充电与数据的传输交错执行，当访问下一个L-Bank时，tRP已过，就可以直接进入行有效状态了。目前VIA声称可以跨P-Bank进行16路内存交错，并以LRU算法进行预充电管理。有关L-Bank交错预充电（存取）的具体执行在本刊2001年第2期已有详细介绍，这里就不再重复了。L-Bank交错自动预充电/读取时序图（可点击放大）：L-Bank 0与L-Bank 3实现了无间隔交错读取，避免了tRP对性能的影响。三、增加PFHR的方法无论是自动预充电还是交错工作的方法都无法消除tRCD所带来的延迟。要解决这个问题，就要尽量让一个工作行在进行预充电前尽可能多的接收多个工作命令，以达到背靠背的效果，此时就只剩下CL所造成的读取延迟了（写入时没有延迟）。如何做到这一点呢？这就是北桥芯片的责任了。在上文的时序图有一个参数tRAS（Active to Precharge Command，行有效至预充电命令间隔周期）。它有一个范围，对于PC133标准，一般是预充电命令至少要在行有效命令5个时钟周期之后发出，最长间隔视芯片而异（基本在120000ns左右），否则工作行的数据将有丢失的危险。那么这也就意味着一个工作行从有效（选通）开始，可以有120000ns的持续工作时间而不用进行预充电。显然，只要北桥芯片不发出预充电（包括允许自动预充电）的命令，行打开的状态就会一直保持。在此期间的对该行的任何读写操作也就不会有tRCD的延迟。可见，如果北桥芯片在能同时打开的行（页）越多，那么PFHR也就越大。需要强调的是，这里的同时打开不是指对多行同时寻址（那是不可能的），而是指多行同时处于选通状态。我们可以看到一些SDRAM芯片组的资料中会指出可以同时打开多少个页的指标，这可以说是决定其内存性能的一个重要因素。Intel 845芯片组MCH的资料：其中表明它可以支持24个页面同时处于打开状态但是，可同时打开的页数也是有限制的。从SDRAM的寻址原理讲，同一L-Bank中不可能有两个打开的行（S-AMP只能为一行服务），这就限制了可同时打开的页面总数。以SDRAM有4个L-Bank，北桥最多支持8个P-Bank为例，理论上最多只能有32个页面能同时处于打开的状态。而如果只有一个P-Bank，那么就只剩下4个页面，因为有几个L-Bank才能有同时打开几个行而互不干扰。Intel 845的MHC虽然可以支持24个打开的页面，那也是指6个P-Bank的情况下（845MCH只支持6个P-Bank）。可见845已经将同时打开页数发挥到了极致。不过，同时打开页数多了，也对存取策略提出了一定的要求。理论上，要尽量多地使用已打开的页来保证最短的延迟周期，只有在数据不存在（读取时）或页存满了（写入时）再考虑打开新的指定页，这也就是变向的连续读/写。而打开新页时就必须要关闭一个打开的页，如果此时打开的页面已是北桥所支持的最大值但还不到理论极限的话，就需要一个替换策略，一般都是用LRU算法来进行，这与VIA的交错控制大同小异。规格播报编辑芯片和模块标准名称内存时脉周期I/O 总线时脉数据速率传输方式模组名称极限传输率DDR-200100MHz10ns100 MHz200Million并列传输PC-16001600MB/sDDR-266133 MHz7.5 ns133 MHz266 Million并列传输PC-21002100 MB/sDDR-333166 MHz6 ns166 MHz333 Million并列传输PC-27002700 MB/sDDR-400200 MHz5 ns200 MHz400 Million并列传输PC-32003200 MB/s记忆芯片DDR-200：DDR-SDRAM 记忆芯片在 100MHz 下运行DDR-266：DDR-SDRAM 记忆芯片在 133MHz 下运行DDR-333：DDR-SDRAM 记忆芯片在 166MHz 下运行DDR-400：DDR-SDRAM 记忆芯片在 200MHz 下运行（JEDEC制定的DDR最高规格）DDR-500：DDR-SDRAM 记忆芯片在 250MHz 下运行（非JEDEC制定的DDR规格）DDR-600：DDR-SDRAM 记忆芯片在 300MHz 下运行（非JEDEC制定的DDR规格）DDR-700：DDR-SDRAM 记忆芯片在 350MHz 下运行（非JEDEC制定的DDR规格）芯片模块PC-1600内存模块指工作在 100MHz 下的DDR-200内存芯片，其拥有 1.600GB/s 的带宽PC-2100内存模块指工作在 133MHz 下的DDR-266内存芯片，其拥有 2.133GB/s 的带宽PC-2700内存模块指工作在 166MHz 下的DDR-333内存芯片，其拥有 2.667GB/s 的带宽PC-3200内存模块指工作在 200MHz 下的DDR-400内存芯片，其拥有 3.200GB/s 的带宽公式播报编辑利用下列公式，就可以计算出DDR SDRAM时脉。DDR I/II内存运作时脉：实际时脉*2。（由于两边数据同时传输，200MHz内存的时脉会以400MHz运作。）内存带宽=内存速度*内存位宽标准公式：内存除频系数=时脉/200→*速算法：外频*（除频频率/同步频率） （使用此公式将会导致4%的误差）取址播报编辑（1）bank块地址---定位逻辑块（2）行地址和列地址---定位存储单元容量定义播报编辑容量定义：地址数*位宽*Bank（存储块）。引脚介绍播报编辑SDRAM在读写数据时重点注意以下信号：（1）CLK：时钟信号，为输入信号。SDRAM所有输入信号的逻辑状态都需要通过CLK的上升沿采样确定。（2）CKE：时钟使能信号，为输入信号，高电平有效。CKE信号的用途有两个：一、关闭时钟以进入省电模式；二、进入自刷新状态。CKE无效时，SDRAM内部所有与输入相关的功能模块停止工作。（3）CS#：片选信号，为输入信号，低电平有效。只有当片选信号有效后，SDRAM才能识别控制器发送来的命令。设计时注意上拉。（4）RAS#：行地址选通信号，为输入信号，低电平有效。（5）CAS#：列地址选通信号，为输入信号，低电平有效。（6）WE#：写使能信号，为输入信号，低电平有效。当然还包括bank[…]地址信号，这个需要根据不同的型号来确定，同样为输入信号；地址信号A[…]，为输入信号；数据信号DQ[…]，为输入/输出双向信号；数据掩码信号DQM，为输入输出双向信号，方向与数据流方向一致，高电平有效。当其有效时，数据总线上出现的对应数据字节被接收端屏蔽。当今主流播报编辑DDR3内存。它属于SDRAM家族的内存产品，提供了相较于DDR2 SDRAM更高的运行效能与更低的电压，是DDR2 SDRAM（四倍资料率同步动态随机存取内存）的后继者（增加至八倍），也是现时流行的内存产品。DDR3 SDRAM为了更省电、传输效率更快，使用了SSTL 15的I/O接口，运作I/O电压是1.5V，采用CSP、FBGA封装方式包装，除了延续DDR2 SDRAM的ODT、OCD、Posted CAS、AL控制方式外，另外新增了更为精进进的CWD、Reset、ZQ、SRT、PASR功能。CWD是作为写入延迟之用，Reset提供了超省电功能的命令，可以让DDR3 SDRAM内存颗粒电路停止运作、进入超省电待命模式，ZQ则是一个新增的终端电阻校准功能，新增这个线路脚位提供了ODCE（On Die Calibration Engline）用来校准ODT（On Die Termination）内部中断电阻，新增了SRT（Self-Reflash Temperature）可编程化温度控制内存时脉功能，SRT的加入让内存颗粒在温度、时脉和电源管理上进行优化，可以说在内存内，就做了电源管理的功能，同时让内存颗粒的稳定度也大为提升，确保内存颗粒不致于工作时脉过高导致烧毁的状况，同时DDR3 SDRAM还加入PASR（Partial Array Self-Refresh）局部Bank刷新的功能，可以说针对整个内存Bank做更有效的资料读写以达到省电功效。工作原理播报编辑SDRAM之所以成为DRAM就是因为它要不断进行刷新（Refresh）才能保留住数据，因为刷新（Refresh）是DRAM最重要的操作。那么要隔多长时间重复一次刷新，目前公认的标准是，存储体中电容的数据有效保存期上限是64ms（毫秒，1/1000秒），也就是说每一行刷新的循环周期是64ms。这样刷新速度就是：64ms/行数量。我们在看内存规格时，经常会看到4096 Refresh Cycles/64ms或8192 Refresh Cycles/64ms的标识，这里的4096与8192就代表这个芯片中每个Bank的行数。刷新命令一次对一行有效，发送间隔也是随总行数而变化，4096行时为15.625μs（微秒，1/1000毫秒），8192行时就为7.8125μs。HY57V561620为8192 refresh cycles / 64ms。SDRAM是多Bank结构，例如在一个具有两个Bank的SDRAM的模组中，其中一个Bank在进行预充电期间，另一个Bank却马上可以被读取，这样当进行一次读取后，又马上去读取已经预充电Bank的数据时，就无需等待而是可以直接读取了，这也就大大提高了存储器的访问速度。为了实现这个功能，SDRAM需要增加对多个Bank的管理，实现控制其中的Bank进行预充电。在一个具有2个以上Bank的SDRAM中，一般会多一根叫做BAn的引脚，用来实现在多个Bank之间的选择。SDRAM具有多种工作模式，内部操作是一个复杂的状态机。SDRAM器件的引脚分为以下几类。（1）控制信号：包括片选、时钟、时钟使能、行列地址选择、读写有效及数据有效。（2）地址信号：时分复用引脚，根据行列地址选择引脚，控制输入的地址为行地址或列地址。。（3）数据信号：双向引脚，受数据有效控制。SDRAM的所有操作都同步于时钟。根据时钟上升沿控制管脚和地址输入的状态，可以产生多种输入命令。模式寄存器设置命令。激活命令。预充命令。读命令。写命令。带预充的读命令。带预充的写命令。自动刷新命令。自我刷新命令。突发停命令。空操作命令。根据输入命令，SDRAM状态在内部状态间转移。内部状态包括模式寄存器设置状态、激活状态、预充状态、写状态、读状态、预充读状态、预充写状态、自动刷新状态及自我刷新状态。SDRAM支持的操作命令有初始化配置、预充电、行激活、读操作、写操作、自动刷新、自刷新等。所有的操作命令通过控制线CS#、RAS#、CAS#、WE#和地址线、体选地址BA输入。1、行激活行激活命令选择处于空闲状态存储体的任意一个行，使之进入准备读/写状态。从体激活到允许输入读/写命令的间隔时钟节拍数取决于内部特征延时和时钟频率。HY57V561620内部有4个体，为了减少器件门数，4个体之间的部分电路是公用的，因此它们不能同时被激活，而且从一个体的激活过渡到另一个体的激活也必须保证有一定的时间间隔。2、预充电预充电命令用于对已激活的行进行预充电即结束活动状态。预充电命令可以作用于单个体，也可以同时作用于所有体（通过所有体预充电命令）。对于突发写操作必须保证在写入预充电命令前写操作已经完成，并使用DQM禁止继续写入数据。预充电结束后回到空闲状态，也可以再次被激活，此时也可以输入进入低功耗、自动刷新、自刷新和模式设置等操作命令。预充电中重写的操作与刷新操作一样，只不过预充电不是定期的，而只是在读操作以后执行的。因为读取操作会破坏内存中的电荷。因此，内存不但要每64ms刷新一次，而且每次读操作之后还要刷新一次。3、自动预充电如果在突发读或突发写命令中，A10/AP位置为“1”，在读写操作完成后自动附加一个预充电动作。操作行结束活动状态，但在内部状态机回到空闲态之前不能给器件发送新的操作命令。4、突发读突发读命令允许某个体中的一行被激活后，连续读出若干个数据。第一个数据在经过指定的CAS延时节拍后呈现在数据线上，以后每个时钟节拍都会读出一个新的数据。突发读操作可以被同体或不同体的新的突发读/写命令或同一体的预充电命令及突发停止命令中止。5、突发写突发写命令与突发读命令类似，允许某个体中的一行被激活后，连续写入若干个数据。第一个写数据与突发写命令同时在数据线上给出，以后每个时钟节拍给出一个新的数据，输入缓冲在突发数据量满足要求后停止接受数据。突发写操作可以被突发读/写命令或DQM数据输入屏蔽命令和预充电命令或突发停止命令中止。6、自动刷新由于动态存储器存储单元存在漏电现象，为了保持每个存储单元数据的正确性，HY57V561620必须保证在64ms内对所有的存储单元刷新一遍。一个自动刷新周期只能刷新存储单元的一个行，每次刷新操作后内部刷新地址计数器自动加“1”。只有在所有体都空闲（因为4个体的对应行同时刷新）并且未处于低功耗模式时才能启动自动刷新操作，刷新操作执行期间只能输入空操作，刷新操作执行完毕后所有体都进入空闲状态。该器件可以每间隔7.8μs执行一次自动刷新命令，也可以在64ms内的某个时间段对所有单元集中刷新一遍。7、自刷新自刷新是动态存储器的另一种刷新方式，通常用于在低功耗模式下保持SDRAM的数据。在自刷新方式下，SDRAM禁止所有的内部时钟和输入缓冲（CKE除外）。为了降低功耗，刷新地址和刷新时间全部由器件内部产生。一旦进入自刷新方式只有通过CKE变低才能激活，其他的任何输入都将不起作用。给出退出自刷新方式命令后必须保持一定节拍的空操作输入，以保证器件完成从自刷新方式的退出。如果在正常工作期间采用集中式自动刷新方式，则在退出自刷新模式后必须进行一遍（对于HY57V561620来说，8192个）集中的自动刷新操作。8、时钟和时钟屏蔽时钟信号是所有操作的同步信号，上升沿有效。时钟屏蔽信号CKE决定是否把时钟输入施加到内部电路。在读写操作期间，CKE变低后的下一个节拍冻结输出状态和突发地址，直到CKE变高为止。在所有的体都处于空闲状态时，CKE变低后的下一个节拍SDRAM进入低功耗模式并一直保持到CKE变高为止。9、DQM操作DQM用于屏蔽输入输出操作，对于输出相当于开门信号，对于输入禁止把总线上的数据写入存储单元。对读操作DQM延迟2个时钟周期开始起作用，对写操作则是当拍有效。 [1]

DMA：
简介播报编辑通常会指定一个内存部分用于直接内存访问。在ISA总线标准中，高达16兆字节的内存可用于DMA。EISA和微通道架构标准允许访问全套内存地址（假设他们可以用32位寻址）。外围设备互连通过使用一个总线主控器来完成直接内存访问。直接内存访问的另一个选择是程控输入输出（PIO）接口。在程控输入输出接口中，设备之间所有的数据传输都要通过处理器。ATA/IDE接口的新协议是Ultra DMA，它提供的突发数据传输速率可达33兆字节每秒。具有Ultra DMA/33的硬盘驱动器也支持PIO模式1、3、4和多字DMA模式2（每秒16.6兆字节）。 [1]原理播报编辑外设与存储器之间以及存储器与存储器之间的数据传输，通常采用程序中断方式、程序查询方式和DMA控制方式。程序中断方式和程序查询方式都需要CPU发出输入/输出（In/Out，I/O）的指令，然后等待I/O设备完成操作之后返回，期间CPU需要等待I/O设备完成操作。DMA在传输存储器和I/O设备的数据时，无须CPU来控制数据的传输，直接通过DMA控制器（direct memory access controller，DMAC）完成外设与存储器之间以及存储器与存储器之间的数据高速传输。 [3]DMA传输原理一个完整的DMA传输包括DMA请求、DMA响应、DMA传输和DMA结束4个步骤。DMA传输原理如图1所示，图中I/O设备为源端设备，由I/O设备向目的端设备（存储器）传输数据，其DMA的基本传输过程如下：①CPU对总线控制器进行初始化，制定工作内存空间，读取DMAC中的寄存器信息，了解DMAC的传输状态[1]；②I/O设备向DMAC发送DMA请求（DMA request，DREQ），DMAC收到此信号后，向CPU发出总线保持信号（HOLD）； ③CPU当前总线周期执行结束后发出总线响应信号保持确认（hold acknowledgment，HLDA）； ④DMAC收到总线授权后，向I/O设备发送DMA响应信号DMA确认（DMA acknowledgment，DACK），表示允许I/O设备进行DMA传送；⑤开始传输时，DMAC首先从源地址读取数据并存入内部缓存中，再写入目的地址，完成总线数据从源地址到目的地址的传输[1]；⑥DMA传输完成后，DMAC向CPU发出结束信号，释放总线，使CPU重新获得总线控制权。一次DMA传输只需要执行一个DMA周期，相当于一个总线读/写周期，因而能够满足外设数据高速传输的需要。 [3]DMA是所有现代电脑的重要特色，它允许不同速度的硬件设备来沟通，而不需要依于中央处理器的大量中断负载。否则，中央处理器需要从来源把每一片段的数据复制到寄存器，然后把它们再次写回到新的地方。在这个时间中，中央处理器对于其他的工作来说就无法使用。DMA传输常使用在将一个内存区从一个设备复制到另外一个。当中央处理器初始化这个传输动作，传输动作本身是由DMA控制器来实行和完成。典型的例子就是移动一个外部内存的区块到芯片内部更快的内存去。像是这样的操作并没有让处理器工作拖延，使其可以被重新调度去处理其他的工作。DMA传输对于高性能嵌入式系统算法和网络是很重要的。 举个例子，个人电脑的ISADMA控制器拥有8个DMA通道，其中的7个通道是可以让计算机的中央处理器所利用。每一个DMA通道有一个16位地址寄存器和一个16位计数寄存器。要初始化数据传输时，设备驱动程序一起设置DMA通道的地址和计数寄存器，以及数据传输的方向，读取或写入。然后指示DMA硬件开始这个传输动作。当传输结束的时候，设备就会以中断的方式通知中央处理器。"分散-收集"（Scatter-gather）DMA允许在一次单一的DMA处理中传输数据到多个内存区域。相当于把多个简单的DMA要求串在一起。同样，这样做的目的是要减轻中央处理器的多次输出输入中断和数据复制任务。 DRQ意为DMA要求；DACK意为DMA确认。这些符号一般在有DMA功能的电脑系统硬件概要上可以看到。它们表示了介于中央处理器和DMA控制器之间的电子信号传输线路。 [1]缓存一致性问题播报编辑DMA会导致缓存一致性问题。想像中央处理器带有缓存与外部内存的情况，DMA的运作则是去访问外部内存，当中央处理器访问外部内存某个地址的时候，暂时先将新的值写入缓存中，但并未将外部内存的数据更新，若在缓存中的数据尚未更新到外部内存前发生了DMA，则DMA过程将会读取到未更新的数据。相同的，如果外部设备写入新的值到外部内存内，则中央处理器若访问缓存时则会访问到尚未更新的数据。这些问题可以用两种方法来解决：1.缓存同调系统（Cache-coherent system）：以硬件方法来完成，当外部设备写入内存时以一个信号来通知缓存控制器某内存地址的值已经过期或是应该更新数据。2.非同调系统（Non-coherent system）：以软件方法来完成，操作系统必须确认缓存读取时，DMA程序已经开始或是禁止DMA发生。第二种的方法会造成DMA的系统负担。 [2]DMA引擎播报编辑除了与硬件交互相关外，DMA也可为昂贵的内存耗费减负。比如大型的拷贝行为或scatter-gather操作，从中央处理器到专用的DMA引擎。Intel的高端服务器包含这种引擎，它被称为I/O加速技术（IOAT）。 [2]RDMA播报编辑在电脑运算领域，远程直接内存访问（英语：remote direct memory access，RDMA）是一种直接存储器访问技术，它将数据直接从一台计算机的内存传输到另一台计算机，无需双方操作系统的介入。这允许高通量、低延迟的网络通信，尤其适合在大规模并行计算机集群中使用。RDMA支持零复制网络传输，通过使网络适配器直接在应用程序内存间传输数据，不再需要在应用程序内存与操作系统缓冲区之间复制数据。这种传输不需要中央处理器、CPU缓存或上下文交换参与，并且传输可与其他系统操作并行。当应用程序执行RDMA读取或写入请求时，应用程序数据直接传输到网络，从而减少延迟并实现快速的消息传输。但是，这种策略也表现出目标节点不会收到请求完成的通知（单向通信）等相关的若干问题。 [2]

非易失性内存：
类型播报编辑非易失性存储器主要有以下类型：ROM（Read-only memory，只读内存）PROM（Programmable read-only memory，可编程只读内存）EAROM（Electrically alterable read only memory，电可改写只读内存）EPROM（Erasable programmable read only memory，可擦可编程只读内存）EEPROM（Electrically erasable programmable read only memory，电可擦可编程只读内存）Flash memory（闪存） [1]只读存储器播报编辑只读存储器（Read-OnlyMemory，ROM）是一种半导体存储器，其特性是一旦存储数据就无法再将之改变或删除，且内容不会因为电源关闭而消失。在电子或电脑系统中，通常用以存储不需经常变更的程序或数据，例如早期的家用电脑如Apple II的监督程序、BASIC语言解释器、与硬件点阵字体，个人电脑IBMPC/XT/AT的BIOS（基本输入输出系统）与IBM PC/XT的BASIC解释器，与其他各种微电脑系统中的固件（Firmware），均存储在ROM内。PROM主条目：PROM可编程只读存储器（Programmable ROM，PROM）其内部有行列式的镕丝，可依用户（厂商）的需要，利用电流将其烧断，以写入所需的数据及程序，镕丝一经烧断便无法再恢复，亦即数据无法再更改。EPROM主条目：EPROM可抹除可编程只读存储器（Erasable Programmable Read Only Memory，EPROM）可利用高电压将数据编程写入，但抹除时需将线路曝光于紫外线下一段时间，数据始可被清空，再供重复使用。因此，在封装外壳上会预留一个石英玻璃所制的透明窗以便进行紫外线曝光。写入程序后通常会用贴纸遮盖透明窗，以防日久不慎曝光过量影响数据。OTPROM一次编程只读存储器（One Time Programmable Read Only Memory，OTPROM）内部所用的芯片与写入原理同EPROM，但是为了节省成本，封装上不设置透明窗，因此编程写入之后就不能再抹除改写。EEPROM主条目：EEPROM电子抹除式可复写只读存储器（Electrically Erasable Programmable Read Only Memory，EEPROM）之运作原理类似EPROM，但是抹除的方式是使用高电场来完成，因此不需要透明窗。 [1]闪存播报编辑快闪存储器（英语：flash memory），是一种电子式可清除程序化只读存储器的形式，允许在操作中被多次擦或写的存储器。这种科技主要用于一般性数据存储，以及在电脑与其他数字产品间交换传输数据，如储存卡与U盘。闪存是一种特殊的、以宏块抹写的EEPROM。早期的闪存进行一次抹除，就会清除掉整颗芯片上的数据。闪存的成本远较可以字节为单位写入的EEPROM来的低，也因此成为非易失性固态存储最重要也最广为采纳的技术。像是PDA、笔记本电脑、数字随身听、数码相机与手机上均可见到闪存。此外，闪存在游戏主机上的采用也日渐增加，藉以取代存储游戏数据用的EEPROM或带有电池的SRAM。闪存是非易失性的内存。这表示单就保存数据而言，它是不需要消耗电力的。与硬盘相比，闪存也有更佳的动态抗震性。这些特性正是闪存被移动设备广泛采用的原因。闪存还有一项特性：当它被制成储存卡时非常可靠──即使浸在水中也足以抵抗高压与极端的温度。闪存的写入速度往往明显慢于读取速度。虽然闪存在技术上属于EEPROM，但是“EEPROM”这个字眼通常特指非快闪式、以小区块为清除单位的EEPROM。它们典型的清除单位是字节。因为老式的EEPROM抹除循环相当缓慢，相形之下快闪记体较大的抹除区块在写入大量数据时带给其显著的速度优势。闪存最常见的封装方式是TSOP48和BGA，在逻辑接口上的标准则由于厂商阵营而区分为两种：ONFI和Toggle。手机上的闪存常常以eMMC的方式存在。 [2]闪存的限制播报编辑区块抹除闪存的一种限制在于即使它可以单一字节的方式读或写入，但是抹除一定是一整个区块。一般来说都是设置某一区中的所有比特为“1”，刚开始区块内的所有部分都可以写入，然而当有任何一个比特被设为“0”时，就只能借由清除整个区块来恢复“1”的状态。换句话说闪存（特别是NOR Flash）能提供随机读取与写入操作，却无法提供任意的随机改写。不过其上的区块可以写入与既存的“0”值一样长的消息（新值的0比特是旧值的0比特的超集）。例如：有一小区块的值已抹除为1111，然后写入1110的消息。接下来这个区块还可以依序写入1010、0010，最后则是0000。可是实际上少有算法可以从这种连续写入兼容性得到好处，一般来说还是整块抹除再重写。尽管闪存的数据结构不能完全以一般的方式做更新，但这允许它以“标记为不可用”的方式删除消息。这种技巧在每单元存储大于1比特数据的MLC设备中必须稍微做点修改。记忆耗损另一项闪存的限制是它有抹写循环的次数限制（大多商业性SLC闪存保证“0”区有十万次的抹写能力，但其他区块不保证）。这个结果部分地被某些固件或文件系统为了在相异区块间分散写入操作而进行的计算写入次数与动态重对映所抵销；这种技巧称为耗损平衡（wear leveling）。另一种处理方法称为坏区管理（Bad Block Management, BBM）。这种方法是在写入时做验证并进行动态重测，如果有验证失败的区块就加以剔除。对多数移动设备而言，这些磨损管理技术可以延长其内部闪存的寿命（甚至超出这些设备的使用年限）。此外，丢失部分数据在这些设备上或许是可接受的。至于会进行大量数据读写循环的高可靠性数据存储应用则不建议使用闪存。不过这种限制不适用于路由器与瘦客户端（Thin clients）等只读式应用，这些设备往往在使用年限内也只会写入一次或少数几次而已。读取干扰所使用的闪存读取方式随着时间的推移会导致在同一区块中相近的记忆单元内容改变（变成写入动作）。这即是所谓的读取干扰。会导致读取干扰现象的读取次数门槛介于区块被抹除间，通常为100,000次。假如连续从一个记忆单元读取，此记忆单元将不会受损，而受损却是接下来被读取的周围记忆单元。为避免读取干扰问题，闪存控制器通常会计算从上次抹除动作后的区块读取动作总次数。当计数值超过所设置的目标值门槛时，受影响的区块会被复制到一个新的区块，然后将原区块抹除后释放到区块回收区中。原区块在抹除动作后就会像新的一样。若是闪存控制器没有即时介入时，读取干扰错误就会发生，如果错误太多而无法被ECC机制修复时就会伴随着可能的数据丢失。写入（编程）干扰写入干扰（编程干扰）是指当对页（page）进行写入时，由于阈值电压接近的关系，相邻的位（bit）也被升高，从而造成相邻的位出错。闪存电荷非常不稳定，相邻存储电荷的悬浮门间会相互干扰，造成相邻悬浮门间的bit错误，MLC由于存在4组接近的电压，与SLC相比更容易受到干扰。 [3]

存储单元：
介绍播报编辑存储单元存储单元：在存储器中有大量的存储元，把它们按相同的位划分为组，组内所有的存储元同时进行读出或写入操作，这样的一组存储元称为一个存储单元。一个存储单元通常可以存放一个字节；存储单元是CPU访问存储器的基本单位。 [1-2]存储单元播报编辑地址上存储单元的过程在计算机中最小的信息单位是bit，也就是一个二进制位，8个bit组成一个Byte，也就是字节。一个存储单元可以存储一个字节，也就是8个二进制位。计算机的存储器容量是以字节为最小单位来计算的，对于一个有128个存储单元的存储器，可以说它的容量为128字节。如果有一个1KB的存储器则它有1024个存储单元，它的编号为从0－1023。存储器被划分成了若干个存储单元，每个存储单元都是从0开始顺序编号，如一个存储器有128个存储单元，则它的编号就是从0-127。存储地址一般用十六进制数表示，而每一个存储器地址中又存放着一组二进制（或十六进制）表示的数，通常称为该地址的内容。值得注意的是，存储单元的地址和地址中的内容两者是不一样的。前者是存储单元的编号，表示存储器中的一个位置，而后者表示这个位置里存放的数据。正如一个是房间号码，一个是房间里住的人一样。存放一个机器字的存储单元，通常称为字存储单元，相应的单元地址叫字地址。而存放一个字节的单元，称为字节存储单元，相应的地址称为字节地址。如果计算机中可以编址的最小单元是字存储单元，则该计算机称为按字寻址的计算机。如果计算机中可编址的最小单位是字节，则该计算机称为按字节寻址的计算机。如果机器字长等于存储器单元的位数，一个机器字可以包含数个字节，所以一个存储单元也可以包含数个能够单独编址的字节地址。例如一个16位二进制的字存储单元可存放两个字节，可以按字地址寻址，也可以按字节地址寻址。当用字节地址寻址时，16位的存储单元占两个字节地址。最小静态存储单元播报编辑世界上最小的静态存储单元2008年8月18日，美国IBM公司、AMD以及纽约州立大学Albany分校的纳米科学与工程学院（CNSE）等机构共同宣布，世界上首个22纳米节点有效静态随机存储器（SRAM）研制成功。这也是全世界首次宣布在300毫米研究设备环境下，制造出有效存储单元。SRAM芯片是更复杂的设备，比如微处理器的“先驱”。SRAM单元的尺寸更是半导体产业中的关键技术指标。最新的SRAM单元利用传统的六晶体管设计，仅占0.1平方微米，打破了此前的SRAM尺度缩小障碍。新的研究工作是在纽约州立大学Albany分校的纳米科学与工程学院（CNSE）完成的，IBM及其他伙伴的许多顶尖的半导体研究都在这里进行。IBM科技研发部副总裁T.C.Chen博士称，“我们正在可能性的终极边缘进行研究，朝着先进的下一代半导体技术前进。新的研究成果对于不断驱动微电子设备小型化的追求，可以说至关重要。”22纳米是芯片制造的下两代，而下一代是32纳米。在这方面，IBM及合作伙伴正在发展它们无与伦比的32纳米高K金属栅极工艺（high-Kmetalgatetechnology）。从传统上而言，SRAM芯片通过缩小基本构建单元，来制造得更加紧密。IBM联盟的研究人员优化了SRAM单元的设计和电路图，从而提升了稳定性，此外，为了制造新型SRAM单元，他们还开发出几种新的制作工艺流程。研究人员利用高NA浸没式光刻（high-NAimmersionlithography）技术刻出了模式维度和密度，并且在先进的300毫米半导体研究环境中制作了相关部件。与SRAM单元相关的关键技术包括：边带高K金属栅极、<25纳米栅极长度晶体管、超薄隔离结构（spacer）、共同掺杂、先进激活技术、极薄硅化物膜以及嵌入式铜触点等。据悉，在2008年12月15至17日美国旧金山将要举行的IEEE国际电子设备（IEDM）年会上，还会有专门的报告来介绍最新成果的细节。相关应用播报编辑在计算机中，由控制器解释，运算器执行的指令集是一个精心定义的数目十分有限的简单指令集合。一般可以分为四类：1）、数据移动 （如：将一个数值从存储单元A拷贝到存储单元B）2）、数逻运算（如：计算存储单元A与存储单元B之和，结果返回存储单元C）3）、 条件验证（如：如果存储单元A内数值为100，则下一条指令地址为存储单元F）4）、指令序列改易（如：下一条指令地址为存储单元F） [1]

中断：
术语解释播报编辑指处理机处理程序运行中出现的紧急事件的整个过程.程序运行过程中，系统外部、系统内部或者现行程序本身若出现紧急事件，处理机立即中止现行程序的运行，自动转入相应的处理程序(中断服务程序)，待处理完后，再返回原来的程序运行，这整个过程称为程序中断;当处理机接受中断时，只需暂停一个或几个周期而不执行处理程序的中断，称为简单中断.中断又可分为屏蔽中断和非屏蔽中断两类.可由程序控制其屏蔽的中断称为屏蔽中断或可屏蔽中断.屏蔽时，处理机将不接受中断.反之，不能由程序控制其屏蔽，处理机一定要立即处理的中断称为非屏蔽中断或不可屏蔽中断.非屏蔽中断主要用于断电、电源故障等必须立即处理的情况.处理机响应中断时，不需执行查询程序.由被响应中断源向CPU发向量地址的中断称为向量中断，反之为非向量中断.向量中断可以提高中断响应速度。 [2]分类播报编辑硬件中断（Hardware Interrupt） [3]：可屏蔽中断（maskable interrupt）。硬件中断的一类，可通过在中断屏蔽寄存器中设定位掩码来关闭。非可屏蔽中断（non-maskable interrupt，NMI）。硬件中断的一类，无法通过在中断屏蔽寄存器中设定位掩码来关闭。典型例子是时钟中断（一个硬件时钟以恒定频率—如50Hz—发出的中断）。处理器间中断（interprocessor interrupt）。一种特殊的硬件中断。由处理器发出，被其它处理器接收。仅见于多处理器系统，以便于处理器间通信或同步。伪中断（spurious interrupt）。一类不希望被产生的硬件中断。发生的原因有很多种，如中断线路上电气信号异常，或是中断请求设备本身有问题。软件中断（Software Interrupt） [3]：软件中断。是一条CPU指令，用以自陷一个中断。由于软中断指令通常要运行一个切换CPU至内核态（Kernel Mode/Ring 0）的子例程，它常被用作实现系统调用（System call）。防止方法播报编辑要防止中断冲突，其实就是要知道什么设备容易产生中断冲突，只要知道了这点，在使用这些设备时稍微注意一下就可以了。下面我列出一些容易冲突的设备，希望对读者有用。1、声卡：一些早期的ISA型声卡，系统很有可能不认，就需要用户手动设置（一般为5）2、内置调制解调器和鼠标：一般鼠标用COM1，内置调制解调器使用COM2的中断（一般为3），这时要注意此时COM2上不应有其它设备3、网卡和鼠标：此问题一般发生在鼠标在COM1口，使用中断为3，这时要注意通常网卡的默认中断为3，两者极有可能发成冲突。4、打印机和EPP扫描仪：在安装扫描仪驱动程序时应将打印机打开，因为两个设备中串联，所以为了防止以后扫描仪驱动程序设置有误，一定要将打印机打开再安装扫描仪驱动程序。5、操作系统和BIOS：如果计算机使用了“即插即用”操作系统（例如win98），应将BIOS中PNP OS Installed设置为Yes这样可让操作系统重新设置中断。6、PS/2鼠标和BIOS：在使用PS/2鼠标时应将BIOS中PS/2 Mouse Function Control打开或设置为Auto，只有这样BIOS才能将IRQ12分配给PS/2鼠标用。功能播报编辑现代计算机中采用中断系统的主要目的是 [4]：①提高计算机系统效率。计算机系统中处理机的工作速度远高于外围设备的工作速度。通过中断可以协调它们之间的工作。当外围设备需要与处理机交换信息时，由外围设备向处理机发出中断请求，处理机及时响应并作相应处理。不交换信息时，处理机和外围设备处于各自独立的并行工作状态。②维持系统可靠正常工作。现代计算机中，程序员不能直接干预和操纵机器，必须通过中断系统向操作系统发出请求，由操作系统来实现人为干预。主存储器中往往有多道程序和各自的存储空间。在程序运行过程中，如出现越界访问，有可能引起程序混乱或相互破坏信息。为避免这类事件的发生，由存储管理部件进行监测，一旦发生越界访问，向处理机发出中断请求，处理机立即采取保护措施。③满足实时处理要求。在实时系统中，各种监测和控制装置随机地向处理机发出中断请求，处理机随时响应并进行处理。④提供故障现场处理手段。处理机中设有各种故障检测和错误诊断的部件，一旦发现故障或错误，立即发出中断请求，进行故障现场记录和隔离，为进一步处理提供必要的依据。中断优先权播报编辑在某一时刻有几个中断源同时发出中断请求时，处理器只响应其中优先权最高的中断源。当处理机正在运行某个中断服务程序期间出现另一个中断源的请求时，如果后者的优先权低于前者，处理机不予理睬，反之，处理机立即响应后者，进入所谓的“嵌套中断”。中断优先权的排序按其性质、重要性以及处理的方便性决定，由硬件的优先权仲裁逻辑或软件的顺序询问程序来实现 [4]。中断过程播报编辑按照事件发生的顺序，中断过程包括 [4]：①中断源发出中断请求;②判断当前处理机是否允许中断和该中断源是否被屏蔽;③优先权排队;④处理机执行完当前指令或当前指令无法执行完，则立即停止当前程序，保护断点地址和处理机当前状态，转入相应的中断服务程序;⑤执行中断服务程序;⑥恢复被保护的状态，执行“中断返回”指令回到被中断的程序或转入其他程序。上述过程中前四项操作是由硬件完成的，后两项是由软件完成的。向量中断播报编辑对应每个中断源设置一个向量。这些向量顺序存在主存储器的特定存储区。向量的内容是相应中断服务程序的起始地址和处理机状态字。在响应中断时，由中断系统硬件提供向量地址，处理机根据该地址取得向量，并转入相应的中断服务程序 [4]。

超标量技术：
超标量CPU不可能再进一步调高性能了，这是由于指令的并行度ILP所决定的，即使编译器可以使用诸如循环展开优化技术，超标量CPU对性能的改善也很有限。

算术逻辑单元：
简介播报编辑算术逻辑单元算术逻辑单元（Arithmetic&logical Unit）是中央处理器(CPU)的执行单元，是所有中央处理器的核心组成部分，由"And Gate"（与门） 和"Or Gate"（或门）构成的算术逻辑单元，主要功能是进行二位元的算术运算，如加减乘(不包括整数除法)。基本上，在所有现代CPU体系结构中，二进制都以补码的形式来表示。发展播报编辑算术逻辑单元算术逻辑单元（arithmetic logic unit，缩写ALU）是进行整数运算的结构。现阶段是用电路来实现，应用在电脑芯片中。在计算机中，算术逻辑单元（ALU）是专门执行算术和逻辑运算的数字电路。ALU是计算机中央处理器的最重要组成部分，甚至连最小的微处理器也包含ALU作计数功能。在现代CPU和GPU处理器中已含有功能强大和复杂的ALU；一个单一的元件也可能含有ALU。1945年数学家冯诺伊曼在一篇介绍被称为EDVAC的一种新型电脑的基础构成的报告中提出ALU的概念。早期发展1946年，冯诺伊曼与同事合作为普林斯顿高等学习学院(IAS)设计计算机。随后IAS计算机成为后来计算机的原形。在论文中，冯诺伊曼提到他所相信的计算机中所需的部件，而其中包括ALU。 冯诺伊曼写到，ALU是计算机的必备组成部分，因为已确定计算机一定要完成基本的数学运算，包括加减乘除。于是他相信「（计算机）应该含有专门完成此类运算的部件。」数字系统ALU必须与数字电路的其他部分使用同样的格式来进行数字处理。对现代处理器而言，数值一律使用二进制补码表示。早期的计算机曾使用过很多种数字系统，包括反码、符号数值码，甚至是十进制码，每一位用十个管子。 以上这每一种数字系统所对应的ALU都有不同的设计，而这也影响了当前对二进制补码的优先选择，因为二进制补码能简化ALU加法和减法的运算。 一个简单的能进行与或非和加运算的2位ALU。可行性分析绝大部分计算机指令都是由ALU执行的。ALU从寄存器中取出数据。数据经过处理将运算结果存入ALU输出寄存器中。其他部件负责在寄存器与内存间传送数据。 控制单元控制着ALU，通过控制电路来告诉ALU该执行什么操作。 [1]简单运算大部分ALU都可以完成以下运算∶整数算术运算（加、减，有时还包括乘和除，不过成本较高）位逻辑运算（与、或、非、异或）移位运算（将一个字向左或向右移位或浮动特定位，而无符号延伸），移位可被认为是乘以2或除以2。复杂运算工程师可设计能完成任何运算的ALU，不论运算有多复杂；问题在于运算越复杂，ALU成本越高，在处理器中占用的空间越大，消耗的电能越多。 于是，工程师们经常计算一个折中的方案，提供给处理器（或其他电路）一个能使其运算高速的ALU，但同时又避免ALU设计的太复杂而价格昂贵。设想你需要计算一个数的平方根，数字工程师将评估以下的选项来完成此操作∶设计一个极度复杂的ALU，它能够一步完成对任意数字的平方根运算。这被称为单时钟脉冲计算。设计一个非常复杂的ALU，它能够分几步完成一个数字的平方根运算。不过，这里有个诀窍，中间结果经过一连串电路，就像是工厂里的生产线。这甚至使得ALU能够在完成前一次运算前就接受新的数字。这使得ALU能够以与单时钟脉冲同样的速度产生数字，虽然从ALU输出的结果有一个初始延迟。这被称为计算流水线。设计一个复杂的ALU，它能够计算分几步计算一个数字的平方根。这被称为互动计算，经常依赖于带有嵌入式微码的复杂控制单元。在处理器中设计一个简单的ALU，去掉一个昂贵的专门用于此运算的处理器，再选择以上三个选项之一。这被称为协处理器。告诉编程人员没有协处理器和仿真设备，于是他们必须自己写出算法来用软件计算平方根。这是由软件库完成的。对协处理器进行仿真，也就是说，只要一个程序想要进行平方根的计算，就让处理器检查当前有没有协处理器。如果有的话就使用其进行计算，如果没有的话，中断程序进程并调用操作系统通过软件算法来完成平方根的计算。这被称为软件仿真。以上给出的选项按最快和最贵到最慢和最经济排列。于是，虽然甚至是最简单的计算机也能计算最复杂的公式，但是最简单的计算机经常需要耗费大量时间，通过若干步才能完成。 强大的处理器，比如英特尔酷睿和AMD64系列对一些简单的运算采用1号选项，对最常见的复杂运算采用2号选项，对极为复杂的运算采用3号选项。这是具有在处理器中构造非常复杂的ALU的能力为前提的。输入和输出ALU的输入是要进行操作的数据（称为操作数）以及来自控制单元的指令代码，用来指示进行哪种运算。它的输出即为运算结果。 在许多设计中ALU也接收或发出输入或输出条件代码到（或来自）状态寄存器。这些代码用来指示一些情况，比如进位或借位、溢出、除数为零等。ALU与FPU浮点单元也对两个数值进行算术运算，但是这种运算已浮点数表示，比在ALU中一般使用的补码表示方式复杂的多。为了完成此类运算，FPU里嵌入了多个复杂电路，包括一些内部ALU。 工程师一般认为ALU是处理整数型（比如补码和BCD码）算术运算的的电路，而对更为复杂的格式（比如浮点型、复数型）进行计算的电路则拥有一个更加匹配的称谓。 [2]特点播报编辑ALU用以计算机指令集中的执行算术与逻辑操作;某些处理器中，将ALU切分为两部分，即算术单元 （AU）与逻辑单元（LU）。某些处理器包含一个以上的AU，如，一个用来进行定点操作，另一个进行浮点操作。（个人计算机中，浮点操作有时由被称为数字协处理器的浮点单元完成）。通常而言，ALU具有对处理器控制器、内存及输入输出设备的直接读入读出权限。输入输出是通过总线进行的。输入指令包含一个指令字，有时被称为机器指令字，其中包括操作码，单个或多个操作数，有时还会有格式码；操作码指示ALU机要执行什么操作，在此操作中要执行多少个操作数。比如，两个操作数可以进行比较，也可以进行加法操作。 格式码可与操作码结合，告知这是一个定点还是浮点指令；输出包括存放在存储寄存器中的结果及显示操作是否成功的设置。如操作失败，则在机器状态字中会有相应的状态显示 。通常，输入操作数、操作数、累加和以及转换结果的存储位置都在ALU中。在算术单元中，乘除操作是通过一系列的加减运算得到的。在机器码中有多种方式用以表示负数。在逻辑单元中，每次执行16个可能的逻辑运算中的一个。ALU的设计是处理器设计中的关键部分。仍在不断研究如何提高指令的处理速度。 [3]逻辑单元播报编辑逻辑单元(LU)是进入IBM系统网络体系结构(SNA)的网络端口，通过它用户可以访问网络资源，或一个程序员与另一个程序员通信。 [4]

并行传输：
概念播报编辑并行传输指的是数据以成组的方式，在多条并行信道上同时进行传输。常用的是将构成一个字符的几位二进制码同时分别在几个并行的信道上传输。另外加一条控制信号即“选通”脉冲，它在数据信号发出之后传送，用以通知接收设备所有位已经发送完毕，可对各条信道上的信号进行取样了。这类传输比较简单，对8位微处理器来说，8位的数据一次同时传送。微处理器本身处理的数据就是并行处理，所以这就不需要对数据进行格式的变化。因此实现这类传输的接口电路也比较简单。 [1]基本原理播报编辑并行传输的编码一个编了码的字符通常是由若干位二进制数表示,如用ASCII码编码的符号是由8位二进制数表示的,则并行传输ASCII编码符号就需要8个传输信道,使表示一个符号的所有数据位能同时沿着各自的信道并排的传输.并行传输时，一次可以传一个字符，收发双方不存在同步的问题。而且速度快、控制方式简单。但是，并行传输需要多个物理通道。所以并行传输只适合于短距离、要求传输速度快的场合使用。这类的接口电路必须具有缓冲寄存器，以便使数据在接口中停留足够的时间以适应外部设备的动作时间，或者是供微处理器在适当的时候来去数。另外还应该有一些控制电路；地址译码选择电路；用于在MPU查询时应答的状态标志电路和由关控制；寄存MPU发来的控制命令的寄存器以及中断逻辑控制电路等等。 [2]并行传输的实现一个采用8单位二进制码构成了一个字符进行并行传输，系统采用8个信道并行传输，一次传送一个字符，因此收、发双方不存在字符同步的问题，不需要额外的措施来实现收发双方的字符同步，这是并行传输的主要优点。但是并行传输必须有多条并行信道，成本比较高，不易远距离传输。这类总线传输速度快，但适用于短距离传送。典型的有S-100总线、MUI.TIBUS总线、标准总线、IEEE-488总线等。并行标准总线通常是用于插件板之间的连接。只有IEEE-488是用于系统和系统之间的连接。 [3]

磁盘：
发展历史播报编辑在过去的50年中，磁盘驱动器走过了很长的一段路。请跟随我们走过这段历史，回首我们按年度列出的磁盘驱动器发展史上50件具有里程碑意义的事件——从最早推出的产品到各种新技术以及在这中间的一切。1956年：IBM向客户交付第一台磁盘驱动器RAMAC 305，可存储5MB数据，每MB成本为10000美元。它有2个冰箱那样大，使用50个24英寸盘片。1961年：IBM发明在空气垫上或“空气支撑物”上“悬浮”的磁盘驱动器磁头。1963年：IBM推出第一个活动磁盘驱动器1311，拥有6个14英寸盘片，可存储2.6MB数据。1966年：IBM推出第一个使用缠绕线圈铁氧记录磁头的驱动器。1970年：通用数据公司(1971年更名为西部数据公司)在加州成立。1973年：IBM宣布推出第一个现代“温彻斯特”磁盘驱动器3340，使用了密封组件、润滑主轴和小质量磁头。1978年：第一个RAID(冗余阵列)驱动器诞生。1979：磁盘制造商希捷科技公司于1979年由Al Shugart挑头创立。1979：IBM的3370使用了7个直径为14英寸的盘片，存储容量可达571MB。3370也是首款使用薄胶片磁头的磁盘，1979：IBM的“Piccolo”电脑磁盘使用了6个直径为8英寸的盘片，存储容量可达64MB。1979：希捷科技公司研发出最早的磁盘接口——ST-506，之后便广泛用于微型计算机中。1980：IBM发布了当时首个存储容量以GB为单位的磁盘，其大小和一台电冰箱大小差不多，重量为250kg，出售价格为40000美元。1980：希捷科技公司发布首个大小为5.25英寸的磁盘。1981：Shugart Associates联手NCR共同研发出一个智能磁盘接口，命名为Shugart Associates Systems Interface (SASI)，该接口是SCSI(Small Computer System Interface，小型计算机系统接口)的前辈。1982：Western Digital宣布推出了首个单芯片“温彻斯特”磁盘控制器——WD1010。1983：Rodime宣布推出了当时首个3.5英寸的磁盘——RO352，它包括有两个盘片，存储容量可达10MB。1984：Western Digital为IBM PC/AT制造出首个“温彻斯特”磁盘控制卡，并成为了当时的一种工业标准。1985：Control Data、Compaq Computer和 Western Digital共同合作，并研发出40-pin的IDE接口。IDE是Intelligent Drive Electronics(智慧电子驱动器)的缩写。1985：磁盘控制器首次整合到磁盘驱动当中。1985：Quantum(昆腾)发布了Plus Hardcard磁盘，它在无需一个可用的插槽，或单独控制卡的情况下，可再多配置一个磁盘。1985：Western Digital宣布推出了首款ESDI(Enhanced Small Device Interface，增强型小型设备接口)控制板，它允许容量更大、速度更快的磁盘用于电脑当中。1986：官方的SCSI规格发布，而苹果电脑公司的Mac Plus也是首台使用该规格的电脑之一。1988：Prairie Tek宣布推出了220磁盘，这是首个2.5英寸的磁盘，主要是针对初生的笔记本电脑市场推出的。220磁盘使用了两个盘片，存储容量可达20MB。1988：Connor发布了首个高为1英寸的3.5英寸磁盘，还有磁盘沿袭了这种设计。1988：Western Digital成功收购Tandon公司，转型为专业的磁盘制造商。1990：Western Digital发布了其首个3.5英寸的Caviar(鱼子酱) IDE磁盘。1991：IBM向外界宣布推出了0663 Corsair，这是首款采用感应式薄胶片磁阻(MR)磁头的磁盘。它设计有8个直径为3.5英寸的盘片，存储容量可达1GB。(MR磁头早在1984年就用于IBM的磁盘驱动器。)1991：Integral Peripherals推出了使用一个直径为1.8英寸的盘片，存储容量可达21MB 的1820 Mustang磁盘。1992：希捷科技公司首次向外界展示了其2.5英寸的磁盘，在当时给了人们极大的震撼。1992：希捷科技公司成功的推出了存储容量为2.1GB的Barracuda(酷鱼)，这是首个采用7200r/min转速马达的磁盘。1992：惠普推出了C3013A Kitty Hawk磁盘，使用了两个直径为1.3英寸的盘片，存储容量可达2.1GB。1994：Western Digital成功研发出Enhanced IDE，这是一个改良版的磁盘接口，并打破了当时528MB存储容量上限的束缚。EIDE同样也允许配置光驱和磁盘驱动器。1996：IBM成功研发出在1个盘片上可存储100亿比特/英寸的磁盘技术。1996：希捷科技公司宣布推出了其Cheetah(捷豹)系列磁盘，这是首个采用10000r/min转速马达的磁盘。1997：IBM宣布推出了首个采用巨磁阻磁头(GMR)的磁盘——Deskstar 16GP Titan，在三个直径为3.5英寸的盘片上可装配16.8GB的存储容量。1998：IBM宣布推出了Microdrive(微磁盘)，这是当时世界上最小的磁盘，一个单一的1英寸盘片的容量可达340MB。2000：Maxtor(迈拓)成功收购了其竞争对手Quantum的磁盘业务。就当时的情况而言，Quantum是世界上第二大磁盘制造商，仅仅位于希捷技术公司之后。而成功收购了Quantum以后，Maxtor便一举成为世界上最大的磁盘制造商。2000：希捷科技公司发布了首款采用15000r/min转速马达的磁盘——Cheetah X15。2002：希捷科技公司在磁盘历史又获得了一个第一的称号，这都是因为它发布了Barracuda ATA V Serial ATA磁盘。2002：希捷科技公司向外界演示了垂直磁性记录技术，每英寸的密度可达100GB。2002：其实，在2002年有很多技术值得我们去记住，但希捷科技公司成功演示的Heat-Assisted Magnetic Recording(热辅助磁记录，HAMR)技术却格外耀眼，HAMR磁性记录技术采用了激光热辅助设计。2003：IBM宣布把其数据存储部门出售给日立，IBM由此也结束了在磁盘领域的辉煌历程。2003：Western Digital推出了首个10000r/min的 SATA磁盘——Raptor(猛禽)，存储容量为37GB。该款产品主要是为企业设计的，但是游戏玩家很快就发现，其实把该磁盘用于双磁盘RAID配置中，使得台式电脑的性能会有很大的提升。2004：东芝宣布推出了世界上首款0.85英寸的磁盘——MK2001MTN，在一个单一的盘片上，存储容量可达2GB。2005：东芝宣布推出了MK4007 GAL，该磁盘采用了直径为1.8英寸的盘片设计，存储容量为40GB。同时，MK4007 GAL也是首款采用垂直磁性记录设计的磁盘。2006：希捷科技公司成功收购了Maxtor，使得其在磁盘制造工业的竞争对手再度缩小。2006：希捷科技公司宣布推出了Momentus 5400.3笔记本电脑磁盘，这是首款采用垂直磁性记录设计的2.5英寸磁盘型号，其存储容量也达到160GB。2006：希捷科技公司发布了当今世界上存储容量最大的磁盘——Barracuda 7200.10，存储容量达到了750GB。2006：Western Digital宣布推出了10000r/min Raptor X SATA磁盘，其存储容量达到了150GB。不仅如此，Raptor X还采用了透明的外观设计，用户可以看到它运作时内部的情况。2006：Cornice和希捷技术这两家公司都在2006年宣布推出了1英寸磁盘，存储容量为12GB。磁盘结构播报编辑盘片一个磁盘（如一个 1T 的机械硬盘）由多个盘片叠加而成。盘片的表面涂有磁性物质，这些磁性物质用来记录二进制数据。因为正反两面都可涂上磁性物质，故一个盘片可能会有两个盘面。磁道、扇区每个盘片被划分为一个个磁道，每个磁道又划分为一个个扇区。其中，最内侧磁道上的扇区面积最小，因此数据密度最大。柱面每个盘面对应一个磁头。所有的磁头都是连在同一个磁臂上的，因此所有磁头只能“共进退”。所有盘面中相对位置相同的磁道组成柱面。技术指标播报编辑磁盘存储器的主要技术指标存储密度、存储容量、存取时间及数据传输率。 存储密度分为道密度、位密度和面密度。                 道密度是沿磁盘半径方向单位长度上的磁道数，单位为道/英寸。                    位密度是磁道单位长度上能记录的二进制代码位数，单位为位/英寸。  面密度是位密度和道密度的乘积，单位为位/平方英寸。存储容量一个磁盘存储器所能存储的字节总数。存取时间存取时间由三种时间构成：寻道时间、等待时间、数据传送时间。寻道时间磁盘定位到指定磁道上所需要的时间等待时间寻道完成后至磁道上需要访问的信息到达磁头下的时间数据传送时间传送数据所需要的时间数据传输率磁盘存储器在单位时间内向主机传送数据的字节数

闪存：
概念播报编辑运用闪存的数码产品闪存是一种非易失性存储器，即断电数据也不会丢失。因为闪存不像RAM（随机存取存储器）一样以字节为单位改写数据，因此不能取代RAM。闪存卡（Flash Card）是利用闪存（Flash Memory）技术达到存储电子信息的存储器，一般应用在数码相机，掌上电脑，MP3等小型数码产品中作为存储介质，所以样子小巧，有如一张卡片，所以称之为闪存卡。根据不同的生产厂商和不同的应用，闪存卡大概有SmartMedia（SM卡）、Compact Flash（CF卡）、MultiMediaCard（MMC卡）、Secure Digital（SD卡）、Memory Stick（记忆棒）、XD-Picture Card（XD卡）和微硬盘（MICRODRIVE）这些闪存卡虽然外观、规格不同，但是技术原理都是相同的。技术特点播报编辑单片机闪存NOR型与NAND型闪存的区别很大，打个比方说，NOR型闪存更像内存，有独立的地址线和数据线，但价格比较贵，容量比较小；而NAND型更像硬盘，地址线和数据线是共用的I/O线，类似硬盘的所有信息都通过一条硬盘线传送一般，而且NAND型与NOR型闪存相比，成本要低一些，而容量大得多。因此，NOR型闪存比较适合频繁随机读写的场合，通常用于存储程序代码并直接在闪存内运行，手机就是使用NOR型闪存的大户，所以手机的“内存”容量通常不大；NAND型闪存主要用来存储资料，我们常用的闪存产品，如闪存盘、数码存储卡都是用NAND型闪存。这里我们还需要端正一个概念，那就是闪存的速度其实很有限，它本身操作速度、频率就比内存低得多，而且NAND型闪存类似硬盘的操作方式效率也比内存的直接访问方式慢得多。因此，不要以为闪存盘的性能瓶颈是在接口，甚至想当然地认为闪存盘采用USB2.0接口之后会获得巨大的性能提升。前面提到NAND型闪存的操作方式效率低，这和它的架构设计和接口设计有关，它操作起来确实挺像硬盘（其实NAND型闪存在设计之初确实考虑了与硬盘的兼容性），它的性能特点也很像硬盘：小数据块操作速度很慢，而大数据块速度就很快，这种差异远比其他存储介质大的多。这种性能特点非常值得我们留意。闪存存取比较快速，无噪音，散热小。用户空间容量需求量小的，打算购置的话可以不考虑太多，同样存储空间买闪存。如果需要容量空间大的（如500G），就买硬盘，较为便宜，也可以满足用户应用的需求。分类播报编辑按种类分U盘、CF卡、SM卡、SD/MMC卡、记忆棒、XD卡、MS卡、TF卡、PCIe闪存卡按品牌分矽统（SIS）、金士顿、索尼、LSI、闪迪、Kingmax、鹰泰、创见、爱国者、纽曼、威刚、联想、台电、微星、SSK、三星、海力士Sandisk【NAND型闪存】内存和NOR型闪存的基本存储单元是bit，用户可以随机访问任何一个bit的信息。而NAND型闪存的基本存储单元是页（Page）（可以看到，NAND型闪存的页就类似硬盘的扇区，硬盘的一个扇区也为512字节）。每一页的有效容量是512字节的倍数。所谓的有效容量是指用于数据存储的部分，实际上还要加上16字节的校验信息，因此我们可以在闪存厂商的技术资料当中看到“（512+16）Byte”的表示方式。2Gb以下容量的NAND型闪存绝大多数是（512+16）字节的页面容量，2Gb以上容量的NAND型闪存则将页容量扩大到（2048+64）字节。NAND型闪存以块（sector）为单位进行擦除操作。闪存的写入操作必须在空白区域进行，如果目标区域已经有数据，必须先擦除后写入，因此擦除操作是闪存的基本操作。一般每个块包含32个512字节的页（page），容量16KB；而大容量闪存采用2KB页时，则每个块包含64个页，容量128KB。每颗NAND型闪存的I/O接口一般是8条，每条数据线每次传输（512+16）bit信息，8条就是（512+16）×8bit，也就是前面说的512字节。但较大容量的NAND型闪存也越来越多地采用16条I/O线的设计，如三星编号K9K1G16U0A的芯片就是64M×16bit的NAND型闪存，容量1Gb，基本数据单位是（256+8）×16bit，还是512字节。寻址时，NAND型闪存通过8条I/O接口数据线传输地址信息包，每包传送8位地址信息。由于闪存芯片容量比较大，一组8位地址只够寻址256个页，显然是不够的，因此通常一次地址传送需要分若干组，占用若干个时钟周期。NAND的地址信息包括列地址（页面中的起始操作地址）、块地址和相应的页面地址，传送时分别分组，至少需要三次，占用三个周期。随着容量的增大，地址信息会更多，需要占用更多的时钟周期传输，因此NAND型闪存的一个重要特点就是容量越大，寻址时间越长。而且，由于传送地址周期比其他存储介质长，因此NAND型闪存比其他存储介质更不适合大量的小容量读写请求。闪存 [1]而比我们平常用的U盘存储量更大，速度更快的闪存产品要属PCIe闪存卡了，它采用低功耗，高性能的闪存存储芯片，以提高应用程序性能。由于它们直接插到服务器中，数据位置接近服务器的处理器，相比其它通过基于磁盘的存储网络路径来获取信息大大节省了时间。企业正在转向这种技术以解决存储密集型工作负载，比如事务处理应用。在PCIe闪存卡方面，LSI公司新的Nytro产品，扩大其基于闪存的应用加速技术到各种规模的企业。LSI推出了三款产品，到一个正变得越来越拥挤的PCIe闪存适配器卡市场。LSI Nytro产品战略中的一部分，LSI公司的WarpDrive卡上，采用闪存存储、LSI的SAS集成控制器和来自公司收购的闪存控制器制造商SandForce的技术。其第二代基于PCIe的应用加速卡容量从200GB到3.2TB不等。Nytro XD应用加速存储解决方案的软件和硬件的组合。它集成了WarpDrive卡与Nytro XD智能高速缓存软件，以提高在存储区域网络(SAN)和直接附加存储(DAS)实现中的I/O速度。最后，还有Nytro MegaRAID应用加速卡，它结合了MegaRAID控制器与板载闪存和缓存软件，LSI公司将Nytro MegaRAID的定位面向低端，针对串行连接SCSI(SAS)DAS环境的性能增强解决方案。微软的SQL Server产品管理主管Claude Lorenson，看好LSI的闪存产品在微软服务器环境中的未来。因为 LSI的闪存产品Nytro MegaRAID可以帮助微软SQL实现了每秒交易的10倍增长， [1]“闪存存储技术，如LSI的Nytro应用加速产品组合，可以用来加速关键业务应用，如SQL Server 2012”，Lorenson在一份公司的声明中表示“随着微软将在Windows Server 8中提供的增强，这些技术的重要性将继续增长。”存储原理播报编辑要讲解闪存的存储原理，还是要从EPROM和EEPROM说起。EPROM是指其中的内容可以通过特殊手段擦去，然后重新写入。其基本单元电路（存储细胞），常采用浮空栅雪崩注入式MOS电路，简称为FAMOS。它与MOS电路相似，是在N型基片上生长出两个高浓度的P型区，通过欧姆接触分别引出源极S和漏极D。在源极和漏极之间有一个多晶硅栅极浮空在SiO2绝缘层中，与四周无直接电气联接。这种电路以浮空栅极是否带电来表示存1或者0，浮空栅极带电后（譬如负电荷），就在其下面，源极和漏极之间感应出正的导电沟道，使MOS管导通，即表示存入0。若浮空栅极不带电，则不形成导电沟道，MOS管不导通，即存入1。EEPROM基本存储单元电路的工作原理如下图所示。与EPROM相似，它是在EPROM基本单元电路的浮空栅的上面再生成一个浮空栅，前者称为第一级浮空栅，后者称为第二级浮空栅。可给第二级浮空栅引出一个电极，使第二级浮空栅极接某一电压VG。若VG为正电压，第一浮空栅极与漏极之间产生隧道效应，使电子注入第一浮空栅极，即编程写入。若使VG为负电压，强使第一级浮空栅极的电子散失，即擦除。擦除后可重新写入。闪存的基本单元电路，与EEPROM类似，也是由双层浮空栅MOS管组成。但是第一层栅介质很薄，作为隧道氧化层。写入方法与EEPROM相同，在第二级浮空栅加以正电压，使电子进入第一级浮空栅。读出方法与EPROM相同。擦除方法是在源极加正电压利用第一级浮空栅与源极之间的隧道效应，把注入至浮空栅的负电荷吸引到源极。由于利用源极加正电压擦除，因此各单元的源极联在一起，这样，快擦存储器不能按字节擦除，而是全片或分块擦除。 到后来，随着半导体技术的改进，闪存也实现了单晶体管（1T）的设计，主要就是在原有的晶体管上加入了浮动栅和选择栅，在源极和漏极之间电流单向传导的半导体上形成贮存电子的浮动棚。浮动栅包裹着一层硅氧化膜绝缘体。它的上面是在源极和漏极之间控制传导电流的选择/控制栅。数据是0或1取决于在硅底板上形成的浮动栅中是否有电子。有电子为0，无电子为1。闪存就如同其名字一样，写入前删除数据进行初始化。具体说就是从所有浮动栅中导出电子。即将有所数据归“1”。写入时只有数据为0时才进行写入，数据为1时则什么也不做。写入0时，向栅电极和漏极施加高电压，增加在源极和漏极之间传导的电子能量。这样一来，电子就会突破氧化膜绝缘体，进入浮动栅。读取数据时，向栅电极施加一定的电压，电流大为1，电流小则定为0。浮动栅没有电子的状态（数据为1）下，在栅电极施加电压的状态时向漏极施加电压，源极和漏极之间由于大量电子的移动，就会产生电流。而在浮动栅有电子的状态（数据为0）下，沟道中传导的电子就会减少。因为施加在栅电极的电压被浮动栅电子吸收后，很难对沟道产生影响。应用前景播报编辑闪存卡“优盘”是闪存走进日常生活的最明显写照，其实早在U盘之前，闪存已经出现在许多电子产品之中。传统的存储数据方式是采用RAM的易失存储，电池没电了数据就会丢失。采用闪存的产品，克服了这一毛病，使得数据存储更为可靠。除了闪存盘，闪存还被应用在计算机中的BIOS、PDA、数码相机、录音笔、手机、数字电视、游戏机等电子产品中。追溯到1998年，优盘进入市场。接口由USB1.0发展到2.0再到最新的USB3.0，速度逐渐提高。U盘的盛行还间接促进了USB接口的推广。为什么U盘这么受到人们欢迎呢？闪存盘可用来在电脑之间交换数据。从容量上讲，闪存盘的容量从16MB到64GB可选，突破了软驱1.44MB的局限性。从读写速度上讲，闪存盘采用USB接口，读写速度比软盘高许多。从稳定性上讲，闪存盘没有机械读写装置，避免了移动硬盘容易碰伤、跌落等原因造成的损坏。部分款式闪存盘具有加密等功能，令用户使用更具个性化。闪存盘外形小巧，更易于携带。且采用支持热插拔的USB接口，使用非常方便。闪存正朝大容量、低功耗、低成本的方向发展。与传统硬盘相比，闪存的读写速度高、功耗较低，市场上已经出现了闪存硬盘，也就是SSD硬盘，该硬盘的性价比进一步提升。随着制造工艺的提高、成本的降低，闪存将更多地出现在日常生活之中。决定因素播报编辑页数量前面已经提到，越大容量闪存的页越多、页越大，寻址时间越长。但这个时间的延长不是线性关系，而是一个一个的台阶变化的。譬如128、256Mb的芯片需要3个周期传送地址信号，512Mb、1Gb的需要4个周期，而2、4Gb的需要5个周期。页容量每一页的容量决定了一次可以传输的数据量，因此大容量的页有更好的性能。前面提到大容量闪存（4Gb）提高了页的容量，从512字节提高到2KB。页容量的提高不但易于提高容量，更可以提高传输性能。我们可以举例子说明。以三星K9K1G08U0M和K9K4G08U0M为例，前者为1Gb，512字节页容量，随机读（稳定）时间12μs，写时间为200μs；后者为4Gb，2KB页容量，随机读（稳定）时间25μs，写时间为300μs。假设它们工作在20MHz。读取性能NAND型闪存的读取步骤分为：发送命令和寻址信息→将数据传向页面寄存器（随机读稳定时间）→数据传出（每周期8bit，需要传送512+16或2K+64次）。K9K1G08U0M读一个页需要：5个命令、寻址周期×50ns+12μs+（512+16）×50ns=38.7μs；K9K1G08U0M实际读传输率：512字节÷38.7μs=13.2MB/s；K9K4G08U0M读一个页需要：6个命令、寻址周期×50ns+25μs+（2K+64）×50ns=131.1μs；K9K4G08U0M实际读传输率：2KB字节÷131.1μs=15.6MB/s。因此，采用2KB页容量比512字节页容量约提高读性能20%。写入性能NAND型闪存的写步骤分为：发送寻址信息→将数据传向页面寄存器→发送命令信息→数据从寄存器写入页面。其中命令周期也是一个，我们下面将其和寻址周期合并，但这两个部分并非连续的。K9K1G08U0M写一个页需要：5个命令、寻址周期×50ns+（512+16）×50ns+200μs=226.7μs。K9K1G08U0M实际写传输率：512字节÷226.7μs=2.2MB/s。K9K4G08U0M写一个页需要：6个命令、寻址周期×50ns+（2K+64）×50ns+300μs=405.9μs。K9K4G08U0M实际写传输率：2112字节/405.9μs=5MB/s。因此，采用2KB页容量比512字节页容量提高写性能两倍以上。块容量块是擦除操作的基本单位，由于每个块的擦除时间几乎相同（擦除操作一般需要2ms，而之前若干周期的命令和地址信息占用的时间可以忽略不计），块的容量将直接决定擦除性能。大容量NAND型闪存的页容量提高，而每个块的页数量也有所提高，一般4Gb芯片的块容量为2KB×64个页=128KB，1Gb芯片的为512字节×32个页=16KB。可以看出，在相同时间之内，前者的擦速度为后者8倍！I/O位宽8gbit闪存以往NAND型闪存的数据线一般为8条，不过从256Mb产品开始，就有16条数据线的产品出现了。但由于控制器等方面的原因，x16芯片实际应用的相对比较少，但将来数量上还是会呈上升趋势的。虽然x16的芯片在传送数据和地址信息时仍采用8位一组，占用的周期也不变，但传送数据时就以16位为一组，带宽增加一倍。K9K4G16U0M就是典型的64M×16芯片，它每页仍为2KB，但结构为（1K+32）×16bit。模仿上面的计算，我们得到如下。K9K4G16U0M读一个页需要：6个命令、寻址周期×50ns+25μs+（1K+32）×50ns=78.1μs。K9K4G16U0M实际读传输率：2KB字节÷78.1μs=26.2MB/s。K9K4G16U0M写一个页需要：6个命令、寻址周期×50ns+（1K+32）×50ns+300μs=353.1μs。K9K4G16U0M实际写传输率：2KB字节÷353.1μs=5.8MB/s可以看到，相同容量的芯片，将数据线增加到16条后，读性能提高近70%，写性能也提高16%。频率工作频率的影响很容易理解。NAND型闪存的工作频率在20～33MHz，频率越高性能越好。前面以K9K4G08U0M为例时，我们假设频率为20MHz，如果我们将频率提高一倍，达到40MHz，则K9K4G08U0M读一个页需要：6个命令、寻址周期×25ns+25μs+（2K+64）×25ns=78μs。K9K4G08U0M实际读传输率：2KB字节÷78μs=26.3MB/s。可以看到，如果K9K4G08U0M的工作频率从20MHz提高到40MHz，读性能可以提高近70%！当然，上面的例子只是为了方便计算而已。在三星实际的产品线中，可工作在较高频率下的应是K9XXG08UXM，而不是K9XXG08U0M，前者的频率可达33MHz。制造工艺制造工艺可以影响晶体管的密度，也对一些操作的时间有影响。譬如前面提到的写稳定和读稳定时间，它们在我们的计算当中占去了时间的重要部分，尤其是写入时。如果能够降低这些时间，就可以进一步提高性能。90nm的制造工艺能够改进性能吗？答案恐怕是否！实际情况是，随着存储密度的提高，需要的读、写稳定时间是呈现上升趋势的。前面的计算所举的例子中就体现了这种趋势，否则4Gb芯片的性能提升更加明显。综合来看，大容量的NAND型闪存芯片虽然寻址、操作时间会略长，但随着页容量的提高，有效传输率还是会大一些，大容量的芯片符合市场对容量、成本和性能的需求趋势。而增加数据线和提高频率，则是提高性能的最有效途径，但由于命令、地址信息占用操作周期，以及一些固定操作时间（如信号稳定时间等）等工艺、物理因素的影响，它们不会带来同比的性能提升。1Page=（2K+64）Bytes；1Block=（2K+64）B×64Pages=（128K+4K）Bytes；1Device=（2K+64）B×64Pages×4096Blocks=4224Mbits其中：A0～11对页内进行寻址，可以被理解为“列地址”。A12～29对页进行寻址，可以被理解为“行地址”。为了方便，“列地址”和“行地址”分为两组传输，而不是将它们直接组合起来一个大组。因此每组在最后一个周期会有若干数据线无信息传输。没有利用的数据线保持低电平。NAND型闪存所谓的“行地址”和“列地址”不是我们在DRAM、SRAM中所熟悉的定义，只是一种相对方便的表达方式而已。为了便于理解，我们可以将上面三维的NAND型闪存芯片架构图在垂直方向做一个剖面，在这个剖面中套用二维的“行”、“列”概念就比较直观了。发展过程播报编辑发展历史在1984年，东芝公司的发明人舛冈富士雄首先提出了快速闪存存储器（此处简称闪存）的概念。与传统电脑内存不同，闪存的特点是非易失性（也就是所存储的数据在主机掉电后不会丢失），其记录速度也非常快。Intel是世界上第一个生产闪存并将其投放市场的公司。1988年，公司推出了一款256K bit闪存芯片。它如同鞋盒一样大小，并被内嵌于一个录音机里。後来，Intel发明的这类闪存被统称为NOR闪存。它结合EPROM（可擦除可编程只读存储器）和EEPROM（电可擦除可编程只读存储器）两项技术，并拥有一个SRAM接口。第二种闪存称为NAND闪存。它由日立公司于1989年研制，并被认为是NOR闪存的理想替代者。NAND闪存的写周期比NOR闪存短90%，它的保存与删除处理的速度也相对较快。NAND的存储单元只有NOR的一半，在更小的存储空间中NAND获得了更好的性能。鉴于NAND出色的表现，它常常被应用于诸如CompactFlash、SmartMedia、 SD、 MMC、 xD、 and PC cards、USB sticks等存储卡上。2021年，第三届中国西部国际投资贸易洽谈会上，长江存储带来128层QLC规格的3D NAND闪存，是业内首款128层QLC 3D NAND 闪存，拥有业内已知型号产品中最高单位面积存储密度，最高I/O传输速度和最高单颗NAND 闪存芯片容量。 [2]市场分析闪存市场仍属于群雄争霸的未成熟时期。三星、日立、Spansion和Intel是这个市场的四大生产商。由于战略上的一些错误，Intel在第一次让出了它的榜首座椅，下落至三星、日立和Spansion之後。AMD闪存业务部门Spansion同时生产NAND和NOR闪存。它上半年的NOR闪存产量几乎与Intel持平，成为NOR闪存的最大制造商。该公司在上半年赢利为13亿美元，几乎是它整个公司利润额（25亿美元）的一半以上。总体而言，Intel和AMD在上半年成绩喜人，但三星和日立却遭受挫折。替代品与许多寿命短小的信息技术相比，闪存以其多年的发展历程，充分显示了其“老前辈”的作风。九十年代初，闪存才初入市场；至2000年，利益额已突破十亿美元。英飞凌科技闪存部门主任，彼得曾说：“就闪存的生命周期而言，我们仍处于一个上升的阶段。”英飞凌相信，闪存的销售仍具有上升空间，并在酝酿加入对该市场的投入。英飞凌宣布，其位于德累斯顿的200毫米DRAM工厂已经开始生产512Mb NAND兼容闪存芯片。到2004年底，英飞凌公司计划采用170纳米制造工艺，每月制造超过10,000片晶圆。而2007年，该公司更希望在NAND市场成为前三甲。尽管对闪存替代品的讨论越来越激励，闪存仍然受到市场的重视。未来的替代品不仅必须是类似闪存一样的非易失性存储器，而且在速度和写周期上略胜一筹。此外，生产成本也应该相对低廉。由于制造技术还不成熟，新的替代品不会对闪存构成绝对的威胁。与硬盘比播报编辑如果单从储存介质上来说 ，闪存比硬盘好。这是指数据传输的速度还有抗震度来说（闪存不存在抗震）。优点：1．闪存的体积小。并不是说闪存的集成度就一定会高。微硬盘做的这么大一块主要原因就是微硬盘不能做的小过闪存，并不代表微硬盘的集成度就不高。2．相对于硬盘来说闪存结构不怕震，更抗摔。硬盘最怕的就是强烈震动。虽然我们使用的时候可以很小心，但老虎也有打盹的时候，不怕一万就怕万一。3．闪存可以提供更快的数据读取速度，硬盘则受到转速的限制。4．闪存存储数据更加安全，原因包括：1.其非机械结构，因此移动并不会对它的读写产生影响；2.广泛应用的机械型硬盘的使用寿命与读写次数和读写速度关系非常大，而闪存受影响不大；3.硬盘的写入是靠磁性来写入，闪存则采用电压，数据不会因为时间而消除。5．质量更轻。缺点：1、材料贵，所以单位容量更贵。2、读写速度相对较慢。问题解决播报编辑1.什么是usb2.0usb 2.0是usb技术的新版本。传输速率高达480mbps，是usb1.1的40倍。适合新型高速外设。它继承了usb 1.1的易用性，即插即用、免安装驱动，完全兼容usb1.1标准，您已经购买的usb1.1 设备和连接线仍然可以继续使用。2.关于USB要知道：USB1.1的闪存盘读速一般为630KB，写速一般为520KB；USB2.0的读速一般为1.5MB，写速一般为1.0MBusb2.0设备接在usb1.1接口上，但受usb1.1的速度限制 发挥不了USB2.0效果。同时使用usb2.0和usb1.1设备，在os 9.x系统中使用usb2.0设备可以，但必须安装驱动程序；但是这些操作系统并不支持usb2.0，该设备在这些系统中只能工作在usb1.1模式6.读写闪存盘时，是否可以运行其它应用程序？可以。7.闪存盘可擦写多少次？闪存盘里的数据能保存多久？闪存盘可擦写1000000次，闪存盘里数据可保存10年8.一台电脑可同时接几个闪存盘？理论上一台电脑可同时接127个闪存盘，但由于驱动器英文字母的排序原因 以及现有的驱动器需占用几个英文字母，故闪存盘最多只可以接23个（除开 A、B、C) 且需要USB HUB的协助。9.闪存盘在DOS状态下能否使用闪存盘支持WINDOWS虚拟DOS方式（启动Windows后在附件中进入）。10.闪存盘支持WINDOWS 95吗闪存盘不支持WINDOWS 95操作系统，建议用户升级操作系统至WINDOWS98或以上版本。11.WINDOWS NT4.0下闪存盘如何使用12.闪存盘可以在什么驱动程序下使用？A9 Windows98、Windows ME、Windows 2000、Windows XP、Windows7、Windows8、Windows8.1、MAC OS、Linux。13.闪存盘是否需要驱动程序？在Mac OS 、Windows 2000以上版本上不需要，在Win 98上需要驱动程序14.闪存盘可以在Windows 98 / Windows 2000 / Mac OS下被格式化吗可以。15.闪存盘的内容能否加密？可以。16.闪存盘在局域网里是否可以共享？可以。17.闪存盘可以存储哪些类型的数据？所有电脑数据都可以存储，包括文件、程序、图象、音乐、多媒体等。18.安装闪存盘时是否需要关闭电脑？不需要，闪存盘是即插即用型产品，可以进行插拔。19.闪存盘可以防水吗？闪存盘是电子类产品，掉入水中后可能会造成闪存盘内部短路而损坏。20.插拔闪存盘时，有哪些注意事项当闪存盘指示灯快闪时，即电脑在读写闪存盘状态下，不要拔下闪存盘；当插入闪存盘后，最好不要立即拔出。特别是不要反复快速插拔，因为操作系统需要一定的反应时间，中间的间隔最好在5秒以上。21.闪存盘是否会感染病毒？闪存盘像所有硬盘一样可能感染病毒22.闪存盘用于桌面电脑时，并且USB接口在电脑的后面时，有什么办法使之更方便？通过一条USB转接电缆（具有 A-Type Plug and A-Type Receptacle）与电脑连接23.存盘的LED灯显示表示什么含义？当LED灯亮的时候，它表示闪存盘连接成功暂时没有数据传输。当LED闪烁的时候，它表示闪存盘正在数据传输过程中。24.当闪存盘的LED还在闪时，是否可以拔出闪存盘？不可以。会使闪存盘的数据丢失或使FAT表破坏且出现蓝屏。当操作系统读闪存盘时它会使电脑出现蓝屏。25.闪存盘上的文件出现乱码或文件打不开使用闪存盘专用工具做格式化26.双击闪存盘盘符时，电脑提示闪存盘需格式化当闪存盘分区表遭到破坏或是闪存盘性能不稳定时，会出现上述现象。出现这种问题，一般可以使用闪存盘专用工具做格式化27.闪存盘写保护不起作用，在写保护关锁状态，数据也能够顺利写入。28.切换闪存写保护开关，需要在断开与电脑的联接的状态下进行。如果是在与电脑联接状态下切换了写保护开关，需要重新插拔一次闪存，才能切实使切换起作用。磁荷随机存储器两家公司都认为，MRAM不仅将是闪存的理想替代品，也是DRAM与SRAM的强有力竞争者。今年六月，英飞凌已将自己的第一款产品投放市场。与此同时，Freescale也正在加紧研发，力争推出4M bit芯片。但是，一些评论者担心MRAM是否能达到闪存存储单元的尺寸。根据英飞凌的报告，闪存存储单元的尺寸为0.1&micro;m²，而16M bit MRAM芯片仅达到1.42 &micro;m²。另外，MRAM的生产成本也是个不小的问题。OUMOUM（Ovonic Unified Memory Ovonyx标准化内存）OUM是由Intel研发的，利用Ge、Sb与Te等化合物为材料制成的薄膜。OUM。OUM的写、删除和读的功能与CD-RW与DVD-RW相似。但CD/DVD使用激光来加热和改变称为硫系化合物（chalcogenides）的材料；而OUM则通过电晶体控制电源，使其产生相变方式来储存资料。OUM的擦写次数为10的12次方，100次数据访问时间平均为200纳秒。OUM的速度比闪存要快。尽管OUM比MRAM的数据访问时间要慢，但是低廉的成本却是OUM的致胜法宝。与MRAM不同，OUM的发展仍处于初期。尽管已制成测试芯片，它们仅仅能用来确认概念而不是说明该技术的可行性。Intel在过去四年一直致力于OUM的研发，并正在努力扩大该市场。闪存式U盘总结除了上文提到的MRAM和OUM，其它可替代的产品还有MRAM （FeRAM）、 Polymer memory （PFRAM）、 PCRAM、 Conductive Bridge RAM （CBRAM）、 Organic RAM （ORAM）以及最近的Nanotube RAM （NRAM）。替代闪存的产品有许多，但是哪条路能够成功，以及何时成功仍然值得怀疑。对大多数公司而言，闪存仍是一个理想的投资。不少公司已决定加大对闪存的投资额。此外，据估计，到2004年，闪存总产值将与DRAM并驾齐驱，到2006年将超越DRAM产品。

DRAM：
简介播报编辑动态随机存取存储器（Dynamic Random Access Memory，DRAM）是一种半导体存储器，主要的作用原理是利用电容内存储电荷的多寡来代表一个二进制比特（bit）是1还是0。由于在现实中晶体管会有漏电电流的现象，导致电容上所存储的电荷数量并不足以正确的判别数据，而导致数据毁损。因此对于DRAM来说，周期性地充电是一个无可避免的要件。由于这种需要定时刷新的特性，因此被称为“动态”存储器。相对来说，静态存储器（SRAM）只要存入数据后，纵使不刷新也不会丢失记忆。与SRAM相比，DRAM的优势在于结构简单——每一个比特的数据都只需一个电容跟一个晶体管来处理，相比之下在SRAM上一个比特通常需要六个晶体管。正因这缘故，DRAM拥有非常高的密度，单位体积的容量较高因此成本较低。但相反的，DRAM也有访问速度较慢，耗电量较大的缺点。与大部分的随机存取存储器（RAM）一样，由于存在DRAM中的数据会在电力切断以后很快消失，因此它属于一种易失性存储器（volatile memory）设备。 [1]工作原理播报编辑DRAM通常以一个电容和一个晶体管为一个单元排成二维矩阵。基本的操作机制分为读(Read)和写(Write)，读的时候先让Bitline(BL)先充电到操作电压的一半，然后再把晶体管打开让BL和电容产生电荷共享的现象，若内部存储的值为1，则BL的电压会被电荷共享抬高到高于操作电压的一半，反之，若内部存储的值为0，则会把BL的电压拉低到低于操作电压的一半，得到了BL的电压后，在经过放大器来判别出内部的值为0和1。写的时候会把晶体管打开，若要写1时则把BL电压抬高到操作电压使电容上存储著操作电压，若要写0时则把BL降低到0伏特使电容内部没有电荷。 [1]随机存取存储器播报编辑随机存取存储器（英语：RandomAccessMemory，缩写：RAM），也叫主存，是与CPU直接交换数据的内部存储器。它可以随时读写（刷新时除外，见下文），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时数据存储媒介。主存（Main memory）即电脑内部最主要的存储器，用来加载各式各样的程序与数据以供CPU直接运行与运用。由于DRAM的性价比很高，且扩展性也不错，是现今一般电脑主存的最主要部分。2014年生产电脑所用的主存主要是DDR3 SDRAM，而2016年开始DDR4 SDRAM逐渐普及化，笔电厂商如华硕及宏碁开始在笔电以DDR4存储器取代DDR3L。 [1]相关条目播报编辑存储器挥发性记忆体静态随机存取存储器动态随机存储器价格操纵SDRAM

硬件：
基本部件播报编辑运算器，控制器，存储器联系计算机由运算器、控制器、存储器、输入设备和输出设备等五个逻辑部件组成运算器硬件(13张)运算器由算术逻辑单元（ALU）、累加器、状态寄存器、通用寄存器组等组成。算术逻辑运算单元（ALU）的基本功能为加、减、乘、除四则运算，与、或、非、异或等逻辑操作，以及移位、求补等操作。控制器控制器（Control Unit），是整个计算机系统的控制中心，它指挥计算机各部分协调地工作，保证计算机按照预先规定的目标和步骤有条不紊地进行操作及处理。控制器从存储器中逐条取出指令，分析每条指令规定的是什么操作以及所需数据的存放位置等，然后根据分析的结果向计算机其它部件发出控制信号，统一指挥整个计算机完成指令所规定的操作。中央处理器中央处理器（CentralProcessingUnit，CPU），由运算器和控制器组成，是任何计算机系统中必备的核心部件。CPU由运算器和控制器组成，分别由运算电路和控制电路实现。存储器存储器（Memory）是计算机系统中的记忆设备，用来存放程序和数据。计算机中全部信息，包括输入的原始数据、计算机程序、中间运行结果和最终运行结果都保存在存储器中。它根据控制器指定的位置存入和取出信息。有了存储器，计算机才有记忆功能，才能保证正常工作。输入部件向计算机输入数据和信息的设备。是计算机与用户或其他设备通信的桥梁。输入设备是用户和计算机系统之间进行信息交换的主要装置之一。输出设备输出设备（Output Device）是计算机的终端设备，用于接收计算机数据的输出显示、打印、声音、控制外围设备操作等。也是把各种计算结果数据或信息以数字、字符、图像、声音等形式表示出来。计算机部件表格计算机基本部件输入设备键盘鼠标扫描仪数码绘图板触摸板轨迹球麦克风摄像头输出设备显示器音箱打印机耳机扬声器投影仪存储设备固态硬盘移动硬盘CDDVD软盘闪存磁带机机箱内的设备中央处理器随机存取存储器显示卡声卡主板电源供应器硬盘接口串行端口并行端口USBFirewirePS/2RJ-45VGADVITRSS/PDIFHDMI关系表播报编辑输入键盘鼠标触摸板轨迹球数字化输入板及输入笔/指向器触控屏幕游戏控制器游戏控制杆麦克风扫描仪条码阅读机摄像头数码相机存储设备可携存储设备CD、CD-ROM、CD-RW、CD-RDVD、DVD/CD-RW Combo、DVD-ROM、DVD-RW、DVD-R、DVD-RAM、DVD+RW、DVD+R软盘磁带机移动硬盘闪存快闪碟存储卡SD、CF、MMC、SM内置存储器硬盘固态硬盘磁盘阵列控制器

微操作：
历史背景播报编辑在即时战略游戏流行之初，并没有微操的概念。在红色警戒时代，玩家们通常更关注如何制造更多的作战单位来取得游戏的胜利。但是随着玩家对游戏熟悉度的增加，有越来越多的人发现，在局部战场上，若是对单位进行适当的操作控制，就能大大优化其作战效率，甚至能达到以少胜多的巨大效果。这是由于游戏设计AI总以某个特定规则进行动作，而事实上，这些规则并不是最优的。例如AI通常会做出类似判断：对一个目标一直攻击直至其消灭，若是目标脱出攻击范围则查找范围内有无其他攻击目标，有则对其攻击直至消灭……而此时第一个目标若是重新回到攻击范围内，AI也会无视它，继续攻击第二个目标。在这个规则下，由AI控制的四个单位分两队厮杀的结果是同归于尽。但是若有一方人为的控制单位，使其分摊伤害，就能够达成2:0的结果。发展播报编辑当游戏平衡性更佳的星际争霸流行之后，微操作的意义变得越来越大。而在职业电子竞技选手之间，微操作能力也成了评价其水准的重要指标。由于微操在局部战场的对抗上能产生巨大影响，双方选手互相施展全力进行操作，这让比赛的观赏性大大提高！一些精妙的操作更被爱好者们津津乐道，微操也成了比赛重要的看点之一。随着魔兽争霸Ⅲ的推出，游戏单位数量的减少也意味着微操作的重要度更高。同时，也有了一群以注重极限优化的微操作来获取比赛胜利的玩家流派出现，既“操作流”，亦会被戏谑为“抽筋流”。APM播报编辑Action Per Minute的缩写，是由软件统计出操作者每分钟各种操作数的数据，从某种程度上能反映玩家的微操作水平。一个职业玩家的APM通常在200以上，甚至超过300。由于APM统计的是所有操作数，并不能表现出操作的有效性，所以仅仅靠APM来说明微操作水平是片面的。围杀播报编辑用己方若干单位（通常是四个以上，若是利用地形的话可以更少）围住并杀死敌方单位，在魔兽争霸中由于单位数量的减少，每个单位的价值也大大提高，这也使得围杀敌方单位有了更大的意义，而且被围杀的单位若是一个具有重大战略意义的英雄的话，这几乎可以左右胜负了。卡位播报编辑在魔兽争霸以及各种即时战略游戏中，由于英雄和小兵都有碰撞体积，而利用自己挡住对方（比如他要追杀你同伴的时候，或者被追杀的时候）叫做卡位。或者是利用视野的盲角，让对方找不到自己，叫做走位。引申义播报编辑不管多么中性含义的词，也有人能听出邪恶的内涵。因此，微操也被极少数人用作人身攻击的词汇。微操（微：精细连贯；操：画面单角色或多角色操作）的含义变成了（微：极短的，不长久的；操：合体）

微指令：
定义播报编辑微指令是指在机器的一个CPU周期中，一组实现一定操作功能的微命令的组合 [1]，描述微操作的语句。微命令是指控制部件通过控制线向执行部件发出各种控制命令。操作微指令是描述受控电路的操作语句 , 分支微指令是描述控制电路的分支语句。一条机器指令的功能是若干条微指令组成的序列来实现的，即一条机器指令所完成的操作分成若干条微指令来完成，由微指令进行解释和执行，这个微指令序列通常叫做微程序。微指令的编译方法是决定微指令格式的主要因素。考虑到速度，成本等原因，在设计计算机时采用不同的编译法 。因此微指令的格式大体分成两类：水平型微指令和垂直型微指令。类型播报编辑水平型微指令一次能定义并执行多个并行操作微命令的微指令，叫做水平型微指令。水平型微指令的一般格式如下：控制字段，判别测试字段和下地址字段。按照控制字段的编码方法不同，水平型微指令又分为三种：一种是全水平型(不译法)微指令，第二种是字段译码法水平型微指令，第三种是直接和译码相混合的水平型微指令。垂直型微指令微指令中设置微操作码字段，采用微操作码编译法，由微操作码规定微指令的功能，称为垂直型微指令。垂直型微指令的结构类似于机器指令的结构.它有操作码，在一条微指令中只有l-2个微操作命令，每条微指令的功能简单，因此，实现一条机器指令的微程序要比水平型微指令编写的微程序长得多 .它是采用较长的微程序结构去换取较短的微指令结构。水平型微指令与垂直型微指令的比较(1)水平型微指令并行操作能力强，指令高效，快速，灵活，垂直型微指令则较差。(2)水平型微指令执行一条指令时间短，垂直型微指令执行时间长。(3)由水平型微指令解释指令的微程序，有微指令字较长而微程序短的特点。垂直型微指令则相反。(4)水平型微指令用户难以掌握，而垂直型微指令与指令比较相似，相对来说，比较容易掌握。规范化描述播报编辑规范化描述就是在指令系统的微指令描述中尽量减小语句使用的随意性，使整个指令系统的描述具有较强的规律性，并使微操作集中的元素最少。事实上只要微指令描述合理规范，从微程序设计角度来看，所描述的功能都是可以通过ASIC技术实现的。在一条指令的描述中， 指令的微操作步数必须与指令所需的时钟周期数相吻合，分配好各微指令序列所占的时钟数，安排好各微指令组和各微指令序列在整个控制序列中的位置，这是指令系统规范化描述的基础。在同类指令的描述中， 完成相同微功能的微指令序列所占的时钟周期数必须相同， 在控制序列中的分配位置必须合理。例如字除法指令比字节除法指令多8个状态周期，因此每位除法只能占用一个状态周期。再例如操作数长度相同的有符号数除法指令和无符号数除法指令相比多增加 4个状态周期，因此有符号除法中被除数和除数、商和余数的符号化处理，只能分别在2个状态周期中实现，且删除这4个状态周期中的所有微指令 [2]。相关指令播报编辑机器指令和微指令的关系一台数字计算机基本上可以划分为两大部分——控制部件和执行部件。控制器就是控制部件，而运算器、存储器、外围设备相对控制器来说就是执行部件。控制部件与执行部件的一种联系就是通过控制线。控制部件通过控制线向执行部件发出各种控制命令，通常这种控制命令叫做微命令，而执行部件接受微命令后所执行的操作就叫做微操作。控制部件与执行部件之间的另一种联系就是反馈信息。执行部件通过反馈线向控制部件反映操作情况，以便使得控制部件根据执行部件的状态来下达新的微命令，这也叫做“状态测试”。微操作在执行部件中是组基本的操作。由于数据通路的结构关系，微操作可分为相容性和相斥性两种。在机器的一个CPU周期中，一组实现一定操作功能的微命令的组合，构成一条微指令。一般的微指令格式由操作控制和顺序控制两部分构成。操作控制部分用来发出管理和指挥全机工作的控制信号。其顺序控制部分用来决定产生下一个微指令的地址。事实上一条机器指令的功能是由许多条微指令组成的序列来实现的。这个微指令序列通常叫做微程序。既然微程序是由微指令组成的，那么当执行当前的一条微指令的时候。必须指出后继微指令的地址，以便当前一条微指令执行完毕以后，取下一条微指令执行。机器指令和微指令的关系归纳如下：1. 一条机器指令对应一个微程序，这个微程序是由若干条微指令构成的。因此，一条机器指令的功能是若干条微指令组成的序列来实现的。简而言之，一条机器指令所完成的操作划分成若干条微指令来完成，由微指令进行解释和执行。2.从指令与微指令，程序与微程序，地址与微地址的一一对应关系上看，前者与内存储器有关，而后者与控制存储器（它是微程序控制器的一部分。微程序控制器主要由控制存储器、微指令寄存器和地址转移逻辑三部分组成。其中，微指令寄存器又分为微地址寄存器和微命令寄存器两部分）有关，与此相关也有相对应的硬设备。3.一条机器指令对应4个CPU周期，每个CPU周期就对于一条微指令。

微程序：
微程序控制器播报编辑微程序控制的提出，其主要目的是希望能实现灵活可变的计算机指令系统。 [2]（1）微程序控制 [2]微程序控制和组合逻辑控制是微命令产生的两种方式。组合逻辑控制方式采用许多门电路，设计复杂，设计效率低，检查调试困难，而微程序控制器改进了其缺点。微程序控制器的核心部件是微地址转移逻辑。 [2]微程序控制器的基本思想包括以下两点： [2]① 将控制器所需的微命令以代码形式编成微指令，存入一个由ROM构成的控制存储器（CM）中。 [2]② 将各种机器指令的操作分解成若干微操作序列。每条微指令包含的微命令控制实现一步操作。若干条微指令组成一小段微程序，解释执行一条机器指令。 [2]（2）常见概念及定义 [2]① 微命令：构成控制信号序列的最小单位。 [2]② 微操作：由微命令控制实现的最基本的操作。 [2]③ 微周期：从控制存储器读取一条微指令并执行相应的一步操作所需的时间。通常一个时钟周期为一个微周期。 [2]④ 控制存储器（微指令存储器）：主要存放控制命令（信号）和下一条要执行的微指令地址。由于计算机的指令系统是固定的，实现这个指令系统的微程序也是固定的，所以控制存储器采用只读存储器（ROM）。 [2]微程序控制器原理播报编辑微程序控制器的基本原理是用多条微指令（Microinstruction）组成的微程序解释执行一条指令的功能，硬件组成的核心电路是“控制存储器”（简称控存，用ROM芯片实现，即固件），用于保存由微指令代码（Microcode）组成的微程序。在指令执行过程中，按照指令及其执行步骤，依次从控制存储器中读出微指令，用微指令控制各执行部件的运行，并用下一地址字段形成下一条微指令的地址，使微指令可以连续运行。 [3]

硬布线控制器：
简介播报编辑硬布线控制器，又称组合逻辑控制器方法原理播报编辑图1一旦控制部件构成后，除非重新设计和物理上对它重新布线，否则要想增加新的控制功能是不可能的。 硬布线控制器是计算机中最复杂的逻辑部件之一。当执行不同的机器指令时，通过激活一系列彼此很不相同的控制信号来实现对指令的解释，其结果使得控制器往往很少有明确的结构而变得杂乱无章。结构上的这种缺陷使得硬布线控制器的设计和调试非常复杂且代价很大。正因为如此，硬布线控制器被微程序控制器所取代。但是，在同样的半导体工艺条件下，硬布线控制器速度要比微程序控制的快，随着新一代机器及VLSI技术的发展与不断进步，硬布线的随机逻辑设计思想又得到了重视，现代新型计算机体系结构如RISC中多采用硬布线控制逻辑。硬布线控制器主要由组合逻辑网络、指令寄存器和指令译码器、节拍电位/节拍脉冲发生器等部分组成，硬布线控制器的结构方框图如图1所示。其中组合逻辑网络产生计算机所需的全部操作命令，是控制器的核心。信号来源播报编辑(1)来自指令操作码译码器的输出I1～Im ,译码器每根输出线表示一条指令，译码器的输出反映出当前正在执行的指令；(2)来自执行部件的反馈信息B1～Bj ；(3)来自时序产生器的时序信号，包括节拍电位信号M1～Mi和节拍脉冲信号T1～Tk。其中节拍电位信号就是机器周期(CPU周期)信号，节拍脉冲信号是时钟周期信号。组合逻辑网络N的输出信号就是微操作控制信号C1～Cn，用来对执行部件进行控制。另有一些信号则根据条件变量来改变时序发生器的计数顺序，以便跳过某些状态，从而可以缩短指令周期。硬布线控制器的基本原理，归纳起来可叙述为：某一微操作控制信号C是指令操作码译码器输出Im 、时序信号(节拍电位Mi ，节拍脉冲Tk )和状态条件信号Bj 的逻辑函数，其数学描述为：C=f(Im，Mi，Tk，Bj )控制信号C是用门电路、触发器等许多器件采用布尔代数方法来设计实现的。当机器加电工作时，某一操作控制信号C在某条特定指令和状态条件下，在某一操作的特定节拍电位和节拍脉冲时间间隔中起作用，从而激活这条控制信号线，对执行部件实施控制。显然，从指令流程图出发，就可以一个不漏地确定在指令周期中各个时刻必须激活的所有操作控制信号。例如，对引起一次主存读操作的控制信号C3来说，当节拍电位M1=1，取指令时被激活；而节拍电位M4=1，三条指令（LDA，ADD，AND）取操作数时也被激活，此时指令译码器的LDA，ADD，AND输出均为1，因此C3的逻辑表达式可由下式确定：C3=M1+M4（LDA+ADD+AND）一般来说，还要考虑节拍脉冲和状态条件的约束，所以每一控制信号C可以由以下形式的布尔代数表达式来确定：Cn=∑(Mi*Tk*Bj*∑Im)与微程序控制相比，硬布线控制的速度较快。其原因是微程序控制中每条微指令都要从控存中读取一次，影响了速度，而硬布线控制主要取决于电路延迟。因此，在某些超高速新型计算机结构中，又选用了硬布线控制器或与微程序控制器混合使用。设计注意播报编辑(1) 采用适宜指令格式，合理分配指令操作码；(2) 确定机器周期、节拍与主频；(3) 确定机器周期数及一周期内的操作；(4) 进行指令综合； 综合所有指令的每一个操作命令，写出逻辑表达式，并进行化简。(5) 明确组合逻辑电路。 将简化后的逻辑表达式用组合逻辑电路来实现。操作命令的控制信号先用逻辑表达式列出，进行化简，考虑各种条件的约束，合理选用逻辑门电路、触发器等器件，采用组合逻辑电路的设计方法产生控制信号。总之，控制信号的设计与实现，技巧性较强，一些专门的开发系统或工具供逻辑设计使用，但是，对全局的考虑主要依靠设计人员的智慧和经验实现。比较硬布线控制器与微程序控制器相比较，在操作控制信号的形成上有较大的区别外，其它没有本质的区别。对于实现相同的一条指令，不管是采用硬布线控制还是采用微程序控制技术，都可以采用多种逻辑设计方案，导致了各种不同的控制器在具体实现方法和手段上的区别，性能差异。硬布线控制与微程序控制的主要区别归纳为如下方面：实现方式微程序控制器的控制功能是在存放微程序存储器和存放当前正在执行的微指令的寄存器直接控制下实现的，而硬布线控制的功能则由逻辑门组合实现。微程序控制器的电路比较规整，各条指令信号的差别集中在控制存储器内容上，因此，无论是增加或修改指令都只要增加或修改控制存储器内容即可，若控制存储器是ROM，则要更换芯片，在设计阶段可以先用RAM或EPROM来实现，验证正确后或成批生产时，再用ROM代替。硬布线控制器的控制信号先用逻辑式列出，经化简后用电路来实现，因此，显得零乱复杂，当需要修改指令或增加指令时就必须重新设计电路，非常麻烦而且有时甚至无法改变。因此，微操作控制取代了硬布线控制并得到了广泛应用，尤其是指令复杂的计算机，一般都采用微程序来实现控制功能。性能方面在同样的半导体工艺条件下，微程序控制的速度比硬布线控制的速度低，因为执行每条微程序指令都要从控制存储器中读取，影响了速度；而硬布线控制逻辑主要取决于电路延时，因而在超高速机器中，对影响速度的关键部分如核心部件CPU，往往采用硬布线逻辑实现。在一些新型计算机系统中，例如，RISC(精简指令系统计算机)中，一般都选用硬布线逻辑电路。

I/O接口：
基本功能播报编辑· 进行端口地址译码设备选择。· 向CPU提供I/O设备的状态信息和进行命令译码。· 进行定时和相应时序控制。· 对传送数据提供缓冲，以消除计算机与外设在“定时”或数据处理速度上的差异。· 提供计算机与外设间有关信息格式的相容性变换。提供有关电气的适配· 还可以中断方式实现CPU与外设之间信息的交换。接口组成播报编辑I/O接口包括硬件电路和软件编程两部分 硬件电路包括基本逻辑电路，端口译码电路和供选电路等。软件编程包括初始化程序段，传送方式处理程序段，主控程序段程序终止与退出程序段及辅助程序段等.接口分类播报编辑I/O接口的功能是负责实现CPU通过系统总线把I/O电路和外围设备联系在一起，按照电路和设备的复杂程度，I/O接口的硬件主要分为两大类：（1）I/O接口芯片这些芯片大都是集成电路，通过CPU输入不同的命令和参数，并控制相关的I/O电路和简单的外设作相应的操作，常见的接口芯片如定时/计数器、中断控制器、DMA控制器、并行接口等。（2）I/O接口控制卡有若干个集成电路按一定的逻辑组成为一个部件，或者直接与CPU同在主板上，或是一个插件插在系统总线插槽上。按照接口的连接对象来分，又可以将他们分为串行接口、并行接口、键盘接口和磁盘接口等。接口功能播报编辑由于计算机的外围设备品种繁多，几乎都采用了机电传动设备，因此，CPU在与I/O设备进行数据交换时存在以下问题：速度不匹配：I/O设备的工作速度要比CPU慢许多，而且由于种类的不 同，他们之间的速度差异也很大，例如硬盘的传输速度就要比打印机快出很多。时序不匹配：各个I/O设备都有自己的定时控制电路，以自己的速度传 输数据，无法与CPU的时序取得统一。信息格式不匹配：不同的I/O设备存储和处理信息的格式不同，例如可以分为串行和并行两种；也可以分为二进制格式、ACSII编码和BCD编码等。信息类型不匹配：不同I/O设备采用的信号类型不同，有些是数字信号，而 有些是模拟信号，因此所采用的处理方式也不同。基于以上原因，CPU与外设之间的数据交换必须通过接口来完成，通常接口有以下一些功能：（1）设置数据的寄存、缓冲逻辑，以适应CPU与外设之间的速度差异，接口通常由一些寄存器或RAM芯片组成，如果芯片足够大还可以实现批量数据的传输；（2）能够进行信息格式的转换，例如串行和并行的转换；（3）能够协调CPU和外设两者在信息的类型和电平的差异，如电平转换驱动器、数/模或模/数转换器等；（4）协调时序差异；（5）地址译码和设备选择功能；（6）设置中断和DMA控制逻辑，以保证在中断和DMA允许的情况下产生中断和DMA请求信号，并在接受到中断和DMA应答之后完成中断处理和DMA传输。控制方式播报编辑CPU通过接口对外设进行控制的方式有以下几种：（1）程序查询方式这种方式下，CPU通过I/O指令询问指定外设当前的状态，如果外设准备就绪，则进行数据的输入或输出，否则CPU等待，循环查询。这种方式的优点是结构简单，只需要少量的硬件电路即可，缺点是由于CPU的速度远远高于外设，因此通常处于等待状态，工作效率很低（2）中断处理方式在这种方式下，CPU不再被动等待，而是可以执行其他程序，一旦外设为数据交换准备就绪，可以向CPU提出服务请求，CPU如果响应该请求，便暂时停止当前程序的执行，转去执行与该请求对应的服务程序，完成后，再继续执行原来被中断的程序。中断处理方式的优点是显而易见的，它不但为CPU省去了查询外设状态和等待外设就绪所花费的时间，提高了CPU的工作效率，还满足了外设的实时要求。但需要为每个I/O设备分配一个中断请求号和相应的中断服务程序，此外还需要一个中断控制器（I/O接口芯片）管理I/O设备提出的中断请求，例如设置中断屏蔽、中断请求优先级等。此外，中断处理方式的缺点是每传送一个字符都要进行中断，启动中断控制器，还要保留和恢复现场以便能继续原程序的执行，花费的工作量很大，这样如果需要大量数据交换，系统的性能会很低。（3）DMA（直接存储器存取）传送方式DMA最明显的一个特点是它不是用软件而是采用一个专门的控制器来控制内存与外设之间的数据交流，无须CPU介入，大大提高CPU的工作效率。在进行DMA数据传送之前，DMA控制器会向CPU申请总线控制 权，CPU如果允许，则将控制权交出，因此，在数据交换时，总线控制权由DMA控制器掌握，在传输结束后，DMA控制器将总线控制权交还给CPU。（4）无条件传送方式（5）I/O通道方式（6）I/O处理机方式

IO：
定义播报编辑输入输出I/O流可以看成对字节或者包装后的字节的读取就是拿出来放进去双路切换；实现联动控制系统的弱电线路与被控设备的强电线路之间的转接、隔离，以防止强电窜入系统，保障系统的安全；与专线控制盘连接，用于控制重要消防设备（如消防泵、喷淋泵、风机等），一只模块可控制一台大型消防设备的启、停控制；插拔式结构，可像安装探测器一样先将底座安装在墙上，布线后工程调试前再将切换模块插入底座。易于施工、维护；通过无源动合接点或切换AC220V电压作为回答信号。确认灯动作灯—红色，回答灯—绿色；动作时，动作灯常亮、回答灯常亮。IO输出口可接继电器，继电器接点负载AC250V/3A、DC30V/7A启动为一组常开/常闭触点、停止为一组常开触点。安装与接线播报编辑安装孔距为65mm，用2只M4螺钉或A4自攻钉固定在安装位置。端子1接多线盘启动端；端子2接多线盘停止端；端子3接多线盘回答端；端子4接电源地G；端子5、6为停止命令对应的常开触点输出；端子11、12接220V回答信号；端子13、14为启动命令对应的常开触点输出；端子14、15为启动命令对应的常闭触点输出；触点输出均为无源。端子16接24V电源正极；应用（接专线控制盘）注意事项：可使用AC220V或无源闭合信号作为回答反馈信号。JBF-151F/D只有1个回答输入，它是启动1的回答。JBF-151F/D启动发出后可提供一组常开或常闭触点，停止命令输出时只输出一对常开点。提高缓存播报编辑衡量性能的几个指标的计算中我们可以看到一个15k转速的磁盘在随机读写访问的情况下IOPS竟然只有140左右，但在实际应用中我们却能看到很多标有5000IOPS甚至更高的存储系统，有这么大IOPS的存储系统怎么来的呢?这就要归结于各种存储技术的使用了，在这些存储技术中使用最广的就是高速缓存(Cache)和磁盘冗余阵列(RAID)了，本文就将探讨缓存和磁盘阵列提高存储IO性能的方法。高速缓存播报编辑在当下的各种存储产品中，按照速度从快到慢应该就是内存＞闪存＞磁盘＞磁带了，然而速度越快也就意味着价格越高，闪存虽然说是发展势头很好，磁盘的速度无疑是计算机系统中最大的瓶颈了，所以在必须使用磁盘而又想提高性能的情况下，人们想出了在磁盘中嵌入一块高速的内存用来保存经常访问的数据从而提高读写效率的方法来折中的解决，这块嵌入的内存就被称为高速缓存。说到缓存，到操作系统层，再到磁盘控制器，还有CPU内部，单个磁盘的内部也都存在缓存，所有这些缓存存在的目的都是相同的，就是提高系统执行的效率。当然在这里我们只提跟IO性能相关的缓存，与IO性能直接相关的几个缓存分别是文件系统缓存(File SySTem Cache)、磁盘控制器缓存(Disk CONtroller Cache)和磁盘缓存(Disk Cache,也称为Disk Buffer)，不过当在计算一个磁盘系统性能的时候文件系统缓存也是不会考虑在内的，我们重点考察的就是磁盘控制器缓存和磁盘缓存。不管是控制器缓存还是磁盘缓存，他们所起的作用主要是分为三部分：缓存数据、预读(Read-ahead)和回写(Write-back)。缓存数据首先是系统读取过的数据会被缓存在高速缓存中，这样下次再次需要读取相同的数据的时候就不用再访问磁盘，直接从缓存中取数据就可以了。当然，使用过的数据也不可能在缓存中永久保留的，缓存的数据一般是采取LRU算法来进行管理，目的是将长时间不用的数据清除出缓存，那些经常被访问的却能一直保留在缓存中，直到缓存被清空。预读预读是指采用预读算法在没有系统的IO请求的时候事先将数据从磁盘中读入到缓存中，然后在系统发出读IO请求的时候，就会实现去检查看看缓存里面是否存在要读取的数据，如果存在(即命中)的话就直接将结果返回，这时候的磁盘不再需要寻址、旋转等待、读取数据这一序列的操作了，这样是能节省很多时间的;如果没有命中则再发出真正的读取磁盘的命令去取所需要的数据。缓存的命中率跟缓存的大小有很大的关系，理论上是缓存越大的话，所能缓存的数据也就越多，这样命中率也自然越高，当然缓存不可能太大，毕竟成本在那儿呢。如果一个容量很大的存储系统配备了一个很小的读缓存的话，这时候问题会比较大的，因为小缓存缓存的数据量非常小，相比整个存储系统来说比例非常低，这样随机读取(数据库系统的大多数情况)的时候命中率也自然就很低，这样的缓存不但不能提高效率(因为绝大部分读IO都还要读取磁盘)，反而会因为每次去匹配缓存而浪费时间。执行读IO操作是读取数据存在于缓存中的数量与全部要读取数据的比值称为缓存命中率(Read Cache Hit Radio)，假设一个存储系统在不使用缓存的情况下随机小IO读取能达到150IOPS，而它的缓存能提供10%的缓存命中率的话，那么实际上它的IOPS可以达到150/(1-10%)=166。回写要先说一下，用于回写功能的那部分缓存被称为写缓存(Write Cache)。在一套写缓存打开的存储中，操作系统所发出的一系列写IO命令并不会被挨个的执行，这些写IO的命令会先写入缓存中，然后再一次性的将缓存中的修改推到磁盘中，这就相当于将那些相同的多个IO合并成一个，多个连续操作的小IO合并成一个大的IO，还有就是将多个随机的写IO变成一组连续的写IO，这样就能减少磁盘寻址等操作所消耗的时间，大大的提高磁盘写入的效率。写缓存虽然对效率提高是很明显的，但是它所带来的问题也比较严重，因为缓存和普通内存一样，掉电以后数据会全部丢失，当操作系统发出的写IO命令写入到缓存中后即被认为是写入成功，而实际上数据是没有被真正写入磁盘的，此时如果掉电，缓存中的数据就会永远的丢失了，这个对应用来说是灾难性的，解决这个问题最好的方法就是给缓存配备电池了，保证存储掉电之后缓存数据能如数保存下来。和读一样，写缓存也存在一个写缓存命中率(Write Cache Hit Radio)，不过和读缓存命中情况不一样的是，尽管缓存命中，也不能将实际的IO操作免掉，只是被合并了而已。控制器缓存和磁盘缓存除了上面的作用之外还承当着其他的作用，比如磁盘缓存有保存IO命令队列的功能，单个的磁盘一次只能处理一个IO命令，但却能接收多个IO命令，这些进入到磁盘而未被处理的命令就保存在缓存中的IO队列中。RAID(Redundant ArrayOf Inexpensive Disks)如果你是一位数据库管理员或者经常接触服务器，那对RAID应该很熟悉了，作为最廉价的存储解决方案，RAID早已在服务器存储中得到了普及。在RAID的各个级别中，应当以RAID10和RAID5(不过RAID5已经基本走到头了，RAID6正在崛起中，看看这里了解下原因)应用最广了。下面将就RAID0，RAID1，RAID5，RAID6，RAID10这几种级别的RAID展开说一下磁盘阵列对于磁盘性能的影响，当然在阅读下面的内容之前你必须对各个级别的RAID的结构和工作原理要熟悉才行，这样才不至于满头雾水，推荐查看wikipedia上面的如下条目：RAID，Standard RAID levels，Nested RAID levels。RAID0播报编辑RAID0将数据条带化(striping)将连续的数据分散在多个磁盘上进行存取，系统发出的IO命令(不管读IO和写IO都一样)就可以在磁盘上被并行的执行，每个磁盘单独执行自己的那一部分请求，这样的并行的IO操作能大大的增强整个存储系统的性能。假设一个RAID0阵列有n(n＞=2)个磁盘组成，每个磁盘的随机读写的IO能力都达到140的话，那么整个磁盘阵列的IO能力将是140*n。同时如果在阵列总线的传输能力允许的话RAID0的吞吐率也将是单个磁盘的n倍。其他RAID区域RAID1镜像磁盘，使用2块硬盘，一般做系统盘的镜像，读IO为一块硬盘的IO，写IO为2块硬盘的IO。RAID10既能增加IO的读写性能又能实现数据的冗余,使用盘的数量为2的倍数且要大于等于4，且硬盘空间相同，这样的缺点是要实现IO扩展就必须增加相应的硬盘数量，实现同样的性能硬盘成本要成倍增长。允许不同硬盘数据的任何一块丢失。RAID3拿出单独一块盘做奇偶校验盘，做到数据的冗余这种情况下允许一块硬盘损坏。由于磁盘的任何数据发生改变都会重新对校验盘进行改写，所以过多的写操作会成为整个系统的瓶颈，此种RAID级别只能用于对读请求相对较高，写请求不多的环境。RAID3已基本淘汰，一般用RAID5技术替代。

直接映射：
定义主要用于主存储器与高速缓存之间的一种地址映射关系，主存储器中的一块只能映射到高速缓存的一个特定块中。 [1]

SSD：
基本简介播报编辑固态硬盘，因为台湾的英语里把固体电容称为Solid而得名。SSD由控制单元和存储单元（FLASH芯片、DRAM芯片）组成。固态硬盘在接口的规范和定义、功能及使用方法上与普通硬盘的完全相同，在产品外形和尺寸上基本与普通硬盘一致（新兴的U.2，M.2等形式的固态硬盘尺寸和外形与SATA机械硬盘完全不同）。被广泛应用于军事、车载、工控、视频监控、网络监控、网络终端、电力、医疗、航空、导航设备等诸多领域。芯片的工作温度范围很大，商规产品（0~70℃）工规产品（-40~85℃）。虽然成本较高，但是正在普及至DIY市场。由于固态硬盘的技术与传统硬盘的技术不同，所以产生了不少新兴的存储器厂商。厂商只需购买NAND颗粒，再配适当的控制芯片，编写主控制器代码，就制造了固态硬盘。新一代的固态硬盘普遍采用SATA-2接口、SATA-3接口、SAS接口、MSATA接口、PCI-E接口、M.2接口、CFast接口、SFF-8639接口和NVME/AHCI协议。 [1]分类播报编辑分类方式：固态硬盘的存储介质分为两种，一种是采用闪存（FLASH芯片）作为存储介质，另外一种是采用DRAM作为存储介质。最新还有英特尔的XPoint颗粒技术。基于闪存的固态硬盘：基于闪存的固态硬盘（IDEFLASH DISK、Serial ATA Flash Disk）：采用FLASH芯片作为存储介质，这也是通常所说的SSD。它的外观可以被制作成多种模样，例如：笔记本硬盘、微硬盘、存储卡、U盘等样式。这种SSD固态硬盘最大的优点就是可以移动，而且数据保护不受电源控制，能适应于各种环境，适合于个人用户使用。寿命较长，根据不同的闪存介质有所不同。SLC闪存普遍达到上万次的PE，MLC可达到3000次以上，TLC也达到了1000次左右，最新的QLC也能确保300次的寿命，普通用户一年的写入量不超过硬盘的50倍总尺寸，即便最廉价的QLC闪存，也能提供6年的写入寿命。可靠性很高，高品质的家用固态硬盘可轻松达到普通家用机械硬盘十分之一的故障率。基于DRAM类：基于DRAM的固态硬盘：采用DRAM作为存储介质，应用范围较窄。它仿效传统硬盘的设计，可被绝大部分操作系统的文件系统工具进行卷设置和管理，并提供工业标准的PCI和FC接口用于连接主机或者服务器。应用方式可分为SSD硬盘和SSD硬盘阵列两种。它是一种高性能的存储器，理论上可以无限写入，美中不足的是需要独立电源来保护数据安全。DRAM固态硬盘属于比较非主流的设备。 [1]基于3D XPoint类基于3D XPoint的固态硬盘：原理上接近DRAM，但是属于非易失存储。读取延时极低，可轻松达到现有固态硬盘的百分之一，并且有接近无限的存储寿命。缺点是密度相对NAND较低，成本极高，多用于发烧级台式机和数据中心。发展历程播报编辑1956年，IBM公司发明了世界上第一块硬盘。1968年，IBM重新提出“温彻斯特”（Winchester）技术的可行性，奠定了硬盘发展方向。1970年，StorageTek公司(Sun StorageTek)开发了第一个固态硬盘驱动器。1984年，东芝发明闪存。1989年，世界上第一款固态硬盘出现。2006年3月，三星率先发布一款32GB容量的固态硬盘笔记本电脑，2007年1月，SanDisk公司发布了1.8寸32GB固态硬盘产品，3月又发布了2.5寸32GB型号。2007年6月，东芝推出了其第一款120GB固态硬盘笔记本电脑。2008年9月，忆正MemoRight SSD的正式发布，标志着中国企业加速进军固态硬盘行业。2009年，SSD井喷式发展，各大厂商蜂拥而来，存储虚拟化正式走入新阶段。2010年2月，镁光发布了全球首款SATA 6Gbps接口固态硬盘，突破了SATAII接口300MB/s的读写速度。2010年底，瑞耐斯Renice推出全球第一款高性能mSATA固态硬盘并获取专利权。 [1]2013年，三星推出VNand 3D闪存。2022年7月21日，三星电子宣布，公司成功研制出第二代智能固态硬盘（SmartSSD），今后将以此抢占未来市场。 [8]基本结构播报编辑基于闪存的固态硬盘是固态硬盘的主要类别，其内部构造十分简单，固态硬盘内主体其实就是一块PCB板，而这块PCB板上最基本的配件就是控制芯片，缓存芯片（部分低端硬盘无缓存芯片）和用于存储数据的闪存芯片。主控芯片市面上比较常见的固态硬盘有LSISandForce、Indilinx、JMicron、Marvell、Phison、Sandisk、Goldendisk、Samsung以及Intel等多种主控芯片。主控芯片是固态硬盘的大脑，其作用一是合理调配数据在各个闪存芯片上的负荷，二则是承担了整个数据中转，连接闪存芯片和外部SATA接口。不同的主控之间能力相差非常大，在数据处理能力、算法，对闪存芯片的读取写入控制上会有非常大的不同，直接会导致固态硬盘产品在性能上差距高达数倍。缓存颗粒主控芯片旁边是缓存颗粒，固态硬盘和传统硬盘一样需要高速的缓存芯片辅助主控芯片进行数据处理。这里需要注意的是，有一些廉价固态硬盘方案为了节省成本，省去了这块缓存芯片，这样对于使用时的性能会有一定的影响，尤其是小文件的读写性能和使用寿命上。闪存芯片除了主控芯片和缓存芯片外，PCB板上其余大部分位置都是NAND Flash闪存芯片。NAND Flash闪存芯片又分为SLC（Single-Level Cell，单层单元）、MLC（Multi-Level Cell，双层单元）、TLC（Trinary-Level Cell，三层单元）、QLC（Quad-Level Cell，四层单元）这四种规格。另还有一种eMLC（Enterprise Multi-Level Cell，企业多层单元）是MLC NAND闪存的一个“增强型”的版本，它在一定程度上弥补了SLC和MLC之间的性能和耐久差距。对比传统硬盘播报编辑固态硬盘的接口规范和定义、功能及使用方法上与普通硬盘几近相同，外形和尺寸也基本与普通的2.5英寸硬盘一致。固态硬盘具有传统机械硬盘不具备的快速读写、质量轻、能耗低以及体积小等特点，同时其劣势也较为明显。尽管IDC认为SSD已经进入存储市场的主流行列，但其价格仍较为昂贵，容量较低，一旦硬件损坏，数据较难恢复等；并且亦有人认为固态硬盘的耐用性（寿命）相对较短。影响固态硬盘性能的几个因素主要是：主控芯片、NAND闪存介质和固件。在上述条件相同的情况下，采用何种接口也可能会影响SSD的性能。主流的接口是SATA（包括3Gb/s和6Gb/s两种）接口，亦有PCIe 3.0接口的SSD问世。由于SSD与普通磁盘的设计及数据读写原理的不同，使得其内部的构造亦有很大的不同。一般而言，固态硬盘（SSD）的构造较为简单，并且也可拆开；所以我们通常看到的有关SSD性能评测的文章之中大多附有SSD的内部拆卸图。而反观普通的机械磁盘，其数据读写是靠盘片的高速旋转所产生的气流来托起磁头，使得磁头无限接近盘片，而又不接触，并由步进电机来推动磁头进行换道数据读取。所以其内部构造相对较为复杂，也较为精密，一般情况下不允许拆卸。一旦人为拆卸，极有可能造成损害，磁盘无法正常工作。这也是为何在对磁盘进行评测时，我们基本看不到关于磁盘拆卸图的原因。 [3]优点播报编辑读写速度快：采用闪存作为存储介质，读取速度相对机械硬盘更快。固态硬盘不用磁头，寻道时间几乎为0。持续写入的速度非常惊人，固态硬盘厂商大多会宣称自家的固态硬盘持续读写速度超过了500MB/s，近年来的NVMe固态硬盘可达到2000MB/s左右，甚至4000MB/s以上。固态硬盘的快绝不仅仅体现于持续读写上，随机读写速度快才是固态硬盘的终极奥义，这最直接体现于绝大部分的日常操作中。与之相关的还有极低的存取时间，最常见的7200转机械硬盘的寻道时间一般为12-14毫秒，而固态硬盘可以轻易达到0.1毫秒甚至更低。 [4]防震抗摔性：传统硬盘都是磁碟型的，数据储存在磁碟扇区里。而固态硬盘是使用闪存颗粒（即MP3、U盘等存储介质）制作而成，所以SSD固态硬盘内部不存在任何机械部件，这样即使在高速移动甚至伴随翻转倾斜的情况下也不会影响到正常使用，而且在发生碰撞和震荡时能够将数据丢失的可能性降到最小。相较传统硬盘，固态硬盘占有绝对优势。 [4]低功耗：固态硬盘的功耗上要低于传统硬盘。无噪音：固态硬盘没有机械马达和风扇，工作时噪音值为0分贝。基于闪存的固态硬盘在工作状态下能耗和发热量较低（但高端或大容量产品能耗会较高）。内部不存在任何机械活动部件，不会发生机械故障，也不怕碰撞、冲击、振动。由于固态硬盘采用无机械部件的闪存芯片，所以具有了发热量小、散热快等特点。 [4]工作温度范围大：典型的硬盘驱动器只能在5到55摄氏度范围内工作。而大多数固态硬盘可在-10~70摄氏度工作。固态硬盘比同容量机械硬盘体积小、重量轻。固态硬盘的接口规范和定义、功能及使用方法上与普通硬盘的相同，在产品外形和尺寸上也与普通硬盘一致。其芯片的工作温度范围很宽（-40~85摄氏度）。轻便：固态硬盘在重量方面更轻，与常规1.8英寸硬盘相比，重量轻20-30克。缺点播报编辑容量：随着MLC、TLC、QLC乃至未来的PLC等多阶存储单元的发展，固态硬盘容量正在迅速增长。截止2021年1月世界上容量最大的固态硬盘是Nimbus Data 推出的 ExaDrive DC100 系列固态硬盘，容量可达100TB。 [5]寿命限制：固态硬盘闪存具有擦写次数限制的问题，这也是许多人诟病其寿命短的所在。闪存完全擦写一次叫做1次P/E，因此闪存的寿命就以P/E作单位。34nm的闪存芯片寿命约是5000次P/E，而25nm的寿命约是3000次P/E。随着SSD固件算法的提升，新款SSD都能提供更少的不必要写入量。一款120G的固态硬盘，要写入120G的文件才算做一次P/E。普通用户正常使用，即使每天写入50G，平均2天完成一次P/E，3000个P/E能用20年，到那时候，固态硬盘早就被替换成更先进的设备了(在实际使用中，用户更多的操作是随机写，而不是连续写，所以在使用寿命内，出现坏道的机率会更高)。另外，虽然固态硬盘的每个扇区可以重复擦写100000次(SLC)，但某些应用，如操作系统的LOG记录等，可能会对某一扇区进行多次反复读写，而这种情况下，固态硬盘的实际寿命还未经考验。不过通过均衡算法对存储单元的管理，其预期寿命会延长。SLC有10万次的写入寿命，成本较低的MLC，写入寿命仅有1万次,而廉价的TLC闪存则更是只有1000-2000次。此外，使用全盘模拟SLC提升写入速度的多阶存储固态会面临写入放大问题，进一步缩短寿命。售价高：截止2021年1月市场上采用TLC存储单元的256GB固态硬盘价格大约为240元人民币左右（采用SATA接口+TLC颗粒），而1TB固态硬盘产品的价格大约在650元人民币左右（NVMe接口+TLC颗粒）。计算下来每GB大约0.6-1元。相比每GB仅为0.2元的机械硬盘高了不少。使用与保养播报编辑对于固态硬盘的使用和保养，最重要的一条就是：在机械硬盘时代养成的“良好习惯”，未必适合固态硬盘。一、不要使用碎片整理碎片整理是对付机械硬盘变慢的一个好方法，但对于固态硬盘来说这完全就是一种“折磨”。消费级固态硬盘的擦写次数是有限制，碎片整理会大大减少固态硬盘的使用寿命。其实，固态硬盘的垃圾回收机制就已经是一种很好的“磁盘整理”，再多的整理完全没必要。Windows的“磁盘整理”功能是机械硬盘时代的产物，并不适用于SSD。除此之外，使用固态硬盘最好禁用Win7的预读(Superfetch)和快速搜索(Windows Search)功能。这两个功能的实用意义不大，而禁用可以降低硬盘读写频率。（在Windows 10中，这一项优化不需要）二、小分区 少分区还是由于固态硬盘的“垃圾回收机制”。在固态硬盘上彻底删除文件，是将无效数据所在的整个区域摧毁，过程是这样的：先把区域内有效数据集中起来，转移到空闲的位置，然后把“问题区域”整个清除。这一机制意味着，分区时不要把SSD的容量都分满。例如一块128G的固态硬盘，厂商一般会标称120G，预留了一部分空间。但如果在分区的时候只分100G，留出更多空间，固态硬盘的性能表现会更好。这些保留空间会被自动用于固态硬盘内部的优化操作，如磨损平衡、垃圾回收和坏块映射。这种做法被称之为“小分区”。“少分区”则是另外一种概念，关系到“4k对齐”对固态硬盘的影响。一方面主流SSD容量都不是很大，分区越多意味着浪费的空间越多，另一方面分区太多容易导致分区错位，在分区边界的磁盘区域性能可能受到影响。最简单地保持“4k对齐”的方法就是用Win7自带的分区工具进行分区，这样能保证分出来的区域都是4K对齐的。三、保留足够剩余空间固态硬盘存储越多性能越慢。而如果某个分区长期处于使用量超过90%的状态，有些固态硬盘崩溃的可能性将大大增加，绝大部分硬盘也会出现性能降低的现象。所以及时清理无用的文件，设置合适的虚拟内存大小，将电影音乐等大文件存放到机械硬盘非常重要，必须让固态硬盘分区保留足够的剩余空间。四、及时刷新固件“固件”好比主板上的BIOS，控制固态硬盘一切内部操作，不仅直接影响固态硬盘的性能、稳定性，也会影响到寿命。优秀的固件包含先进的算法能减少固态硬盘不必要的写入，从而减少闪存芯片的磨损，维持性能的同时也延长了固态硬盘的寿命。因此及时更新官方发布的最新固件显得十分重要。不仅能提升性能和稳定性，还可以修复之前出现的bug。五、学会使用恢复指令固态硬盘的Trim重置指令可以把性能完全恢复到出厂状态。 [6]随着互联网的飞速发展，人们对数据信息的存储需求也在不断提升，多家存储厂商推出了自己的便携式固态硬盘，更有支持Type-C接口的移动固态硬盘和支持指纹识别的固态硬盘推出。相关资讯播报编辑2021年11月15日，铭瑄发布旗下首款电竞之心系列 SSD，国产主控 + 国产 TLC 颗粒。 [7]

磁带：
简介播报编辑定义英文名称：magnetic tapetitle在中国大陆，通常“磁带”或者“录音带”一词都指紧凑音频盒带，因为它的应用非常广泛。在中国台湾，reel-to-reel tape被称为盘式录音带、紧凑音频盒带（Compact audio cassette）被称为卡式录音带、8轨软片（8-track cartridges)）被称为匣式录音带。磁带主要由磁粉、带基、粘合剂三种材料组成，其中磁性层尤为重要，它是磁带记录和存贮信息的主体部分，而且磁带质量的好坏主要由磁性层决定。 [7]磁带54年1963-2017年：承载记忆的AB面磁带，作为承载一个时代记忆的载体，已有50年的历史，即从最初的数据存储到主流的音乐存储介质。磁带1963年，荷兰飞利浦公司研制成了全球首盘盒式磁带，大小仅为早期的菲德里派克（Fidelipac）循环卡式录音机的1/4，磁带双面都由塑料外壳包裹，可最大程度保护其中的数据，每一面可容纳30到45分钟的立体声音乐。1965年，8声轨磁带诞生。3年后，TDK的超级动态系列上市，宣告了第一款“高保真”磁带诞生。1970年，第一盘120分钟磁带诞生，即每一面可容纳60分钟的音频数据。1971年，Advent公司推出了201型磁带机——其搭载杜比B型降噪系统，磁带才被更加认真地用于录制音乐，为之后开始的高保真卡带和播放器时代奠定了坚实基础。20世纪80年代，以索尼Walkman系列为代表的便携式随身听出现，造就了磁带在全世界范围内的风靡。正是在这个时期，音乐磁带的销售开始取代密纹唱片，随身听一跃成为便携式音乐市场的象征。然而好景不长，在很多西方国家，磁带市场在经历了上世纪80年代末的销售高峰后，就开始急速萎缩。到了90年代初期，CD的销售就超过了预录制卡带。1998年，韩国三星公司推出了全球首台MP3播放器。在随后的几年时间里，尤其是进入了千禧年之后，MP3格式开始在市场上大行其道。2007年，当英国一个主要的电器零售商宣告停止销售磁带后，《太阳报》就自作主张宣告了磁带的死亡。2009年，网络杂志Pop Matters认为磁带已经可以圆满退场了：“一些媒介就是注定要灭亡且永无复兴之日，磁带注定是这种命运。”2010年秋，美国媒体报道了磁带的“重生”：美国25个音乐厂牌开始重新制作磁带，著名的音乐网站Pitchfork也早就进行了类似的尝试，并且这些磁带不是老专辑的翻录，而是新发行的专辑。一些独立乐队，如Animal Collective、Deerhoof、the Mountain Goats也推出了磁带专辑。如今，磁带变为一种收藏，依旧在市场上活跃。据业内行家称，老磁带的大部分品种发行量小，外加绞带、受潮等自然损耗和人为损耗，其收藏价值会越来越高。尺寸磁带尺寸广义上讲包括磁带的宽度、长度或者磁带盒的规格。磁带盒常见的规格：AIT磁带多为3.5英寸（8.89厘米）、DLT磁带多为5.25英寸（13.335厘米）。工作原理为什么磁带可以存储音频信号呢？它的工作原理是什么呢？原来，录音磁头实际上是个蹄形电磁铁，两极相距很近，中间只留个狭缝。整个磁头封在金属壳内。录音磁带的带基上涂着一层磁粉，实际上就是许多铁磁性小颗粒。磁带紧贴着录音磁头走过，音频电流使得录音头缝隙处磁场的强弱、方向不断变化，磁带上的磁粉也就被磁化成一个个磁极方向和磁性强弱各不相同的“小磁铁”，声音信号就这样记录在磁带上了。放音头的结构和录音头相似。当磁带从放音头的狭缝前走过时，磁带上“小磁铁”产生的磁场穿过放音头的线圈。由于“小磁铁”的极性和磁性强弱各不相同，它在线圈内产生的磁通量也在不断变化，于是在线圈中产生感应电流，放大后就可以在扬声器中发出声音。技术磁带卷轴螺旋扫描记录技术的历史可追溯到40多年前。1956年，ampex公司将螺旋扫描设备作为一种可靠的存储设备推向了视频市场。该设备每平方英寸磁带可存储的数据大幅度增长，读数据的速度比当时线性磁带技术还要快。螺旋扫描技术的高性能和大容量迅速使螺旋扫描技术成为视频广播业的标准。许多电视台仍使用类似的螺旋扫描磁带驱动器，每套磁带系统的价格超过了10万美元。第一种高性能、高容量磁带驱动器exabyte 8200于1987年被引入到unix开放系统市场中，该驱动器传输速率为240kb/s，容量为2．4gb。这种螺旋驱动器使用8毫米磁带，利用不同的读、写磁头从磁带读取数据并向磁带写入数据。写后读技术，即在安装磁头的磁鼓每转一圈时，使用一个磁头写数据，随后再利用读磁头来校验数据。这种技术是用来校验写入操作正确性的通用方法。如果检测到错误的话，就对数据进行重写，直到读出的数据没有错误为止。这类驱动器的高密度、高速度以及错误检测和纠正等特性使螺旋扫描技术非常流行。对螺旋扫描技术的改进包括1990年推出的硬件压缩，它可以将存储在磁带上的数据密度增加一倍。1990年，人们还对螺旋扫描技术进行了另一项改进，即使用方位角记录技术。这项技术利用以不同角度安装在扫描器上的磁头在磁带上生成的人字形或v形轨迹。这就使高密磁轨容错技术成为可能。这项技术在历史上曾使螺旋扫描技术在性能和容量上处于领先位置。此外，磁带介质上的改进则进一步增加了螺旋扫描磁带的数据密度。新型驱动器的发展提供了更高的记录速率、更大的磁带容量，并提高了数据密度。 [1]分类播报编辑根据用途不同，磁带按用途可大致分成录音带、录像带、计算机带和仪表磁带四种。 [7]录音带磁带20世纪30年代开始出现，是用量最大的一种磁带。1963年，荷兰飞利浦公司研制成盒式录音带,由于具有轻便、 耐用、 互换性强等优点而得到迅速发展。1973年，日本研制成功Avilyn包钴磁粉带。1978年，美国生产出金属磁粉带。由日本日立玛克赛尔公司创造的MCMT技术（即特殊定向技术、超微粒子及其分散技术）制成了微型及数码盒式录音带，又使录音带达到一个新的水平，并使音频记录进入了数字化时代。中国在60年代初开始生产录音带，1975年试制成盒式录音带，并已达较高水平。录像带自从1956年美国安佩克斯公司制成录像机以来，录像带已从电视广播逐步进入到科学技术、文化教育、电影和家庭娱乐等领域。除了用二氧化铬包钴磁粉以及金属磁粉制成录像带外，日本还制成微型镀膜录像带，并开发了钡铁氧体型垂直磁化录像带。计算机带盒式磁带（线性磁带开放协议，即LTO技术）计算机带作为数字信息的存贮具有容量大、价格低的优点。主要大量用于计算机的外存贮器。如今仅在专业设备上使用（比如计算机磁带存储器、车床控制机）。线性磁带开放协议（LTO-2的滤芯）仪表磁带也称仪器磁带或精密磁带。近代科学技术，常需要把人们无法接近的测量数据自动而连续地记录下来，即所谓遥控遥测技术。如原子弹爆炸和卫星空间探测都要求准确无误地同时记录上百、上千个数据。仪表磁带就是在上述需要下发展起来的，它是自动化和磁记录技术相结合的产物。对这种磁带的性能和制造都有着严格的要求。此外，还有其他磁带和打字机用磁性染色带等。 [2]挑战播报编辑读写记录线性磁带技术在时间上早于螺旋扫描记录技术十多年。在使用线性记录技术时，磁带被安装在两个磁带轴上，通过磁带轴的转动使磁带高速经过磁头。如今线性技术已经成为非常流行的技术，并对螺旋扫描技术发起了挑战。磁带写后读技术被广泛地应用在线性磁带中。利用隔开一小段距离的写磁头和读磁头，完成先写后读的操作。读磁头读取写磁头刚刚写入的数据，以保证数据完整地写到磁带上。错误处理的方式与螺旋扫描使用的方式相似。记录介质、磁头设计和固件上的改进，使线性技术超越了每条磁带36条磁轨的人为限制。这就使蛇形记录成为可能。在使用蛇形记录技术时，磁带机先沿整条磁带写入一个磁轨集后，再重新定位磁头；然后反方向再沿整个磁带写入另一个磁轨集。线性技术可以在一条磁带上这样写52遍，写入208条磁轨。利用这种方法，可以增加记录密度。但是，即使利用这种技术，与螺旋扫描相比，数据密度仍很有限。数据磁轨之间的距离越小，磁轨之间串音的可能性就越高。dlt7000磁带机针对这一问题采用轻微地旋转磁头的方法，产生一个有角度的写形式，类似人字形或v形。这种形式与螺旋扫描驱动器的写形式非常相似。磁头对磁带的精确校准，尤其在磁轨的数量不断增加的情况下，对磁带驱动器设计者一直是一个挑战。用于校准磁头与磁带位置的伺服器已经成为了当今和下一代线性驱动器的通用特性。线性记录已经成为一种成功的产品，并且已在许多应用中取代了螺旋磁带驱动器。不过，事物是在不断变化的，三十年河东，三十年河西。襟抱堂网络策划机构评论，光盘媒介对磁带的取代已成为了历史发展的必然趋势，磁带的发展由其产品的制约性必然被科技所淘汰。传输速率提高8毫米传输速率在过去的五年中，螺旋扫描驱动器的性能已经滞后于线性驱动器。例如，dlt7000磁带的传输速率为5mb/s，而mammoth 1和ait磁带驱动器只具有3mb/s的传输速率。通过增加更多的并行磁带通道，线性磁带驱动器上的传输速率一直不断地改进。例如，dlt7000使用了四条磁轨达到了5mb/s，而螺旋扫描驱动器不能超过两条通道的限制。以前，螺旋扫描设计上的部分限制是由于它们与用于消费类视频技术之间存在过近关系。mammoth是第一种与基于消费用8毫米设计脱钩的8毫米驱动器。这项技术在1996年推出。该解决方案特别定位于满足企业数据存储需要。工业化的机芯设计利用轴对轴伺服系统取代了早期设计中造成很多麻烦的绞盘和压紧导辊。这有助于取得精确的磁带速度，实现准确的张力控制。螺旋扫描技术方面两项最新的进展克服了8毫米螺旋扫描驱动器在传输速率上的限制。这两项改进将成为安百特（exabyte）下一代驱动器必不可少的部分。下一代驱动器性能将增至四倍，存储容量将增至三倍。边写边读和加电转子这两项技术消除了以前制约传输速率和数据密度的限制。这些限制曾使螺旋扫描的传输速率和数据密度低于线性磁带技术。这两种新技术结合的结果是，现可以在磁鼓上（扫描器）安装更多的磁头（大大多于以前螺旋扫描可能安装的磁头数量），实现更高的传输率并增加了密度。在未来的产品中，可以在扫描器上安装8个通道（16个磁头）。在使用8个通道时，系统可以达到超过100mb/s的传输速率。最新一代的扫描器，其设计将读/写电器件集成到扫描仪（带电转子）上，缩短了电子器件与磁头的距离，同时提高了驱动器的性能和可靠性。技术提升mammoth 2的设计在其它许多方面超过了线性磁带。定义性能两个主要因素：一是磁头对磁带的速度；二是螺旋扫描驱动器可以方便地升级。为使螺旋扫描驱动器增加磁头对磁带的速度，扫描器应当更快地旋转。与此相比，线性驱动器已接近磁带运动的极限。例如，线性磁带驱动器耗电约为35瓦。将磁带的速度提高一倍，会使耗电达到60至65瓦，产生的烤箱效应足以烤熟磁带和驱动器。靠线性驱动器以更高速的磁带运动使性能得到改进是不可能的。相反，螺旋扫描驱动器速度的提高，相应的耗电增加仅为不到1瓦。因此，对螺旋扫描技术来说，增加扫描器马达速度耗电将从12瓦升至12．5瓦。此外，螺旋磁带密度比线性磁带的密度更高。螺旋扫描磁轨在实时伺服控制下写磁轨的偏差为0．2微米，而线性磁轨偏差为10微米。事实上，螺旋驱动器可以在3微米宽的磁轨上进行读写。这样细的磁轨尺寸对线性磁带驱动器几乎是不可能的，其原因是线性蛇形记录本身固有的容错问题。存储容量至于容量，最大的线性磁带驱动器的存储容量要比8毫米磁带高，这已是公认的事实了。这主要是由于在磁带盒中使用了五倍的磁带。老实说，与dlt相比较，8毫米磁带的格式（它的磁带盒和磁带都比较小）可以被认为每盒磁带的容量受到了限制。但是，如果mammoth与dlt有同样多的磁带的话，那么，它的容量可以达到100gb。然而，部分是由于磁带盒较小的原因，同样更紧凑的磁带机也是可能做到的。换句话说，在相同空间里，可以比dlt驱动器和磁带放入更多的8毫米驱动器和磁带。此外，较小的磁带盒使8毫米驱动器可以具有更快的文件访问时间，因而对数据的访问也更快。日本富士胶片公司和瑞士苏黎世的研究人员研发出一种新型超密磁带，被称之为“线性磁带文件系统”。这种存储系统存储密度更高，能耗更低，能够取代当前的硬盘。他们研制的原型超密磁带覆盖钡铁氧体颗粒图层，所使用的带盒长10厘米，宽10厘米，高2厘米，能够存储35TB数据，大约相当于3500个图书馆所涵盖的信息。技术比较如果将线性和螺旋磁带并排放在一起比较的话，这两种技术一些有趣的方面就会变得非常明显。在安百特（exabyte）公司的磁带库解决方案中，在同样大小的机箱内既可采用mammoth，也可采用dlt。dlt库提供了30mb/s的性能和6．3tb的容量。相比之下，8毫米库也提供了同样的30mb/s的传输速率，但可以多存储1．7tb的数据，即总存储量为8tb。在过去的几年里，8毫米技术已经得到了改进，它所具有的传输速率与竞争对手线性技术一样好，或许甚至更好。这些改进包括工业强度的机械和创新的工程上的进步，使采用最多8个磁头成为可能。通过增加磁带对磁头的速度来进一步提高传输速率可以方便地利用mammoth技术实现，而这种改进对于线性驱动技术而言将越来越困难。录音磁带第二次世界大战虽然造成了78转唱片市场的萎缩，却阴差阳错地从另一个方面对流行音乐市场提供了帮助。德国的工程师们为了更好地广播希特勒的讲话，在磁带录音技术上取得了革命性的进步。二战后，美国把这一技术原样拿了过来，并很快就运用在流行音乐领域。磁带录音方便可靠，价钱便宜，质量又好，使得投资不多的小型录音公司得以生存下去，为五十年代独立唱片公司的发展壮大立下了汗马功劳，这些小型公司的兴起直接促成了摇滚乐的诞生。六十年代中期，RCA发明了可以在汽车上使用的八轨磁带（8-Track），这一发明立刻吸引了众多以前不怎么买唱片的消费者的注意，美国的音乐销售也从这一时期开始直线上升。七十年代初，一批自称是“低者”（Downer,相对于传统的“Higher”）的吸毒群体高速行驶中的汽车里听震耳欲聋的重摇滚对达到“状态”很有帮助。这种说法很快在听众中流传开来，并很大程度上造就了七十年代初期重摇滚的流行。一批重摇滚乐队因此受益匪浅，如“深紫”、“黑色安息日”（BlackSabbath）和“AC/DC”等，他们的磁带销售往往会占到总销售额的70%以上。磁带原理图后来，杜比技术的发明让可录音的卡式磁带走进了消费者的家中。这一新技术使得盗版磁带开始在地下泛滥。唱片商不得不象当年对抗广播业一样，又开始借助法律手段进行抵制。不过，磁带的录音质量比不上黑胶唱片（LP），再加上因为各种原因，六七十年代的美国流行音乐市场格外繁荣，因此盗版的影响不算太坏，倒是一些歌迷在地下市场交换私自录制的歌手实况演唱录音，算是弥补了录音室唱片的不足。这些非法录音不但为乐队造就了一批批铁杆歌迷，而且为后来音乐史学家们研究这段历史帮助很大。 [3]相关资料播报编辑常见故障声音不清晰（国标允许误差范围≤ -48dB ）可能的故障部位：①放音机磁头上有脏物。解决方法：用酒精棉球轻擦放音磁头，注意不要用镊子，螺丝刀等金属物。②磁带顶部毛粘垫松动或脱落。解决方法：要修复，更换毛粘垫。③磁带变形、打折、受潮或被磁化。解决方法：调换、报废。④放音磁头磨损严重。解决方法：更换放音头，业余更换时要求磁头电感量和阻值一样。⑤磁头方位角偏移不正确。调校：把录音带放入盒中放音，用小螺丝刀调校磁头上带弹簧的螺丝里外移动，使放音量最大为宜。声音忽大忽小可能的故障部位：①检查放音机机械传动是否正常，如压带轮是否老化，主导轴是否有脏物，皮带是否老化、错槽。解决方法：清洗或更换。②盒式录音带白轮是否变形，毛粘垫是否松动。解决方法：修复、更换左右声道串音（国标允许误差范围≤ -34dB ）可能的故障部位：①检查放音机磁头是否磨损。解决方法：更换磁头。②检查走动机械是否正常。解决方法：修复。③磁带不合格。解决方法：调换磁带AB 面串音（国标允许误差范围≤ -52dB ）可能的故障部位：①检查放音磁头，机械传动是否正常。解决方法：修复。②磁头磁叉是否松动。解决方法：调校粘牢。③是否使用了特大功率放大器。解决方法：旋小放音量。④磁带不合格。解决方法：调换。走带过程中断带可能的故障部位：①检查机器供带轮是否转动。解决方法：修复或更换。②检查磁带两个轮是否转动灵活。解决方法：修复或更换。③放音机收带轮是否转动。解决方法：修复或更换。④操作放音机方法是否正确。解决方法：改变操作方法。声音模糊听不清可能的故障部位：①检查磁头是否有脏物，是否磨损、老化以及磁头方位角是否正确。解决方法：清洗磁头，修复，调校。②检查磁带顶部毛粘垫是否脱落。解决方法：修复，更换。③工厂加工是反带不合格。解决方法：反带修复，也就是把磁带前轮和后轮翻面倒个相位重新倒带即可。声音断话、停顿可能的故障部位：①检查放音操作方法是否失误，按错按键。（如误按了录音键）解决方法：改正操作方法。②放音机传动机械是否正常，电源电压是否稳定，电池夹是否氧化，接触是否不良。解决方法：修复、更换电池或电源。③磁带转动是否灵活。解决方法：修复，把磁带正面五个螺丝松动录音带左右声道电平不一致，AB 面声音不一样（国标允许左右相差1dB，AB面相差2dB）可能的故障部位：①音头有脏物、磁头磨损或出厂不合格。解决方法：清洗磁头或更换放音机。②放音头老化、磁头偏磨、方位角不平或磁带不合格。解决方法：更换或修复放音机或调换磁带。磁带保管磁带保管不善，也会使磁带变形、变脆，机械强度降低，电磁性能下降，严重影响和缩短磁带使用寿命，甚至报废。磁带保管中要注意几个问题： [7]1.远离磁场2.保持适宜的温湿度3.防光、防尘4.正确卷绕5.正确放置6.严禁磁带同有害的液体、气体接触盗版区别正版（原版）磁带一般均采用进口原材料，包括饼带粘带胶条，AB 贴和外封都是正规印刷厂印刷，工厂机器刷标，机器包装。仪器设备均采用进口数码母机和采用 10MHZ 偏磁独立放大、频响宽、噪音低、失真小的高速复录子机。分切灌带机均由国标正规厂家所生产，分切可靠，头尾一致，前后没有空白，声音清晰，不跑调。另外，歌曲磁带中，正版（原版）磁带的特点是唱片充足，磁带里的歌曲完整、歌曲长，有的歌曲甚至长达将近5分钟、5分多钟或者更长，有些磁带甚至出现了超长版的歌曲。盗版磁带使用低劣饼带，各轮（前轮、后轮、白轮等）不耐磨，贴片滑纸质量很差，复制设备极差（一般都是小机复录），录出的声音电平偏低，噪音很大，频响很差，声音不清晰，跑调，前后空白较长。另外，歌曲磁带中，盗版磁带的特点是唱片不足，磁带里的歌曲残缺、歌曲短，有的歌曲甚至被缩短的只有1分多钟或者更短。录制的时候，有的歌曲被录制的有头无尾，还有的歌曲被录制的时候是从中间省略一部分，个别的歌曲甚至唱了没一会儿就没了。 [4]

组相联映射：
定义播报编辑主要用于主存储器与高速缓存之间的一种地址映射关系，将主存储器和高速缓存按同样大小分组，组内再分成同样大小的块，组间采用直接映射，组内的块之间采用全相联映射。出处播报编辑《计算机科学技术名词 》第三版。 [1]

存取周期：
概念播报编辑存储器的两个基本操作为“读出”与“写入”，是指将存储单元与存储寄存器(MDR)之间进行读写。存储器从接收读出命令到被读出信息稳定在MDR的输出端为止的时间间隔，称为“取数时间TA”。两次独立的存取操作之间所需最短时间称为“存储周期TMC”。半导体存储器的存取周期一般为6ns～10ns。 [1]其中存储单元(memory location)简称“单元”。为存储器中存储一机器字或一字节的空间位置。一个存储器划分为若干存储单元，并按一定顺序编号，称为“地址”。如一存储单元存放一有独立意义的代码。即存放作为一个整体来处理或运算的一组数字，则称为“字”。字的长度，即字所包含的位数，称为“字长”。如以字节来划分存储单元，则一机器字常须存放在几个存储单元中。存储单元中的内容一经写入，虽经反复使用，仍保持不变。如须写入新内容，则原内容被“冲掉”，而变成新写入的内容。 [2]存储器（storage）又称“记忆装置”。能按一定地址随机存取计算程序、原始数据、中间和最后结果的装置。存储器须保存大量表示数据或程序的二进制代码，故“按地址”存取的组织方式，将其分成众多单元(称“存储单元”)，并按一定顺序编号(称“地址”)，以便按地址根据指令的要求存取。常以存储容量和存取周期作为衡量性能的主要指标。 [2]意义播报编辑为存储器的性能指标之一，直接影响电子计算机的技术性能。存储周期愈短，运算速度愈快，但对存储元件及工艺的要求也愈高。 [2]存取周期例如磁芯存储器的存取周期为零点几到几个微秒。半导体存储器的存取周期通常在几十到几百毫微秒之间。那么半导体存储器的性能一般比磁芯存储器的性能要好。 [3]

缓存：
简介播报编辑缓存的工作原理缓存是指可以进行高速数据交换的存储器，它先于内存与CPU交换数据，因此速率很快。L1 Cache（一级缓存）是CPU第一层高速缓存。内置的L1高速缓存的容量和结构对CPU的性能影响较大，不过高速缓冲存储器均由静态RAM组成，结构较复杂，在CPU管芯面积不能太大的情况下，L1级高速缓存的容量不可能做得太大。一般L1缓存的容量通常在32—256KB。L2　Cache（二级缓存）是CPU的第二层高速缓存，分内部和外部两种芯片。内部的芯片二级缓存运行速率与主频相同，而外部的二级缓存则只有主频的一半。L2高速缓存容量也会影响CPU的性能，原则是越大越好，普通台式机CPU的L2缓存一般为128KB到2MB或者更高，笔记本、服务器和工作站上用CPU的L2高速缓存最高可达1MB-3MB。由于高速缓存的速度越高价格也越贵，故有的计算机系统中设置了两级或多级高速缓存。紧靠CPU的一级高速缓存的速度最高，而容量最小，二级高速缓存的容量稍大，速度也稍低 [1]。缓存只是内存中少部分数据的复制品，所以CPU到缓存中寻找数据时，也会出现找不到的情况（因为这些数据没有从内存复制到缓存中去），这时CPU还是会到内存中去找数据，这样系统的速率就慢下来了，不过CPU会把这些数据复制到缓存中去，以便下一次不要再到内存中去取。随着时间的变化，被访问得最频繁的数据不是一成不变的，也就是说，刚才还不频繁的数据，此时已经需要被频繁的访问，刚才还是最频繁的数据，又不频繁了，所以说缓存中的数据要经常按照一定的算法来更换，这样才能保证缓存中的数据是被访问最频繁的。工作原理播报编辑缓存工作原理缓存的工作原理是当CPU要读取一个数据时，首先从CPU缓存中查找，找到就立即读取并送给CPU处理；没有找到，就从速率相对较慢的内存中读取并送给CPU处理，同时把这个数据所在的数据块调入缓存中，可以使得以后对整块数据的读取都从缓存中进行，不必再调用内存。正是这样的读取机制使CPU读取缓存的命中率非常高（大多数CPU可达90%左右），也就是说CPU下一次要读取的数据90%都在CPU缓存中，只有大约10%需要从内存读取。这大大节省了CPU直接读取内存的时间，也使CPU读取数据时基本无需等待。总的来说，CPU读取数据的顺序是先缓存后内存。RAM(Random-Access Memory)和ROM(Read-Only Memory)相对的，RAM是掉电以后，其中的信息就消失那一种，ROM在掉电以后信息也不会消失那一种。RAM又分两种，一种是静态RAM，SRAM(Static RAM)；一种是动态RAM，DRAM(Dynamic RAM)。前者的存储速率要比后者快得多，使用的内存一般都是动态RAM。为了增加系统的速率，把缓存扩大就行了，扩的越大，缓存的数据越多，系统就越快了，缓存通常都是静态RAM，速率是非常的快， 但是静态RAM集成度低（存储相同的数据，静态RAM的体积是动态RAM的6倍）， 价格高（同容量的静态RAM是动态RAM的四倍）， 由此可见，扩大静态RAM作为缓存是一个非常愚蠢的行为， 但是为了提高系统的性能和速率，必须要扩大缓存， 这样就有了一个折中的方法，不扩大原来的静态RAM缓存，而是增加一些高速动态RAM做为缓存， 这些高速动态RAM速率要比常规动态RAM快，但比原来的静态RAM缓存慢， 把原来的静态RAM缓存叫一级缓存，而把后来增加的动态RAM叫二级缓存。功能作用播报编辑硬盘的缓存主要起三种作用：预读取数据缓存当硬盘受到CPU指令控制开始读取数据时，硬盘上的控制芯片会控制磁头把正在读取的簇的下一个或者几个簇中的数据读到缓存中（由于硬盘上数据存储时是比较连续的，所以读取命中率较高），当需要读取下一个或者几个簇中的数据的时候，硬盘则不需要再次读取数据，直接把缓存中的数据传输到内存中就可以了，由于缓存的速率远远高于磁头读写的速率，所以能够达到明显改善性能的目的。写入缓存(16张)当硬盘接到写入数据的指令之后，并不会马上将数据写入到盘片上，而是先暂时存储在缓存里，然后发送一个“数据已写入”的信号给系统，这时系统就会认为数据已经写入，并继续执行下面的工作，而硬盘则在空闲（不进行读取或写入的时候）时再将缓存中的数据写入到盘片上。虽然对于写入数据的性能有一定提升，但也不可避免地带来了安全隐患——数据还在缓存里的时候突然掉电，那么这些数据就会丢失。对于这个问题，硬盘厂商们自然也有解决办法：掉电时，磁头会借助惯性将缓存中的数据写入零磁道以外的暂存区域，等到下次启动时再将这些数据写入目的地。临时存储有时候，某些数据是会经常需要访问的，像硬盘内部的缓存（暂存器的一种）会将读取比较频繁的一些数据存储在缓存中，再次读取时就可以直接从缓存中直接传输。缓存就像是一台计算机的内存一样，在硬盘读写数据时，负责数据的存储、寄放等功能。这样一来，不仅可以大大减少数据读写的时间以提高硬盘的使用效率。同时利用缓存还可以让硬盘减少频繁的读写，让硬盘更加安静，更加省电。更大的硬盘缓存，你将读取游戏时更快，拷贝文件时候更快，在系统启动中更为领先。硬盘缓存缓存容量的大小不同品牌、不同型号的产品各不相同，早期的硬盘缓存基本都很小，只有几百KB，已无法满足用户的需求。16MB和32MB缓存是现今主流硬盘所采用，而在服务器或特殊应用领域中还有缓存容量更大的产品，甚至达到了64MB、128MB等。大容量的缓存虽然可以在硬盘进行读写工作状态下，让更多的数据存储在缓存中，以提高硬盘的访问速率，但并不意味着缓存越大就越出众。缓存的应用存在一个算法的问题，即便缓存容量很大，而没有一个高效率的算法，那将导致应用中缓存数据的命中率偏低，无法有效发挥出大容量缓存的优势。算法是和缓存容量相辅相成，大容量的缓存需要更为有效率的算法，否则性能会大大折扣，从技术角度上说，高容量缓存的算法是直接影响到硬盘性能发挥的重要因素。更大容量缓存是未来硬盘发展的必然趋势。技术发展播报编辑缓存集群的配置最早先的CPU缓存是个整体的，而且容量很低，英特尔公司从Pentium时代开始把缓存进行了分类。当时集成在CPU内核中的缓存已不足以满足CPU的需求，而制造工艺上的限制又不能大幅度提高缓存的容量。因此出现了集成在与CPU同一块电路板上或主板上的缓存，此时就把 CPU内核集成的缓存称为一级缓存，而外部的称为二级缓存。一级缓存中还分数据缓存（Data Cache，D-Cache）和指令缓存（Instruction Cache，I-Cache）。二者分别用来存放数据和执行这些数据的指令，而且两者可以同时被CPU访问，减少了争用Cache所造成的冲突，提高了处理器效能。英特尔公司在推出Pentium 4处理器时，用新增的一种一级追踪缓存替代指令缓存，容量为12KμOps，表示能存储12K条微指令。随着CPU制造工艺的发展，二级缓存也能轻易的集成在CPU内核中，容量也在逐年提升。再用集成在CPU内部与否来定义一、二级缓存，已不确切。而且随着二级缓存被集成入CPU内核中，以往二级缓存与CPU大差距分频的情况也被改变，此时其以相同于主频的速率工作，可以为CPU提供更高的传输速率。二级缓存是CPU性能表现的关键之一，在CPU核心不变化的情况下，增加二级缓存容量能使性能大幅度提高。而同一核心的CPU高低端之分往往也是在二级缓存上有差异，由此可见二级缓存对于CPU的重要性。CPU在缓存中找到有用的数据被称为命中，当缓存中没有CPU所需的数据时（这时称为未命中），CPU才访问内存。从理论上讲，在一颗拥有二级缓存的CPU中，读取一级缓存的命中率为80%。也就是说CPU一级缓存中找到的有用数据占数据总量的80%，剩下的20%从二级缓存中读取。由于不能准确预测将要执行的数据，读取二级缓存的命中率也在80%左右（从二级缓存读到有用的数据占总数据的16%）。那么还有的数据就不得不从内存调用，但这已经是一个相当小的比例了。较高端的CPU中，还会带有三级缓存，它是为读取二级缓存后未命中的数据设计的—种缓存，在拥有三级缓存的CPU中，只有约5%的数据需要从内存中调用 [5]，这进一步提高了CPU的效率。为了保证CPU访问时有较高的命中率，缓存中的内容应该按一定的算法替换。一种较常用的算法是“最近最少使用算法”（LRU算法），它是将最近一段时间内最少被访问过的行淘汰出局。因此需要为每行设置一个计数器，LRU算法是把命中行的计数器清零，其他各行计数器加1。当需要替换时淘汰行计数器计数值最大的数据行出局。这是一种高效、科学的算法，其计数器清零过程可以把一些频繁调用后再不需要的数据淘汰出缓存，提高缓存的利用率。CPU产品中，一级缓存的容量基本在4KB到64KB之间，二级缓存的容量则分为128KB、256KB、512KB、1MB、2MB、4MB等。一级缓存容量各产品之间相差不大，而二级缓存容量则是提高CPU性能的关键。二级缓存容量的提升是由CPU制造工艺所决定的，容量增大必然导致CPU内部晶体管数的增加，要在有限的CPU面积上集成更大的缓存，对制造工艺的要求也就越高。主流的CPU二级缓存都在2MB左右，其中英特尔公司07年相继推出了台式机用的4MB、6MB二级缓存的高性能CPU，不过价格也是相对比较高的，对于对配置要求不是太高的朋友，一般的2MB二级缓存的双核CPU基本也可以满足日常上网需要了。2022年，新一代的奔腾处理器采用了与 12 代酷睿一样的 Intel 7 工艺，但没有大小核架构。参数方面，奔腾 G7400 为 2 核 4 线程，3.7GHz，6MB 三级缓存，46W TDP，支持 DDR4-3200 内存和 DDR5-4800 内存。核显为 UHD 710，16 EU 1.35GHz。 [4]主要意义播报编辑缓存的工作方式缓存工作的原则，就是“引用的局部性”，这可以分为时间局部性和空间局部性。空间局部性是指CPU在某一时刻需要某个数据，那么很可能下一步就需要其附近的数据；时间局部性是指当某个数据被访问过一次之后，过不了多久时间就会被再一次访问。对于应用程序而言，不管是指令流还是数据流都会出现引用的局部性现象。举个简单的例子，比如在播放DVD影片的时候，DVD数据由一系列字节组成，这个时候CPU会依次从头处理到尾地调用DVD数据，如果CPU这次读取DVD数据为1分30秒，那么下次读取的时候就会从1分31秒开始，因此这种情况下有序排列的数据都是依次被读入CPU进行处理。从数据上来看，对于Word一类的应用程序通常都有着较好的空间局部性。用户在使用中不会一次打开7、8个文档，不会在其中某一个文档中打上几个词就换另一个。大多数用户都是打开一两个文档，然后就是长时间对它们进行处理而不会做其他事情。这样在内存中的数据都会集中在一个区域中，也就可以被CPU集中处理。从程序代码上来考虑，设计者通常也会尽量避免出现程序的跳跃和分支，让CPU可以不中断地处理大块连续数据。游戏、模拟和多媒体处理程序通常都是这方面的代表，以小段代码连续处理大块数据。不过在办公运用程序中，情况就不一样了。改动字体，改变格式，保存文档，都需要程序代码不同部分起作用，而用到的指令通常都不会在一个连续的区域中。于是CPU就不得不在内存中不断跳来跳去寻找需要的代码。这也就意味着对于办公程序而言，需要较大的缓存来读入大多数经常使用的代码，把它们放在一个连续的区域中。如果缓存不够，就需要内存中的数据，而如果缓存足够大的话，所有的代码都可以放入，也就可以获得最高的效率。同理，高端的数据应用以及游戏应用则需要更高容量的缓存。CPU缓存播报编辑CPU缓存CPU缓存（Cache Memory）是位于CPU与内存之间的临时存储器，它的容量比内存小的多但是交换速率却比内存要快得多。缓存的出现主要是为了解决CPU运算速率与内存读写速率不匹配的矛盾，因为CPU运算速率要比内存读写速率快很多，这样会使CPU花费很长时间等待数据到来或把数据写入内存。在缓存中的数据是内存中的一小部分，但这一小部分是短时间内CPU即将访问的，当CPU调用大量数据时，就可避开内存直接从缓存中调用，从而加快读取速率。由此可见，在CPU中加入缓存是一种高效的解决方案，这样整个内存储器（缓存+内存）就变成了既有缓存的高速率，又有内存的大容量的存储系统了。缓存对CPU的性能影响很大，主要是因为CPU的数据交换顺序和CPU与缓存间的带宽引起的。缓存基本上都是采用SRAM存储器，SRAM是英文Static RAM的缩写，它是一种具有静态存取功能的存储器，不需要刷新电路即能保存它内部存储的数据。不像DRAM内存那样需要刷新电路，每隔一段时间，固定要对DRAM刷新充电一次，否则内部的数据即会消失，因此SRAM具有较高的性能，但是SRAM也有它的缺点，即它的集成度较低，相同容量的DRAM内存可以设计为较小的体积，但是SRAM却需要很大的体积，这也是不能将缓存容量做得太大的重要原因。它的特点归纳如下：优点是节能、速率快、不必配合内存刷新电路、可提高整体的工作效率，缺点是集成度低、相同的容量体积较大、而且价格较高，只能少量用于关键性系统以提高效率。工作原理1、读取顺序CPU要读取一个数据时，首先从Cache中查找，如果找到就立即读取并送给CPU处理；如果没有找到，就用相对慢的速度从内存中读取并送给CPU处理，同时把这个数据所在的数据块调入Cache中，可以使得以后对整块数据的读取都从Cache中进行，不必再调用内存。正是这样的读取机制使CPU读取Cache的命中率非常高（大多数CPU可达90%左右），也就是说CPU下一次要读取的数据90%都在Cache中，只有大约10%需要从内存读取。这大大节省了CPU直接读取内存的时间，也使CPU读取数据时基本无需等待。总的来说，CPU读取数据的顺序是先Cache后内存。2、缓存分类Intel从Pentium开始将Cache分开，通常分为一级高速缓存L1和二级高速缓存L2。在以往的观念中，L1 Cache是集成在CPU中的，被称为片内Cache。在L1中还分数据Cache（D-Cache）和指令Cache（I-Cache）。它们分别用来存放数据和执行这些数据的指令，而且两个Cache可以同时被CPU访问，减少了争用Cache所造成的冲突，提高了处理器效能。3、读取命中率CPU在Cache中找到有用的数据被称为命中，当Cache中没有CPU所需的数据时（这时称为未命中），CPU才访问内存。从理论上讲，在一颗拥有2级Cache的CPU中，读取L1 Cache的命中率为80%。也就是说CPU从L1 Cache中找到的有用数据占数据总量的80%，剩下的20%从L2 Cache读取。由于不能准确预测将要执行的数据，读取L2的命中率也在80%左右（从L2读到有用的数据占总数据的16%）。那么还有的数据就不得不从内存调用，但这已经是一个相当小的比例了。在一些高端领域的CPU（像Intel的Itanium）中，我们常听到L3 Cache，它是为读取L2 Cache后未命中的数据设计的—种Cache，在拥有L3 Cache的CPU中，只有约5%的数据需要从内存中调用，这进一步提高了CPU的效率。一级缓存播报编辑一级缓存（Level 1 Cache）简称L1 Cache，位于CPU内核的旁边，是与CPU结合最为紧密的CPU缓存，也是历史上最早出现的CPU缓存。由于一级缓存的技术难度和制造成本最高，提高容量所带来的技术难度增加和成本增加非常大，所带来的性能提升却不明显，性价比很低，而且现有的一级缓存的命中率已经很高，所以一级缓存是所有缓存中容量最小的，比二级缓存要小得多。一级缓存可以分为一级数据缓存（Data Cache，D-Cache）和一级指令缓存（Instruction Cache，I-Cache）。二者分别用来存放数据以及对执行这些数据的指令进行即时解码，而且两者可以同时被CPU访问，减少了争用Cache所造成的冲突，提高了处理器效能。大多数CPU的一级数据缓存和一级指令缓存具有相同的容量，例如AMD的Athlon XP就具有64KB的一级数据缓存和64KB的一级指令缓存，其一级缓存就以64KB+64KB来表示，其余的CPU的一级缓存表示方法以此类推。Intel的采用NetBurst架构的CPU（最典型的就是Pentium 4）的一级缓存有点特殊，使用了新增加的一种一级追踪缓存（Execution Trace Cache，T-Cache或ETC）来替代一级指令缓存，容量为12KμOps，表示能存储12K条即12000条解码后的微指令。一级追踪缓存与一级指令缓存的运行机制是不相同的，一级指令缓存只是对指令作即时的解码而并不会储存这些指令，而一级追踪缓存同样会将一些指令作解码，这些指令称为微指令（micro-ops），而这些微指令能储存在一级追踪缓存之内，无需每一次都作出解码的程序，因此一级追踪缓存能有效地增加在高工作频率下对指令的解码能力，而μOps就是micro-ops，也就是微型操作的意思。它以很高的速率将μops提供给处理器核心。Intel NetBurst微型架构使用执行跟踪缓存，将解码器从执行循环中分离出来。这个跟踪缓存以很高的带宽将uops提供给核心，从本质上适于充分利用软件中的指令级并行机制。Intel并没有公布一级追踪缓存的实际容量,只知道一级追踪缓存能储存12000条微指令（micro-ops）。所以，不能简单地用微指令的数目来比较指令缓存的大小。实际上，单核心的NetBurst架构CPU使用8Kμops的缓存已经基本上够用了，多出的4kμops可以大大提高缓存命中率。而要使用超线程技术的话，12KμOps就会有些不够用，这就是为什么有时候Intel处理器在使用超线程技术时会导致性能下降的重要原因。例如Northwood核心的一级缓存为8KB+12KμOps，就表示其一级数据缓存为8KB，一级追踪缓存为12KμOps；而Prescott核心的一级缓存为16KB+12KμOps，就表示其一级数据缓存为16KB，一级追踪缓存为12KμOps。在这里12KμOps绝对不等于12KB，单位都不同，一个是μOps，一个是Byte（字节），而且二者的运行机制完全不同。所以那些把Intel的CPU一级缓存简单相加，例如把Northwood核心说成是20KB一级缓存，把Prescott核心说成是28KB一级缓存，并且据此认为Intel处理器的一级缓存容量远远低于AMD处理器128KB的一级缓存容量的看法是完全错误的，二者不具有可比性。在架构有一定区别的CPU对比中，很多缓存已经难以找到对应的东西，即使类似名称的缓存在设计思路和功能定义上也有区别了，此时不能用简单的算术加法来进行对比；而在架构极为近似的CPU对比中，分别对比各种功能缓存大小才有一定的意义。二级缓存播报编辑二级缓存结构剖析二级缓存（Level2cache），它是处理器内部的一些缓冲存储器，其作用跟内存一样。上溯到上个世纪80年代，由于处理器的运行速率越来越快，慢慢地，处理器需要从内存中读取数据的速率需求就越来越高了。然而内存的速率提升速率却很缓慢，而能高速读写数据的内存价格又非常高昂，不能大量采用。从性能价格比的角度出发，英特尔等处理器设计生产公司想到一个办法，就是用少量的高速内存和大量的低速内存结合使用，共同为处理器提供数据。这样就兼顾了性能和使用成本的最优。而那些高速的内存因为是处于cpu和内存之间的位置，又是临时存放数据的地方，所以就叫做缓冲存储器了，简称“缓存”。它的作用就像仓库中临时堆放货物的地方一样，货物从运输车辆上放下时临时堆放在缓存区中，然后再搬到内部存储区中长时间存放。货物在这段区域中存放的时间很短，就是一个临时货场。 最初缓存只有一级，后来处理器速率又提升了，一级缓存不够用了，于是就添加了二级缓存。二级缓存是比一级缓存速率更慢，容量更大的内存，主要就是做一级缓存和内存之间数据临时交换的地方用。为了适应速率更快的处理器p4ee，已经出现了三级缓存了，它的容量更大，速率相对二级缓存也要慢一些，但是比内存可快多了。 缓存的出现使得cpu处理器的运行效率得到了大幅度的提升，这个区域中存放的都是cpu频繁要使用的数据，所以缓存越大处理器效率就越高，同时由于缓存的物理结构比内存复杂很多，所以其成本也很高。大量使用二级缓存带来的结果是处理器运行效率的提升和成本价格的大幅度不等比提升。举个例子，服务器上用的至强处理器和普通的p4处理器其内核基本上是一样的，就是二级缓存不同。至强的二级缓存是2mb～16mb，p4的二级缓存是512kb，于是最便宜的至强也比最贵的p4贵，原因就在二级缓存不同。即l2cache。由于l1级高速缓存容量的限制，为了再次提高cpu的运算速率，在cpu外部放置一高速存储器，即二级缓存。工作主频比较灵活，可与cpu同频，也可不同。cpu在读取数据时，先在l1中寻找，再从l2寻找，然后是内存，在后是外存储器。所以l2对系统的影响也不容忽视。最早先的cpu缓存是个整体的，而且容量很低，英特尔公司从pentium时代开始把缓存进行了分类。当时集成在cpu内核中的缓存已不足以满足cpu的需求，而制造工艺上的限制又不能大幅度提高缓存的容量。因此出现了集成在与cpu同一块电路板上或主板上的缓存，此时就把cpu内核集成的缓存称为一级缓存，而外部的称为二级缓存。随着cpu制造工艺的发展，二级缓存也能轻易的集成在cpu内核中，容量也在逐年提升。再用集成在cpu内部与否来定义一、二级缓存，已不确切。而且随着二级缓存被集成入cpu内核中，以往二级缓存与cpu大差距分频的情况也被改变，此时其以相同于主频的速率工作，可以为cpu提供更高的传输速率。三级缓存播报编辑L3 Cache(三级缓存)，分为两种，早期的是外置，逐渐都变为内置的。而它的实际作用即是，L3缓存的应用可以进一步降低内存延迟，同时提升大数据量计算时处理器的性能。降低内存延迟和提升大数据量计算能力对游戏都很有帮助。而在服务器领域增加L3缓存在性能方面仍然有显著的提升。比方具有较大L3缓存的配置利用物理内存会更有效，故它比较慢的磁盘I/O子系统可以处理更多的数据请求。具有较大L3缓存的处理器提供更有效的文件系统缓存行为及较短消息和处理器队列长度。其实最早的L3缓存被应用在AMD发布的K6-III处理器上，当时的L3缓存受限于制造工艺，并没有被集成进芯片内部，而是集成在主板上。在只能够和系统总线频率同步的L3缓存同主内存其实差不了多少。后来使用L3缓存的是英特尔为服务器市场所推出的Itanium处理器。接着就是P4EE和至强MP。Intel还打算推出一款9MB L3缓存的Itanium2处理器，和以后24MB L3缓存的双核心Itanium2处理器。但基本上L3缓存对处理器的性能提高显得不是很重要，比方配备1MB L3缓存的Xeon MP处理器却仍然不是Opteron的对手，由此可见前端总线的增加，要比缓存增加带来更有效的性能提升。超级缓存播报编辑SuperCache，也就是超级缓存，计算机的速度瓶颈主要在于机械硬盘的读写速度，SuperCache就是给硬盘的读写用高速内存来做缓存，是大内存机器的提速首选，服务器的必备利器。工作原理：对于SuperCache而言，硬盘上没有文件的概念，只是用户指定大小的一个一个小格子，例如32k，硬盘上某个小格子里面的内容被读取了，则被缓存在内存里面，下次还读这个小格子的时候，直接从内存读取，硬盘没有任何动作，从而达到了加速的目的。有两种缓存模式，1、MFU模式，每个小格子被读取的时候，做一个简单的计数，当缓存满的时候，计数值小的先被清出缓存；2、MRU模式，简单的队列，先进先出。系统缓存播报编辑缓存设计将CPU比作一个城里的家具厂，而将存储系统比作郊区的木料厂，那么实际情况就是木料厂离家具厂越来越远，即使使用更大的卡车来运送木料，家具厂也得停工来等待木料送来。在这样的情况下，一种解决方法是在市区建立一个小型仓库，在里面放置一些家具厂最常用到的木料。这个仓库实际上就是家具厂的“Cache”，家具厂就可以从仓库不停的及时运送需要的木料。当然，仓库越大，存放的木料越多，效果就越好，因为这样即使是些不常用的东西也可以在仓库里找到。需要的木料仓库里没有，就要从城外的木料厂里继续找，而家具厂就得等着了。仓库就相对于L1缓存，可以由CPU及时快速的读写，所以存储的是CPU最常用代码和数据（后面会介绍一下如何挑选“最常用”）。L1缓存的速率比系统内存快的多是因为使用的是SRAM，这种内存单晶元使用四到六个晶体管。这也使得SRAM的造价相当的高，所以不能拿来用在整个存储系统上。在大多数CPU上，L1缓存和核心一起在一块芯片上。在家具厂的例子中，就好比工厂和仓库在同一条街上。这样的设计使CPU可以从最近最快的地方得到数据，但是也使得“城外的木料厂”到“仓库”和到“家具厂”的距离差不多远。这样CPU需要的数据不在L1缓存中，也就是“Cache Miss”，从存储设备取数据就要很长时间了。处理器速率越快，两者之间的差距就越大。使用Pentium4那样的高频率处理器，从内存中取得数据就相当于“木料厂”位于另一个国家。其实，缓存是CPU的一部分，它存在于CPU中 CPU存取数据的速率非常的快，一秒钟能够存取、处理十亿条指令和数据（术语：CPU主频1G），而内存就慢很多，快的内存能够达到几十兆就不错了，可见两者的速率差异是多么的大 缓存是为了解决CPU速率和内存速率的速率差异问题 内存中被CPU访问最频繁的数据和指令被复制入CPU中的缓存，这样CPU就可以不经常到象“蜗牛”一样慢的内存中去取数据了，CPU只要到缓存中去取就行了，而缓存的速率要比内存快很多。这里要特别指出的是： 1  因为缓存只是内存中少部分数据的复制品，所以CPU到缓存中寻找数据时，也会出现找不到的情况（因为这些数据没有从内存复制到缓存中去），这时CPU还是会到内存中去找数据，这样系统的速率就慢下来了，不过CPU会把这些数据复制到缓存中去，以便下一次不要再到内存中去取。 2  因为随着时间的变化，被访问得最频繁的数据不是一成不变的，也就是说，刚才还不频繁的数据，此时已经需要被频繁的访问，刚才还是最频繁的数据，后来又不频繁了，所以说缓存中的数据要经常按照一定的算法来更换，这样才能保证缓存中的数据是被访问最频繁的。 3 关于一级缓存和二级缓存为了分清这两个概念，我们先了解一下RAM ram和ROM相对的，RAM是掉电以后，其中信息才消失的那一种，ROM是在掉电以后信息也不会消失的那一种。RAM又分两种： 一种是静态RAM、SRAM；一种是动态RAM、DRAM。磁盘缓存播报编辑磁盘缓存磁盘缓存分为读缓存和写缓存。读缓存是指，操作系统为已读取的文件数据，在内存较空闲的情况下留在内存空间中（这个内存空间被称之为“内存池”），当下次软件或用户再次读取同一文件时就不必重新从磁盘上读取，从而提高速率。写缓存实际上就是将要写入磁盘的数据先保存于系统为写缓存分配的内存空间中，当保存到内存池中的数据达到一个程度时，便将数据保存到硬盘中。这样可以减少实际的磁盘操作，有效的保护磁盘免于重复的读写操作而导致的损坏，也能减少写入所需的时间。根据写入方式的不同，有写通式和回写式两种。写通式在读硬盘数据时，系统先检查请求指令，看看所要的数据是否在缓存中，在的话就由缓存送出响应的数据，这个过程称为命中。这样系统就不必访问硬盘中的数据，由于SDRAM的速率比磁介质快很多，因此也就加快了数据传输的速率。回写式就是在写入硬盘数据时也在缓存中找，找到就由缓存就数据写入盘中，多数硬盘都是采用的回写式缓存，这样就大大提高了性能。缓存英文名为 Cache。CPU 缓存也是内存的一种，其数据交换速率快且运算频率高。磁盘缓存则是操作系统为磁盘输入输出而在普通物理内存中分配的一块内存区域。硬盘的缓冲区，硬盘的缓冲区是硬盘与外部总线交换数据的场所。硬盘的读数据的过程是将磁信号转化为电信号后，通过缓冲区一次次地填充与清空，再填充，再清空，一步步按照PCI总线的周期送出，可见，缓冲区的作用是相当重要的。它的作用也是提高性能，但是它与缓存的不同之处在于：一   它是容量固定的硬件，而不像缓存是可以由操作系统在内存中动态分配的。二   它对性能的影响大大超过磁盘缓存对性能的影响，因为没有缓冲区，就会要求每传一个字（通常是4字节）就需要读一次磁盘或写一次磁盘。缓存映射播报编辑高速缓存可以被分为直接映射缓存，组相联缓存和全相联缓存。直接映射缓存缓存的映射这种缓存中，每个组只有一行，E = 1，结构很简单，整个缓存就相当于关于组的一维数组。不命中时的行替换也很简单，就一个行嘛，哪不命中替换哪。为了适应容量小的情况，第n+1层存储器中的某个数据块，你只能被替换到上一层（也就是第n层）存储器中的某个位置的子集中。假设一个直接映射的高速缓存，（S，E，B，m) = ( 4,1,2,4 )，也就是说，地址是4位（16个），有四个组，每个组一行，每个块两个字节。由于有16个地址，表征16个字节，所以总共有8个块，但只有4个组，也就是4行。只能把多个块映射到相同的缓存组，比如0和4都映射到组1，1和5都映射到组2，等等。这下问题就来了，比如先读块0，此时块0的数据被cache到组0。然后我再读块4，因为块4也是被映射到组0的，组0又只有一行，那就只有把以前块0的数据覆盖了，要是之后我又读块0，就数据丢失了，只能到下级的存储器去找。实际的循环程序中，很容易引起这种情况，称其为抖动。这种情况的存在，自然大大影响了性能。所以，需要更好的映射方案。组相联缓存在组相联缓存里，E大于1，就是说一个组里面有多个cacheline。E等于多少，就叫有多少路，所以叫E路组相联。组相联的行匹配就要复杂一些了，因为要检查多个行的标记位和有效位。如果最终找到了，还好。当然，找不到会从下一级存储器中取出包含所需求数据的行来替换，但一个组里面这么多行，替换哪个行。如果有一个空行，自然就是替换空行，如果没有空行，那就引发了一些其他的替换策略了。除了刚才介绍过的随机策略，还有最不常使用策略，最近最少使用策略。这些策略本身是需要一定开销的，但要知道，不命中的开销是很大的，所以为了保证命中率，采取一些相对复杂的策略是值得的。全相联缓存所谓全相联，就是由一个包含所有缓存行的组组成的缓存（块可以放在高速缓存中的任意位置） [2]。由于只有一个组，所以组选择特别简单，此时地址就没有组索引了，只有标记和偏移，也就是t部分和b部分。其他的步骤，行匹配和数据选择，和组相联原理是一样的，只是规模大得多了。如果说上面关于这三种映射方法的描述非常抽象，为了能理解得更加透彻，把存储器比作一家大超市，超市里面的东西就是一个个字节或者数据。为了让好吃好玩受欢迎的东西能够容易被看到，超市可以将这些东西集中在一块放在一个专门的推荐柜台中，这个柜台就是缓存。如果仅仅是把这些货物放在柜台中即完事，那么这种就是完全关联的方式。可是如果想寻找自己想要的东西，还得在这些推荐货物中寻找，而且由于位置不定，甚至可能把整个推荐柜台寻找个遍，这样的效率无疑还是不高的。于是超市老总决定采用另一种方式，即将所有推荐货物分为许多类别，如“果酱饼干”，“巧克力饼干”，“核桃牛奶”等，柜台的每一层存放一种货物。这就是直接关联的访问原理。这样的好处是容易让顾客有的放矢，寻找更快捷，更有效。但这种方法还是有其缺点，那就是如果需要果酱饼干的顾客很多，需要巧克力饼干的顾客相对较少，显然对果酱饼干的需求量会远多于对巧克力饼干的需求量，可是放置两种饼干的空间是一样大的，于是可能出现这种情况：存放的果酱饼干的空间远不能满足市场需求的数量，而巧克力饼干的存放空间却被闲置。为了克服这个弊病，老板决定改进存货方法：还是将货物分类存放，不过分类方法有所变化，按“饼干”，“牛奶”，“果汁”等类别存货，也就是说，无论是什么饼干都能存入“ 饼干”所用空间中，这种方法显然提高了空间利用的充分性，让存储以及查找方法更有弹性。技术指标播报编辑CPU缓存CPU产品中，一级缓存的容量基本在4kb到64kb之间，二级缓存的容量则分为128kb、256kb、512kb、1mb、2mb等。一级缓存容量各产品之间相差不大，而二级缓存容量则是提高cpu性能的关键。二级缓存容量的提升是由cpu制造工艺所决定的，容量增大必然导致cpu内部晶体管数的增加，要在有限的cpu面积上集成更大的缓存，对制造工艺的要求也就越高缓存(cache)大小是CPU的重要指标之一，其结构与大小对CPU速率的影响非常大。简单地讲，缓存就是用来存储一些常用或即将用到的数据或指令，当需要这些数据或指令的时候直接从缓存中读取，这样比到内存甚至硬盘中读取要快得多，能够大幅度提升cpu的处理速率。所谓处理器缓存，通常指的是二级高速缓存，或外部高速缓存。即高速缓冲存储器，是位于CPU和主存储器dram(dynamic ram)之间的规模较小的但速率很高的存储器，通常由sram（静态随机存储器）组成。用来存放那些被cpu频繁使用的数据，以便使cpu不必依赖于速率较慢的dram（动态随机存储器）。l2高速缓存一直都属于速率极快而价格也相当昂贵的一类内存，称为sram(静态ram)，sram(static ram)是静态存储器的英文缩写。由于sram采用了与制作cpu相同的半导体工艺，因此与动态存储器dram比较，sram的存取速率快，但体积较大，价格很高。处理器缓存的基本思想是用少量的sram作为cpu与dram存储系统之间的缓冲区，即cache系统。80486以及更高档微处理器的一个显著特点是处理器芯片内集成了sram作为cache，由于这些cache装在芯片内，因此称为片内cache。486芯片内cache的容量通常为8k。高档芯片如pentium为16kb，power pc可达32kb。pentium微处理器进一步改进片内cache，采用数据和双通道cache技术，相对而言，片内cache的容量不大，但是非常灵活、方便，极大地提高了微处理器的性能。片内cache也称为一级cache。由于486，586等高档处理器的时钟频率很高，一旦出现一级cache未命中的情况，性能将明显恶化。在这种情况下采用的办法是在处理器芯片之外再加cache，称为二级cache。二级cache实际上是cpu和主存之间的真正缓冲。由于系统板上的响应时间远低于cpu的速率，没有二级cache就不可能达到486，586等高档处理器的理想速率。二级cache的容量通常应比一级cache大一个数量级以上。在系统设置中，常要求用户确定二级cache是否安装及尺寸大小等。二级cache的大小一般为128kb、256kb或512kb。在486以上档次的微机中，普遍采用256kb或512kb同步cache。所谓同步是指cache和cpu采用了相同的时钟周期，以相同的速率同步工作。相对于异步cache，性能可提高30%以上。pc及其服务器系统的发展趋势之一是cpu主频越做越高，系统架构越做越先进，而主存dram的结构和存取时间改进较慢。因此，缓存（cache）技术愈显重要，在pc系统中cache越做越大。广大用户已把cache做为评价和选购pc系统的一个重要指标。光驱缓存播报编辑光存储驱动器都带有内部缓冲器或高速缓存存储器。这些缓冲器是实际的存储芯片，安装在驱动器的电路板上，它在发送数据给PC之前可能准备或存储更大的数据段。CD/DVD典型的缓冲器大小为128KB，不过具体的驱动器可大可小（通常越多越好）。可刻录CD或DVD驱动器一般具有2MB-4MB以上的大容量缓冲器，用于防止缓存欠载（buffer underrun）错误，同时可以使刻录工作平稳、恒定的写入。一般来说，驱动器越快，就有更多的缓冲存储器，以处理更高的传输速率。CD/DVD驱动器带有缓冲或高速缓存具有很多好处。缓冲可以保证PC以固定速率接收数据。当一个应用程序从驱动器请求数据时，数据可能位于分散在光盘上不同地方。因为驱动器的访问速率相对较慢，在数据读取时会使驱动器不得不间隔性向PC发送数据。驱动器的缓冲在软件的控制下可以预先读取并准备光盘的内容目录，从而加速第一次数据请求。光驱读取数据的规律是首先在缓存里寻找，如果在缓存中没有找到才会去光盘上寻找，大容量的缓存可以预先读取的数据越多，但在实际应用中CD-ROM、DVD-ROM等读取操作时，读取重复信息的机会是相对较少的，大部分的光盘更多的时候是一次读取数量较多的文件内容，因此在CD-ROM、DVD-ROM驱动器上缓存重要性得不到体现，因此大多此类产品采用较小的缓存容量。CD-ROM一般有128KB、256KB、512KB几种；而DVD一般有128KB、256KB、512KB，只有个别的外置式DVD光驱采用了较大容量的缓存。在刻录机或COMMBO产品上，缓存就变得十分重要了。在刻录光盘时，系统会把需要刻录的数据预先读取到缓存中，然后再从缓存读取数据进行刻录，缓存就是数据和刻录盘之间的桥梁。系统在传输数据到缓存的过程中，不可避免的会发生传输的停顿，如在刻录大量小容量文件时，硬盘读取的速率很可能会跟不上刻录的速率，就会造成缓存内的数据输入输出不成比例，如果这种状态持续一段时间，就会导致缓存内的数据被全部输出，而得不到输入，此时就会造成缓存欠载错误，这样就会导致刻录光盘失败。因此刻录机和COMMBO产品都会采用较大容量的缓存容量，再配合防刻死技术，就能把刻坏盘的几率降到最低。同时缓存还能协调数据传输速率，保证数据传输的稳定性和可靠性。刻录机产品一般有2MB、4MB、8MB，COMBO产品一般有2MB、4MB、8MB的缓存容量，受制造成本的限制，缓存不可能制作到足够大，但适量的缓存容量还是选择光储需要考虑的关键之一。网络缓存播报编辑概念WWW是互联网上最受欢迎的应用之一，其快速增长造成网络拥塞和服务器超载，导致客户访问延迟增大，WWW服务质量日益显现出来。缓存技术被认为是减轻服务器负载、降低网络拥塞、增强WWW可扩展性的有效途径之一，其基本思想是利用客户访问的时间局部性（Temproral Locality）原理，将客户访问过的内容在Cache中存放一个副本，当该内容下次被访问时，不必连接到驻留网站，而是由Cache中保留的副本提供。Web内容可以缓存在客户端、代理服务器以及服务器端。研究表明，缓存技术可以显著地提高WWW性能，它可以带来以下好处：（1）减少网络流量，从而减轻拥塞。（2）降低客户访问延迟，其主要原因有：①缓存在代理服务器中的内容，客户可以直接从代理获取而不是从远程服务器获取，从而减小了传输延迟；②没有被缓存的内容由于网络拥塞及服务器负载的减轻而可以较快地被客户获取。（3）由于客户的部分请求内容可以从代理处获取，从而减轻了远程服务器负载。（4）如果由于远程服务器故障或者网络故障造成远程服务器无法响应客户的请求，客户可以从代理中获取缓存的内容副本，使得WWW服务的鲁棒性得到了加强。Web缓存系统也会带来以下问题：（1）客户通过代理获取的可能是过时的内容。（2）如果发生缓存失效，客户的访问延迟由于额外的代理处理开销而增加。因此在设计Web缓存系统时，应力求做到Cache命中率最大化和失效代价最小化。（3）代理可能成为瓶颈。因此应为一个代理设定一个服务客户数量上限及一个服务效率下限，使得一个代理系统的效率至少同客户直接和远程服务器相连的效率一样。影响Internet访问速率访问网站的过程是通过建立在TCP/IP协议之上的HTTP协议来完成的。从客户端发出一个HTTP请求开始，用户所经历的等待时间主要决定于DNS和网站的响应时间。网站域名首先必须被DNS服务器解析为IP地址，HTTP的延时则由在客户端和服务器间的若干个往返时间所决定。往返时间是指客户端等待每次请求的响应时间，平均往返时间取决于三个方面：网站服务器的延时网站服务器造成的延时在往返时间中占主要比例。当某个服务器收到多个并发HTTP请求时，会产生排队延时。由于响应一个HTTP请求，往往需要多次访问本地硬盘，所以即使是一台负载并不大的服务器，也可能产生几十或几百微秒的延时。由路由器、网关、代理服务器和防火墙引入的延时通常在客户端和服务器之间的路径上会存在多个网络设备，如路由器、网关、代理和防火墙等。它们对经过的IP包都要做存储/转发的操作，于是会引入排队延时和处理延时。在网络拥塞时，这些设备甚至会丢包，此时会寄希望于客户端和服务器通过端到端的协议来恢复通信。不同通信链路上的数据传输速率在广域网中，从一个网络设备到另一个网络设备间的数据传输速率是决定往返时间的一个重要因素。但基本带宽的作用并不是像人们想象的那么重要，一项测试表明，当网站采用T3速率接入Internet时，也仅有2%的网页或对象能以64kbps的速率提供给客户端，这显然表明，带宽在网络性能上不是最关键的因素。Internet在向世界的每一个角落延伸，用户向一个服务器发出的 请求可能会经过8000公里到1.6万公里的距离，光速带来的延时和网络设备的延时是网络如此缓慢的最根本原因。网络缓存解决根本问题既然影响网络速率的原因是由距离和光速引起，那么加速Web访问的唯一途径就是缩短客户端与网站之间的距离。通过将用户频繁访问的页面和对象存放在离用户更近的地方，才能减少光速引入的延时，同时由于减少了路由中的环节，也相应地减少了路由器、防火墙和代理等引入的延时。传统的解决办法是建立镜像服务器来达到缩短距离的目的。但这个办法存在很大的不足，对于某个站点而言，不可能在离每个用户群较近的地方都建立镜像站点，若对大多数网站都用这样的办法就更不经济，同时管理和维护镜像站点是一项非常困难的工作。网络缓存是一种降低Internet流量和提高终端用户响应时间的新兴网络技术。它的观念来自于计算机和网络的其他领域，如流行的Intel架构的CPU中就存在缓存，用于提高内存存取的速率；各种操作系统在进行磁盘存取时也会利用缓存来提高速率；分布式文件系统通常也通过缓存来提高客户机和服务器之间的速率。类型静态页面的缓存可能有2种形式：其实主要区别就是CMS是否自己负责关联内容的缓存更新管理。1 静态缓存：是在新内容发布的同时就立刻生成相应内容的静态页面，比如：2003年3月22日，管理员通过后台内容管理界面录入一篇文章后，并同步更新相关索引页上的链接。2 动态缓存：是在新内容发布以后，并不预先生成相应的静态页面，直到对相应内容发出请求时，如果前台缓存服务器找不到相应缓存，就向后台内容管理服务器发出请求，后台系统会生成相应内容的静态页面，用户第一次访问页面时可能会慢一点，但是以后就是直接访问缓存了。静态缓存的缺点：网络缓存系统结构图复杂的触发更新机制：这两种机制在内容管理系统比较简单的时候都是非常适用的。但对于一个关系比较网络缓存系统结构图复杂的网站来说，页面之间的逻辑引用关系就成为一个非常非常复杂的问题。最典型的例子就是一条新闻要同时在新闻首页和相关的3个新闻专题中出现，在静态缓存模式中，每发一篇新文章，除了这篇新闻内容本身的页面外，还需要系统通过触发器生成多个新的相关静态页面，这些相关逻辑的触发也往往就会成为内容管理系统中最复杂的部分之一。旧内容的批量更新： 通过静态缓存发布的内容，对于以前生成的静态页面的内容很难修改，这样用户访问旧页面时，新的模板根本无法生效。在动态缓存模式中，每个动态页面只需要关心，而相关的其他页面能自动更新，从而大大减少了设计相关页面更新触发器的需要。网络缓存可以在客户端，也可以在网络上，由此我们将缓存分为两类：浏览器缓存和代理缓存。几乎所有的浏览器都有一个内置的缓存，它们通常利用客户端本地的内存和硬盘来完成缓存工作，同时允许用户对缓存的内容大小作控制。浏览器缓存是网络缓存的一个极端的情况，因为缓存设在客户机本地。通常一个客户端只有一个用户或几个共享计算机用户，浏览器缓存要求的硬盘空间通常在5MB到50MB的范围内。但是浏览器缓存在用户之间难以共享，不同客户端的缓存无法实现交流，因而缓存的内容与效果相当有限。代理缓存则是一种独立的应用层网络服务,它更像E－mail、Web、DNS等服务。许多用户不仅可以共享缓存，而且可以同时访问缓存中的内容。企业级代理缓存一般需要配置高端的处理器和存储系统，采用专用的软件，要求的硬盘空间在5MB到50GB左右，内存为64MB到512MB。代理处于客户端与网站服务器之间,在某些情况下，这种连接是不允许的，如网站在防火墙内,这时客户端必须与代理建立TCP连接，然后由代理建立与网站服务器的TCP连接。代理在服务器和客户端之间起到了数据接力的作用。代理发出的HTTP请求与一般的HTTP请求有细小的不同，主要在于它包含了完整的URL，而不只是URL的路径。代理缓存的工作原理当代理缓存收到客户端的请求时，它首先检查所请求的内容是否已经被缓存。如果没有找到，缓存必须以客户端的名义转发请求，并在收到服务器发出的文件时，将它以一定的形式保存在本地硬盘，并将其发送给客户端。如果客户端请求的内容已被缓存，还存在两种可能：其一，缓存的内容已经过时，即缓存中保存的内容超过了预先设定的时限，或网站服务器的网页已经更新，这时缓存会要求原服务器验证缓存中的内容，要么更新内容，要么返回“未修改”的消息；其二，缓存的内容是新的，即与原网站的内容保持同步，此时称为缓存命中，这时缓存会立即将已保存的内容送给客户端。在客户端的请求没有命中时，反而增加了缓存存储和转发的处理时间。在这种情况下，代理缓存是否仍有意义呢？实际上，代理缓存能够同时与网站服务器建立多个并发的TCP/IP连接，并行获取网站上的内容。缓存的存在从整体上降低了对网站访问的次数，也就降低了单位时间内服务器端的排队数目，因而这时并发连接的排队延时要小得多。优秀的缓存甚至能实现对网页内相关链接内容的预取以加快连接的速率。代理缓存的策略当原服务器的文件修改或被删除后，缓存又如何知道它保存的拷贝已经作废呢？HTTP协议为缓存服务提供了基本的支持，它使缓存能向原服务器查询，某个文件是否更改,如果缓存的拷贝过时则进行有条件下载。仅当原服务器文件超过指定的日期时，才会发出新的文件。但是这些询问操作对网络服务器造成的负载几乎和获取该文件差不多,因此不可能在客户端向缓存发起请求时都执行这样的操作。HTTP协议使得服务器可以有选择地为每个文档指定生存时间,即清楚地指出某个文件的有效生命周期，生存时间很短即意味着“不要对其缓存”。拷贝的保留时间可以是固定的，也可以是通过这个文件的大小、来源、生存时间或内容计算出来的。 [3]分布缓存播报编辑分布式缓存系统是为了解决数据库服务器和web服务器之间的瓶颈。如果一个网站的流量很大，这个瓶颈将会非常明显，每次数据库查询耗费的时间将会非常可观。对于更新速度不是很快的网站，我们可以用静态化来避免过多的数据库查询。对于更新速度以秒计的网站，静态化也不会太理想，可以用缓存系统来构建。如果只是单台服务器用作缓存，问题不会太复杂，如果有多台服务器用作缓存，就要考虑缓存服务器的负载均衡。使用Memcached分布式缓存服务来达到保存用户的会话数据，而达到各个功能模块都能够跨省份、跨服务器共享本次会话中的私有数据的目的。每个省份使用一台服务器来做为Memcached服务器来存储用话的会话中的数据，当然也可以多台服务器，但必须确保每个省份的做Memcached服务器数量必须一致，这样才能够保证Memcached客户端操作的是同一份数据，保证数据的一致性。会话数据的添加、删除、修改Memcached客户端，添加、删除和、修改会话信息数据时，不仅要添加、删除、修改本省的Memcached服务器数据，而且同时要对其它省份的Memcahed服务器做同样的操作，这样用户访问其它省份的服务器的功能模块进也能读取到相同的会话数据。Memcached客户端服务器的列表使用局域网的内网IP（如：192.168.1.179）操作本省的Memcahed服务器，使用公网的IP（（如：202.183.62.210））操作其它省份的Memcahe服务器。会话数据的读取系统所有模块读取会话数据的Memcached客户端服务器列表都设为本省Memcached服务器地址的内网IP来向Memcahed服务器中读取会话数据。同一会话的确认使用Cookie来保持客户与服务端的联系。每一次会话开始就生成一个GUID作为SessionID，保存在客户端的Cookie中，作用域是顶级域名，这样二级、三级域名就可以共享到这个Cookie，系统中就使用这个SessionID来确认它是否是同一个会话。会话数据的唯一ID会话数据存储在Memcached服务器上的唯一键Key也就是会话数据数据的唯一ID定义为：SessionID_Name, SessionID就是保存在客户端Cookie中的SessionID,Name就是会话数据的名称，同一次会话中各个会话数据的Name必须是唯一的，否则新的会话数据将覆盖旧的会话数据。会话的失效时间会话的失效通过控制Cookie的有效时间来实现，会话的时间设为SessionID或Cookie中的有效时间，且每一次访问SessionID时都要重新设置一下Cookie的有效时间，这样就达到的会话的有效时间就是两次间访问Cookie中SessionID值的的最长时间，如果两次访问的间隔时间超过用效时间，保存在SessionID的Cookie将会失效，并生成新的SessionID存放在Cookie中, SessionID改变啦，会话就结束啦。Memcached服务器中会话数据的失效，每一次向Memcache服务器中添加会话数据时，都把有效时间设为一天也就是24小时，让Memcached服务使用它内部的机制去清除，不必在程序中特别做会话数据的删除操作。数据在Memcache服务器中有有效时间只是逻辑上的，就算是过了24 小时，如果分配给Memcached服务的内存还够用的话，数据还是保存在内存当中的，只是Memcache客户端读取不到而已。只有到了分配给Memcached服务的内存不够用时，它才会清理没用或者比较旧的数据，也就是懒性清除。增加缓存的方法播报编辑CPU的缓存CPU的缓存分二级：L1（一级缓存）和L2（二级缓存），当处理器要读取数据时，首先要在L1缓存中查找，其次才是L2缓存，最后才是系统内存。如果有一天你发觉自己的电脑慢了很多，进入到Windows桌面也要几分钟，这时候就要检查一下CPU的一、二级缓存有没有打开。在BIOS设置中的StandardCMOSSetup（标准CMOS设定）有两项是用来打开或关闭缓存的：CPUInternalCache设为Enable时开启CPU内部的一级缓冲区，若设置为Disabl则为关闭，这时系统性能将大大降低；ExternalCache选项是控制主板上二级缓冲区，如果主板上有二级缓存则应设成Enable。硬盘的缓存点击电脑桌面上的“开始”/“运行”，键入“Msconfig”启动“系统配置实用程序”，跟着选中“system．ini”标签下的“Vcache”项，就可以根据系统的实际情况来调节硬盘的缓存了。在该选项中一般会有三行内容：ChunkSize=1024、MaxFileCache=10240和MinFileCache=10240；其中第一行是缓冲区读写单元值，第二、三行是硬盘的最大和最小缓冲值，等号后的数值都是可以修改的，只要右键单击选中任一行就可以进行修改了。如果你的内存是128MB的话，上面这三行的取值就比较合理了，当然也可以自定。如果不知道该如何设置合适的缓冲值，请“Windows优化大师”帮忙吧，这个软件中有一个“磁盘缓存优化”项，用鼠标就可以方便地设置好缓存；又或者让“Windows优化大师”自动帮你进行优化设置。当硬盘的缓存值足够大时，硬盘就不用频繁地读写磁盘，一来可以延长硬盘的寿命，二来也可以提高数据的传输速度。另外，将硬盘的“文件系统缓存”设置为“网络服务器”，可以加快系统对硬盘的访问速度，因为文件系统缓存里存放了硬盘最近被访问过的文件名和路径，缓存越大所能储存的内容也就越多。如果点击“控制面板”/“系统”/“性能”/“文件系统”/“硬盘”，将“此计算机的主要用途”由“台式机”改为“网络服务器”，可以将原来10K左右的缓存增加至近50K左右。软驱和光驱的缓存一般来说，软驱读写数据的速度都比较慢，这是因为盘片的转速不能太高，但是，我们可以提高软驱的读写缓存，让软驱一次读写更多的数据。方法是：在桌面上的“开始”/“运行”框中键入“Regedit”运行注册表编辑器，依次进入HKEY－LOCAL－MACHINE\System\CurrentControlSet\Services\Class\FDC\0000，新建一个为ForeFifo的“DWORD值”，将其值设为“0”，这样就对软驱进行了软提速。很多人都知道右键单击桌面“我的电脑”图标，选“属性”/“性能”/“文件系统”/“CD－ROM”，将最佳的访问方式设为“四倍速或更高速”，将追加的高速缓存大小滑块拖到最大处，可以明显提高光驱的读盘速度。除了这种方式，我们还可以在注册表中设置缓冲值，方法是：进入到注册表，在HKEY－LOCAL－MACHINE\System\CurrentControlSet\Control\FileSystem\CDFS下，将CacheSize（缓存值的大小）和Prefetch（预读文件大小）两项进行手工调整，只要右键单击要选的项就可以进行修改了。 [3]

总线仲裁：
简介播报编辑总线上的设备一般分为总线主设备和总线从设备。总线主设备是指具有控制总线能力的模块，通常是CPU或以CPU为中心的逻辑模块，在获得总线控制权之后能启动数据信息的传输；与之相对应的总线从设备，是指能够对总线上的数据请求做出响应，但本身不具备总线控制能力的模块。在早期的计算机系统中，一条总线上只有一个主设备，总线一直由它占用，技术简单，实现也比较容易。随着应用的发展，主要是工业控制、科学计算的需求，多个主设备共享总线的情况越来越多，这对总线技术提出了新的要求。根据这类系统的特点，需要解决各个主设备之间资源争用等问题，这使得总线的复杂性大为增加。总线仲裁就是在多个总线主设备的环境中提出来的。在多处理机系统中，每个处理机都可以作为总线主设备，都要共享资源，它们都必须通过系统总线才能访问其它资源，总线也可视为是一种重要的公共资源。由于每个处理机都会随机地提出对总线使用的要求，这样就可能发生总线竞争现象。为了防止多个处理机同时控制总线，就要在总线上设立一个处理上述总线竞争的机构，按优先级次序，合理地分配资源，这就是总线仲裁问题。用硬件来实现总线分配的逻辑电路称为总线仲裁器(Bus Arbiter)。它的任务是响应总线请求，通过对分配过程的正确控制，达到最佳使用总线。总线判优控制按照仲裁控制机构的设置可分为集中控制和分散控制两种。其中就集中控制而言，常用的总线仲裁方式有：菊花链仲裁、二维仲裁、同步通信方式、异步通信方式和半同步通信方式。连接到总线上的功能模块有主动和被动两种形态，CPU可以做主方也可以做从方，而存取器模块只能用作从方。主方可以启动一个总线周期，而从方只能响应主方的请求。对多个主设备提出的占用总线请求，一般采用优先级或公平策略进行仲裁 [1]。仲裁方式分类播报编辑按照总线仲裁电路的位置不同，仲裁方式分为集中式仲裁和分布式仲裁两类：1.集中式总线仲裁的控制逻辑基本集中在一处，需要中央仲裁器，分为链式查询方式、计数器定时查询方式、独立请求方式；(1) 链式查询方式链式查询方式的主要特点：总线授权信号BG串行地从一个I/O接口传送到下一个I/O接口。假如BG到达的接口无总线请求，则继续往下查询；假如BG到达的接口有总线请求，BG信号便不再往下查询，该I/O接口获得了总线控制权。离中央仲裁器最近的设备具有最高优先级，通过接口的优先级排队电路来实现。链式查询方式的优点: 只用很少几根线就能按一定优先次序实现总线仲裁，很容易扩充设备。链式查询方式的缺点: 对询问链的电路故障很敏感，如果第i个设备的接口中有关链的电路有故障，那么第i个以后的设备都不能进行工作。查询链的优先级是固定的，如果优先级高的设备出现频繁的请求时，优先级较低的设备可能长期不能使用总线。(2)计数器定时查询方式总线上的任一设备要求使用总线时，通过BR线发出总线请求。中央仲裁器接到请求信号以后，在BS线为“0”的情况下让计数器开始计数，计数值通过一组地址线发向各设备。每个设备接口都有一个设备地址判别电路，当地址线上的计数值与请求总线的设备地址相一致时，该设备 置“1”BS线，获得了总线使用权，此时中止计数查询。每次计数可以从“0”开始，也可以从中止点开始。如果从“0”开始，各设备的优先次序与链式查询法相同，优先级的顺序是固定的。如果从中止点开始，则每个设备使用总线的优先级相等。计数器的初值也可用程序来设置，这可以方便地改变优先次序，但这种灵活性是以增加线数为代价的。(3)独立请求方式每一个共享总线的设备均有一对总线请求线BRi和总线授权线BGi。当设备要求使用总线时，便发出该设备的请求信号。中央仲裁器中的排队电路决定首先响应哪个设备的请求，给设备以授权信号BGi。独立请求方式的优点：响应时间快，确定优先响应的设备所花费的时间少，用不着一个设备接一个设备地查询。其次，对优先次序的控制相当灵活，可以预先固定也可以通过程序来改变优先次序；还可以用屏蔽(禁止)某个请求的办法，不响应来自无效设备的请求。2.分布式仲裁不需要中央仲裁器，每个潜在的主方功能模块都有自己的仲裁号和仲裁器。当它们有总线请求时，把它们唯一的仲裁号发送到共享的仲裁总线上，每个仲裁器将仲裁总线上得到的号与自己的号进行比较。如果仲裁总线上的号大，则它的总线请求不予响应，并撤消它的仲裁号。最后，获胜者的仲裁号保留在仲裁总线上。显然，分布式仲裁是以优先级仲裁策略为基础 [2]。总线分配技术播报编辑对总线仲裁问题的解决是以优先级(又称优先权)的概念为基础的，通常有三种总线分配的优先级技术──串联、并联和循环。串联优先级判别法图1 串联优先级判别法图1中有Ⅰ、Ⅱ、…、N等N个模块，都可作为总线主设备，各个模块中的“请求”输出端采用集电极(漏极)开路门，“请求”端用“线或”方式接到仲裁器“请求”输入端，每个模块的“忙”端同仲裁器的“总线忙”状态线相连，这是一个输入输出双向信号线。当一个模块占有总线控制权时，该模块的“忙”信号端成为输出端，向系统的“忙”状态线送出有效信号(例如低电平)。其它模块的“忙”信号端全部作为输入端工作，检测“忙”线上状态。一个模块若要提出总线“请求”，其必要条件是选检测到“忙”信号输入端处于无效状态。与此相应，仲裁器接受总线请求输入的条件，也是“忙”线处于无效状态。进一步可以规定仲裁器输出“允许”信号的条件首先是“忙”线无效，表示总线没有被任一模块占用；其次才是有模块提出了总线请求。“允许”信号在链接的模块之间传输，直到提出总线“请求”的那个模块为止。这里用“允许”信号的边沿触发，它把共享总线的各模块要使用总线时，便发生信号禁止后面的部件使用总线。通过这种方式，就确定了请求总线各模块中优先级最高的模块。显然，在这种方式中，当优先级高的模块频繁请求时，优先级别低的模块可能很长时间都无法获得总线。一旦有模块占用总线后，“允许”信号就不再存在。并联优先级别判别法图2 并联优先级别判别法图2中有N个模块，都可作为总线主设备，每个模块都有总线“请求”线和总线“允许”线，模块之间是独立的，没有任何控制关系。这些信号接到总线优先控制器(仲裁器)，任一模块使用总线，都要通过“请求”线向仲裁器发出“请求”信号。仲裁器一般由一个优先级编码器和一个译码器组成。该电路接到某个模块或多个模块发来的请求信号后，首先优先级编码器进行编码，然后由译码器产生相应的输出信号，发往请求总线模块中优先级最高的模块，并把“允许”信号送给该模块。被选中的模块撤销总线“请求”信号，输出总线“忙”信号，通知其余模块，总线已经占用。在一个模块占用总线的传输结束以后，就把总线“忙”信号撤销，仲裁器也撤销“允许”信号。根据各请求输入的情况，仲裁器重新分配总线控制权。循环优先级判别法循环优先级判别方法类似于并联优先级判别方法，只是其中的优先级是动态分配的，原来的优先级编码器由一个更为复杂的电路代替，该电路把占用总线的优先级在发出总线请求的那些模块之间循环移动，从而使每个总线模块使用总线的机会相同。

总线仲裁：
简介播报编辑总线上的设备一般分为总线主设备和总线从设备。总线主设备是指具有控制总线能力的模块，通常是CPU或以CPU为中心的逻辑模块，在获得总线控制权之后能启动数据信息的传输；与之相对应的总线从设备，是指能够对总线上的数据请求做出响应，但本身不具备总线控制能力的模块。在早期的计算机系统中，一条总线上只有一个主设备，总线一直由它占用，技术简单，实现也比较容易。随着应用的发展，主要是工业控制、科学计算的需求，多个主设备共享总线的情况越来越多，这对总线技术提出了新的要求。根据这类系统的特点，需要解决各个主设备之间资源争用等问题，这使得总线的复杂性大为增加。总线仲裁就是在多个总线主设备的环境中提出来的。在多处理机系统中，每个处理机都可以作为总线主设备，都要共享资源，它们都必须通过系统总线才能访问其它资源，总线也可视为是一种重要的公共资源。由于每个处理机都会随机地提出对总线使用的要求，这样就可能发生总线竞争现象。为了防止多个处理机同时控制总线，就要在总线上设立一个处理上述总线竞争的机构，按优先级次序，合理地分配资源，这就是总线仲裁问题。用硬件来实现总线分配的逻辑电路称为总线仲裁器(Bus Arbiter)。它的任务是响应总线请求，通过对分配过程的正确控制，达到最佳使用总线。总线判优控制按照仲裁控制机构的设置可分为集中控制和分散控制两种。其中就集中控制而言，常用的总线仲裁方式有：菊花链仲裁、二维仲裁、同步通信方式、异步通信方式和半同步通信方式。连接到总线上的功能模块有主动和被动两种形态，CPU可以做主方也可以做从方，而存取器模块只能用作从方。主方可以启动一个总线周期，而从方只能响应主方的请求。对多个主设备提出的占用总线请求，一般采用优先级或公平策略进行仲裁 [1]。仲裁方式分类播报编辑按照总线仲裁电路的位置不同，仲裁方式分为集中式仲裁和分布式仲裁两类：1.集中式总线仲裁的控制逻辑基本集中在一处，需要中央仲裁器，分为链式查询方式、计数器定时查询方式、独立请求方式；(1) 链式查询方式链式查询方式的主要特点：总线授权信号BG串行地从一个I/O接口传送到下一个I/O接口。假如BG到达的接口无总线请求，则继续往下查询；假如BG到达的接口有总线请求，BG信号便不再往下查询，该I/O接口获得了总线控制权。离中央仲裁器最近的设备具有最高优先级，通过接口的优先级排队电路来实现。链式查询方式的优点: 只用很少几根线就能按一定优先次序实现总线仲裁，很容易扩充设备。链式查询方式的缺点: 对询问链的电路故障很敏感，如果第i个设备的接口中有关链的电路有故障，那么第i个以后的设备都不能进行工作。查询链的优先级是固定的，如果优先级高的设备出现频繁的请求时，优先级较低的设备可能长期不能使用总线。(2)计数器定时查询方式总线上的任一设备要求使用总线时，通过BR线发出总线请求。中央仲裁器接到请求信号以后，在BS线为“0”的情况下让计数器开始计数，计数值通过一组地址线发向各设备。每个设备接口都有一个设备地址判别电路，当地址线上的计数值与请求总线的设备地址相一致时，该设备 置“1”BS线，获得了总线使用权，此时中止计数查询。每次计数可以从“0”开始，也可以从中止点开始。如果从“0”开始，各设备的优先次序与链式查询法相同，优先级的顺序是固定的。如果从中止点开始，则每个设备使用总线的优先级相等。计数器的初值也可用程序来设置，这可以方便地改变优先次序，但这种灵活性是以增加线数为代价的。(3)独立请求方式每一个共享总线的设备均有一对总线请求线BRi和总线授权线BGi。当设备要求使用总线时，便发出该设备的请求信号。中央仲裁器中的排队电路决定首先响应哪个设备的请求，给设备以授权信号BGi。独立请求方式的优点：响应时间快，确定优先响应的设备所花费的时间少，用不着一个设备接一个设备地查询。其次，对优先次序的控制相当灵活，可以预先固定也可以通过程序来改变优先次序；还可以用屏蔽(禁止)某个请求的办法，不响应来自无效设备的请求。2.分布式仲裁不需要中央仲裁器，每个潜在的主方功能模块都有自己的仲裁号和仲裁器。当它们有总线请求时，把它们唯一的仲裁号发送到共享的仲裁总线上，每个仲裁器将仲裁总线上得到的号与自己的号进行比较。如果仲裁总线上的号大，则它的总线请求不予响应，并撤消它的仲裁号。最后，获胜者的仲裁号保留在仲裁总线上。显然，分布式仲裁是以优先级仲裁策略为基础 [2]。总线分配技术播报编辑对总线仲裁问题的解决是以优先级(又称优先权)的概念为基础的，通常有三种总线分配的优先级技术──串联、并联和循环。串联优先级判别法图1 串联优先级判别法图1中有Ⅰ、Ⅱ、…、N等N个模块，都可作为总线主设备，各个模块中的“请求”输出端采用集电极(漏极)开路门，“请求”端用“线或”方式接到仲裁器“请求”输入端，每个模块的“忙”端同仲裁器的“总线忙”状态线相连，这是一个输入输出双向信号线。当一个模块占有总线控制权时，该模块的“忙”信号端成为输出端，向系统的“忙”状态线送出有效信号(例如低电平)。其它模块的“忙”信号端全部作为输入端工作，检测“忙”线上状态。一个模块若要提出总线“请求”，其必要条件是选检测到“忙”信号输入端处于无效状态。与此相应，仲裁器接受总线请求输入的条件，也是“忙”线处于无效状态。进一步可以规定仲裁器输出“允许”信号的条件首先是“忙”线无效，表示总线没有被任一模块占用；其次才是有模块提出了总线请求。“允许”信号在链接的模块之间传输，直到提出总线“请求”的那个模块为止。这里用“允许”信号的边沿触发，它把共享总线的各模块要使用总线时，便发生信号禁止后面的部件使用总线。通过这种方式，就确定了请求总线各模块中优先级最高的模块。显然，在这种方式中，当优先级高的模块频繁请求时，优先级别低的模块可能很长时间都无法获得总线。一旦有模块占用总线后，“允许”信号就不再存在。并联优先级别判别法图2 并联优先级别判别法图2中有N个模块，都可作为总线主设备，每个模块都有总线“请求”线和总线“允许”线，模块之间是独立的，没有任何控制关系。这些信号接到总线优先控制器(仲裁器)，任一模块使用总线，都要通过“请求”线向仲裁器发出“请求”信号。仲裁器一般由一个优先级编码器和一个译码器组成。该电路接到某个模块或多个模块发来的请求信号后，首先优先级编码器进行编码，然后由译码器产生相应的输出信号，发往请求总线模块中优先级最高的模块，并把“允许”信号送给该模块。被选中的模块撤销总线“请求”信号，输出总线“忙”信号，通知其余模块，总线已经占用。在一个模块占用总线的传输结束以后，就把总线“忙”信号撤销，仲裁器也撤销“允许”信号。根据各请求输入的情况，仲裁器重新分配总线控制权。循环优先级判别法循环优先级判别方法类似于并联优先级判别方法，只是其中的优先级是动态分配的，原来的优先级编码器由一个更为复杂的电路代替，该电路把占用总线的优先级在发出总线请求的那些模块之间循环移动，从而使每个总线模块使用总线的机会相同。

存储容量：
单位简介播报编辑网络上的所有信息都是以“位”（bit）为单位传递的，一个位就代表一个0或1。每8个位（bit）组成一个字节（byte）。字节是什么概念呢？一个英文字母就占用一个字节，也就是8位，一个汉字占用两个字节。一般位简写为小写字母“b”，字节简写为大写字母“B”。存贮容量的设计播报编辑根据要求，福建理工大学监控系统采用集中式存储解决方案。具体设计为：在监控中心部署H3C EX1000S IPSAN存储服务器，前端所有摄像头的图像通过监控专网传输到监控中心，集中存储到IPSAN服务器上。 监控平台建成后，还需针对存储需求进行不同码流设计： CIF：图像分辨率为352×288   D1：图像分辨率为720×576 采用CIF方式：每路每秒是采用512K进行存储，我们参考512k存储系统按照160个摄像头存储30天的需求，共需要存储容量； (计算公式：存储容量（GB)=（码流/1024/1024/8）×CBR影响系数×60秒×60分钟×24小时×天数）  以512K单路视频图像码流，计算图像存储容量。  每小时容量=3600秒×（512/1024/1024/8）×1.10=0.242G/小时 每路图像一天24小时 一天容量=24 Hour×0.242GB/Hour=5.801GB/天 一月容量=30 天×5.801GB/天=174.03GB 160个摄像头保存30天容量=160×174.03GB =27844.8GB=27.8TB 采用Full D1方式：每路每秒是采用2M进行存储，我们参考2M存储系统按照14个摄像头存储30天的需求，共需要存储容量； (计算公式：存储容量（GB)=（码流/1024/1024/8）×CBR影响系数×60秒×60分钟×24小时×天数）  以1M单路视频图像码流，视频图像分辨率为D1 720*576 PAL 25帧,计算图像存储容量。  每小时容量=3600秒×（2048/1024/1024/8）×1.10=0.967G/小时 每路图像一天24小时 一天容量=24 Hour×0.967GB/Hour=23.203GB/天 一月容量=30 天*23.203GB/天=696.09GB 14个摄像头保存30天容量=14×696.09GB =9745.26GB=9.75TB  [1]存贮容量的计算播报编辑每一千个字节称为1KB，注意，这里的“千”不是我们通常意义上的1000，而是指1024。即：1KB=1024B。但如果不要求严格计算的话，也可以忽略地认为1K就是1000。 4）每1024个KB就是1MB（同样这里的K是指1024），即：1MB=1024KB=1024×1024B=1,048,576B这是准确的计算。如果不精确要求的话，也可认为1MB=1,000KB=1,000,000B另外需要注意的是，存储产品生产商会直接以1GB=1000MB，1MB=1000KB ，1KB=1000B的计算方式统计产品的容量，这就是为何买回的存储设备容量达不到标称容量的主要原因（如320G的硬盘只有300G左右）每1024MB就是1GB，即1GB=1024MB，至于等于多少字节，自己算吧。我们搞清楚了，常听人说什么一张软盘是1.44MB、一张CD光盘是650MB、一块硬盘是120GB是什么意思了。打个比方，一篇10万汉字的小说，如果我们把存到磁盘上，需要占用多少空间呢？100,000汉字=200,000B=200,000B÷1024≈195.3KB≈195.3KB÷1024≈0.19MB硬盘计算：750GB SATA实际容量为667（698.5）GB（少于的部分用于操作系统）； CBR影响系数：是指CBR(恒定码流）正误差给存储容量带来的影响系数。 存储设备采用RAID5+1的方式布置，每台存储需要损耗2块硬盘，如果IPSAN的硬盘为500GB的侧每台存储有效容量为6.316TB；如果IPSAN的硬盘为750GB的侧每台存储有效容量为9.119TB 存储模式与硬盘数量关系： 模式1:部署JBOD盘，采用750G硬盘（有效容量667GB)，单机16个有效盘位总容量为10.672TB，不考虑存储数据可靠性为最经济模式。 模式2：部署RAID5但不配热备盘，采用750G硬盘（有效容量667GB），单机15个有效盘位总容量为9.771TB，不考虑RAID5重建对存储性能影响，这是最经济的模式。 模式3：部署RAID5且配热备盘，采用750G硬盘（有效容量667GB)，单机14个有效盘位总容量为9.119TB，不考虑RAID5重建对存储性能影响（允许在坏掉一个硬盘后短时间内再坏掉一个硬盘）。 根据各布点区域监控点的数量可具体计算出所需的存储容量。（方案存储数据）方案中我们IP SAN存储，可以根据需要随时增加存储设备，并进行统一管理。 [1]单位换算介绍播报编辑随着存贮信息量的增大，有更大的单位表示存贮容量单位，比吉字节（GB, gigabyte）更高的还有：太字节（TB ，terabyte）、PB(Petabyte)、EB(Exabyte)、ZB(Zettabyte)和YB(yottabyte)等，其中，1PB=1024TB，1EB=1024PB，1ZB=1024EB，1YB=1024ZB。那么，这些单位的容量究竟有多大呢？请看一下表示： Ki1obyte(KB)=1024B相当于一则短篇故事的内容。Megabyte(MB)=1024KB相当于一则短篇小说的文字内容。Gigabyte(GB)=1024MB相当于贝多芬第五乐章交响曲的乐谱内容。Terabyte(TB)=1024GB相当于一家大型医院中所有的X光图片资讯量。Petabyte(PB)=1024TB相当于50%的全美学术研究图书馆藏书资讯内容。Exabyte (EB)=1024PB；5EB相当于至今全世界人类所讲过的话语。Zettabyte(ZB)=1024EB如同全世界海滩上的沙子数量总和。Yottabyte(YB)=1024ZB相当于7000位人类体内的微细胞总和。常用单位播报编辑存储容量是指该便携存储产品最大所能存储的数据量，是便携存储产品最为关键的参数。一般U盘的容量有1GB、2GB、4GB、8GB、16GB、32GB、64GB，还有部分更高容量的产品，但价格已超出了用户可以接受的地步。其中1GB～2GB的便携存储，已基本被市场淘汰；而4GB～16GB的产品是市场中的主流，价格在普通用户可以接受的范围内，也是厂家推出产品类型最多的容量类型；32GB以上的产品，因为价格昂贵，用户群体较少，产品种类也较少。磁盘存储容量播报编辑如上面所说，一块磁盘通常采用三级编址，因此，磁盘存储器的存储容量可以用如下公式来计算：存储容量C=柱面(磁道)数T x 磁盘面(磁头)数H x 扇区数S应当指出，这里所说的存储容量是指磁盘存储器能够保存的有效数据量，在磁盘上记录的许多其他信息不计算在存储容量之内。有些人可能已经注意到，新购买的硬盘，格式化之后显示的存储容量与磁盘上实际标称的存储容量并不符合。其主要原因是：磁盘上的标称容量是用十进制给出的，而计算机内部实际上是用二进制来表示存储容量的。例如，1KB=1024B，1MB=1 048 576B等，如果用MB来表示磁盘存储器的容量，则磁盘的标称容量与实际显示的容量之间有近5%的误差，如果用GB来表示，则有7.4%的误差，如果用TB表示，则误差高达10%。数据库避免存储容量浪费播报编辑数据库存储容量大量浪费的表现之一是数据冗余，指的是一个字段在多个表里重复出现。举个例子，如果每条客户购买商品的信息里都连带记录了客户自身的信息，这样的数据冗余可能造成不一致，因为客户自身的信息可能不一样。数据冗余会导致数据异常和损坏，一般来说设计上应该被避免。数据库规范化防止了冗余而且不浪费存储容量。适当的使用外键可以使得数据冗余和异常降到最低。但是，如果考虑效率和便利，有时候也会设计冗余数据，而不考虑数据被破坏的风险。 [1-2]

多核处理器：
技术发展播报编辑256线程的CPU英特尔工程师们开发了多核芯片，使之满足“横向扩展”（而非“纵向扩充”）方法，从而提高性能。该架构实现了“分治法”战略。通过划分任务，线程应用能够充分利用多个执行内核，并可在特定的时间内执行更多任务。多核处理器是单枚芯片（也称为“硅核”），能够直接插入单一的处理器插槽中，但操作系统会利用所有相关的资源，将每个执行内核作为分立的逻辑处理器。通过在两个执行内核之间划分任务，多核处理器可在特定的时钟周期内执行更多任务。多核架构能够使软件更出色地运行，并创建一个促进未来的软件编写更趋完善的架构。尽管认真的软件厂商还在探索全新的软件并发处理模式，但是，随着向多核处理器的移植，现有软件无需被修改就可支持多核平台。操作系统专为充分利用多个处理器而设计，且无需修改就可运行。为了充分利用多核技术，应用开发人员需要在程序设计中融入更多思路，但设计流程与对称多处理 (SMP)系统的设计流程相同，并且现有的单线程应用也将继续运行。得益于线程技术的应用在多核处理器上运行时将显示出卓越的性能可扩充性。此类软件包括多媒体应用（内容创建、编辑，以及本地和数据流回放）、工程和其他技术计算应用以及诸如应用服务器和数据库等中间层与后层服务器应用。多核技术能够使服务器并行处理任务，而在以前，这可能需要使用多个处理器，多核系统更易于扩充，并且能够在更纤巧的外形中融入更强大的处理性能，这种外形所用的功耗更低、计算功耗产生的热量更少。多核技术是处理器发展的必然。推动微处理器性能不断提高的因素主要有两个：半导体工艺技术的飞速进步和体系结构的不断发展。半导体工艺技术的每一次进步都为微处理器体系结构的研究提出了新的问题，开辟了新的领域；体系结构的进展又在半导体工艺技术发展的基础上进一步提高了微处理器的性能。这两个因素是相互影响，相互促进的。一般说来，工艺和电路技术的发展使得处理器性能提高约20倍，体系结构的发展使得处理器性能提高约4倍，编译技术的发展使得处理器性能提高约1.4倍。但是今天，这种规律性的东西却很难维持。多核的出现是技术发展和应用需求的必然产物。发展历程播报编辑1971年，英特尔推出的全球第一颗通用型微处理器4004，由2300个晶体管构成。当时，公司的联合创始人之一戈登摩尔(Gordon Moore)，就提出后来被业界奉为信条的“摩尔定律”——每过18个月，芯片上可以集成的晶体管数目将增加一倍。在一块芯片上集成的晶体管数目越多，意味着运算速度即主频就更快。今天英特尔的奔腾(Pentium)四至尊版840处理器，晶体管数量已经增加至2.5亿个，相比当年的4004增加了10万倍。其主频也从最初的740kHz(每秒钟可进行74万次运算)，增长到3.9GHz(每秒钟运算39亿次)以上。当然，CPU主频的提高，或许在一定程度上也要归功于1975年进入这个领域的AMD公司的挑战。正是这样的“双雄会”，使得众多计算机用户有机会享受不断上演的“速度与激情”。一些仍不满足的发烧友甚至选择了自己超频，因为在玩很多游戏时，更快的速度可以带来额外的饕餮享受。但到了2005年，当主频接近4GHz时，英特尔和AMD发现，速度也会遇到自己的极限：那就是单纯的主频提升，已经无法明显提升系统整体性能。以英特尔发布的采用NetBurst架构的奔腾四CPU为例，它包括Willamette、Northwood和Prescott等三种采用不同核心的产品。利用冗长的运算流水线，即增加每个时钟周期同时执行的运算个数，就达到较高的主频。这三种处理器的最高频率，分别达到了2.0G、3.4G和3.8G。按照当时的预测，奔腾四在该架构下，最终可以把主频提高到10GHz。但由于流水线过长，使得单位频率效能低下，加上由于缓存的增加和漏电流控制不利造成功耗大幅度增加，3.6GHz奔腾四芯片在性能上反而还不如早些时推出的3.4GHz产品。所以，Prescott产品系列只达到3.8G，就戛然而止。英特尔上海公司一位工程师在接受记者采访时表示，Netburst微架构的好处在于方便提升频率，可以让产品的主频非常高。但性能提升并不明显，频率提高50%，性能提升可能微不足道。因为Netburst微架构的效率较低，CPU计算资源未被充分利用，就像开车时“边踩刹车边踩油门”。此外，随着功率增大，散热问题也越来越成为一个无法逾越的障碍。据测算，主频每增加1G，功耗将上升25瓦，而在芯片功耗超过150瓦后，现有的风冷散热系统将无法满足散热的需要。3.4GHz的奔腾四至尊版，晶体管达1.78亿个，最高功耗已达135瓦。实际上，在奔腾四推出后不久，就在批评家那里获得了“电炉”的美称。更有好事者用它来玩煎蛋的游戏。很显然，当晶体管数量增加导致功耗增长超过性能增长速度后，处理器的可靠性就会受到致命性的影响。就连戈登摩尔本人似乎也依稀看到了“主频为王”这条路的尽头——2005年4月，他曾公开表示，引领半导体市场接近40年的“摩尔定律”，在未来10年至20年内可能失效。多核心CPU解决方案(多核)的出现，似乎给人带来了新的希望。早在上世纪90年代末，就有众多业界人士呼吁用CMP(单芯片多处理器)技术来替代复杂性较高的单线程CPU。IBM、惠普、Sun等高端服务器厂商，更是相继推出了多核服务器CPU。不过，由于服务器价格高、应用面窄，并未引起大众广泛的注意。直到AMD抢先手推出64位处理器后，英特尔才想起利用“多核”这一武器进行“帝国反击战”。2005年4月，英特尔仓促推出简单封装双核的奔腾D和奔腾四至尊版840。AMD在之后也发布了双核皓龙(Opteron)和速龙(Athlon) 64 X2和处理器。但真正的“双核元年”，则被认为是2006年。这一年的7月23日，英特尔基于酷睿(Core)架构的处理器正式发布。2006年11月，又推出面向服务器、工作站和高端个人电脑的至强(Xeon)5300和酷睿双核和四核至尊版系列处理器。与上一代台式机处理器相比，酷睿2 双核处理器在性能方面提高40%，功耗反而降低40%。作为回应，7月24日，AMD也宣布对旗下的双核Athlon64 X2处理器进行大降价。由于功耗已成为用户在性能之外所考虑的首要因素，两大处理器巨头都在宣传多核处理器时，强调其“节能”效果。英特尔发布了功耗仅为50瓦的低电压版四核至强处理器。而AMD的“Barcelona”四核处理器的功耗没有超过95瓦。在英特尔高级副总裁帕特基辛格(Pat Gelsinger)看来，从单核到双核，再到多核的发展，证明了摩尔定律还是非常正确的，因为“从单核到双核，再到多核的发展，可能是摩尔定律问世以来，在芯片发展历史上速度最快的性能提升过程”。技术优势播报编辑从应用需求上去看，越来越多的用户在使用过程中都会涉及到多任务应用环境，日常应用中用到的非常典型的有两种应用模式。一种应用模式是一个程序采用了线程级并行编程，那么这个程序在运行时可以把并行的线程同时交付给两个核心分别处理，因而程序运行速度得到极大提高。这类程序有的是为多路工作站或服务器设计的专业程序，例如专业图像处理程序、非线视频编缉程序、动画制作程序或科学计算程序等。对于这类程序，两个物理核心和两颗处理器基本上是等价的，所以，这些程序往往可以不作任何改动就直接运行在双核电脑上。还有一些更常见的日常应用程序，例如Office、IE等，同样也是采用线程级并行编程，可以在运行时同时调用多个线程协同工作，所以在双核处理器上的运行速度也会得到较大提升。例如，打开IE浏览器上网。看似简单的一个操作，实际上浏览器进程会调用代码解析、Flash播放、多媒体播放、Java、脚本解析等一系列线程，这些线程可以并行地被双核处理器处理，因而运行速度大大加快（实际上IE浏览器的运行还涉及到许多进程级的交互通信，这里不再详述）。由此可见，对于已经采用并行编程的软件，不管是专业软件，还是日常应用软件，在多核处理器上的运行速度都会大大提高。日常应用中的另一种模式是同时运行多个程序。许多程序没有采用并行编程，例如一些文件压缩软件、部分游戏软件等等。对于这些单线程的程序，单独运行在多核处理器上与单独运行在同样参数的单核处理器上没有明显的差别。但是，由于日常使用的最最基本的程序——操作系统——是支持并行处理的，所以，当在多核处理器上同时运行多个单线程程序的时候，操作系统会把多个程序的指令分别发送给多个核心，从而使得同时完成多个程序的速度大大加快。另外，虽然单一的单线程程序无法体现出多核处理器的优势，但是多核处理器依然为程序设计者提供了一个很好的平台，使得他们可以通过对原有的单线程序进行并行设计优化，以实现更好的程序运行效果。上面介绍了多核心处理器在软件上面的应用，但游戏其实也是软件的一种，作为一种特殊的软件，对PC发展作出了较大的贡献。一些多线程游戏已经能够发挥出多核处理器的优势，对于单线程游戏，相信游戏厂商也将会改变编程策略，例如，一些游戏厂商正在对原来的一些单线程游戏进行优化，采用并行编程使得游戏运行得更快。有的游戏可以使用一个线程实现人物动画，而使用另一个线程来载入地图信息。或者使用一个线程来实现图像渲染中的矩阵运算，而使用另一个来实现更高的人工智能运算。如今，大量的支持多核心的游戏涌现出来，从而使得多核处理器的优势能得到进一步的发挥。技术瓶颈播报编辑布赖恩特直言不讳地指出，要想让多核完全发挥效力，需要硬件业和软件业更多革命性的更新。其中，可编程性是多核处理器面临的最大问题。一旦核心多过八个，就需要执行程序能够并行处理。尽管在并行计算上，人类已经探索了超过40年，但编写、调试、优化并行处理程序的能力还非常弱。易观国际分析师李也认为，“出于技术的挑战，双核甚至多核处理器被强加给了产业，而产业却并没有事先做好准备”。或许正是出于对这种失衡的担心，中国国家智能计算机中心主任孙凝辉告诉《财经》记者，“十年以后，多核这条道路可能就到头了”。在他看来，一味增加并行的处理单元是行不通的。并行计算机的发展历史表明，并行粒度超过100以后，程序就很难写，能做到128个以上的应用程序很少。CPU到了100个核以上后，并行计算机系统遇到的问题，在CPU一样会存在。“如果解决不了主流应用并行化的问题，主流CPU发展到100个核就到头了。还不知道什么样的革命性的进展能解决这些问题。”孙补充说。实际上，市场研究公司In-Stat分析师吉姆克雷格(Jim McGregor)就承认，虽然英特尔已向外界展示了80核处理器原型，但尴尬的是，还没有能够利用这一处理器的操作系统。中科院软件所并行计算实验室副主任张云泉也持类似的观点。他对《财经》记者表示，这个问题实际一直就存在，但原来在超级计算机上才会遇到，所以，讨论也多局限在学术界。所有用户都要面对这样的问题。多核心技术在应用上的优势有两个方面：为用户带来更强大的计算性能；更重要的，则是可满足用户同时进行多任务处理和多任务计算环境的要求。两大巨头都给消费者描绘出了使用多核处理器在执行多项任务时的美妙前景：同时可以检查邮件、刻录CD、修改照片、剪辑视频，并且同时可以运行杀毒软件。或者利用同一台电脑，父亲在查看财务报表，女儿在打游戏，母亲在给远方的朋友打网络电话。但并不是所有家庭只有一台电脑，也不是所有用户都要用电脑一下子做那么多事，更何况大部分应用程序还并不能自动分割成多任务，分别交给多个核心去执行。所以，对于大多数用户来说，多核所带来的实际益处，很可能并不明显。而多核所带来的挑战，或者说麻烦，却是实实在在的。美国卡内基梅隆大学计算机系教授朗道布赖恩特(Randal E Bryant)在接受《财经》记者采访时就坦称，“这给软件业制造了巨大的问题”。技术原理播报编辑多核CPU就是基板上集成有多个单核CPU，早期PD双核需要北桥来控制分配任务，核心之间存在抢二级缓存的情况，后期酷睿自己集成了任务分配系统，再搭配操作系统就能真正同时开工，2个核心同时处理2“份”任务，速度快了，万一1个核心死机，起码另一个U还可以继续处理关机、关闭软件等任务。技术关键播报编辑与单核处理器相比，多核处理器在体系结构、软件、功耗和安全性设计等方面面临着巨大的挑战，但也蕴含着巨大的潜能。多核处理器CMP和SMT一样，致力于发掘计算的粗粒度并行性。CMP可以看做是随着大规模集成电路技术的发展，在芯片容量足够大时，就可以将大规模并行处理机结构中的SMP（对称多处理机）或DSM（分布共享处理机）节点集成到同一芯片内，各个处理器并行执行不同的线程或进程。在基于SMP结构的单芯片多处理机中，处理器之间通过片外Cache或者是片外的共享存储器来进行通信。而基于DSM结构的单芯片多处理器中，处理器间通过连接分布式存储器的片内高速交叉开关网络进行通信。由于SMP和DSM已经是非常成熟的技术了，CMP结构设计比较容易，只是后端设计和芯片制造工艺的要求较高而已。正因为这样，CMP成为了最先被应用于商用CPU的“未来”高性能处理器结构。虽然多核能利用集成度提高带来的诸多好处，让芯片的性能成倍地增加，但很明显的是原来系统级的一些问题便引入到了处理器内部。核结构研究同构还是异构CMP的构成分成同构和异构两类，同构是指内部核的结构是相同的，而异构是指内部的核结构是不同的。为此，面对不同的应用研究核结构的实现对未来微处理器的性能至关重要。核本身的结构，关系到整个芯片的面积、功耗和性能。怎样继承和发展传统处理器的成果，直接影响多核的性能和实现周期。同时，根据Amdahl定理，程序的加速比决定于串行部分的性能，所以，从理论上来看似乎异构微处理器的结构具有更好的性能。核所用的指令系统对系统的实现也是很重要的，多核之间采用相同的指令系统还是不同的指令系统，能否运行操作系统等，也将是研究的内容之一。程序执行模型处理器设计的首要问题是选择程序执行模型。程序执行模型的适用性决定多核处理器能否以最低的代价提供最高的性能。程序执行模型是编译器设计人员与系统实现人员之间的接口。编译器设计人员决定如何将一种高级语言程序按一种程序执行模型转换成一种目标机器语言程序; 系统实现人员则决定该程序执行模型在具体目标机器上的有效实现。当目标机器是多核体系结构时，产生的问题是: 多核体系结构如何支持重要的程序执行模型？是否有其他的程序执行模型更适于多核的体系结构？这些程序执行模型能多大程度上满足应用的需要并为用户所接受？Cache设计多级Cache设计与一致性问题处理器和主存间的速度差距对CMP来说是个突出的矛盾，因此必须使用多级Cache来缓解。有共享一级Cache的CMP、共享二级Cache的CMP以及共享主存的CMP。通常，CMP采用共享二级Cache的CMP结构，即每个处理器核心拥有私有的一级Cache，且所有处理器核心共享二级Cache。Cache自身的体系结构设计也直接关系到系统整体性能。但是在CMP结构中，共享Cache或独有Cache孰优孰劣、需不需要在一块芯片上建立多级Cache，以及建立几级Cache等等，由于对整个芯片的尺寸、功耗、布局、性能以及运行效率等都有很大的影响，因而这些都是需要认真研究和探讨的问题。另一方面，多级Cache又引发一致性问题。采用何种Cache一致性模型和机制都将对CMP整体性能产生重要影响。在传统多处理器系统结构中广泛采用的Cache一致性模型有: 顺序一致性模型、弱一致性模型、释放一致性模型等。与之相关的Cache一致性机制主要有总线的侦听协议和基于目录的目录协议。CMP系统大多采用基于总线的侦听协议。核间通信技术CMP处理器的各CPU核心执行的程序之间有时需要进行数据共享与同步，因此其硬件结构必须支持核间通信。高效的通信机制是CMP处理器高性能的重要保障，比较主流的片上高效通信机制有两种，一种是基于总线共享的Cache结构，一种是基于片上的互连结构。总线共享Cache结构是指每个CPU内核拥有共享的二级或三级Cache，用于保存比较常用的数据，并通过连接核心的总线进行通信。这种系统的优点是结构简单，通信速度高，缺点是基于总线的结构可扩展性较差。基于片上互连的结构是指每个CPU核心具有独立的处理单元和Cache，各个CPU核心通过交叉开关或片上网络等方式连接在一起。各个CPU核心间通过消息通信。这种结构的优点是可扩展性好，数据带宽有保证; 缺点是硬件结构复杂，且软件改动较大。也许这两者的竞争结果不是互相取代而是互相合作，例如在全局范围采用片上网络而局部采用总线方式，来达到性能与复杂性的平衡。总线设计传统微处理器中，Cache不命中或访存事件都会对CPU的执行效率产生负面影响，而总线接口单元（BIU）的工作效率会决定此影响的程度。当多个CPU核心同时要求访问内存或多个CPU核心内私有Cache同时出现Cache不命中事件时，BIU对这多个访问请求的仲裁机制以及对外存储访问的转换机制的效率决定了CMP系统的整体性能。因此寻找高效的多端口总线接口单元（BIU）结构，将多核心对主存的单字访问转为更为高效的猝发（burst）访问; 同时寻找对CMP处理器整体效率最佳的一次Burst访问字的数量模型以及高效多端口BIU访问的仲裁机制将是CMP处理器研究的重要内容，Inter推出了最新的英特尔智能互连技术(QPI)技术总线，更大程度发掘了多核处理器的实力 。操作系统设计任务调度、中断处理、同步互斥对于多核CPU，优化操作系统任务调度算法是保证效率的关键。一般任务调度算法有全局队列调度和局部队列调度。前者是指操作系统维护一个全局的任务等待队列，当系统中有一个CPU核心空闲时，操作系统就从全局任务等待队列中选取就绪任务开始在此核心上执行。这种方法的优点是CPU核心利用率较高。后者是指操作系统为每个CPU内核维护一个局部的任务等待队列，当系统中有一个CPU内核空闲时，便从该核心的任务等待队列中选取恰当的任务执行，这种方法的优点是任务基本上无需在多个CPU核心间切换，有利于提高CPU核心局部Cache命中率。多数多核CPU操作系统采用的是基于全局队列的任务调度算法。多核的中断处理和单核有很大不同。多核的各处理器之间需要通过中断方式进行通信，所以多个处理器之间的本地中断控制器和负责仲裁各核之间中断分配的全局中断控制器也需要封装在芯片内部。另外,多核CPU是一个多任务系统。由于不同任务会竞争共享资源，因此需要系统提供同步与互斥机制。而传统的用于单核的解决机制并不能满足多核，需要利用硬件提供的“读－修改－写”的原子操作或其他同步互斥机制来保证。低功耗设计半导体工艺的迅速发展使微处理器的集成度越来越高，同时处理器表面温度也变得越来越高并呈指数级增长，每三年处理器的功耗密度就能翻一番。低功耗和热优化设计已经成为微处理器研究中的核心问题。CMP的多核心结构决定了其相关的功耗研究是一个至关重要的课题。低功耗设计是一个多层次问题，需要同时在操作系统级、算法级、结构级、电路级等多个层次上进行研究。每个层次的低功耗设计方法实现的效果不同——抽象层次越高，功耗和温度降低的效果越明显。当前Intel的CPU的功耗相对较低，得益于先进的英特尔构架和45纳米、32纳米制程工艺，同时Intel还专门为CPU开发了不少节能技术，比如C6深度节能技、英特尔智能功效管理 和主动管理技术 等等，Intel在移动CPU市场，更是凭借超低电压处理器（ULV）和凌动（Atom）系列处理器，遥遥领先于对手。存储器墙为了使芯片内核充分地工作，最起码的要求是芯片能提供与芯片性能相匹配的存储器带宽，虽然内部Cache的容量能解决一些问题，但随着性能的进一步提高，必须有其他一些手段来提高存储器接口的带宽，如增加单个管脚带宽的DDR、DDR2、QDR、XDR等。同样，系统也必须有能提供高带宽的存储器。所以，芯片对封装的要求也越来越高，虽然封装的管脚数每年以20%的数目提升，但还不能完全解决问题，而且还带来了成本提高的问题，为此，怎样提供一个高带宽，低延迟的接口带宽，是必须解决的一个重要问题。可靠性及安全性设计随着技术革新的发展，处理器的应用渗透到现代社会的各个层面，但是在安全性方面却存在着很大的隐患。一方面，处理器结构自身的可靠性低下，由于超微细化与时钟设计的高速化、低电源电压化，设计上的安全系数越来越难以保证，故障的发生率逐渐走高。另一方面，来自第三方的恶意攻击越来越多，手段越来越先进，已成为具有普遍性的社会问题。可靠性与安全性的提高在计算机体系结构研究领域备受注目。今后，CMP这类处理器芯片内有多个进程同时执行的结构将成为主流，再加上硬件复杂性、设计时的失误增加，使得处理器芯片内部也未必是安全的，因此，安全与可靠性设计任重而道远。技术意义播报编辑多核处理器代表了计算技术的一次创新。由于数字数据和互联网的全球化，商业和消费者开始要求多核处理器带来性能改进，这个重要创新就开始了；因为多核处理器比单核处理器具有性能和效率优势，多核处理器将会成为被广泛采用的计算模型。在驱动pc安全性和虚拟化技术的重大进程过程中，多核处理器扮演着中心作用，这些安全性和虚拟化技术的开发用于为商业计算市场提供更大的安全性、更好的资源利用率、创造更大价值。普通消费者用户也期望得到前所未有的性能，这将极大地扩展其家庭pc和数字媒体计算系统的使用。多核处理器具有不增加功耗而提高性能的好处，实现更大的性能/能耗比。在一个处理器中放入两个或多个功能强大的计算核产生了一个重大的可能性。由于多核处理器能提供比单核处理器更好的性能和效率，下一代的软件应用程序很有可能是基于多核处理器而开发的。不管这些应用是帮助专业的电影公司以更少的投入和更少的时间完成更真实的电影，还是以更彻底的方法使得pc更自然和直观，多核处理器技术将永远改变计算世界。多核处理器表达了amd了解顾客需求并且开发最能满足客户要求产品的意愿。微软多核计算的主管Dan Reed称，整个世界上很缺乏那些并行计算的研究人员，而一个间接的原因就是学院里对于并行计算的关注度不够，而这些学院正是下一代软件开发人员诞生的地方。越来越高的时钟频率导致应用程序的代码运行的越来越快，而对于当前多核处理器来讲这一规则虽然成立，但却有所不同。而这种不同可以做一个形象的比喻，那就是一部跑车和一辆学校的巴士。当跑车能够以很快的速度飞奔时，巴士虽然比较慢，但它可以载着更多的人前行。问题就是，简单地在计算机CPU上增加多个核并不能增加传统应用程序代码的运行速度，这一结果是根据一项来自于Forrester研究公司的报告得出的。换句话说，复杂的工作需要拆分来填充这辆巴士上的空座位。Forrester的报告还谈到：同时，当前四核处理器会激发更多的多处理器设计的思想，我们期待着2009年x86的服务器使用64个处理器核，并且2012年台式机也可以实现这一梦想。使得芯片的制造商以及主要的板级应用的软件厂商意识到多核编程的机遇和挑战。 [1]技术种类播报编辑单芯片多处理器(CMP)与同时多线程处理器(SimultaneousMultithreading，SMT)，这两种体系结构可以充分利用这些应用的指令级并行性和线程级并行性，从而显著提高了这些应用的性能。从体系结构的角度看，SMT比CMP对处理器资源利用率要高，在克服线延迟影响方面更具优势。CMP相对SMT的最大优势还在于其模块化设计的简洁性。复制简单设计非常容易，指令调度也更加简单。同时SMT中多个线程对共享资源的争用也会影响其性能，而CMP对共享资源的争用要少得多，因此当应用的线程级并行性较高时，CMP性能一般要优于SMT。此外在设计上，更短的芯片连线使CMP比长导线集中式设计的SMT更容易提高芯片的运行频率，从而在一定程度上起到性能优化的效果。总之，单芯片多处理器通过在一个芯片上集成多个微处理器核心来提高程序的并行性。每个微处理器核心实质上都是一个相对简单的单线程微处理器或者比较简单的多线程微处理器，这样多个微处理器核心就可以并行地执行程序代码，因而具有了较高的线程级并行性。由于CMP采用了相对简单的微处理器作为处理器核心，使得CMP具有高主频、设计和验证周期短、控制逻辑简单、扩展性好、易于实现、功耗低、通信延迟低等优点。此外，CMP还能充分利用不同应用的指令级并行和线程级并行，具有较高线程级并行性的应用如商业应用等可以很好地利用这种结构来提高性能。技术应用播报编辑并行计算技术是云计算的核心技术，也是最具挑战性的技术之一。多核处理器的出现增加了并行的层次性能使得并行程序的开发比以往更难。而当前业内并无有效的并行计算解决方案，无论是编程模型、开发语言还是开发工具，距离开发者的期望都有很大的差距。自动的并行化解决方案在过去的30年间已经被证明基本是死胡同，但传统的手工式的并行程序开发方式又难以为普通的程序员所掌握。Intel、微软、SUN、Cray等业内巨头正投入大量人力物力进行相关的研究，但真正成熟的产品在短期内很难出现。可扩展性是云计算时代并行计算的主要考量点之一，应用性能必须能随着用户的请求、系统规模的增大有效的扩展。当前大部分并行应用在超过一千个的处理器(核)上都难以获得有效的加速性能，未来的许多并行应用必须能有效扩展到成千上万个处理器上。这对开发者是巨大的挑战。 [2]产品应用播报编辑从Power、UltraSPARC T1、安腾到双核Opteron、至强Xeon，各个领域都显示出，多核处理器计算平台势必成为服务器的主流或者说是强势计算平台，但这只是上游硬件厂商的乐观预计。并不是所有的操作系统和应用软件都做好了迎接多核平台的准备，尤其是在数十年来均为单一线程开发应用的x86服务器领域。微软软件架构师HerbSutter曾指出:软件开发者对多核处理器时代的来临准备不足。他说，软件开发社区认识到处理器厂商被迫采用多核设计以应对处理器速度提升带来的发热问题，但却没有清楚地了解这样的设计为软件开发带来多少额外的工作。在过去一段长时间里，x86系统上软件的性能随着来自Intel和AMD处理器速度越来越快而不断提高，开发者只需对现有软件程序作轻微改动就能坐观其性能在随着硬件性能的上升而不断提升。不过，多核设计概念的出现迫使软件世界不得不直面并行性(将单个任务拆分成多个小块以便分别处理之后再重新组合的能力)问题。当然，为服务器设计软件的开发者已经解决了一些此类难题，因为多核处理器和多路系统在服务器市场已经存在多年(在传统的Unix领域)，一些运行在RISC架构多核多路系统上的应用程序已经被设计成多线程以利用系统的并行处理能力。但是，在x86领域，应用程序开发者多年来一直停留在单线程世界，生产所谓的“顺序软件”。情况是软件开发者必须找出新的开发软件的方法，面向对象编程的兴起增加了汇编语言的复杂性，并行编程也需要新的抽象层次。另一方面，处理器设计厂商在设计产品时也应该将软件开发者考虑在内，“处理器的首要着眼点应该是可编程性，而不是速度。”Sutter说。多核处理器要想发挥出威力，关键在于并行化软件支持，多核设计带动并行化计算的推进，而给软件带来的影响更是革命性的。Intel很早就通过超线程技术实现了逻辑上的双处理器系统，可以并行计算，但这不过是对处理器闲置资源的一种充分利用而已，并且这种充分利用只有在特定的条件下，尤其是针对流水线比较长且两种运算并不相互交叉的时候，才会有较高的效率，如编码解码、长期重复某种矩阵运算以及一些没有经过仔细编写的软件等。即使IBM的Power5架构，也需要跟最新的操作系统进行融合，加上运行在其上的软件，才有可能利用并发多线程。虚拟化技术在一定程度上能够处理一些因为多核带来的问题，可以让应用软件和操作系统在透明的环境下对处理器资源进行分配和管理。在对称多处理器方面，操作系统对资源的分配和管理并没有本质的改变，多以对称的方式进行平均分配。也就是说，在操作系统层面，当一个任务到来时，剥离成为两个并行的线程，因为线程之间需要交流以及操作系统监管，它导致的效率损失要比硬件层面大得多。并且，多数软件并没有充分考虑到双核乃至多核的运行情况，导致线程的平均分配时间以及线程之间的沟通时间都会大大增加，尤其是当线程需要反复访问内存的时候。多数操作系统还没有完全实现自由的资源分配，如IBM是通过AIX 5.3L来支持Power5上的虚拟化功能，才实现了资源的动态调配和划分的。从长远来看，需要使用虚拟化技术才可能实现操作系统对任务的具体划分，这很可能改变一些通用的编程模式。 [3]英特尔播报编辑2009年9月6日下午，英特尔在北京发布了业界首款专为多路(MP)服务器设计的四核英特尔&reg;至强&reg;7300系列服务器处理器。与英特尔前代双核产品相比，此次发布的六款全新四核至强&reg;7300系列处理器的性能和性能功耗比分别提升了两倍和三倍之多。而随着这些产品的发布，英特尔在不到15个月的时间内完成了向创新和高能效的英特尔&reg;酷睿™微体系架构的快速切换。据了解，此次推出的至强&reg;7300系列产品包括主频高达2.93GHz处理器(功耗为130瓦)，几款80瓦处理器，和一款针对四插槽刀片式服务器和高密度机架式服务器优化的50瓦版处理器(主频为1.86GHz)。具备数据流量优化(Data Traffic Optimizations)特性的英特尔&reg;7300芯片组采用平衡的平台设计，具有多项全新技术，以改善数据在处理器、内存和I/O之间的传输能力。此外，英特尔还发布了一款50瓦(每内核12.5瓦)的处理器，以推动四插槽刀片式服务器和高密度机架式服务器等高能效超密度部署产品的发展。在芯片设计方面，除内核数量增加一倍之外，相对于前代英特尔多路平台，至强&reg;7300系列处理器和英特尔&reg;7300芯片组所支持的内存容量是原来的4倍，并能支持非常高的整合比例，以减少空间、降低功耗和运营成本。预计今后将有超过50家的系统制造商发售基于英特尔&reg;至强&reg;7300系列处理器的服务器，其中包括戴尔、Egenera、富士通、富士通-西门子、日立、惠普、IBM、NEC、Sun、超微和优利等。针对需要基于全新英特尔&reg;至强&reg;7300系列处理器的完整平台的渠道客户，英特尔特别为其提供了英特尔&reg;S7000FC4UR服务器平台。该款平台可提供强劲的可扩展性能、业经验证的企业级可靠性，用于基础设施的虚拟化和整合。许多软件厂商也为基于英特尔&reg;至强&reg;7300系列处理器的平台提供了创新性的支持虚拟化和性能扩展的解决方案，如BEA、微软、甲骨文、SAP和VMware等。此外，Solaris 操作系统和其上运行的数千款应用能够充分利用英特尔&reg;至强&reg;7300 系列处理器平台的领先性能优势，为英特尔&reg;至强&reg;服务器用户提供企业级、支持关键任务的UNIX操作系统环境。这些全新四核处理器的定价根据主频、特性和客户定购数量的不同，其千枚单价从856美元至2,301美元不等。

指令集架构：
CISC播报编辑x86架构微处理器如Intel的Pentium/Celeron/Xeon与AMD的Athlon/Duron/Sempron；以及其64位扩展系统的x86-64的架构的EM64T的Pentium/Xeon与AMD64的Athlon 64/Opteron都属于CISC系列。主要针对的操作系统是微软的Windows。另外Linux，一些UNIX等都可以运行在x86（CISC）架构的微处理器。RISC播报编辑RISC这种指令集运算包括HP的PA-RISC，IBM的PowerPC，Compaq（被并入HP）的Alpha，MIPS公司的MIPS，SUN公司的SPARC等。只有UNIX，Linux，MacOS等操作系统运行在RISC处理器上。EPIC播报编辑EPIC是先进的全新指令集运算，只有Intel的IA-64架构的纯64位微处理器的Itanium/Itanium 2。EPIC指令集运算的IA-64架构主要针对的操作系统是微软64位安腾版的Windows XP以及64位安腾版的Windows Server 2003。另外一些64位的Linux，一些64位的UNIX也可以运行IA-64（EPIC）架构。VLIW播报编辑通过将多条指令放入一个指令字，有效的提高了CPU各个计算功能部件的利用效率，提高了程序的性能。

