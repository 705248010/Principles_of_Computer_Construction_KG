计算机系统不同于一般的电子设备，它是一个由硬件、软件组成的复杂的自动化设备。
本章先说明计算机的分类，然后采用自上而下的方法，简要地介绍硬件、软件的概念和组
成，目的在于使读者先有一个粗略的总体概念，以便于展开后续各章内容。 

电子计算机从总体上来说分为两大类。一类是电子模拟计算机。“模拟”就是相似的意
思，例如计算尺是用长度来标示数值；时钟是用指针在表盘上转动来表示时间；电表是用
角度来反映电量大小，这些都是模拟计算装置。模拟计算机的特点是数值由连续量来表示，
运算过程也是连续的。 
另一类是电子数字计算机，它是在算盘的基础上发展起来的，是用数字来表示数量的
大小。数字计算机的主要特点是按位运算，并且不连续地跳动计算。表 1.1 列出了电子数字
计算机与电子模拟计算机的主要区别。 

电子模拟计算机由于精度和解题能力都有限，所以应用范围较小。电子数字计算机则
与电子模拟计算机不同，它是以近似于人类的“思维过程”来进行工作的，所以有人把它
叫做电脑。它的发明和发展是 20 世纪人类最伟大的科学技术成就之一，也是现代科学技术
发展水平的主要标志。习惯上所称的电子计算机，一般是指现在广泛应用的电子数字计算机。 
电子数字计算机进一步又可分为专用计算机和通用计算机。专用和通用是根据计算机
的效率、速度、价格、运行的经济性和适应性来划分的。专用计算机是最有效、最经济和


最快速的计算机，但是它的适应性很差。通用计算机适应性很强，但是牺牲了效率、速度
和经济性。 
通用计算机可分为超级计算机、大型机、服务器、PC 机、单片机和多核机六类，它们
的区别在于体积、简易性、功率损耗、性能指标、数据存储容量、指令系统规模和机器价
格，见图 1.1。一般来说，超级计算机主要用于科学计算，其运算速度在每秒万亿次以上，
数据存储容量很大，结构复杂，价格昂贵。而单片机是只用一片集成电路做成的计算机，
体积小，结构简单，性能指标较低，价格便宜。介于超级计算机和多核机之间的是大型机、
服务器、PC 机和单片机，它们的结构规模和性能指标依次递减。但随着巨大规模集成电路
的迅速发展，单片机、多核机等彼此之间的概念也在发生变化，因为今天的单片机可能就
是明天的多核机。专用计算机是针对某一任务设计的计算机，一般来说，其结构要比通用
计算机简单。目前已经出现了多种型号的单片专用机及嵌入式单片机，用于测试或控制，
成为计算机应用领域中最热门的产品。多核机是多于一个处理器的计算机芯片，具有更强
的能力。 
 

世界上第一台电子数字计算机是 1946 年在美国宾夕法尼亚大学制成的。这台机器用了
18000 多个电子管，占地 170m2，重量达 30 吨，而运算速度只有 5000 次/秒。用今天的眼
光来看，这台计算机耗费既大又不完善，但却是科学史上一次划时代的创新，它奠定了电
子计算机的基础。自从这台计算机问世 70 多年来，从使用器件的角度来说，计算机的发展
大致经历了五代的变化。 
第一代为 1946～1957 年，电子管计算机。计算机运算速度为每秒几千次至几万次，体
积庞大，成本很高，可靠性较低。在此期间，形成了计算机的基本体系，确定了程序设计
的基本方法，数据处理机开始得到应用。 
第二代为 1958～1964 年，晶体管计算机。运算速度提高到每秒几万次至几十万次，可
靠性提高，体积缩小，成本降低。在此期间，工业控制机开始得到应用。 


第三代为 1965～1971 年，中小规模集成电路计算机。可靠性进一步提高，体积进一步
缩小，成本进一步下降，运算速度提高到每秒几十万次至几百万次。在此期间形成机种多
样化，生产系列化，使用系统化，小型计算机开始出现。 
第四代为 1972～1990 年，大规模和超大规模集成电路计算机。可靠性更进一步提高，
体积更进一步缩小，成本更进一步降低，速度提高到每秒 1000 万次至 1 亿次。由几片大规
模集成电路组成的微型计算机开始出现。 
第五代为 1991 年开始的巨大规模集成电路计算机。运算速度提高到每秒 10 亿次。由
一片巨大规模集成电路实现的单片计算机开始出现。 
总之，从 1946 年计算机诞生以来，大约每隔五年运算速度提高 10 倍，可靠性提高 10
倍，成本降低为 1/10，体积缩小为 1/10。而 20 世纪 70 年代以来，计算机的生产数量每年
以 25%的速度递增。 
计算机从第三代起，与集成电路技术的发展密切相关。LSI 的采用，一块集成电路芯片
上可以放置 1000 个元件，VLSI 达到每个芯片 1 万个元件，现在的 ULSI 芯片超过了 100 万
个元件。1965 年摩尔观察到芯片上的晶体管数量每年翻一番，1970 年这种态势减慢成每 18
个月翻一番，这就是人们所称的摩尔定律。 
在国际超级计算机 500 强排序中，中国 2004 年“曙光 4000A”位居第 10；2009 年“星
云号”位居第 2；2010 年“天河 1 号”位居第 1，运算速度达 2500 万亿次/秒。 

20 世纪 50～60 年代，所有计算机存储器都是由微小的铁磁体环(磁芯)做成，每个磁芯
直径约 1mm。这些小磁芯处在计算机内用三条细导线穿过网格板上。每个磁芯的一种磁化
方向代表一个 1，另一个磁化方向则代表一个 0。磁芯存储器速度相当快，读存储器中的一
位只需 1 微秒。但是磁芯存储器价格昂贵，体积大，而且读出是破坏性的，因此必须有读
出后立即重写数据的电路。更重要的在于工艺复杂，甚至手工制作。 
1970 年，仙童半导体公司生产出了第一个较大容量半导体存储器。一个相当于单个磁
芯大小的芯片，包含了 256 位的存储器。这种芯片是非破坏性的，而且读写速度比磁芯快
得多，读出一位只要 70 纳秒，但是其价格比磁芯要贵。 
1974 年每位半导体存储器的价格低于磁芯。这以后，存储器的价格持续快速下跌，但
存储密度却不断增加。这导致了新的机器比它之前的机器更小、更快、存储容量更大，价
格更便宜。存储器技术的发展，与处理器技术的发展一起，在不到 10 年的时间里改变了计
算机的生命力。虽然庞大昂贵的计算机仍然存在，但计算机已经走向了个人电脑时代。 
从 1970 年起，半导体存储器经历了 11 代：单个芯片 1KB、4KB、16KB、64KB、256KB、
1MB、4MB、16MB、64MB、256MB 和现在的 1GB。其中 1K=210，1M=220，1G=230。每
一代比前一代存储密度提高 4 倍，而每位价格和存取时间都在下降。 

与存储器芯片一样，处理器芯片的单元密度也在不断增加。随着时间的推移，每块芯
片上的单元个数越来越多，因此构建一个计算机处理器所需的芯片越来越少。表 1.2 列出了
Intel 公司微处理器的演化。 


1971 年 Intel 公司开发出 Intel 4004。这是第一个将 CPU 的所有元件都放入同一块芯片
内的产品，于是，微处理器诞生了。 
第 1 章  计算机系统概论       5 
Intel 4004 能完成两个 4 位数相加，通过重复相加能完成乘法。按今天的标准，4004 虽
然过于简单，但是它却成为微处理器的能力和功能不断发展的奠基者。 
微处理器演变中的另一个主要进步是 1972 年出现的 Intel 8008，这是第一个 8 位微处理
器，它比 4004 复杂一倍。 
1974 年出现了 Intel 8080。这是第一个通用微处理器，而 4004 和 8008 是为特殊用途而
设计的。8080 是为通用微机而设计的中央处理器。它与 8008 一样，都是 8 位微处理器，但
8080 更快，有更丰富的指令系统和更强的寻址能力。 
大约在同时，16 位微机被开发出来。但是直到 20 世纪 70 年代末才出现强大的通用 16
位微处理器，Intel 8086 便是其中之一。这一发展趋势中的另一阶段是在 1981 年，贝尔实验
室和 HP 公司开发出了 32 位单片微处理器。Intel 于 1985 年推出了 32 位微处理器 Intel 80386。 

吞吐量  表征一台计算机在某一时间间隔内能够处理的信息量。 
响应时间  表征从输入有效到系统产生响应之间的时间度量，用时间单位来度量。 
利用率  在给定的时间间隔内系统被实际使用的时间所占的比率，用百分比表示。 
处理机字长  指处理机运算器中一次能够完成二进制数运算的位数，如 32 位、64 位。 
总线宽度  一般指 CPU 中运算器与存储器之间进行互连的内部总线二进制位数。 
存储器容量  存储器中所有存储单元的总数目，通常用 KB、MB、GB、TB 来表示。 
存储器带宽  单位时间内从存储器读出的二进制数信息量，一般用字节数/秒表示。 
主频/时钟周期  CPU 的工作节拍受主时钟控制，主时钟不断产生固定频率的时钟，主
时钟的频率(f )叫 CPU 的主频。度量单位是 MHz(兆赫兹)、GHz(吉赫兹)。 
主频的倒数称为 CPU 时钟周期(T)，T=1/f，度量单位是 μs、ns。 
CPU 执行时间  表示 CPU 执行一般程序所占用的 CPU 时间，可用下式计算： 
CPU 执行时间=CPU 时钟周期数×CPU 时钟周期 
CPl  表示每条指令周期数，即执行一条指令所需的平均时钟周期数。用下式计算： 
CPI=执行某段程序所需的 CPU 时钟周期数÷程序包含的指令条数 
MIPS  (Million Instructions Per Second)的缩写，表示平均每秒执行多少百万条定点指
令数，用下式计算： 
MIPS=指令数÷(程序执行时间×106) 
FLOPS  (Floating-point Operations Per Second)的缩写，表示每秒执行浮点操作的次
数，用来衡量机器浮点操作的性能。用下式计算： 
FLOPS=程序中的浮点操作次数÷程序执行时间(s) 

要了解数字计算机的主要组成和工作原理，可从打算盘说起。假设给一个算盘、一张
带横格的纸和一支笔，要求计算 y=ax+b–c 这样一个题目。为了和下面讲到的内容做比较，
不妨按以下方法把使用算盘进行解题的过程步骤事先用笔详细地记录在带横格的纸上。 
首先，将横格纸编上序号，每一行占一个序号，如 1, 2, 3, …, n，如表 1.3 所示。其次，
把计算式中给定的四个数 a、b、c 和 x 分别写到横格纸的第 9、10、11、12 行上，每一行只
写一个数。接着详细列出给定题目的解题步骤，而解题步骤也需要记在横格纸上，每一步
也只写一行。第一步写到横格纸的第 1 行，第二步写到第 2 行，……以此类推。 

在完成 y=ax+b–c 的计算过程中，用到了什么东西呢? 
首先，用到了带横格且编有序号的纸，把原始的数据
及解题步骤记录在纸上，即纸“存储”了算题的原始信息。
其次，用到了算盘，它用来对数据进行加、减、乘、除等
算术运算。再次，用到了笔，利用笔把原始数据和解题步
骤记录到纸上，还可把计算结果写出来告诉人。最后，用
到了我们人本身，这主要是人的脑和手。在人的控制下，
按照解题步骤一步一步进行操作，直到完成全部运算。 
电子数字计算机进行解题的过程完全和人用算盘解
题的情况相似，也必须有运算工具，解题步骤和原始数据
的输入与存储，运算结果的输出及整个计算过程的调度控
制。和打算盘不同的是，以上这些部分都是由电子线路和
其他设备自动进行的。在电子计算机里，相当于算盘功能
的部件，我们称之为运算器；相当于纸那样具有“记忆”
功能的部件，我们称之为存储器；相当于笔那样把原始解
题信息送到计算机或把运算结果显示出来的设备，我们称
之为输入设备或输出设备；而相当于人的大脑，能够自动
控制整个计算过程的，称之为控制器。图 1.2 所示为数字计算机的主要组成结构，其中双线
及箭头表示数据代码传送通路。 
 


运算器就好像是一个由电子线路构成的算盘，图 1.3 是它的示意图。它的主要功能是进
行加、减、乘、除等算术运算。除此以外，还可以进行逻辑运算，因此通常称为 ALU(算术
逻辑运算部件)。 
人们习惯于十进制数的运算，但是考虑到电子器件的特
性，计算机中通常采用二进制数。二进制数是以 2 为基数来
计数，也就是“逢二进一”。在二进制数中，只有 0 和 1 两个
数字。1 和 0 可以用电压的高低、脉冲的有无来表示。这种
电压的高低，脉冲的有无，在电子器件中很容易实现，而且
设备也最省。 
二进制数的运算规律非常简单。例如，加法：0+0=0，
0+1=1，1+0=1，1+1=10，最后一个加式中等号右边的“1”表
示向上一位的进位。又如，乘法：0×0=0，0×1=0，1×0=0，
1×1=1。正是由于二进制数运算规律简单，在电子器件中比
较容易实现，因此，在电子数字计算机中广泛采用二进制数。 
二进制数和十进制数一样，在运算中，当数的位数越多
时，计算的精度就越高。理论上讲，数的位数可以任意多。但是位数越多，所需的电子器
件也越多，因此计算机的运算器长度一般是 8 位、16 位、32 位、64 位。 

存储器的功能是保存或“记忆”解题的原始数据和解题步骤。为此，在运算前需要把
参加运算的数据和解题步骤通过输入设备送到存储器中保存起来。 
注意，不论是数据，还是解题步骤，在存放到存储器以
前，它们全已变成 0 或 1 表示的二进制代码。因此，存储器
存储的也全是 0 或 1 表示的二进制代码。那么大量的 0、1
代码在存储器中如何保存呢? 
目前采用半导体器件来担当此任务。我们知道，一个半
导体触发器由于有 0 和 1 两个状态，可以记忆一个二进制代
码。一个数假定用 16 位二进制代码来表示，那么就需要有 
16 个触发器来保存这些代码。通常，在存储器中把保存一
个数的 16 个触发器称为一个存储单元。存储器是由许多存
储单元组成的。每个存储单元都有编号，称为地址。向存储
器中存数或者从存储器中取数，都要按给定的地址来寻找所
选的存储单元，这相当于上面所讲的横格纸每一行存放一个
数一样。图 1.4 所示为存储器的结构示意图。 
存储器所有存储单元的总数称为存储器的存储容量，通常用单位 KB、MB 来表示，如
64KB、128MB。存储容量越大，表示计算机记忆储存的信息越多。 
半导体存储器的存储容量毕竟有限，因此计算机中又配备了存储容量更大的磁盘存储
 


器和光盘存储器，称为外存储器。相对而言，半导体存储器称为内存储器，简称内存。 

控制器是计算机中发号施令的部件，它控制计算机的各部件有条不紊地进行工作。更
具体地讲，控制器的任务是从内存中取出解题步骤加以分析，然后执行某种操作。 

运算器只能完成加、减、乘、除四则运算及其他一些辅助操作。对于比较复杂的计算
题目，计算机在运算前必须化成一步一步简单的加、减、乘、除等基本操作来做。每一个
基本操作就叫做一条指令，而解算某一问题的一串指令序列，叫做该问题的计算程序，简
称为程序。 
例如，在前述求解 y=ax+b–c 的例子中，我们在横格纸上列出了它的解题步骤。解题步
骤的每一步，只完成一种基本操作，所以就是一条指令，而整个解题步骤就是一个简单的
计算程序。 
正如我们在横格纸上按行的序号记下解题步骤一样，计算机中为了顺利运算，也必须
事先把程序和数据按地址安排到存储器里去。注意，程序中的指令通常按顺序执行，所以
这些指令是顺次放在存储器里。这就相当于我们把表 1.3 所示的横格纸的内容原封不动地搬
到存储器，因而所编的程序如表 1.4 所示。 

由表 1.4 可知，每条指令应当明确告诉控制器，从存储器的哪个单元取数，并进行何种
操作。这样可知指令的内容由两部分组成，即操作的性质和操作数的地址。前者称为操作


码，后者称为地址码。因而上述指令的形式如下： 
操作码 
地址码 
其中操作码指出指令所进行的操作，如加、减、乘、除、取数、存数等；而地址码表示参
加运算的数据应从存储器的哪个单元中取来，或运算的结果应该存到哪个单元中去。 
指令的操作码和地址码用二进制代码来表示，其中地址码部分和数据一样，是二进制
数的数码，而操作码部分则是二进制代码的编码。假定只有 8 种基本指令，那么这 8 种指
令的操作码可用 3 位二进制代码来定义，如表 1.5 所示。 
这样一来，表 1.5 中指令的操作码部分就可以变成二进制代码。假如把地址码部分和数
据也换成二进制数，那么整个存储器的内容全部变成了二进制的代码或数码，如图 1.5 所示。 

由图 1.5 可知，指令数码化以后，就可以和数据一样放入存储器。存储器的任何位置
既可以存放数据也可以存放指令，不过一般是将指令和数据分开存放。将解题的程序(指
令序列)存放到存储器中称为存储程序，而控制器依据存储的程序来控制全机协调地完成
计算任务叫做程序控制。存储程序并按地址顺序执行，这就是冯·诺依曼型计算机的设计
思想，也是机器自动化工作的关键。由于指令和数据放在同一个存储器，称为冯·诺依曼
结构；如果指令和数据分别放在两个存储器，称为哈佛结构。显然后者结构的计算机速度
更快。 
一台计算机通常有几十种基本指令，从而构成了该计算机的指令系统。指令系统不仅
是硬件设计的依据，而且是软件设计的基础。因此，指令系统是衡量计算机性能的一个重
要标志。 

由表 1.4 可知，计算机进行计算时，指令必须是按一定的顺序一条接一条地进行。控
制器的基本任务，就是按照计算程序所排的指令序列，先从存储器取出一条指令放到控制
器中，对该指令的操作码由译码器进行分析判别，然后根据指令性质，执行这条指令，进
行相应的操作。接着从存储器取出第二条指令，再执行这第二条指令。以此类推。通常把 

取指令的一段时间叫做取指周期，而把执行指
令的一段时间叫做执行周期。因此，控制器反
复交替地处在取指周期与执行周期之中，如  
图 1.6 所示。每取出一条指令，控制器中的指
令计数器就加 1，从而为取下一条指令做好准
备，这也就是指令在存储器中顺序存放的   
原因。 
在计算机系统中，运算器和控制器通常被组合在一个集成电路芯片中，合称为中央处
理器（中央处理机），简称处理器，英文缩写为 CPU。 

由于计算机仅使用 0 和 1 两个二进制数字，所以使用“位”(bit)作为数字计算机的最
小信息单位。当 CPU 向存储器送入或从存储器取出信息时，不能存取单个的“位”，而用
B(字节)和 W(字)等较大的信息单位来工作。一个“字节”由 8 位二进制信息组成，而一个
“字”则至少由一个以上的字节组成。通常把组成一个字的二进制位数叫做字长。例如，微
型机的字长可以是 8 位，也可以达到 64 位。 
由于计算机使用的信息既有指令又有数据，所以计算机字既可以代表指令，也可以代
表数据。如果某字代表要处理的数据，则称为数据字；如果某字为一条指令，则称为指令字。 
我们已经看到，指令和数据统统放在内存中，从形式上看，它们都是二进制数码，似
乎很难分清哪些是指令字，哪些是数据字。然而控制器完全可以区分开哪些是指令字，哪
些是数据字。一般来讲，取指周期中从内存读出的信息流是指令流，它流向控制器；而在
执行周期中从内存读出的信息流是数据流，它由内存流向运算器。例如，图 1.5 中从地址 1～
7 号单元读出的信息流是指令流，而从地址 9～12 号单元读出的信息流是数据流。显然，某
些指令进行过程中需要两次访问内存，一次是取指令，另一次是取数据，如表 1.4 中取数、
乘法、加法、减法、存数指令就是如此。 

理想的计算机输入设备应该是“会看”和“会听”，即能够把人们用文字或语言所表达
的问题直接送到计算机内部进行处理，但是现在这种理想的输入设备还未大规模投入应用。
目前常用的输入设备是键盘、鼠标、数字扫描仪及模数转换器等。它们的作用是把人们所
熟悉的某种信息形式变换为机器内部所能接收和识别的二进制信息形式。 
输出设备的作用是把计算机处理的结果变换为人或其他机器设备所能接收和识别的信
息形式。理想的输出设备应该是“会写”和“会讲”。“会写”已经做到，如目前广为使用
的激光印字机、绘图仪、CRT 显示器等。这些设备不仅能输出文字符号，而且还能画图作
曲线。至于“会讲”即输出语言的设备，目前也有高级产品问世。 
计算机的输入/输出设备通常称为外围设备。这些外围设备有高速的也有低速的，有机
电结构的，也有全电子式的。由于种类繁多且速度各异，因而它们不是直接与高速工作的
主机相连接，而是通过适配器部件与主机相联系。适配器的作用相当于一个转换器。它可
以保证外围设备用计算机系统特性所要求的形式发送或接收信息。 
一个典型的计算机系统具有各种类型的外围设备，因而有各种类型的适配器，它使得
 

被连接的外围设备通过系统总线与主机进行联系，以便使主机和外围设备并行协调地工作。 
除了上述各部件外，计算机系统中还必须有总线。系统总线是构成计算机系统的骨架，
是多个系统部件之间进行数据传送的公共通路。借助系统总线，计算机在各系统部件之间
实现传送地址、数据和控制信息的操作。 
以上是我们对一台计算机硬件组成的概貌了解，其目的在于使读者对计算机的整体先
有一个粗略的印象，为后面讲授各章提供一些方便。 

上面说过，现代电子计算机是由运算器、存储器、控制器、适配器、总线和输入/输出
设备组成的。这些部件或设备都是由元器件构成的有形物体，因而称为硬件或硬设备。 
我们知道，使用算盘进行运算时，要按运算法则和计算步骤， 利用珠算口诀来进行。
如果只有算盘，没有运算法则和计算步骤，就不能用算盘来计算。电子计算机更是如此。
如果只有上述硬件，计算机并不能进行运算，它仍然是一个“死”东西。那么计算机靠什
么东西才能变“活”，从而高速自动地完成各种运算呢?这就是前面讲过的计算程序。因为
它是无形的东西，所以称为软件或软设备。比方说，用算盘进行运算，算盘本身就是硬件，
而运算法则和解题步骤等就是软件。 
事实上，利用电子计算机进行计算、控制或做其他工作时，需要有各种用途的程序。
因此，凡是用于一台计算机的各种程序，统称为这台计算机的程序或软件系统。 
计算机软件一般分为两大类：一类叫系统程序，一类叫应用程序。 
系统程序用来简化程序设计，简化使用方法，提高计算机的使用效率，发挥和扩大计
算机的功能及用途。它包括以下四类：①各种服务性程序，如诊断程序、排错程序、练习
程序等；②语言程序，如汇编程序、编译程序、解释程序等；③操作系统；④数据库管理
系统。 
应用程序是用户利用计算机来解决某些问题而编制的程序，如工程设计程序、数据处
理程序、自动控制程序、企业管理程序、情报检索程序、科学计算程序等。随着计算机的
广泛应用，这类程序的种类越来越多。 

如同硬件一样，计算机软件也是在不断发展的。下面以系统程序为例，简要说明软件
的发展演变过程。 
在早期的计算机中，人们是直接用机器语言(即机器指令代码)来编写程序的，这种方
式编写的程序称为手编程序。这种用机器语言书写的程序，计算机完全可以“识别”并能
执行，所以又叫做目的程序。但直接用机器语言编写程序是一件很烦琐的工作，需要耗费
大量的人力和时间，而且又容易出错，出错后寻找错误也相当费事。这些情况大大限制了
计算机的使用。 
后来，为了编写程序方便和提高机器的使用效率，人们想了一种办法，用一些约定的

文字、符号和数字按规定的格式来表示各种不同的指令，然后再用这些特殊符号表示的指
令来编写程序。这就是所谓的汇编语言，它是一种能被转化为二进制文件的符号语言。对
人来讲，符号语言简单直观、便于记忆，比二进制数表示的机器语言方便了许多。但计算
机只“认识”机器语言而不认识这些文字、数字、符号，为此人们创造了一种程序，叫汇
编器。如同英汉之间对话需要“翻译”一样，汇编器的作用相当于一个“翻译员”。借助于
汇编器，计算机本身可以自动地把符号语言表示的程序(称为汇编语言程序)翻译成用机器
语言表示的目的程序，从而实现了程序设计工作的部分自动化。 
使用符号语言编程序比用机器语言编程序是进了一步，但符号语言还是一种最初级的
语言，和数学语言的差异很大，并且仍然面向一台具体的机器。由于不同的计算机其指令
系统也不同，所以人们使用计算机时必须先花很多时间熟悉这台机器的指令系统，然后再
用它的符号语言来编写程序，因此还是很不方便，节省的人力时间也有限。为了进一步实
现程序自动化和便于程序交流，使不熟悉具体计算机的人也能很方便地使用计算机，人们
又创造了各种接近于数学语言的算法语言。 
所谓算法语言，是指按实际需要规定好的一套基本符号及由这套基本符号构成程序的
规则。算法语言比较接近数学语言，它直观通用，与具体机器无关，只要稍加学习就能掌
握，便于推广使用计算机。有影响的算法语言有 BASIC、FORTRAN、C、C++、Java 等。 
用算法语言编写的程序称为源程序。但是，这种源程序如同汇编语言程序一样，是不
能由机器直接识别和执行的，也必须给计算机配备一个既懂算法语言又懂机器语言的“翻
译”，才能把源程序翻译为机器语言。通常采用的方法是给计算机配制一套用机器语言写的
编译程序，它把源程序翻译成目的程序，然后机器执行目的程序，得出计算结果。但由于
目的程序一般不能独立运行，还需要一种叫做运行系统的辅助程序来帮助。通常，把编译
程序和运行系统合称为编译器。 
图 1.7 描述了一个在硬盘文件中的 C 语言程序，被转换成计算机上可运行的机器语言
程序的四个步骤：C 语言程序通过编译器首先被编译为汇编语言程序，然后通过汇编器汇
编为机器语言的目标模块。链接器将多个模块与库程序组合在一起以解析所有的应用。加
载器将机器代码放入合适的内存位置以便处理器执行。 
随着计算机技术的日益发展，原始的操作方式越来越不适应，特别是用户直接使用大
型机器并独占机器，无论是对机器的效率来说还是对方便用户来说都不适宜。用户直接使
用机器总觉得机器“太硬了”，很多情况都得依附它。而计算机又觉得用户及外部设备“太
笨”，常常使它处于无事可做的状态，因此，迫切需要摆脱这种情况。显然人的思维速度跟
不上计算机的计算速度，要摆脱这种情况还要依靠计算机来管理自己和管理用户，于是人
们又创造出一类程序，叫做操作系统。它是随着硬件和软件的不断发展而逐渐形成的一套
软件系统，用来管理计算机资源(如处理器，内存，外部设备和各种编译、应用程序)和自
动调度用户的作业程序，而使多个用户能有效地共用一套计算机系统。操作系统的出现，
使计算机的使用效率成倍地提高，并且为用户提供了方便的使用手段和令人满意的服务
质量。 
根据不同使用环境要求，操作系统目前大致分为批处理操作系统、分时操作系统、网
络操作系统、实时操作系统等多种。个人计算机中广泛使用微软公司的“视窗”操作系统。 

随着计算机在信息处理、情报检索及各种管理系统中应用的发展，要求大量处理某些
数据，建立和检索大量的表格。这些数据和表格按一定的规律组织起来，使得处理更方便，
检索更迅速，用户使用更方便，于是出现了数据库。所谓数据库，就是实现有组织地、动
态地存储大量相关数据，方便多用户访问的计算机软、硬件资源组成的系统。数据库和数
据库管理软件一起，组成了数据库管理系统。 
数据库管理系统有各种类型。目前许多计算机包括微型机，都配有数据库管理系统。 
随着软件的进一步发展，将开发更高级的计算机语言。这是因为目前所有的高级语言
编写程序时，程序比较复杂，开发成本高。计算机语言发展的方向是标准化、积木化、产
品化，最终是向自然语言发展，它们能够自动生成程序。 

从前两节讲述可知，计算机不能简单地认为是一种电子设备，而是一个十分复杂的硬、
软件结合而成的整体。它通常由五个以上不同的级组成，每一级都能进行程序设计。
第 1 级是微程序设计级或逻辑电路级。这是一个实在的硬件级，由硬件直接执行。如
果某一个应用程序直接用微指令来编写，那么可在这一级上运行应用程序。 
第 2 级是一般机器级，也称为机器语言级，它由微程序解释机器指令系统。这一级也
是硬件级。 
第 3 级是操作系统级，它由操作系统程序实现。这些操作系统由机器指令和广义指令
组成，广义指令是操作系统定义和解释的软件指令，所以这一级也称为混合级。 

第 4 级是汇编语言级，它给程序人员提供一种符
号形式语言，以减少程序编写的复杂性。这一级由汇
编程序支持和执行。如果应用程序采用汇编语言编写，
则机器必须要有这一级的功能；如果应用程序不采用
汇编语言编写，则这一级可以不要。 
第 5 级是高级语言级，它是面向用户的，为方便
用户编写应用程序而设置的。这一级由各种高级语言
编译程序支持和执行。 
图 1.8 中，除第 1 级外，其他各级都得到它下级
的支持，同时也受到运行在下面各级上的程序的支持。
第 1 级到第 3 级编写程序采用的语言，基本是二进制
数字化语言，机器执行和解释容易。第 4、5 两级编写程序所采用的语言是符号语言，用英
文字母和符号来表示程序，因而便于大多数不了解硬件的人们使用计算机。 
显然，采用这种用一系列的级来组成计算机的概念和技术，对了解计算机如何组成提
供了一种好的结构和体制。而且用这种分级的观点来设计计算机，对保证产生一个良好的
系统结构也是很有帮助的。 

然而，随着大规模集成电路技术的发展和软件硬化的趋势，计算机系统的软、硬件界
限已经变得模糊了。因为任何操作可以由软件来实现，也可以由硬件来实现；任何指令的
执行可以由硬件完成，也可以由软件来完成。对于某一机器功能采用硬件方案还是软件方
案，取决于器件价格、速度、可靠性、存储容量和变更周期等因素。 
当研制一台计算机的时候，设计者必须明确分配每一级的任务，确定哪些情况使用硬
件，哪些情况使用软件，而硬件始终放在最低级。就目前而言，一些计算机的特点是，把
原来明显地在一般机器级通过编制程序实现的操作，如整数乘除法指令、浮点运算指令、
处理字符串指令等，改为直接由硬件完成。总之，随着大规模集成电路和计算机系统结构
的发展，实体硬件机的功能范围在不断扩大。换句话说，第一级和第二级的边界范围，要
向第三级乃至更高级扩展。这是因为容量大、价格低、体积小、可以改写的只读存储器提
供了软件固化的良好物质手段。现在已经可以把许多复杂的、常用的程序制作成所谓的固
件。就它的功能来说，是软件，但从形态上来说，又是硬件。其次，目前在一片硅单晶芯
片上制作复杂的逻辑电路已经是实际可行的，这就为扩大指令的功能提供了物质基础，因
此本来通过软件手段来实现的某种功能，现在可以通过硬件来直接解释执行。进一步的发
展，就是设计所谓面向高级语言的计算机。这样的计算机，可以通过硬件直接解释执行高
级语言的语句而不需要先经过编译程序的处理。因此传统的软件部分，今后完全有可能“固
化”甚至“硬化”。 

习惯上所称的“电子计算机”是指现在广泛应用的电子数字计算机，它分为专用计算
 
机和通用计算机两大类。专用和通用是根据计算机的效率、速度、价格、运行的经济性和
适应性来划分的。通用计算机分为超级计算机、大型机、服务器、PC 机、单片机、多核机
六类，其结构复杂性、性能、价格依次递减。 
计算机的硬件是由有形的电子器件等构成的，它包括运算器、存储器、控制器、适配
器、输入输出设备。早期将运算器和控制器合在一起称为 CPU(中央处理器)。目前的 CPU
包含了存储器，因此称为中央处理机。存储程序并按地址顺序执行，这是冯·诺依曼型计
算机的工作原理，也是 CPU 自动工作的关键。 
计算机的软件是计算机系统结构的重要组成部分，也是计算机不同于一般电子设备的
本质所在。计算机软件一般分为系统程序和应用程序两大类。系统程序用来简化程序设计，
简化使用方法，提高计算机的使用效率，发挥和扩大计算机的功能和用途，它包括：①各
种服务性程序；②语言类程序；③操作系统；④数据库管理系统。应用程序是针对某一应
用课题领域开发的软件。 
计算机系统是一个由硬件、软件组成的多级层次结构，它通常由微程序级、一般机器
级、操作系统级、汇编语言级、高级语言级组成，每一级上都能进行程序设计，且得到下
面各级的支持。 
计算机的性能指标主要是 CPU 性能指标、存储器性能指标和 I/O 吞吐率。 

在选择计算机的数的表示方式时，需要考虑以下几个因素：①要表示的数的类型(小数、
整数、实数和复数)；②可能的数值范围；③数值精确度；④数据存储和处理所需要的硬件
代价。 
计算机中常用的数据表示格式有两种，一是定点格式，二是浮点格式。一般来说，定
点格式容许的数值范围有限，要求的处理硬件比较简单。而浮点格式容许的数值范围很大，
要求的处理硬件比较复杂。 
1. 定点数的表示方法 
所谓定点格式，即约定机器中所有数据的小数点位置是固定不变的。由于约定在固定
的位置，小数点就不再使用记号“.”来表示。原理上讲，小数点位置固定在哪一位都可以，
但是通常将数据表示成纯小数或纯整数。 
假设用一个 n+1 位字来表示一个定点数 x，其中一位 xn 用来表示数的符号，其余位数
代表它的量值。为了将整个 n+1 位统一处理，符号位 xn 放在最左位置，并用数值 0 和 1 分
别代表正号和负号，这样，对于任意定点数 x=xnxn–1…x1x0，在定点机中可表示为如下形式： 
 

目前计算机中多采用定点纯整数表示，因此将定点数表示的运算简称为整数运算。 

电子的质量(9×10–28g)和太阳的质量(2×1033g)相差甚远，在定点计算机中无法直接来
表示这个数值范围。要使它们送入定点计算机进行某种运算，必须对它们分别取不同的比
例因子，使其数值部分的绝对值小于 1，即 
9×10–28 = 0.9×10–27 
2×1033 = 0.2×1034 
这里的比例因子 10–27 和 1034 要分别存放在机器的某个存储单元中，便于以后对计算结果按
这个比例增大。显然这要占用一定的存储空间和运算时间。 
从定点机取比例因子中我们得到一个启示，在计算机中还可以这样来表示数据：把一
个数的有效数字和数的范围在计算机的一个存储单元中分别予以表示。这种把数的范围和
精度分别表示的方法，相当于数的小数点位置随比例因子的不同而在一定范围内可以自由
浮动，所以称为浮点表示法。 
任意一个十进制数 N 可以写成 

同样，在计算机中一个任意二进制数 N 可以写成 
 

其中 M 称为浮点数的尾数，是一个纯小数。e 是比例因子的指数，称为浮点数的指数，是
一个整数。比例因子的基数 2 对二进记数制的机器是一个常数。 
在机器中表示一个浮点数时，一是要给出尾数，用定点小数形式表示。尾数部分给出
有效数字的位数，因而决定了浮点数的表示精度。二是要给出指数，用整数形式表示，常
称为阶码，阶码指明小数点在数据中的位置，因而决定了浮点数的表示范围。浮点数也要
有符号位。计算机中，一个机器浮点数由阶码和尾数及其符号位组成： 
 

大多数通用性较强的计算机都能直接处理十进制形式表示的数据。十进制数串在计算
机内主要有两种表示形式： 
(1)字符串形式，即 1 字节存放一个十进制的数位或符号位。在主存中，这样的一个十
进制数占用连续的多字节，故为了指明这样一个数，需要给出该数在主存中的起始地址和
位数(串的长度)。这种方式表示的十进制字符串主要用在非数值计算的应用领域中。 
(2)压缩的十进制数串形式，即 1 字节存放两个十进制的数位。它比前一种形式节省存
储空间，又便于直接完成十进制数的算术运算，是广泛采用的较为理想的方法。 
用压缩的十进制数串表示一个数，也要占用主存连续的多字节。每个数位占用半字节
(即 4 个二进制位)，其值可用二-十编码(BCD 码)或数字符的 ASCII 码的低 4 位表示。符
第 2 章  运算方法和运算器       19 
号位也占半字节并放在最低数字位之后，其值选用四位编码中的六种冗余状态中的有关值，
如用 12(C)表示正号，用 13(D)表示负号。在这种表示中，规定数位加符号位之和必须为
偶数，当和不为偶数时，应在最高数字位之前补一个 0。例如，+123 和–12 分别被表示成： 
 
在上述表示中，一个实线框表示 1 字节，虚线把一个字节分为高低各半字节，每一个小框
内给出一个数值位或符号位的编码值(用十六进制形式给出)。符号位在数字位之后。 
与第一种表示形式类似，要指明一个压缩的十进制数串，也得给出它在主存中的首地
址和数字位个数(不含符号位)，又称位长，位长为 0 的数其值为 0。十进制数串表示法的优
点是位长可变，许多机器中规定该长度为 0～31，有的甚至更长。 
2.1.2  数的机器码表示 
前面介绍了数的小数点表示，下面还需要解决数的机器码表示问题。 
在计算机中对数据进行运算操作时，符号位如何表示呢?是否也同数值位一道参加运算
操作呢?如参加，会给运算操作带来什么影响呢?为了妥善地处理好这些问题，就产生了把
符号位和数值位一起编码来表示相应的数的各种表示方法，如原码、补码、反码、移码。
为了区别一般书写表示的数和机器中这些编码表示的数，通常将前者称为真值，后者称为
机器数或机器码。 

若定点整数的原码形式为 xnxn–1…x1x0(xn 为符号位)，则原码表示的定义是 
 

式中，[x]原是机器数，x 是真值。 
例如，x = +1001，则[x]原 = 01001 
      x = –1001，则[x]原 = 11001 
一般情况下，对于正数 x=+xn–1…x1x0，则有 
[x]原= 0 xn–1…x1x0 
对于负数 x= –xn–1…x1x0，则有 
[x]原= 1 xn–1…x1x0 
对于 0，原码机器中往往有“+0”、“–0”之分，故有两种形式： 
[+0]原= 0000…0 
[–0]原= 1000…0 
采用原码表示法简单易懂，即符号位加上二进制数的绝对值，但它的最大缺点是加法
运算复杂。这是因为，当两数相加时，如果是同号则数值相加；如果是异号，则要进行减
法。而在进行减法时，还要比较绝对值的大小，然后大数减去小数，最后还要给结果选择
恰当的符号。为了解决这些矛盾，人们找到了补码表示法。 
      计算机组成原理 
20

我们先以钟表对时为例说明补码的概念。假设现在的标准时间为 4 点正，而有一只表
已经 7 点了，为了校准时间，可以采用两种方法：一是将时针退 7–4=3 格；一是将时针向
前拨 12–3=9 格。这两种方法都能对准到 4 点，由此看出，减 3 和加 9 是等价的。就是说 9
是(–3)对 12 的补码，可以用数学公式表示为 
–3= +9    (mod 12) 
mod 12 的意思就是 12 为模数，这个“模”表示被丢掉的数值。上式在数学上称为同余式。 
上例中 7–3 和 7+9(mod 12)等价，原因就是表指针超过 12 时，将 12 自动丢掉，最后得
到 16–12=4。同样地，以 12 为模时， 
–4 = +8    (mod 12) 
–5 = +7    (mod 12) 
从这里可以得到一个启示，就是负数用补码表示时，可以把减法转化为加法。这样，在计
算机中实现起来就比较方便。 
对定点整数，补码形式为 xnxn–1…x1x0，xn 为符号位，则补码表示的定义是 
 
采用补码表示法进行减法运算比原码方便多了。因为不论数是正或负，机器总是做加
法，减法运算可变成加法运算。但根据补码定义，求负数的补码还要做减法，这显然不方
便，为此可通过反码来解决。 
我们先引出数的补码表示与真值的关系。设一个二进制整数补码有 n+1 位(含 1 位符号
位 xn)，即 
[x]补= xnxn–1xn–2…x1x0 
则其补码表示的真值为 


由此可知，式(2.7)统一表示了正负整数的补码与真值的关系。 

下面说明由原码表示法变成补码表示法的方法。 
在定点数的反码表示法中，正数的机器码仍然等于其真值；而负数的机器码符号位为 1，
尾数则将真值的各个二进制位取反。由于原码变反码很容易实现(触发器互补输出端得到)，
所以用反码作为过渡，就可以很容易得到补码。 
一个正整数，当用原码、反码、补码表示时，符号位都固定为 0，用二进制表示的数位
值都相同，即三种表示方法完全一样。 
一个负整数，当用原码、反码、补码表示时，符号位都固定为 1，用二进制表示的数位
值都不相同。此时由原码表示法变成补码表示法的规则如下： 
(1)原码符号位为 1 不变，整数的每一位二进制数位求反得到反码； 
(2)反码符号位为 1 不变，反码数值位最低位加 1，得到补码。 

3. 移码表示法 
移码通常用于表示浮点数的阶码。由于阶码是个 k 位的整数，假定定点整数移码形式
为 ekek–1…e2e1e0(最高位为符号位)时，移码的传统定义是 
 
[e]移=2k+e，  2k＞e≥–2k 
(2.8) 
式中，[e]移为机器数，e 为真值，2k 是一个固定的偏移值常数。 
若阶码数值部分为 5 位，以 e 表示真值，则 
[e]移=25+e，  25＞e≥–25 
例如，当正数e = +10101 时，[e]移 = 1，10101；当负数e = –10101 时，[e]移 = 25+e = 25–10101 = 
0，01011。移码中的逗号不是小数点，而是表示左边一位是符号位。显然，移码中符号位
ek 表示的规律与原码、补码、反码相反。 
移码表示法对两个指数大小的比较和对阶操作都比较方便，因为阶码域值大者其指数
值也大。 
4. 浮点数的机器表示 
早期，各个计算机系统的浮点数使用不同的机器码表示阶和尾数，给数据的交换和比

较带来很大麻烦。当前的计算机都采用统一的 IEEE754 标准中的格式表示浮点数。IEEE754
标准规定的 32 位短浮点数和 64 位长浮点数的标准格式为 
 
不论是 32 位浮点数还是 64 位浮点数，由于基数 2 是固定常数，对每一个浮点数都一
样，所以不必用显式方式来表示它。 
32 位的浮点数中，S 是浮点数的符号位，占 1 位，安排在最高位，S=0 表示正数，S=1
表示负数。M 是尾数，放在低位部分，占用 23 位，小数点位置放在尾数域最左(最高)有效
位的右边。E 是阶码，占用 8 位，阶符采用隐含方式，即采用移码方法来表示正负指数。采
用这种方式时，将浮点数的指数真值 e 变成阶码 E 时，应将指数 e 加上一个固定的偏置常
数 127，即 E=e+127。 
若不对浮点数的表示作出明确规定，同一个浮点数的表示就不是唯一的。例如，(1.75)10
可以表示成 1.11×20、0.111×21、0.0111×22 等多种形式。为了提高数据的表示精度，当尾数
的值不为 0 时，尾数域的最高有效位应为 1，这称为浮点数的规格化表示。对于非规格化浮
点数，一般可以通过修改阶码同时右移动小数点位置的办法，使其变成规格化数的形式。 
在 IEEE754 标准中，一个规格化的 32 位浮点数 x 的真值表示为 

其中尾数域所表示的值是 1.M。由于规格化的浮点数的尾数域最左位(最高有效位)总是 1，
故这一位无需存储，而认为隐藏在小数点的左边。于是用 23 位字段可以存储 24 位有效数。 
对 32 位浮点数 N，IEEE754 定义： 
(1)若 E=255 且 M<>0，则 N=NaN。符号 NaN 表示无定义数据，采用这个标志的目的
是让程序员能够推迟进行测试及判断的时间，以便在方便的时候进行。 
(2)若 E=255 且 M=0，则 N=(–1)S∞。当阶码 E 为全 1 且尾数 M 为全 0 时，表示的真
值 N 为无穷大，结合符号位 S 为 0 或 1，也有+∞和–∞之分。 
(3)若 E=0 且 M=0，则 N=(–1)S0。当阶码 E 为全 0 且尾数 M 也为全 0 时，表示的真值
N 为零(称为机器 0)，结合符号位 S 为 0 或 1，有正零和负零之分。 
(4)若 0<E<255，则 N=(–1)S×(1.M)×2E–127(规格化数)。除去用 E 为全 0 和全 1(即十进
制 255)表示零和无穷大的特殊情况，指数的偏移值不选 27=128(10000000)，而选 27–1= 
127(01111111)。对于规格化浮点数，阶码 E 的范围变为 1～254，指数值 e 则为–126～+127。
因此 32 位浮点数表示的绝对值的范围是 10–38～1038。 
(5)若 E=0 且 M<>0，则 N=(–1)S×(0.M)×2–126(非规格化数)。对于规格化无法表示的数
据，可以用非规格化形式表示。 
64 位的浮点数中符号位 1 位，阶码域 11 位，尾数域 52 位，指数偏移值是 1023。因此
规格化的 64 位浮点数 x 的真值为 

浮点数所表示的范围远比定点数大。一般在高档微机以上的计算机中同时采用定点、
浮点表示，由使用者进行选择。而单片机中多采用定点表示。 

现代计算机不仅处理数值领域的问题，而且处理大量非数值领域的问题。这样一来，
必然要引入文字、字母及某些专用符号，以便表示文字语言、逻辑语言等信息。例如，人
机交换信息时使用英文字母、标点符号、十进制数及诸如＄，%，+等符号。然而数字计算
机只能处理二进制数据，因此，上述信息应用到计算机中时，都必须编写成二进制格式的
代码，也就是字符信息用数据表示，称为符号数据。 
目前国际上普遍采用的一种字符系统是七单位的 IRA 码。其美国版称为 ASCII 码(美国
国家信息交换标准字符码)，它包括 10 个十进制数码，26 个英文字母和一定数量的专用符
号，如＄，%，+等，总共 128 个元素，因此二进制编码需要 7 位，加上一个偶校验位，共
8 位，刚好为 1 字节。 

ASCII 码规定 8 个二进制位的最高一位为 0，余下的 7 位可以给出 128 个编码，表示
128 个不同的字符。其中 95 个编码，对应着计算机终端能敲入并且可以显示的 95 个字符，
打印机设备也能打印这 95 个字符，如大小写各 26 个英文字母，0～9 这 10 个数字符，通用
的运算符和标点符号+，–，*，\，>，=，<等。 
另外的 33 个字符，其编码值为 0～31 和 127，则不对应任何一个可以显示或打印的实
际字符，它们被用作控制码，控制计算机某些外围设备的工作特性和某些计算机软件的运
行情况。 
ASCII 编码和 128 个字符的对应关系如表 2.1 所示。表中编码符号的排列次序为
b7b6b5b4b3b2b1b0，其中 b7 恒为 0，表中未给出，b6b5b4 为高位部分，b3b2b1b0 为低位部分。可
以看出，十进制的 8421 码可以去掉 b6b5b4(=011)而得到。 

字符串是指连续的一串字符，通常方式下，它们占用主存中连续的多字节，每字节存
一个字符。当主存字由 2 或 4 字节组成时，在同一个主存字中，既可按从低位字节向高位
字节的顺序存放字符串内容，也可按从高位字节向低位字节的顺序存放字符串内容。这两
种存放方式都是常用方式，不同的计算机可以选用其中任何一种。例如下述字符串： 
IF   A>B   THEN   READ(C) 
就可以按图 2.1 所示从高位字节到低位字节依次存放在
主存中。其中主存单元长度由 4 字节组成。每字节中存放相
应字符的 ASCII 值，文字表达式中的空格“   ”在主存中也
占 1 字节的位置。因而每字节分别存放十进制的 73，70，32，
65，62，66，32，84，72，69，78，32，82，69，65，68，
 

为了能直接使用西文标准键盘把汉字输入到计算机，就必须为汉字设计相应的输入编
码方法。当前采用的方法主要有以下三类： 
数字编码  常用的是国标区位码，用数字串代表一个汉字输入。区位码是将国家标准
局公布的 6763 个两级汉字分为 94 个区，每个区分 94 位，实际上把汉字表示成二维数组，
每个汉字在数组中的下标就是区位码。区码和位码各两位十进制数字，因此输入一个汉字
需按键四次。例如“中”字位于第 54 区 48 位，区位码为 5448。 
数字编码输入的优点是无重码，且输入码与内部编码的转换比较方便，缺点是代码难
以记忆。 
拼音码  拼音码是以汉语拼音为基础的输入方法。凡掌握汉语拼音的人，不需训练和
记忆，即可使用。但汉字同音字太多，输入重码率很高，因此按拼音输入后还必须进行同
音字选择，影响了输入速度。 
字形编码  字形编码是用汉字的形状进行的编码。汉字总数虽多，但是由一笔一画组
成，全部汉字的部件和笔画是有限的。因此，把汉字的笔画部件用字母或数字进行编码，
按笔画的顺序依次输入，就能表示一个汉字。例如五笔字型编码是最有影响的一种字形编
码方法。 
除了上述三种编码方法之外，为了加快输入速度，在上述方法基础上，发展了词组输
入、联想输入等多种快速输入方法。但是都利用了键盘进行“手动”输入。理想的输入方
式是利用语音或图像识别技术“自动”将拼音或文本输入到计算机内，使计算机能认识汉
字，听懂汉语，并将其自动转换为机内代码表示。目前这种理想已经成为现实。 

汉字内码是用于汉字信息的存储、交换、检索等操作的机内代码，一般采用 2 字节表
示。英文字符的机内代码是七位的 ASCII 码，当用 1 字节表示时，最高位为“0”。为了与
英文字符能相互区别，汉字机内代码中 2 字节的最高位均规定为“1”。例如，汉字操作系
统 CCDOS 中使用的汉字内码是一种最高位为“1”的两字节内码。 
有些系统中字节的最高位用于奇偶校验位，这种情况下用 3 字节表示汉字内码。 

字模码是用点阵表示的汉字字形代码，它是汉字的输出形式。 
根据汉字输出的要求不同，点阵的多少也不同。简易型汉字为 16×16 点阵，提高型汉
字为 24×24 点阵、32×32 点阵，甚至更高。因此字模点阵的信息量是很大的，所占存储空
间也很大。以 16×16 点阵为例，每个汉字要占用 32 字节，国标两级汉字要占用 256K 字节。
因此字模点阵只能用来构成汉字库，而不能用于机内存储。字库中存储了每个汉字的点阵
代码。当显示输出或打印输出时才检索字库，输出字模点阵，得到字形。注意，汉字的输
入编码、汉字内码、字模码是计算机中用于输入、内部处理、输出三种不同用途的编码，
不要混为一谈。 

元件故障、噪声干扰等各种因素常常导致计算机在处理信息过程中出现错误。例如，
将 1 位二进制数 x 从部件 A 传送到部件 B，可能由于传送信道中的噪声干扰而受到破坏，
以至于在接收部件 B 收到的是 x 而不是 x。为了防止这种错误，可将信号采用专门的逻辑电
路进行编码以检测错误，甚至校正错误。通常的方法是，在每个字上添加一些校验位，用
来确定字中出现错误的位置。计算机中常用这种检错或纠错技术进行存储器读写正确性或
传输信息的检验。这里仅介绍检错码。 
最简单且应用广泛的检错码是采用一位校验位的奇校验或偶校验。设 X=(x0x1…xn–1)是
一个 n 位字，则奇校验位C 定义为 
 
2.1 节已介绍了数的补码表示法，负数用补码表示后，就可以和正数一样来处理。这样，
运算器里只需要一个加法器就可以了，不必为了负数的加法运算，再配一个减法器。 
      计算机组成原理 
28
补码加法的公式是 
 
[x]补+[y]补=[x+y]补    (mod 2n+1) 
(2.13) 
可分四种情况来证明。假设采用定点整数表示，因此证明的先决条件是： x ＜(2n–1)，
y ＜(2n–1)， x
 y
＜(2n–1)。 
(1)x≥0，y≥0，则 x+y≥0。 
相加两数都是正数，故其和也一定是正数。正数的补码和原码是一样的，根据数据补
码定义可得 
[x]补+[y]补=x+y=[x+y]补    (mod 2n+1) 
(2)x≥0，y＜0，则 x+y≥0 或 x+y＜0。 
相加的两数一个为正，一个为负，因此相加结果有正、负两种可能。根据补码定义， 
[x]补=x，  [y]补=2n+1+y 
所以 
[x]补+[y]补=x+2n+1+y=2n+1+(x+y)=[x+y]补    (mod 2n+1) 
(3)x＜0，y≥0，则 x+y≥0 或 x+y＜0。 
这种情况和第(2)种情况一样，把 x 和 y 的位置对调即得证。 
(4)x＜0，y＜0，则 x+y＜0。 
相加两数都是负数，则其和也一定是负数。 
[x]补=2n+1+x，  [y]补=2n+1+y 
所以 
[x]补+[y]补=2n+1+x+2n+1+y=2n+1+(2n+1+x+y)=[x+y]补    (mod 2n+1) 
式(2.13)说明，在模 2n+1 意义下，任意两数的补码之和等于该两数之和的补码。这是补
码加法的理论基础。 

在定点整数机器中，数的表示范围 x <(2n–1)。在运算过程中如出现大于字长绝对值的
现象，称为“溢出”。在定点机中，运算过程中出现溢出时其结果是不正确的，故运算器必
须能检测出溢出。 
【
之所以发生错误，是因为运算结果产生了溢出。两个正数相加，结果大于机器字长所
能表示的最大正数，称为正溢。而两个负数相加，结果小于机器所能表示的最小负数，称
为负溢，如图 2.2 所示。 
 

为了判断“溢出”是否发生，可采用两种检测方法。第一种方法是采用双符号位法，
这称为“变形补码”，从而可使模 2n+1 补码所能表示的数的范围扩大一倍。 
数的变形补码用同余式表示时 

为了得到两数变形补码之和等于两数和的变形补码，同样必须：①两个符号位都看做数码
一样参加运算；②两数进行以 2n+2 为模的加法，即最高符号位上产生的进位要丢掉。 
采用变形补码后，任何正数，两个符号位都是“0”，即 00 xn–1xn–2…x1x0；任何负数，
两个符号位都是“1”，即 11 xn–1xn–2…x1x0。如果两个数相加后，其结果的符号位出现“01”
或“10”两种组合时，表示发生溢出。最高符号位永远表示结果的正确符号。 

两个符号位出现“10”，表示负溢出，即结果小于–2n。 
由此，我们可以得出如下结论： 
(1)当以变形补码运算，运算结果的二符号位相异时，表示溢出；相同时，表示未溢出。
故溢出逻辑表达式为 V=Sf1Sf2，其中 Sf1 和 Sf2 分别为最高符号位和第二符号位。此逻辑
表达式可用异或门实现。 
(2)模 2n+2 补码相加的结果，不论溢出与否，最高符号位始终指示正确的符号。 
第二种溢出检测方法是采用单符号位法。从例 17 和例 18 中看到，当最高有效位产生
进位而符号位无进位时，产生正溢；当最高有效位无进位而符号位有进位时，产生负溢。
故溢出逻辑表达式为 V=Cf C0，其中 Cf 为符号位产生的进位，C0 为最高有效位产生的进位。
此逻辑表达式也可用异或门实现。 
在定点机中，当运算结果发生溢出时表示出错，机器通过逻辑电路自动检查出这种溢
出，并进行中断处理。 

图2.3(a)示出了补码运算的二进制加法/减法器逻辑结构图。由图看到，n个1位的全加器(FA)
可级联成一个 n 位的行波进位加减器。M 为方式控制输入线，当 M=0 时，做加法(A+B)运算；
当 M=1，做减法(A–B)运算，在后一种情况下，A–B 运算转化成[A]补+[–B]补运算，求补过程由
B +1 来实现。因此，图中最右边的全加器的起始进位输入端被连接到功能方式线 M 上，做减法
时 M=1，相当于在加法器的最低位上加 1。另外，图中左边还表示出单符号位法的溢出检测逻辑：
当 Cn=Cn–1时，运算无溢出；而当 Cn≠Cn–1时，运算有溢出，经异或门产生溢出信号。 

两个二进制数字 Ai，Bi 和一个进位输入 Ci 相
加，产生一个和输出 Si，以及一个进位输出 Ci+1。
表 2.2 中列出一位全加器 FA 进行加法运算的输入
输出真值表。 
根据表 2.2 所示的真值表，三个输入端和两个
输出端可按如下逻辑方程进行联系： 
Si=AiBiCi 
  Ci+1=AiBi+BiCi+CiAi=AiBi+(AiBi)Ci   (2.20) 
按此表达式组成的 FA 示于图 2.3(a)，进位链采用
1 个与门和 1 个或门。 
对图 2.3(a)所示的一位全加器(FA)来说，求
和结果 Si 的时间延迟为 6T(每级异或门延迟 3T)。 
 

Ci+1 的时间延迟为 2T，其中 T 被定义为相应于单级逻辑电路的单位门延迟。T 通常采
用一个“与”门或一个“或”门的时间延迟来作为度量单位，因此多级进位链的时间延迟
可以用与-或门的级数或者 T 的数目来计算得到。 
现在计算 n 位行波进位加法器图 2.3(b)的时间延迟。假如采用图 2.3(a)所示的一位全
加器 FA 并考虑溢出检测，那么 n 位行波进位加法器的延迟时间 ta 为 
 

其中，9T 为最低位上的两级“异或”门再加上溢出“异或”门的总时间，2T 为每级进位链
的延迟时间。ta 意味着加法器的输入端输入加数和被加数后，在最坏情况下加法器输出端得
到稳定的求和输出所需的最长时间。显然这个时间越小越好。注意，加数、被加数、进位
与和数都是用电平来表示的，因此，所谓稳定的求和输出，就是指稳定的电平输出。 
思考题  为什么一套加法器可以实现加法和减法操作？创新点在何处？ 

在定点计算机中，两个原码表示的数相乘的运算规则是：乘积的符号位由两数的符号
位按异或运算得到，而乘积的数值部分则是两个正数相乘之积。 
设 n 位被乘数和乘数用定点整数表示 
                         被乘数  [x]原=xf xn–1…x1x0 
                           乘数  [y]原=yf yn–1…y1y0 
                           乘积  [z]原=(xf yf)+(xn–1…x1x0)(yn–1…y1y0) 
(2.22) 
式中，xf 为被乘数符号，yf 为乘数符号。 
乘积符号的运算法则是：同号相乘为正，异号相乘为负。由于被乘数和乘数的符号组
合只有四种情况(xf yf =00,01,10,11)，因此积的符号可按“异或”(按位加)运算得到。 
数值部分的运算方法与普通的十进制小数乘法相类似，不过对于用二进制表达的数来
说，其乘法规则更为简单一些。 
设 x=1101，y=1011，先用习惯方法求其乘积，其过程如下： 
                                   1  1  0  1(x) 
             ×                    1  0  1  1(y) 
                                   1  1  0  1 
                                1  1  0  1 
                             0  0  0  0 
             +            1  1  0  1 
                       1  0  0  0  1  1  1  1(z) 
运算的过程与十进制乘法相似：从乘数 y 的最低位开始，若这一位为“1”，则将被乘数 x
写下；若这一位为“0”，则写下全 0。然后再对乘数 y 的高一位进行乘法运算，其规则同上，
不过这一位乘数的权与最低位乘数的权不一样，因此被乘数 x 要左移一位。以此类推，直
到乘数各位乘完为止，最后将它们统统加起来，便得到最后乘积 z。 
如果被乘数和乘数用定点小数表示，我们也会得到同样的结果。 
但是人们习惯的算法对机器并不完全适用。原因之一，机器通常只有 n 位长，两个 n
位数相乘，乘积可能为 2n 位。原因之二，只有两个操作数相加的加法器，难以胜任将 n 个
位积一次相加起来的运算。因此，在早期计算机中为了简化硬件结构，采用串行的 1 位乘
法方案，即多次执行“加法-移位”操作来实现。这种方法并不需要很多器件。然而串行方
法毕竟太慢，不能满足科学技术对高速乘法所提出的要求。 
由于乘法运算大约占全部算术运算的 1/3，因此采用高速乘法部件，无论从速度上来说
还是从效率上来说，都是十分必要的。自从大规模集成电路问世以来，高速的单元阵列乘
法器应运而生，出现了各种形式的流水式阵列乘法器，它们属于并行乘法器。鉴于串行乘
法器已被淘汰，下面只介绍并行乘法器。 
      
设有两个不带符号的二进制整数： 
A=am–1…a1a0 
B=bn–1…b1b0 

上述过程说明了在 m 位×n 位不带符号整数的阵列乘法中加法-移位操作的被加数矩
阵。每一个部分乘积项(位积)aibj 叫做一个被加数。这 m×n 个被加数｛aibj｜0≤i≤m–1 和
0≤j≤n–1｝可以用 m×n 个“与”门并行地产生，如图 2.4 的上半部分所示。显然，设计高
速并行乘法器的基本问题，就在于缩短被加数矩阵中每列所包含的 1 的加法时间。 
现以 5 位×5 位不带符号的阵列乘法器(m=n=5)为例来说明并行阵列乘法器的基本原
理。图 2.5 示出了 5 位×5 位阵列乘法器的逻辑电路图，其中 FA 是前面讲过的一位全加器，
FA 的斜线方向为进位输出，竖线方向为和输出，而所有被加数项的排列和前述 A×B=P 乘
法过程中的被加数矩阵相同。图中用虚线围住的阵列中最后一行构成了一个行波进位加法
器，其求和时间延迟为(n–1)2T+3T(异或门)。当然，为了缩短加法时间，最下一行的行波
进位加法器也可以用先行进位加法器来代替。 
 

这种乘法器要实现 n 位×n 位时，需要 n(n–1)个全加器和 n2 个与门。该乘法器的总的
乘法时间可以估算如下： 
令 Ta 为与门的传输延迟时间，Tf 为全加器(FA)的进位传输延迟时间，假定用 2 级“与
或”逻辑来实现 FA 的进位链功能，那么就有 
Ta=T，  Tf=2T 
从图 2.5 可见，最坏情况下的延迟途径，即是沿着矩阵 p4 垂直线和最下面的一行进位
及 p8 求和。参见图 2.3(b)，n 位×n 位不带符号的阵列乘法器总的乘法时间估算为 
 

对带符号的阵列乘法器的结构来说，按其所用的数的表示方法而有所不同。 
在介绍带符号的阵列乘法器基本原理以前，
我们先来看看算术运算部件设计中经常用到的
求补电路。图 2.6 示出了一个具有使能控制的二
进制对 2 求补器电路图，其逻辑表达式如下： 
C–1=0，  Ci=ai+Ci–1 
*
ia =aiECi –1，  0≤i≤n 
对 2 求补时，采用按位扫描技术来执行所需
要的求补操作。令 A=an…a1a0 是给定的(n+1)位
带符号的数，要求确定它的补码形式。进行求补的方法就是从数的最右端 a0 开始，由右向
左，直到找出第一个“1”，例如，ai=1，0≤i≤n。这样，ai 以右的每一个输入位，包括 ai
自己，都保持不变，而 ai 以左的每一个输入位都求反，即 1 变 0，0 变 1。鉴于此，横向链
式线路中的第 i 扫描级的输出 Ci 为 1 的条件是：第 i 级的输入位 ai=1，或者第 i 级链式输入
(来自右起前 i–1 级的链式输出)Ci–1=1。另外，最右端的起始链式输入 C–1 必须永远置成“0”。
当控制信号线 E 为“1”时，启动对 2 求补的操作；当控制信号线 E 为“0”时，输出将和
输入相等。显然，我们可以利用符号位来作为控制信号。 
例如，在一个 4 位的对 2 求补器中，如果输入数为 1010，那么输出数应是 0110，其中
从右算起的第 2 位，就是所遇到的第一个“1”的位置。用这种对 2 求补器来转换一个(n+1)
位带符号的数，所需的总时间延迟为 
 
tTC=n·2T+5T=(2n+5)T 
(2.24) 
其中每个扫描级需 2T 延迟，而 5T 则是由于“与”门和“异或”门引起的。 
现在让我们来讨论带符号的阵列乘法器。图 2.7 给出了(n+1)位×(n+1)位带求补器的
阵列乘法器逻辑方框图。通常，把包含这些求补级的乘法器又称为符号求补的阵列乘法器。
在这种逻辑结构中，共使用了三个求补器。其中两个算前求补器的作用是：将两个操作数 A
和 B 在被不带符号的乘法阵列(核心部件)相乘以前，先变成正整数。而算后求补器的作用
则是：当两个输入操作数的符号不一致时，把运算结果变换成带符号的数。 
设 A=anan–1…a1a0 和 B=bnbn–1…b1b0 均为用定点表示的(n+1)位带符号整数。由图 2.7 看
到，在必要的求补操作以后，A 和 B 的码值输送给 n 位×n 位不带符号的阵列乘法器，并由
此产生 2n 位乘积为 
A·B = P = p2n–1…p1p0 
p2n = anbn 
其中，p2n 为符号位。 
图 2.7 所示的带求补器的阵列乘法器既适用于原码乘法，也适用于间接的补码乘法。不
过在原码乘法中，算前求补和算后求补都不需要，因为输入数据都是立即可用的。而间接
的补码阵列乘法却需要使用三个求补器。为了完成所必需的求补与乘法操作，时间大约比
原码阵列乘法增加 1 倍。 

1. 可控加法/减法(CAS)单元 
和阵列乘法器相似，阵列除法器也是一种并行运算部件，采用大规模集成电路制造。
与早期的串行除法器相比，阵列除法器不仅所需要的控制线路少，而且能够提供令人满意
的高速运算速度。 

阵列除法器有多种形式，如加减交替阵列除法器、补码阵列除法器等。这里以加减交
替阵列除法器为例，来说明这类除法器的组成原理。 
在介绍加减交替阵列除法器以前，首先介绍可控加法/减法(CAS)单元，因为它将被采
用于下面所介绍的除法流水逻辑阵列中。图 2.8(a)示出了可控加法/减法(CAS)单元的逻辑
电路图，它有四个输出端和四个输入端。当输入线 P=0 时，CAS 做加法运算；当 P=1 时，
CAS 做减法运算。 
 


在这两个表达式中，每一个都能用一个三级组合逻辑电路(包括反相器)来实现。因此
每一个基本的 CAS 单元的延迟时间为 3T 单位。后面将利用这个单元的延迟时间来精确确
定除法时间。 
2. 加减交替的阵列除法器 
现在转入讨论加减交替的阵列除法器，假定所有被处理的数都是正小数。 
在加减交替的除法阵列中，每一行所执行的操作究竟是加法还是减法，取决于前一行
输出的符号与被除数的符号是否一致。当出现不够减时，部分余数相对于被除数来说要改
变符号。这时应该产生一个商位“0”，除数首先沿对角线右移，然后加到下一行的部分余
数上。当部分余数不改变它的符号时，即产生商位“1”，下一行的操作应该是减法。 
图 2.8(b)示出了 4 位除 4 位的加减交替阵列除法器的逻辑原理图。其中 
                       被除数  x = 0.x6x5x4x3x2x1(双倍长) 
                       除数    y = 0.y3y2y1 
                       商数    q = 0.q3q2q1 
                       余数    r = 0.00r6r5r4r3 
                       字长    n+1 = 4 
由图 2.8 看出，该阵列除法器是用一个可控加法/减法(CAS)单元所组成的流水阵列来
实现的。推广到一般情况，一个(n+1)位÷(n+1)位的加减交替除法阵列由(n+1)2 个 CAS 单
元组成，其中两个操作数(被除数与除数)都是正的。 
单元之间的互联是用 n=3 的阵列来表示的。这里被除数 x 是一个 6 位的小数(双倍长
数值)： 
x = 0.x6x5x4x3x2x1 
它是由顶部一行和最右边的对角线上的垂直输入线来提供的。 
除数 y 是一个 3 位的小数： 
y = 0.y3y2y1 
它沿对角线方向进入这个阵列。这是因为，在除法中将所需要的部分余数保持固定，而将
除数沿对角线右移。 
商 q 是一个 3 位的小数： 

q = 0.q3q2q1 
它在阵列的左边产生。 
余数 r 是一个 6 位的小数： 
r = 0.00r6r5r4r3 
它在阵列的最下一行产生。 
最上面一行所执行的初始操作一定是减法。因此最上面一行的控制线 P 固定置成“1”。
减法是用 2 的补码运算来实现的，这时右端各 CAS 单元上的反馈线用作初始的进位输入，
即最低位上加“1”。每一行最左边的单元的进位输出决定着商的数值。将当前的商反馈到
下一行，我们就能确定下一行的操作。由于进位输出信号指示出当前的部分余数的符号，
因此，正如前面所述，它决定下一行的操作将进行加法还是减法。 
对加减交替阵列除法器来说，在进行运算时，沿着每一行都有进位(或借位)传播，同
时所有行在它们的进位链上都是串行连接。而每个 CAS 单元的延迟时间为 3T 单元，因此，
对一个 2n 位除以 n 位的加减交替阵列除法器来说，单元的数量为(n+1)2，考虑最大情况下
的信号延迟，其除法执行时间为 
 
td=3(n+1)2T 
(2.27) 
其中 n 为尾数数位。 

运算器是数据的加工处理部件，是 CPU 的重要组成部分。尽管各种计算机的运算器结
构可能有这样或那样的不同，但是它们的最基本的结构中必须有算术/逻辑运算单元、数据
缓冲寄存器、通用寄存器、多路转换器和数据总线等逻辑构件。 

计算机中除了进行加、减、乘、除等基本算术运算以外，还可对两个或一个逻辑数进
行逻辑运算。所谓逻辑数，是指不带符号的二进制数。利用逻辑运算可以进行两个数的比
较，或者从某个数中选取某几位等操作。例如，当利用计算机做过程控制时，我们可以利
用逻辑运算对一组输入的开关量做出判断，以确定哪些开关是闭合的，哪些开关是断开的。
总之，在非数值应用的广大领域中，逻辑运算是非常有用的。 
计算机中的逻辑运算，主要是指逻辑非、逻辑加、逻辑乘、逻辑异等四种基本运算。 

逻辑非也称求反。对某数进行逻辑非运算，就是按位求它的反，常用变量上方加一横
来表示。 
设一个数 x 表示成： 
x = x0x1x2…xn 
对 x 求逻辑非，则有 
x = z = z0z1z2…zn 
zi =
ix ，    i = 0,1,2,…,n 

我们在 2.2.4 节中曾介绍由一位全加器(FA)构成的行波进位加法器，它可以实现补码数
的加法运算和减法运算。但是这种加法/减法器存在两个问题。一是由于串行进位，它的运
算时间很长。假如加法器由 n 位全加器构成，每一位的进位延迟时间为 20ns，那么最坏情
况下，进位信号从最低位传递到最高位而最后输出稳定，至少需要 n×20ns，这在高速计算
中显然是不利的。二是就行波进位加法器本身来说，它只能完成加法和减法两种操作而不
能完成逻辑操作。为此，本节先介绍多功能算术/逻辑运算单元(ALU)，它不仅具有多种算
术运算和逻辑运算的功能，而且具有先行进位逻辑，从而能实现高速运算。 
1. 基本思想 
2.2.4 节中给出一位全加器(FA)的逻辑表达式
(2.20)为 
Fi=AiBiCi 
Ci+1=AiBi+BiCi+CiAi 
式中，Fi 是第 i 位的和数，Ai、Bi 是第 i 位的被加
数和加数，Ci 是第 i 位的进位输入，Ci+1 为第 i 位
的进位输出。 
为了将全加器的功能进行扩展以完成多种算
术/逻辑运算，我们先不将输入 Ai、Bi 和下一位的
进位数 Ci 直接进行全加，而是将 Ai 和 Bi 先组合成
由控制参数 S0、S1、S2、S3 控制的组合函数 Xi 和
Yi(图 2.9)，然后再将 Xi、Yi 和下一位进位数通过全加器进行全加。这样，不同的控制参数
可以得到不同的组合函数，因而能够实现多种算术运算和逻辑运算。 
因此，一位算术/逻辑运算单元的逻辑表达式修改为 
Fi=XiYiCn+i 
 
Cn+i+1=XiYi+YiCn+i+Cn+iXi 
(2.28) 
式(2.28)中进位下标用 n+i 代替原来一位全加器中的 i，i 代表集成在一片电路上的
ALU 的二进制位数，对于 4 位一片的 ALU，i=0, 1, 2, 3。n 代表若干片 ALU 组成更大字长
的运算器时每片电路的进位输入，如当 4 片组成 16 位字长的运算器时，n=0, 4, 8, 12。 
2. 逻辑表达式 
控制参数 S0、S1、S2、S3 分别控制输入 Ai 和 Bi，产生 Yi 和 Xi 的函数。其中 Yi 是受 S0、
S1 控制的 Ai 和 Bi 的组合函数，而 Xi 是受 S2、S3 控制的 Ai 和 Bi 的组合函数，其函数关系如
表 2.3 所示。 
根据上面所列的函数关系，即可列出 Xi 和 Yi 的逻辑表达式 
 
这样，对一片 ALU 来说，可有三个进位输出。其中 G 称为进位发生输出，P 称为进位传送
输出。在电路中多加这两个进位输出的目的，是为了便于实现多片(组)ALU 之间的先行进
位，为此还需一个配合电路，称为先行进位发生器(CLA)，将在下面介绍。 
Cn+4 是本片(组)的最后进位输出。逻辑表达式表明，这是一个先行进位逻辑。换句话
说，第 0 位的进位输入 Cn 可以直接传送到最高进位位上去，因而可以实现高速运算。 
图 2.10 示出了用正逻辑表示的 4 位算术/逻辑运算单元(ALU)的逻辑电路图，它是根据
上面的原始推导公式用 TTL 电路实现的。这个器件的商业标号为 74181ALU。 
3. 算术逻辑运算的实现 
图 2.11 中除了 S0～S3 四个控制端外，还有一个控制端 M，它用来控制 ALU 进行算术
运算还是进行逻辑运算。 
当 M=0 时，M 对进位信号没有任何影响。此时 Fi 不仅与本位的被操作数 Yi 和操作数
Xi 有关，而且与向本位的进位值 Cn+i 有关，因此 M=0 时，进行算术操作。 
当 M=1 时，封锁了各位的进位输出，即 Cn+i=0，因此各位的运算结果 Fi 仅与 Yi 和 Xi
有关，故 M=1 时，进行逻辑操作。 

表 2.4 列出了 74181ALU 的运算功能表，它有两种工作方式。对正逻辑操作数来说，算
术运算称高电平操作，逻辑运算称正逻辑操作(即高电平为“1”，低电平为“0”)。对于负
逻辑操作数来说，正好相反。由于 S0～S3 有 16 种状态组合，因此对正逻辑输入与输出而言，
有 16 种算术运算功能和 16 种逻辑运算功能。同样，对于负逻辑输入与输出而言，也有 16
种算术运算功能和 16 种逻辑运算功能。表 2.4 中只列出了正逻辑的 16 种算术运算和 16 种
逻辑运算功能。 
注意，表 2.4 中算术运算操作是用补码表示法来表示的。其中“加”是指算术加，运算
时要考虑进位，而符号“+”是指“逻辑加”。其次，减法是用补码方法进行的，其中数的
反码是内部产生的，而结果输出“A 减 B 减 1”，因此做减法时须在最末位产生一个强迫进
位(加 1)，以便产生“A 减 B”的结果。另外，“A=B”输出端可指示两个数相等，因此它
与其他 ALU 的“A=B”输出端按“与”逻辑连接后，可以检测两个数的相等条件。 

4. 两级先行进位的 ALU 
前面说过，74181ALU 设置了 P 和 G 两个本组先行进位输出端。如果将四片 74181 的 P，
G 输出端送入到 74182 先行进位部件(CLA)，又可实现第二级的先行进位，即组与组之间
的先行进位。 
假设 4 片(组)74181 的先行进位输出依次为 P0，G0，P1，G1，P2，G2，P3，G3，那么参
考式(2.29)的进位逻辑表达式，先行进位部件 74182CLA 所提供的进位逻辑关系如下： 
           Cn+x=G0+P0Cn 
           Cn+y=G1+P1Cn+x=G1+G0P1+P0P1Cn 
           Cn+z=G2+P2Cn+y=G2+G1P2+G0P1P2+P0P1P2Cn                     (2.31) 
           Cn+4=G3+P3Cn+z=G3+G2P3+G1P2P3+G0P1P2P3+P0P1P2P3Cn=G*+P*Cn 
其中 
P*=P0P1P2P3 
                          G*=G3+G2P3+G1P2P3+G0P1P2P3 
根据以上表达式，用 TTL 器件实现的成组先行进位部件 74182 的逻辑电路图如图 2.11
所示。其中 G*称为成组进位发生输出，P*称为成组进位传送输出。 
下面介绍如何用若干个 74181ALU 位片，与配套的 74182 先行进位部件 CLA 在一起，
构成一个全字长的 ALU。 

图 2.12 示出了用两个 16 位全先行进位部件级联组成的 32 位 ALU 逻辑方框图。在这个
电路中使用了八个 74181ALU 和两个 74182CLA 器件。很显然，对一个 16 位来说，CLA 部
件构成了第二级的先行进位逻辑，即实现四个小组(位片)之间的先行进位，从而使全字长
ALU 的运算时间大大缩短。 
 


由于计算机内部的主要工作过程是信息传送和加工的过程，因此在机器内部各部件之
间的数据传送非常频繁。为了减少内部数据传送线并便于控制，通常将一些寄存器之间数
据传送的通路加以归并，组成总线结构，使不同来源的信息在此传输线上分时传送。 
根据总线所处的位置，总线分为内部总线和外部总线两类。内部总线是指 CPU 内各部
件的连线，而外部总线是指系统总线，即 CPU 与存储器、I/O 系统之间的连线。本节只讨
论内部总线。 
按总线的逻辑结构来说，总线可分为单向
传送总线和双向传送总线。所谓单向总线，就
是信息只能向一个方向传送。所谓双向总线，
就是信息可以向两个方向传送，既可以发送数
据，也可以接收数据。 
图 2.13 是带有缓冲驱动器的 4 位双向数据
总线。其中所用的基本电路就是三态逻辑电路。
当“发送”信号有效时，数据从左向右传送。
反之，当“接收”信号有效时，数据从右向左
传送。这种类型的缓冲器通常根据它们如何使
用而叫做总线扩展器、总线驱动器、总线接收
器等等。 

运算器包括 ALU、阵列乘除器、寄存器、多路开关、三态缓冲器、数据总线等逻辑部
件。运算器的设计，主要是围绕着 ALU 和寄存器同数据总线之间如何传送操作数和运算结
果而进行的。在决定方案时，需要考虑数据传送的方便性和操作速度，在微型机和单片机
 

中还要考虑在硅片上制作总线的工艺。计算机的运算器大体有如下三种结构形式： 
1. 单总线结构的运算器 
单总线结构的运算器如图 2.14(a)所示。由于所有部件都接到同一总线上，所以数据可
以在任何两个寄存器之间，或者在任一个寄存器和 ALU 之间传送。如果具有阵列乘法器或
除法器，那么它们所处的位置应与 ALU 相当。 
 

对这种结构的运算器来说，在同一时间内，只能有一个操作数放在单总线上。为了把
两个操作数输入到 ALU，需要分两次来做，而且还需要 A、B 两个缓冲寄存器。例如，执
行一个加法操作时，第一个操作数先放入 A 缓冲寄存器，然后再把第二个操作数放入 B 缓
冲寄存器。只有两个操作数同时出现在 ALU 的两个输入端，ALU 才执行加法。当加法结果
出现在单总线上时，由于输入数已保存在缓冲寄存器中，它并不会打扰输入数。然后，再
由第三个传送动作，以便把加法的“和”选通到目的寄存器中。由此可见，这种结构的主
要缺点是操作速度较慢。 
虽然在这种结构中输入数据和操作结果需要三次串行的选通操作，但它并不会对每种
指令都增加很多执行时间。例如，如果有一个输入数是从存储器来的，且运算结果又送回
存储器，那么限制数据传送速度的主要因素是存储器访问时间。只有在对全都是 CPU 寄存
器中的两个操作数进行操作时，单总线结构的运算器才会造成一定的时间损失。但是由于
它只控制一条总线，故控制电路比较简单。 
2. 双总线结构的运算器 
双总线结构的运算器如图 2.14(b)所示。在这种结构中，两个操作数同时加到 ALU 进
行运算，只需要一次操作控制，而且马上就可以得到运算结果。图中，两条总线各自把其


数据送至 ALU 的输入端。专用寄存器分成两组，它们分别与一条总线交换数据。这样，通
用寄存器中的数就可以进入到任一组专用寄存器中去，从而使数据传送更为灵活。 
ALU 的输出不能直接加到总线上去。这是因为，当形成操作结果的输出时，两条总线
都被输入数占据，因而必须在 ALU 输出端设置缓冲寄存器。为此，操作的控制要分两步来
完成：第一步，在 ALU 的两个输入端输入操作数，形成结果并送入缓冲寄存器；第二步，
把结果送入目的寄存器。假如在总线 1、2 和 ALU 输入端之间再各加一个输入缓冲寄存器，
并把两个输入数先放至这两个缓冲寄存器，那么，ALU 输出端就可以直接把操作结果送至
总线 1 或总线 2 上去。 
3. 三总线结构的运算器 
三总线结构的运算器如图 2.14(c)所示。在三总线结构中，ALU 的两个输入端分别由两
条总线供给，而 ALU 的输出则与第三条总线相连。这样，算术逻辑操作就可以在一步的控
制之内完成。由于 ALU 本身有时间延迟，所以打入输出结果的选通脉冲必须考虑到这个延
迟。另外，设置了一个总线旁路器(桥)。如果一个操作数不需要修改，而直接从总线 2 传
送到总线 3，那么可以通过总线旁路器把数据传出；如果一个操作数传送时需要修改，那么
就借助于 ALU。三总线运算器的特点是操作时间快。 
思考题  你能评价三种运算器的结构特点吗？ 

设有两个浮点数 x 和 y，它们分别为 
x = 2Ex·Mx 
y = 2Ey·My 
其中，Ex 和 Ey 分别为数 x 和 y 的阶码，Mx 和 My 分别为数 x 和 y 的尾数。 
两浮点数进行加法和减法的运算规则是 
 
z = x±y = (Mx2Ex－Ey±My)2Ey，    Ex≤Ey 
(2.32) 
完成浮点加减运算的操作过程大体分为四步：第一步，0 操作数检查；第二步，比较阶
码大小并完成对阶；第三步，尾数进行加或减运算；第四步，结果规格化并进行舍入处理。
图 2.15 示出浮点加减运算的操作流程。 
1)0 操作数检查 
浮点加减运算过程比定点运算过程复杂。如果判知两个操作数 x 或 y 中有一个数为 0，
即可得知运算结果而没有必要再进行后续的一系列操作，以节省运算时间。0 操作数检查步
骤则用来完成这一功能。 
2)比较阶码大小并完成对阶 
两浮点数进行加减，首先要看两数的阶码是否相同，即小数点位置是否对齐。若两数
阶码相同，表示小数点是对齐的，就可以进行尾数的加减运算。反之，若两数阶码不同，
表示小数点位置没有对齐，此时必须使两数的阶码相同，这个过程叫做对阶。 

 
要对阶，首先应求出两数阶码 Ex 和 Ey 之差，即 
ΔE = Ex–Ey 
若 ΔE=0，表示两数阶码相等，即 Ex=Ey；若 ΔE>0，表示 Ex>Ey；若 ΔE<0，表示 Ex<Ey。 
当 Ex≠Ey 时，要通过尾数的移动以改变 Ex 或 Ey，使之相等。原则上，既可以通过 Mx
移位以改变 Ex 来达到 Ex=Ey，也可以通过 My 移位以改变 Ey 来实现 Ex=Ey。但是，由于浮点
表示的数多是规格化的，尾数左移会引起最高有效位的丢失，造成很大误差。而尾数右移
虽引起最低有效位的丢失，但造成的误差较小。因此，对阶操作规定使尾数右移，尾数右
移后使阶码作相应增加，其数值保持不变。很显然，一个增加后的阶码与另一个阶码相等，
所增加的阶码一定是小阶。因此在对阶时，总是使小阶向大阶看齐，即小阶的尾数向右移
位(相当于小数点左移)，每右移一位，其阶码加 1，直到两数的阶码相等为止，右移的位数
等于阶差 ΔE。 
3)尾数加减运算 
对阶结束后，即可进行尾数的加减运算。不论是加法运算还是减法运算，都按加法进
行操作，其方法与定点加减运算完全一样。 
4)结果规格化 
在浮点加减运算时，尾数求和的结果也可以得到 01.φ…φ或 10.φ…φ，即两符号位
不相等，这在定点加减运算中称为溢出，是不允许的。但在浮点运算中，它表明尾数求和
结果的绝对值大于 1，向左破坏了规格化。此时将尾数运算结果右移以实现规格化表示，称
为向右规格化，即尾数右移 1 位，阶码加 1。当尾数不是 1.M 时须向左规格化。 
5)舍入处理 
在对阶或向右规格化时，尾数要向右移位，这样，被右移的尾数的低位部分会被丢掉，
从而造成一定误差，因此要进行舍入处理。 
在 IEEE754 标准中，舍入处理提供了四种可选办法。 
就近舍入  其实质就是通常所说的“四舍五入”。例如，尾数超出规定的 23 位的多余
位数字是 10010，多余位的值超过规定的最低有效位值的一半，故最低有效位应增 1。若多

余的 5 位是 01111，则简单的截尾即可。对多余的 5 位 10000 这种特殊情况：若最低有效位
现为 0，则截尾；若最低有效位现为 1，则向上进 1 位使其变为 0。 
朝 0 舍入  即朝数轴原点方向舍入，就是简单的截尾。无论尾数是正数还是负数，截
尾都使取值的绝对值比原值的绝对值小。这种方法容易导致误差累积。 
朝+∞舍入  对正数来说，只要多余位不全为 0 则向最低有效位进 1；对负数来说，则
是简单的截尾。 
朝–∞舍入  处理方法正好与朝+∞舍入情况相反。对正数来说，则是简单截尾；对负
数来说，只要多余位不全为 0，则向最低有效位进 1。 
6)溢出处理 
浮点数的溢出是以其阶码溢出表现出来的。在加、减运算过程中要检查是否产生了溢
出：若阶码正常，加(减)运算正常结束；若阶码溢出，则要进行相应的处理。另外对尾数
的溢出也需要处理。图 2.16 表示了 32 位格式浮点数的溢出概念。 
 

阶码上溢  超过了阶码可能表示的最大值的正指数值，一般将其认为是+∞和–∞。 
阶码下溢  超过了阶码可能表示的最小值的负指数值，一般将其认为是 0。 
尾数上溢  两个同符号尾数相加产生了最高位向上的进位，要将尾数右移，阶码增 1
来重新对齐。 
尾数下溢  在将尾数右移时，尾数的最低有效位从尾数域右端流出，要进行舍入处理。 
图 2.17 示出浮点加减法运算电路的硬件框图。首先，两个加数的指数部分通过 ALU1
相减，从而判断出哪一个的指数较大、大多少。指数相减所得的差值控制着下面的三个多
路开关；按从左到右的顺序，这三个多路开关分别挑选出较大的指数、较小加数的有效数
位以及较大加数的有效数位。较小加数的有效数位部分右移适当的位数，然后再在 ALU2
中与另一个加数的有效数位部分相加。接下来对结果进行规格化，这是通过将求得的和向
左或向右做适当的移位操作(同时相应地增大或减小和的指数部分)来实现的。最后对结果
进行舍入，舍入之后可能还需要再次进行规格化，才能得到最终的结果。 

2. 浮点乘、除法运算步骤 
浮点数的乘除运算大体分为六步：第一步，0 操作数检查，如果被除数 x 为 0，则商为
0，如果除数 y 为 0，则商为∞；第二步，阶码加/减操作；第三步，尾数乘/除操作；第四步，
结果规格化；第五步，舍入处理；第六步，确定积的符号。 
1)浮点数的阶码运算 
浮点乘除法中，对阶码的运算有+1、–1、两阶码求和、两阶码求差四种，运算时还必
须检查结果是否溢出。 
2)尾数处理 
浮点加减法对结果的规格化及舍入处理也适用于浮点乘除法。 
第一种简单办法是，无条件地丢掉正常尾数最低位之后的全部数值。这种办法被称为
截断处理，其好处是处理简单，缺点是影响结果的精度。 
第二种简单办法是，运算过程中保留右移中移出的若干高位的值，最后再按某种规则
用这些位上的值修正尾数。这种处理方法被称为舍入处理。 
当尾数用原码表示时，舍入规则比较简单。最简便的方法，是只要尾数最低位为 1，或
移出的几位中有为 1 的数值位，就使最低位的值为 1。另一种是 0 舍 1 入法，即当丢失的最
高位的值为 1 时，把这个 1 加到最低数值位上进行修正。 

第 3 步，规格化与溢出检查。 
乘积的有效数位已经规格化，由于指数–3 处在 127≥–3≥–126，故没有发生上溢和下
溢。 
第 4 步，舍入到 4 位有效数字。这一步无需做任何操作，结果仍为 
1.1102×2–3 
第 5 步，确定积的符号：x 和 y 符号相反，乘积为负数，即 
(x×y)浮= –1.1102×2–3 
十进制浮点数验证： 
–1.110×2–3= –0.0011102= –0.001112= –0.2187510 
0.5×(–0.4375)= –0.21875 

1. 流水线原理 
计算机的流水处理过程同工厂中的流水装配线类似。为了实现流水，首先必须把输入
的任务分割为一系列子任务，使各子任务能在流水线的各个阶段并发地执行。将任务连续
不断地输入流水线，从而实现了子任务级的并行。因此流水处理大幅度地改善了计算机的
系统性能，是在计算机上实现时间并行性的一种非常经济的方法。 
在流水线中，原则上要求各个阶段的处理时间都相同。若某一阶段的处理时间较长，
势必造成其他阶段的空转等待。因此对子任务的划分，是决定流水线性能的一个关键因素，
它取决于操作部分的效率、所期望的处理速度，以及成本价格等。 
假设作业 T 被分成 k 个子任务，可表达为 
T=｛T1, T2, …, Tk｝ 
各个子任务之间有一定的优先关系：若 i＜j，则必须在 Ti 完成以后，Tj 才能开始工作。具
有这种线性优先关系的流水线称为线性流水线。线性流水线的硬件基本结构如图 2.19 所示。 

图 2.19 中，处理一个子任务的过程为过程段(Si)。线性流水线由一系列串联的过程段
组成，各个过程之间设有高速的缓冲寄存器(L)，以暂时保存上一过程子任务处理的结果。
在一个统一的时钟(C)控制下，数据从一个过程段流向相邻的过程段。 
设过程段 Si 所需的时间为 τi，缓冲寄存器的延时为  ，线性流水线的时钟周期定义为 
 
τ=max｛
i ｝+  =τm+   
(2.35) 
故流水线处理的频率为 f =1/τ。 
在流水线处理中，当任务饱满时，任务源源不断地输入流水线，不论有多少级过程段，
每隔一个时钟周期都能输出一个任务。从理论上说，一个具有 k 级过程段的流水线处理 n
个任务需要的时钟周期数为 
 
Tk=k＋(n–1) 
(2.36) 
其中 k 个时钟周期用于处理第一个任务。k 个周期后，流水线被装满，剩余的 n–1 个任务只
需 n–1 个周期就能完成。如果用非流水线的硬件来处理这 n 个任务，时间上只能串行进行，
则所需时钟周期数为 
 
TL=n·k 
(2.37) 

当 n  k 时，Ck→k。这就是说，理论上 k 级线性流水线处理几乎可以提高 k 倍速度。 
思考题  你能举出工厂中的生产流水线实例吗？ 
2. 流水线浮点加法器 
从图 2.15 看出，浮点加减法由 0 操作数检查、对阶操作、尾数操作、结果规格化及舍
入处理共 4 步完成，因此流水线浮点加法器可由 4 个过程段组成。图 2.20 仅示出了除 0 操
作数检查之外的 3 段流水线浮点加法器框图。 
 

假设有两个规格化的浮点数 
x=1.1000×22，  y=1.1100×24 
第 2 章  运算方法和运算器       59 
当此二数相加时，因 x 具有较小的阶码，首先应使它向 Y 对阶，从而得到 x=0.0110×24，
然后尾数再相加，即 
 0. 0 1 1 0×24 
+  1. 1 1 0 0×24 
  1 0 . 0 0 1 0×24 
其结果要进行规格化，将尾数向右移 1 位，阶码增 1。即规格化的结果为 1.0001×25。 
在图 2.20 所示的流水线浮点加法器框图中，标出了上述例子在每一个过程段和锁存器
L 中保存的流水运算结果值。 

本 章 小 结 
一个定点数由符号位和数值域两部分组成。按小数点位置不同，定点数有纯小数和纯
整数两种表示方法。 
按 IEEE754 标准，一个浮点数由符号位 S、阶码 E、尾数 M 三个域组成。其中阶码 E
的值等于指数的真值 e 加上一个固定偏移值。 
为了使计算机能直接处理十进制形式的数据，采用两种表示形式：①字符串形式，主要用
在非数值计算的应用领域；②压缩的十进制数串形式，用于直接完成十进制数的算术运算。 
数的真值变成机器码时有四种表示方法：原码表示法、反码表示法、补码表示法、移
码表示法。其中移码主要用于表示浮点数的阶码 E，以利于比较两个指数的大小和对阶操作。 
字符信息属于符号数据，是处理非数值领域的问题。国际上采用的字符系统是七单位
的 ASCII 码。 
直接使用西文标准键盘输入汉字，进行处理，并显示打印汉字，是一项重大成就。为
此要解决汉字的输入编码、汉字内码、字模码等三种不同用途的编码。 
为运算器构造的简单性，运算方法中算术运算通常采用补码加、减法，原码乘除法或
补码乘除法。为了运算器的高速性和控制的简单性，采用了先行进位、阵列乘除法、流水
线等并行技术措施。运算方法和运算器是本章的重点。 
定点运算器和浮点运算器的结构复杂程度有所不同。早期微型机中浮点运算器放在
CPU 芯片外，随着高密度集成电路技术的发展，现已移至 CPU 内部。 

在冯·诺依曼体系结构中，存储器是计算机系统的五大组成部件之一。早期的计算机
系统只有单一的存储器存放为数不多的数据和指令。但是，随着软件复杂度的提高、多媒
体技术和网络技术的普及，对存储器容量的要求不断提高。而微电子技术的发展又为大幅
度提升存储器的存储密度提供了可能性，这反过来又促使对存储器容量的需求进一步提升。 
由于存储器的价格相对较高，而且在整机成本中占有较大的比例，因而从性能价格比
的角度不能通过简单配置更大容量的存储器满足用户的需求。为此，必须使用某种策略解
决成本和性能之间的矛盾。这一策略就是存储器分层，即利用不同容量、成本、功耗和速
度的多种存储器构成有机结合的多级存储系统。构成多级存储系统的依据就是程序的局部
性原理。 
1. 程序的局部性原理 
统计表明，无论是访问存取指令还是存储数据，在一个较短的时间间隔内，程序所访
问的存储器地址在很大比例上集中在存储器地址空间的很小范围内。这种在某一段时间内
频繁访问某一局部的存储器地址空间，而对此范围以外的地址空间则很少访问的现象称为
程序的局部性原理。 
程序的局部性可以从两个角度分析。 
(1)时间局部性：最近被访问的信息很可能还要被访问。 
(2)空间局部性：最近被访问的信息邻近地址的信息也可能被访问。 
2. 多级存储系统的组成 
在 CPU 内部有少量的寄存器可以存储正在执行的指令或者正在参加运算的数据，寄存

器的访问速度非常快，但数量较少。正在执行的程序的指令和数据存储在 CPU 能直接访问
的存储器中，这种狭义的存储器就是内存储器。内存储器速度高、容量小、价格高，由半
导体器件构成。 
为了扩大存储容量，在内存储器之外增加容量更大但访问速度稍慢的外存储器(外存)，
或者称为辅助存储器(辅存)。相对而言，外存储器速度低、容量大、价格便宜，可以由磁盘
存储器、光盘存储器等非半导体器件或者固态半
导体存储器构成。CPU 不能直接访问外存储器，
外存储器的信息必须调入内存储器后才能由CPU
处理。 
内存储器和外存储器构成了两级存储系统。 
随着半导体技术的发展，CPU 和内存储器的
工作速度都在提高，但 CPU 速度提高得更快，而
更高速度的内存储器价格非常高。为此，人们在
常规内存储器与 CPU 之间增加了速度更高但容
量更小的半导体高速缓冲存储器，即 cache，用
于存放常规内存中正在使用的一些信息块的副
本。常规的内存被称为主存。这样，内存储器就
分为 cache 和主存两部分，由此构成了三级存储
系统，其结构如图 3.1 所示。 
在三级存储系统中，cache 用于提升访问
速度，以便使存取速度和 CPU 的运算速度相匹
配；外存储器则主要解决存储容量问题，以满
足计算机的大容量存储要求；主存储器介于
cache 与外存之间，要求选取适当的存储容量
和存取周期，使它能容纳系统的核心软件和较
多的用户程序。多级存储系统的出发点是提高
存储系统的性能/价格比，让整个存储系统在速
度上接近 cache，而在容量和价格上接近外存。 
对性能要求更高的系统还可以将 cache 分
成一级(L1)cache 和二级(L2)cache，甚至更多
级。对存储容量要求更多的系统还可以用磁带等可更换介质实现无容量限制的存储。如图 3.2
所示，在由 cache、主存、磁盘和磁带构成的多级存储体系中，存储容量、存储密度逐级提
升，访问速度和价格逐级降低，构成金字塔式的存储结构。 

构成存储器的存储介质，目前主要采用半导体器件和磁性材料。一个双稳态半导体电
路或一个 CMOS 晶体管或磁性材料的存储元，均可以存储一位二进制代码。这个二进制代
码位是存储器中最小的存储单位，称为存储位元。由若干存储位元组成一个存储单元，然
后再由许多存储单元组成一个存储器。 
 


根据存储材料的性能及使用方法不同，存储器有各种不同的分类方法。 
存储介质  作为存储介质的基本要求，必须有两个明显区别的物理状态，分别用来表
示二进制的代码 0 和 1。另外，存储器的存取速度又取决于这种物理状态的改变速度。目前
使用的存储介质主要是半导体器件、磁性材料和光存储器。用半导体器件组成的存储器称
为半导体存储器。用磁性材料做成的存储器称为磁表面存储器，如磁盘存储器和磁带存储
器。光存储器是指只读光盘或者读写光盘。磁盘和光盘的共同特点是存储容量大，储存的
信息不易丢失。 
存取方式   如果存储器中任何存储单元的内容都能被随机存取，且存取时间和存储单
元的物理位置无关，这种存储器称为随机存取存储器。如果存储器只能按某种顺序来存取，
也就是说存取时间和存储单元的物理位置有关，这种存储器称为顺序存取存储器。如磁带
存储器就是顺序存取存储器，它的存取周期较长。磁盘存储器则是半顺序(直接)存取存储器，
沿磁道方向顺序存取，垂直半径方向随机存取。 
读写功能  有些半导体存储器存储的内容在存储器工作过程中只能读出而不能写入，
这种半导体存储器称为只读存储器(ROM)。在存储器工作过程中既能读出又能写入的半导
体存储器称为读写存储器或随机存取存储器(RAM)。 
信息易失性   断电后信息消失的存储器，称为易失性存储器。断电后仍能保存信息的
存储器，称为非易失性存储器。半导体存储器中，RAM 是易失性存储器，一旦掉电，储存
信息全部丢失。而 ROM 是非易失性存储器。磁性材料做成的存储器是非易失性存储器。 
与 CPU 的耦合程度  根据存储器在计算机系统中所处的位置，可分为内部存储器和外
部存储器。内存又可分为主存和高速缓冲存储器。 
计算机系统的主存习惯上被分为 RAM 和 ROM 两类。RAM 用来储存当前运行的程序
和数据，并可以在程序运行过程中反复更改其内容。而 ROM 常用来储存不变或基本不变的
程序和数据(如监控程序、引导加载程序及常数表格等)。RAM 可以根据信息储存方法分为
静态 RAM(SRAM)和动态 RAM(DRAM)。SRAM 是用半导体管的“导通”或“截止”来记
忆的，只要不掉电，储存信息就不会丢失。而 DRAM 的信息是用电荷储存在电容上，随着
时间的推移，电荷会逐渐漏掉，储存信息也会丢失，因此要周期性地对其“刷新”。根据
工艺和特性的不同，只读存储器又分为掩膜 ROM、一次可编程 ROM(PROM)和可擦除
PROM(EPROM)，后者又分为紫外线擦除EPROM(UV-EPROM)、电擦除EPROM(EEPROM
或 E2PROM)和闪速(Flash)只读存储器。 
存放一个机器字的存储单元，通常称为字存储单元，相应的单元地址称为字地址。而
存放一字节的单元，称为字节存储单元，相应的地址称为字节地址。编址方式是存储器地
址的组织方式，一般在设计处理器时就已经确定了。如果计算机中编址的最小单位是字存
储单元，则该计算机称为按字编址的计算机。如果计算机中编址的最小单位是字节，则该
计算机称为按字节编址的计算机。一个机器字可以包含数字节，所以一个存储单元也可占
用数个能够单独编址的字节地址。例如，一个 16 位二进制的字存储单元包含两字节，当采
用字节编址方式时，该字占两字节地址。 
当一个存储字的字长高于八位时，就存在一个存储字内部的多字节的排列顺序问题，
第 3 章  存 储 系 统       65 
其排列方式称为端模式。大端(big-endian)模式将一个字的高有效字节放在内存的低地址端，
低有效字节放在内存的高地址端，而小端(little-endian)模式则将一个字的低有效字节放在内
存的低地址端，高有效字节放在内存的高地址端。如图 3.3 所示，如果一个 32 位数
(0A0B0C0D)16 按照大端模式存放在内存中，则最低地址存放最高有效字节(0A)16，最高地
址存放最低有效字节(0D)16；而按照小端模式存放时，字节顺序刚好相反。 

常用的英特尔 64 系列处理器采用小端模式。ARM 系列的处理器一般默认采用小端模
式，但可以随时在程序中进行大小端模式的切换。 
许多处理器允许在 CPU 每次访问存储器时动态确定读写的信息量大小，相应地选择不
同的寻址宽度。例如，字寻址每次访存读写一个存储字，半字寻址每次访存读写半个存储
字，字节寻址则每次访存读写一字节。 

内存储器的性能指标主要是存储容量和存取速度，后者通常可以用存取时间、存储周
期和存储器带宽描述。 
存储容量  存储容量指一个存储器中可存储的信息比特数，常用比特数(bit)或字节数
(B)来表示，也可使用 KB、MB、GB、TB 等单位。其中 1KB=210B，1MB=220B，1GB=230B，
1TB=240B。为了清楚地表示其组织结构，存储容量也可表示为：存储字数(存储单元数)×
存储字长(每单元的比特数)。例如，1Mbit 容量的存储器可以组织成 1M×1bit，也可组织
成 128K×8bit，或者 512K×4bit。 
存取时间  又称存储器访问时间，是从存储器接收到读/写命令开始到信息被读出或写
入完成所需的时间，取决于存储介质的物理特性和寻址部件的结构。 
存储周期(存取周期)  是在存储器连续读写过程中一次完整的存取操作所需的时间，
即 CPU 连续两次访问存储器的最小间隔时间。通常，存储周期略大于存取时间。 
存储器带宽(数据传送速率，频宽)  单位时间里存储器所存取的信息量，通常以位/秒
或字节/秒做度量单位。若系统的总线宽度为 W 位，则带宽=W/存取周期(bit/s)。 

静态随机存取存储器(SRAM)的优点是存取速度快，但存储密度和容量不如 DRAM 大。

本节先讨论 SRAM。 

图 3.4 表示基本的静态存储元阵列。SRAM 用锁存器(触发器)作为存储元。只要直流供
电电源一直加在这个记忆电路上，它就无限期地保持记忆的 1 状态或 0 状态。如果电源断
电，则存储的数据(1 或 0)就会丢失。 
 

任何一个 SRAM，都有三组信号线与外部打交道：①地址线，本例中有 6 条，即 A0、
A1、A2、A3、A4、A5，它指定了存储器的容量是 26=64 个存储单元。②数据线，本例中有
4 条，即 I/O0、I/O1、I/O2 和 I/O3，说明存储器的字长是 4 位，因此存储位元的总数是 64×4=256。
③控制线，本例中 R/W 控制线，它指定了对存储器进行读( R/W 高电平)，还是进行写( R/W
低电平)。注意，读写操作不会同时发生。 
地址译码器输出有 64 条选择线，称为行线，其作用是打开每个存储位元的输入与非门。
当外部输入数据为 1 时，锁存器便记忆了 1；当外部输入数据为 0 时，锁存器便记忆了 0。 

目前的 SRAM 芯片采用双译码方式，以便组织更大的存储容量。这种译码方式的实质
是采用了二级译码：将地址分成 x 向、y 向两部分，第一级进行 x 向(行译码)和 y 向(列译
码)的独立译码，然后在存储阵列中完成第二级的交叉译码。而数据宽度有 1 位、4 位、8
位，甚至有更多的字节。 
图 3.5(a)表示存储容量为 32K×8 位的 SRAM 逻辑结构图。它的地址线共 15 条，其中 x
方向 8 条(A0～A7)，经行译码输出 256 行，y 方向 7 条(A8～A14)，经列译码输出 128 列，
存储阵列为三维结构，即 256 行×128 列×8 位。双向数据线有 8 条，即 I/O0～I/O7。向 SRAM

写入时，8 个输入缓冲器被打开，而 8 个输出缓冲器被关闭，因而 8 条 I/O 数据线上的数据
写入存储阵列中。从 SRAM 读出时，8 个输出缓冲器被打开，8 个输入缓冲器被关闭，读出
的数据送到 8 条 I/O 数据线上。 

控制信号中 CS是片选信号，CS有效时(低电平)，门 G1、G2 均被打开。OE 为读出使
能信号， OE 有效时(低电平)，门 G2 开启，当写命令 WE =1 时(高电平)，门 G1 关闭，存
储器进行读操作。写操作时， WE =0，门 G1 开启，门 G2 关闭。注意，门 G1 和 G2 是互锁
的，一个开启时另一个必定关闭，这样保证了读时不写，写时不读。图 3.5(b)为 32K×8 位
SRAM 的逻辑图。 

如图 3.6 所示，读/写周期波形图精确地反映了 SRAM 工作的时间关系。我们把握住地
址线、控制线、数据线三组信号线何时有效，就能很容易看懂这个周期时序图。 
在读周期中，地址线先有效，以便进行地址译码，选中存储单元。为了读出数据，片
选信号 CS 和读出使能信号 OE 也必须有效(由高电平变为低电平)。从地址有效开始经
tAQ(读出)时间，数据总线 I/O 上出现了有效的读出数据。之后 CS、 OE 信号恢复高电平，
tRC 以后才允许地址总线发生改变。tRC 时间即为读周期时间。 
在写周期中，也是地址线先有效，接着片选信号 CS有效，写命令 WE 有效(低电平)，
此时数据总线 I/O 上必须置写入数据，在 tWD时间段将数据写入存储器。之后撤销写命令 WE
和 CS。为了写入可靠，I/O 线的写入数据要有维持时间 thD，CS的维持时间也比读周期长。
tWC 时间称为写周期时间。为了控制方便，一般取 tRC=tWC，通常称为存取周期。 

当单个存储器芯片的容量不能满足系统要求时，需要把多片存储器芯片组合起来，组
成更大容量的存储器。所需芯片数为：d=设计要求的存储器容量/已知芯片存储容量。 
1. 位扩展 
若给定的芯片的字数(地指数)符合要求，但位数较短，不满足设计要求的存储器字长，
则需要进行位扩展，让多片给定芯片并行工作。三组信号线中，地址线和控制线公用而数
据线单独分开连接。 

3. 字位扩展 
若给定的芯片的字数和位数均不符合要求，则需要先进行位扩展，再进行字扩展。 

SRAM 的存储元是一个触发器，它具有两个稳定的状态。而动态随机存取存储器
(DRAM)简化了每个存储元的结构，因而 DRAM 的存储密度很高，通常用作计算机的主存储器。 

作为开关使用，而所存储的信息 1 或 0 则是由电
容器上的电荷量来体现——当电容器充满电荷
时，代表存储了 1，当电容器放电没有电荷时，
代表存储了 0。 
写 1 到存储元时，输出缓冲器关闭、刷新缓
冲器关闭，输入缓冲器打开( R/W 为低)，输入
数据 DIN=1 送到存储元位线上，而行选线为高，
打开 MOS 管，于是位线上的高电平给电容器充
电，表示存储了 1。写 0 到存储元时，输出缓冲
器和刷新缓冲器关闭，输入缓冲器打开，输入数
据 DIN=0 送到存储元位线上；行选线为高，打开
MOS 管，于是电容上的电荷通过 MOS 管和位线放电，表示存储了 0。 
从存储元读出时，输入缓冲器和刷新缓冲器关闭，输出缓冲器/读放打开( R/W 为高)。
行选线为高，打开 MOS 管，若当前存储的信息为 1，则电容上所存储的 1 送到位线上，通
过输出缓冲器/读出放大器发送到 DOUT，即 DOUT=1。 
读出过程破坏了电容上存储的信息，所以要把信息重新写入，即刷新。读出的过程中
可以完成刷新。读出 1 后，输入缓冲器关闭，刷新缓冲器打开，输出缓冲器/读放打开，读
出的数据 DOUT=1 又经刷新缓冲器送到位线上，再经 MOS 管写到电容上，存储元重写 1。 
注意，输入缓冲器与输出缓冲器总是互锁的。这是因为读操作和写操作是互斥的，不
会同时发生。 
与 SRAM 相比，DRAM 的存储元所需元件更少，所以存储密度更高。但是 DRAM 的
附属电路比较复杂，访问时需要额外的电路和操作支持。 

与 SRAM 不同的是，图中增加了行地址锁存器和列地址锁存器。由于 DRAM 容量很大，
地址线的数目相当多，为减少芯片引脚的数量，将地址分为行、列两部分分时传送。存储
容量为 1M 字，共需 20 位地址线。此芯片地址引脚的数量为 10 位，先传送行地址码 A0～
A9，由行选通信号 RAS 打入到行地址锁存器；然后传送列地址码 A10～A19，由列选通信号
CAS 打入到列地址锁存器。片选信号的功能也由增加的 RAS 和 CAS 信号实现。 
3.3.3  DRAM 读/写时序 
图 3.11(a)为 DRAM 的读周期波形。当地址线上行地址有效后，用行选通信号 RAS 打
入行地址锁存器；接着地址线上传送列地址，并用列选通信号 CAS 打入列地址锁存器。此
时经行、列地址译码，读/写命令 R/W =1(高电平表示读)，数据线上便有输出数据。 
图 3.11(b)为 DRAM 的写周期波形。此时读/写命令 R/W =0(低电平表示写)，在此期间，
数据线上必须送入欲写入的数据 DIN(1 或 0)。 
从图中可以看出，每个读周期或写周期是从行选通信号 RAS 下降沿开始，到下一个
RAS 信号的下降沿为止的时间，也就是连续两个读/写周期的时间间隔。通常为控制方便，
读周期和写周期时间相等。 
 

DRAM 存储位元是基于电容器上的电荷量存储信息的，DRAM 的读操作是破坏性的，
读操作会使电容器上的电荷流失，因而读出后必须刷新。而未读写的存储元也要定期刷新，
因为电荷量会逐渐泄漏而减少。从外部看，刷新操作与读操作类似，只是刷新时无须送出
数据，并且可以将一行的所有存储元同时刷新。 
现代的 DRAM 芯片通常会在一次读操作之后自动地刷新选中行中的所有存储位元。但
是读操作出现的时间不是固定的，因此必须对 DRAM 进行周期性的刷新，以保持其记忆的
信息不丢失。 
早期的 DRAM 需要由存储器控制器从外部向 DRAM 芯片送入刷新行地址并启动一次
刷新，而现代的 DRAM 都支持自动刷新功能，由芯片内部提供刷新行地址。故图 3.10 中增
加了刷新计数器(刷新行地址发生器)和相应的控制电路。刷新计数器的宽度等于行地址锁
存器的宽度。由于自动刷新不需要给出列地址，而行地址由片内刷新计数器自动生成，故

可利用 CAS 信号先于 RAS 信号有效来启动一次刷新操作，此时地址线上的地址无效。 
当前主流的 DRAM 器件的刷新间隔时间(刷新周期)为 64ms。周期性的刷新操作是与
读/写操作交替进行的，所以通过 2 选 1 多路开关选择刷新行地址或正常读/写的行地址。常
用的刷新策略有集中式刷新和分散式刷新两种。例如，对于一片有 8192 行、刷新周期为 64ms
的 DRAM 内存来说： 
在集中式刷新策略中，每一个刷新周期中集中一段时间对 DRAM 的所有行进行刷新。
64ms 的刷新周期时间可以分为两部分：前一段时间进行正常的读/写操作；后一段时间作为
集中刷新操作时间，连续刷新 8192 行。由于刷新操作的优先级高，刷新操作时正常的读/
写操作被暂停，数据线输出被封锁。等所有行刷新结束后，又开始正常的读/写周期。由于
在刷新的过程中不允许读/写操作，集中式刷新策略存在“死时间”。 
在分散式刷新策略中，每一行的刷新操作被均匀地分配到刷新周期时间内。由于 64ms
除以 8192 约等于 7.8μs，所以 DRAM 每隔 7.8μs 刷新一行。 
由于 CPU 送出的访存地址要分行地址和列地址两次送入 DRAM 芯片，并且 DRAM 还
要实现定时刷新，因而使用 DRAM 做系统主存的系统通常要通过存储器控制器或者 DRAM
控制器产生 DRAM 访问和刷新时序控制与地址信号。 

DRAM 存储密度高，大容量 DRAM 价格相对较低，因而适合用作系统主存。但是，
DRAM 的访问速度相对要低一些，提升其访问速度是改进系统性能的重要途径之一。近年
来，人们在传统 DRAM 的基础上应用了诸多技术提升其访问速度。 
突发(Burst，猝发)访问指的是在存储器同一行中对相邻的存储单元进行连续访问的方
式，突发长度可以从几字节到数千字节不等。由于访问地址是连续的，因而只需要向存储
器发送一次访问地址。突发访问时先激活一行，然后按照一定的顺序依次发出列选择信号，
访问相应的目标存储单元。突发方式可以消除地址建立时间及第一次存取之后的行、列线
的预充电时间。在第一次存取后，一系列数据能够快速地输出。 
通过支持突发模式、快速页模式和扩展数据输出等方式，可以允许重复存取 DRAM 存
储矩阵的行缓冲区而无须增加另外的行存取时间，以提升等效数据访问速度。 

传统的 DRAM 是异步工作的，处理器送地址和控制信号到存储器后，等待存储器进行
内部操作(选择行线和列线读出信号放大并送输出缓冲器等)，处理器需等待一段存取延时
时间后才能存取数据，因而必须消耗较长时间以确保数据传输可靠，影响了系统性能。在
DRAM 接口上增加时钟信号则可以降低存储器芯片与控制器同步的开销，优化 DRAM 与
CPU 之间的接口，这是同步 DRAM(SDRAM)的最主要改进。 
1. SDRAM 的特征 
SDRAM 存储体的存储单元电路仍然是标准的 DRAM 存储体结构，只是在工艺上进行
了改进，如功耗更低、集成度更高等。与传统的 DRAM 相比，SDRAM 在存储体的组织方
式和对外操作上作了重大改进。图 3.12 显示了 SDRAM 的逻辑结构，其主要特性如下。 

同步操作  处理器访问 SDRAM 时，SDRAM 的所有输入信号均在系统时钟 CLK 的
上升沿被存储器内部电路锁定；SDRAM 的所有输出信号均在系统时钟 CLK 的上升沿被
输出。这样做的目的是使 SDRAM 的操作在系统时钟 CLK 的控制下，与系统的高速操作
严格同步进行。CKE 为时钟使能信号，只有该信号有效时，时钟输入才能作用于 SDRAM
芯片。 
多存储体配置  为了进一步提高存取速度和减少内部操作冲突，SDRAM 的存储体被拆
分为多个相互独立的存储体(bank)。这种内部组织结构可以支持流水线方式的并行操作。
各存储体可同时和独立工作，也可选择顺序工作或交替工作。例如，当一个存储体正在刷
新时，另一个存储体可以进行正常的读写操作，从而提高存取速度。通常由片内地址线的
最高一位或若干位选择存储体。 
命令控制  传统的异步 DRAM 是根据控制信号的电平组合选择工作方式的，而
SDRAM 将一组控制信号的电平编码组合为“命令”。例如， RAS 、 CAS 、 WE 、 CS以
及特定地址线的不同组合分别代表激活存储体(active，所有存储体在读/写之前都必须被激
活)、读、写、预充等不同的命令。 
模式寄存器  在 SDRAM 加电后必须先对模式寄存器进行设置，控制 SDRAM 工作在
不同的操作模式下。在模式寄存器中可以设置CAS 延迟、突发类型、突发长度和测试模式等。 
表 3.1 比较了传统异步 DRAM 和 SDRAM 的功能差异。 
 
2. SDRAM 的控制方式 
下面以读周期为例说明 SDRAM 的控制方式。图 3.13 对比了异步 DRAM 和 SDRAM 的
读操作时序。 
 

在 SDRAM 操作过程中，所有的动作都是以时钟信号为依据的。 
在 T1 时钟的上升沿(图中<1>处)，激活命令 ACT 和行地址首先被锁存，表明开始一次
存取操作。而异步 DRAM 并没有时钟信号，对应的动作为 RAS 有效(低)。 
第 3 章  存 储 系 统       75 
在 T3 时钟的上升沿(<2>处)，读命令和列地址被锁存，表明当前是一次读操作。对应
异步 DRAM 的 CAS 有效(低)。 
此后，SDRAM 将完成内部准备操作，并在 2 个时钟周期之后送出数据。从列地址被锁
存到数据有效输出的时间间隔称为 CAS 延迟 CL，图中 CL = 2。 
在 T6 时钟的上升沿(<3>处)，控制器送入预充命令。对应异步 DRAM 的 RAS 和 CAS 无
效(变高)。 
SDRAM 的操作时序都是确定的，在系统时钟控制下，CPU 向 SDRAM 送出地址和控
制命令后，需等待事先确定好的一定数量的时钟周期。在此期间，SDRAM 完成读或写的内
部操作(如行列选择、地址译码、数据读出或写入、数据放大等)，处理器则可照常安全地
执行其他任务，不必单纯等待，以此来提高系统效率。  
3. SDRAM 的命令 
图 3.14 给出了 SDRAM 读和写命令操作的时序，可以看出 SDRAM 的命令发送方式。 
 

在 T1 时钟的上升沿，控制器发出存储体 A 的激活命令。存储体激活命令通过在时钟上
升沿发出下列信号组合发出： CS=0、 RAS =0、 CAS =1、 WE =1，地址线 A11=0 选择存储
体 A。 
在 T3 时钟的上升沿，控制器发出存储体 A 的读命令。读命令通过在时钟上升沿发出下
列信号组合发出： CS=0、 RAS =1、 CAS =0、 WE =1。 
经过 2 个时钟周期的内部操作，数据在 T5 时钟的上升沿开始送出。此例中，突发长度
BL=4，故在随后的四个时钟周期内分别送出一个数据字。 
在 T9 时钟的上升沿，DQ 输出被设置为高阻状态。在 T10 时钟的上升沿，控制器发出
存储体 A 的写命令。写命令通过在时钟上升沿发出下列信号组合发出： CS=0、 RAS =1、
      
CAS =0、 WE =0。 
在 T14 时钟的上升沿开始下一次读操作。 
3.3.7  双倍数据率 SDRAM(DDR SDRAM) 
在 SDRAM 出现之后，又出现了双数据率的 DDR SDRAM，故后来将单数据率的
SDRAM 称为 SDR SDRAM。狭义的 SDRAM 仅指 SDR SDRAM。 
DDR SDRAM 沿袭了 SDR SDRAM 内存的制造体系，又能够提供更快的操作速度和更
低的功耗。SDRAM 仅能在时钟上升沿传输数据，而 DDR SDRAM 的最大特点便是在时钟
的上升沿和下降沿都能传输数据。  
双倍数据率结构本质上是一个
2n 预取结构，如图 3.15 所示。内部
总线宽度是外部总线宽度的两倍，从
存储矩阵到 I/O 缓冲区每个时钟周
期传输 2n 比特数据，从 I/O 缓冲区
到数据总线则在时钟触发沿的上、下
沿都能进行数据传输。 
差分时钟也是 DDR SDRAM 的
一个必要设计。由于数据是在 CK 的
上下沿触发，因而传输周期缩短了一
半，因此必须要保证传输周期的稳定，以确保数据的正确传输。因为温度和电阻特性的改
变等原因，CK 上下沿间距可能发生变化，此时与其反相的 CK 就起到触发时钟校准的作用。 
在第一代 DDR SDRAM 出现之后，相继又出现了 DDR2、DDR3 和 DDR4 等 SDRAM
技术。这些技术的主要改进点在于提升存储矩阵输出的数据率。例如，DDR2 技术采用 4n
预取结构，将数据总线的时钟频率提升至内部传输频率的 2 倍，从而使外部总线数据率比
DDR SDRAM 提升一倍。类似地，DDR3 SDRAM 则采用 8n 预取结构。 
DDR4 SDRAM 仍然采用 8n 预取，但是允许使用两个或者四个存储体组，每个存储体
组都有独立的激活、读取、写入和刷新操作。因此，如果设计两个独立的存储体组，相当
于将内存预取值提高到了 16n；如果是四个独立的存储体组，则预取值提高到了 32n。 
3.3.8  DRAM 读/写校验 
DRAM 通常用作主存储器，其读/写操作的正确性与可靠性至关重要。为此除了正常的
数据位宽度，还增加了附加位，用于读/写操作正确性校验。增加的附加位也要同数据位一
起写入 DRAM 中保存。显然这增加了 DRAM 的位成本。 
图 3.16 表示 DRAM 正确性校验的概念示意图。最简单的校验是奇偶校验，除了数据位
外只需增加 1 位附加位(k=1)，进行奇校验或偶校验即可。图中的 F 部分为进行奇校验或偶
校验的异或运算电路，如果存储器读/写正确，那么写入存储器前与读出存储器后两部分的
F 运算结果应该一致，否则给出错误信号。奇偶校验只能检出 1 位错误，不能纠正错误。但
是由于技术简单，成本较低，所以在早期主存储器中常常使用。 

为了能纠正错误，纠错码设成 k 位，如果数据字为 m 位，则实际存储的字长为 m+k 位。
最简单的纠错码是汉明码。为了能纠错，汉明码要求的校验位长度如表 3.2 所示。 

由表 3.2 可见，数据位 8 位时，附加的校验码要求为 4 位，存储器字长变成 12 位，位
成本增加了 50%。但是数据位 64 位时，校验码要求为 7 位，字长变成 71 位，位成本只增
加约 11%。 
在汉明码校验中，F 电路的运算要比奇偶校验复杂，如 8 位数据时，F 部分有 4 位，所
以有 4 个异或运算表达式。纠正器电路部分则是新、老校验位比较时形成的故障字，它也
通过异或运算形成。 
3.3.9  CDRAM 
CDRAM(Cached DRAM)是一种附带高速缓冲存储器的动态存储器，它是在常规的
DRAM 芯片封装内又集成了一个小容量 SRAM 作为高速缓冲存储器，从而使 DRAM 芯片
的访问速度得到显著提升。 
1. CDRAM 芯片的结构 
图 3.17 为 1M×4 位 CDRAM 芯片的结构框图。一片 512×4 位的 SRAM 构成 cache，保
存最近访问的一行数据。另外增加了最后读出行地址锁存器和行地址比较器，如果后续访问
的数据就在最近访问过的行中，则可直接从 cache 中读出数据而无须访问 DRAM 存储体。 
访问 1M×4 位的 CDRAM 芯片需 20 位内存地址。在行选通信号 RAS 作用下，内存地
址的高 11 位行地址经 A0～A10 地址线输入，并被锁存在行地址锁存器和最后读出行地址锁
存器中。在 DRAM 阵列的 2048 行中，此地址指定行的全部 512×4 位数据被读取到 SRAM
中暂存。然后，内存地址的低 9 位列地址在列选通信号 CAS 有效时经 A0～A10 地址线输入，
并被锁存到列地址锁存器中。如果是首次读操作，则在读命令信号有效时，SRAM 中 512
个 4 位组内的某一个 4 位组被此列地址选中，经 D0～D3 送出芯片。 

下一次读取时，输入的 11 位行地址立即与最后读出行地址锁存器的内容进行比较：若
相符则 SRAM 命中，由输入的列地址直接从 SRAM 中选择某一 4 位组送出即可；只在比较
不相符时，才需要再次访问 DRAM 阵列，更新 SRAM 和最后读出行地址锁存器的内容，并
送出指定的 4 位组。 
CDRAM 在常规 DRAM 的基础上增加了一点成本，但是有几个明显的优点。一是突发
操作的速度高，如果连续访问的地址的高 11 位相同(属于同一行地址)，那么只需连续变动
9 位列地址就能从 SRAM 中快速读出数据。二是在 SRAM 读出期间可同时对 DRAM 阵列
进行刷新。三是允许在写操作完成的同时启动同一行的读操作，因为芯片内的数据输出路
径(由 SRAM 到 I/O)与数据输入路径(由 I/O 到读出放大和列写选择)是分开的。 
2. CDRAM 存储模块 
8 片容量为 1M×4 位的 CDRAM 芯片可以组成 1M×32 位(4MB)的存储模块，如图 3.18 所示。 
 
8 个芯片共用片选信号Sel 、行选通信号 RAS 、刷新信号 Ref 和地址输入信号 A0～A10。
每两片 1M×4 位的 CDRAM 芯片的列选通信号 CAS 接在一起，形成一个 1M×8 位(1MB)的
片组。4 个片组组合成一个 1M×32 位的存储模块。 
数据总线宽度为 32 位。为了 CPU 与存储器交换数据方便，每次访存时可以由 CPU 选
择实现字存取(32 位)、半字存取(高 16 位或低 16 位)或字节存取(任意 8 位)。由于存储器
按字节编址，因而每次访存数据总线上可能会传输 4 个地址(字)、2 个地址(半字)或者 1
个地址(字节)的数据。为此，CPU 送出的地址线中最低两位的 A1 和 A0 并不送出，而是送
出由连续四字节组成的一个 32 位字的字地址(字地址的最低两位固定为 00)，外加 4 个字节

低位地址排列关系。该模块按小端模式安排
地址，故每个字的最低有效字节(与数据线
D7~D0 对应)安排在低地址(最低两位地址为
00)，而每个字的最高有效字节(与数据线
D31~D24 对应)安排在高地址(最低两位地址
为 11)。4 个片组的列选通信号
3
CAS ～
0
CAS 分别与 CPU 送出的 4 个字节允许信号
3
BE ～
0
BE 相对应。 
当某模块被选中并完成 32 位存取时，此模块的 8 个 CDRAM 芯片同时动作。8 个 4 位
数据 I/O 端口 D3～D0 同时与 32 位数据总线交换数据，完成一次 32 位的存取。此 32 位存储
字的模块内地址对应系统存储地址中的 A21～A2。这 20 位地址分为 11 位的行地址和 9 位的
列地址，分别在 RAS 和 CAS 有效时同时输入到 8 个芯片的地址引脚端。 
系统存储地址的最高两位 A23、A22 作为模块选择地址，译码输出可以分别驱动 4 个这
样的 4MB 模块的Sel 信号。即系统可配置 4 个这样的模块，存储器容量达到 16MB。 
上述存储模块具有高速的突发存取能力。如果连续访问的数据块的高 13 位地址相同(同
一行)，那么只是第一个存储字需要一个完整的存取周期(如 6 个总线时钟周期)，而后续存
储字的存取因内容已在 SRAM 中，故存取周期大为缩短(如 2 个总线时钟周期)。这样，读
取 4 个 32 位字只使用了 6-2-2-2 个总线时钟周期。存储器写入也有相似的速度提高。 

半导体只读存储器(ROM)最大的特点是其非易失性，其访问速度比 RAM 稍低，可以
按地址随机访问并在线执行程序，因而在计算机中用于储存固件、引导加载程序、监控程
序及不变或很少改变的数据。“只读”的意思是在其工作时只能读出，不能写入。早期的
只读存储器中存储的原始数据必须在其工作以前离线存入芯片中，现代的许多只读存储器

都能够支持在线更新其存储的内容，但更新操作与 RAM 的写操作完全不同，不仅控制复杂，
而且耗时长，更新所需的时间比 ROM 的读操作时间长很多，可以重复更新的次数也相对较
少。因此，这种更新 ROM 存储内容的操作实际上不是“写入”，而是编程。 
狭义的 ROM 仅指掩模 ROM。掩模 ROM 实际上是一个存储内容固定的 ROM，由半导
体生产厂家根据用户提供的信息代码在生产过程中将信息存入芯片内。一旦 ROM 芯片做
成，就不能改变其中的存储内容。掩模 ROM 一般用于存储广泛使用的具有标准功能的程序
或数据，或用户定做的具有特殊功能的程序或数据，当然这些程序或数据均转换成二进制
码。由于成本很低，在没有更新需求的大批量的应用中适宜使用掩模 ROM。 
为了让芯片的用户能更新 ROM 中存储的内容，可以使用可编程 ROM(PROM)。一次
性编程 ROM、紫外线擦除 PROM、E2PROM 和闪速存储器均可由用户编程。 
狭义的 PROM 即指一次性编程 ROM(OTP ROM)，只能编程一次。紫外线擦除 PROM 
(UV-EPROM)通常简称 EPROM，器件的上方有一个石英窗口，通常将其从电路板上的插座
上拔下后，在专用的擦除器中使用一定波长的紫外线照射数分钟至十余分钟即可擦除存储
的信息，且可在通用编程器或电路板上实现多次编程和验证。 
电可擦 PROM(EEPROM，E2PROM)采用电擦除，因而不需要离线擦除，且擦除速度
快，可以单字节编程和擦除(或者擦除块尺寸很小)，使用更方便。E2PROM 通常容量比较
小，单位成本高，但可重复擦除的次数多，一般在一百万次左右，一般用于存储偶尔需要
更新的系统配置信息、系统参数、加密保护数据或历史信息等。许多单片机或者简单电子
模块往往会内置 E2PROM 芯片。常规并行总线 E2PROM 访问速度快，接口简单，但引脚数
量多，封装尺寸较大，故近年来更多地被串行 E2PROM(SEEPROM)或闪存取代。常见的串
行 E2PROM 支持 SPI、I²C、Microwire 或 1-Wire 等 1 至 4 线的串行总线，芯片封装只需 8
个或者更少的引脚。 
闪速(Flash)存储器(闪存)也属于电可擦、可在线编程的非易失性只读存储器。Flash 意
为擦除速度高，其擦除时间远高于传统的 UV-EPROM 和 E2PROM。闪速存储器的存储密度
高，工作速度快，擦除块尺寸较大(通常在 512 字节以上)，可擦除的次数相对较少(NOR
闪存为一万到十万次)。闪存自 20 世纪 80 年代末出现以来，应用已经极为普遍，在很多情
况下取代了传统的其他 ROM。 
根据存储元工作原理和制造工艺的不同，闪存可以分为 NOR 技术、DINOR 技术、AND
技术和 NAND 技术等不同类别。其中应用最普遍的是 NOR 技术和 NAND 技术。 
NOR 闪存通常被称为线性闪存，最早由英特尔和 AMD 等公司生产。相对于其他技术
的闪存，其特点是：可以像 SRAM 和传统 ROM 那样随机读出任意地址的内容，读出速度
高；存储在其中的指令代码可以直接在线执行；可以对单字节或单字进行编程(在重新编程
之前需要先进行擦除操作)；以区块(sector)或芯片为单位执行擦除操作；拥有独立的数据
线和地址线，因而接口方式与 SRAM 相似；信息存储的可靠性高。因此，NOR 闪存更适用
于擦除和编程操作较少而直接执行代码的场合，尤其是纯代码存储应用。由于擦除和编程
速度相对较慢，且区块尺寸较大，NOR 闪存不太适合纯数据存储和文件存储等应用场景。
NOR 闪存可在线“写入”数据，又具有 ROM 的非易失性，因而可以取代全部的 UV-EPROM
和大部分的 E2PROM，存储监控程序、引导加载程序等不经常改变的程序代码，或者储存
在掉电时需要保持的系统配置等不常改变的数据。 
 
NAND 闪存通常被称为非线性闪存，最早由三星和东芝等公司生产。相对于其他技术
的闪存，其特点是：每次读出以页(page)为单位，因而属于非随机访问的存储器；存储在
其中的指令代码不能够直接在线执行；以页为单位进行编程操作；以数十页组成的块(block)
为单位进行擦除操作；快速编程和快速擦除；数据线、地址线和控制线复用在同一组总线
信号上，故其接口方式与传统 ROM 不同；位成本低、位密度高；由于工艺的限制，存在较
高的比特错误率，通常需要软件处理坏块。NAND 闪存不能够随机读出，所以一般不能直
接用于存储在线执行的代码；但是由于其存储密度高，价格低，通常容量较大，增加 NAND
闪存控制器后也可用于程序代码存储。由于 NAND 闪存有 10 倍于 NOR 闪存的可擦除次数，
故适用于大容量存储设备，如存储卡、优盘(USB 闪存盘)、固态盘等应用。由于 NAND 闪
存的数据存取无机械运动，可靠性高，存取速度快，体积小巧，因而已经部分取代了磁介
质辅存。 

1. NOR 闪存的外部接口与逻辑结构 
下面以飞索公司(现赛普拉斯公司)生产的 S29AL016J 系列 16Mbit 闪存为例，说明 NOR
闪存的接口和工作方式。 
图 3.20(a)给出了其外部引脚。该芯片有两种工作模式：字模式组织成 1M×16bit，需要
A19~A0 共 20 位地址，DQ15～DQ0 共 16 位数据线；字节模式组织成 2M×8bit，需要 A19~A –1
共 21 位地址，DQ7～DQ0 共 8 位数据线。引脚 BYTE#(#代表低电平有效)为低时选择字节
模式，为高时选择字模式。此外，CE#为片选信号线，OE#为输出允许线，WE#为写使能信
号。闪存芯片内部需要有状态机支持其操作，复位信号 RESET#可以让其通过硬件复位恢复
到初始状态。由于闪存经常存放系统上电引导程序，为了防止误操作或其他原因导致存储
的信息被删除，WP#信号为低电平时可以让芯片处于写保护状态。为了获取闪存内部的工
作状态，可以读取 RY/BY#(Ready/Busy#)信号的电平：高表示芯片准备好接收新的命令，
低表示芯片内部正忙于处理上一操作。 

从图 3.20(b)的逻辑结构图可以看出，闪存芯片的核心仍然是存储矩阵。该芯片由单一
3.3V 电源供电，内部集成了编程电压发生器和擦除电压发生器，无需外接高电压电源。与
传统只读存储器不同，闪存可以通过命令寄存器接收外部命令。而且，闪存内部有状态机，
需要有状态控制逻辑，并且通过定时器给出内部操作定时。 
2. NOR 闪存的区块划分 
S29AL016J 是一种区块(sector)式闪存，外部按 1M×16bit 或 2M×8bit 组织，内部组织
为 35 个区块。表 3.4 给出了底部引导区版本的区块地址表，其低地址区通常存放系统引导

程序和一些参数，因而区块尺寸较小并且可以附加特定的写保护措施，前 4 个区块大小分别为
16KB、8KB、8KB、32KB。SA4～SA34 大小均为 64KB。每个存储单元的地址由高位的区块
地址(A12以上)和低位的区块内偏移地址两个字段组成，两个字段的长度与区块尺寸相关。 
表 3.4  S29AL016J 的区块地址表(底部引导区) 

 
3. NOR 闪存的总线操作与工作方式 
表 3.5 给出了 S29AL016J 的部分总线操作。NOR 闪存的外部接口信号线与 SRAM 类似，

但除了读出和编程写入这些常规的 PROM 操作外，NOR 闪存还具有内部控制寄存器和状态
寄存器，可以通过“命令写”和“状态读”操作进行灵活的控制。为了在保持与传统 ROM
兼容的情况下实现更多新功能，闪存内部通过状态机控制其操作状态。 

*WP#=0 时，最外区块保持保护状态；WP#=1 时，最外区块的保护状态由先前的保护/去保护状态决定。 
RESET#信号为低时为硬件复位。上电或复位之后，芯片内部的状态机使器件自动进入
“读存储矩阵”操作状态。在该状态下，NOR 闪存的读出操作与传统 ROM 芯片相同，只
需给出片选信号和一定的地址并使读信号(输出允许)线有效即可。因而其读操作与传统
ROM 完全兼容。 
如果需要执行传统 ROM 不支持的其他操作，需要执行特定的命令序列，使 NOR 闪存
转入其他状态，进行芯片擦除、区块擦除、编程写入、软件数据保护或者读标识码等操作。 
为防止状态机的误动作，闪存的各种命令是以“向特定地址写入特定内容的命令序
列”方式定义的。命令寄存器本身并不占据单独的存储器片内地址，而是通过特定的地址
和特定的数据组合给出不同的命令。表 3.5 中的“写”操作是指总线上的写入操作，并非直
接写入存储矩阵，而是写命令寄存器的“写周期”操作。不同命令通常要占用长短不一的若
干个总线写周期。在每一次命令操作之后，可以查询状态寄存器，以使 CPU 能够了解命令的
执行情况。 
不同厂商生产的芯片支持的命令序列不同，常见的有 AMD/Fujitsu 的标准命令集和
Intel/Sharp 的扩展命令集。表 3.6 给出了字模式下 S29AL016J 使用的标准命令集中的部分命
令。表中地址和数据均为十六进制。其中的地址是指芯片地址线上应该给出的地址模式， 

形式上为片内偏移地址，但并非向存储矩阵的相应单元写入，而是与其他地址和数据模式
组合代表特定命令。 
例如，芯片擦除命令将所有存储元擦除到存储 1 的状态。当芯片在连续的 6 个总线写
周期中依次从其地址线/数据线上接收到 555/AA、2AA/55、555/80、555/AA、2AA/55 和 555/10
这组信息时，将会把内部状态机转到“整片擦除”状态，并启动整片擦除操作。区块擦除
操作与此类似，但最后一个写周期需给出欲擦除的区块的地址，且数据线送入 30，芯片收
到此命令后将启动该区块的擦除操作。 
编程命令需要四个写总线周期，依次送入 555/AA、2AA/55、555/A0 和欲编程地址/欲
编程数据后，芯片将转入“编程”状态。 
需要注意的是，无论是擦除操作还是编程操作都不是能在接到命令后立即完成的，闪
存收到擦除或编程命令后需要执行内嵌擦除/编程算法进行费时的内部复杂操作才有可能完
成操作任务。在闪存完成上一命令之前，不能接收新的命令。 
为了让 CPU 知晓闪存的内部操作是否完成，芯片支持多种编程/擦除状态判定方法。例
如，通过通用 I/O 引脚读取 RY/BY#信号的电平可以获知闪存是处于“准备好”状态还是
“忙”状态。 
还有一种常用的判定编程和写入的状态的
方法称为 data# polling，如图 3.21 所示。在发
出编程或擦除命令之后，对欲编程的存储单元地
址或者欲擦除的任意存储单元的地址 VA 发出读
命令，并检查数据线返回的状态值。设欲向该地
址编程的数据的第七位为 D7，若编程未完成时，
读 出 的 DQ7=
7
D ； 而 编 程 结 束 后 ， 读 出 的
DQ7=D7。擦除操作可以看作写全 1 的操作，故
擦除过程中，DQ7=0；擦除完成时，DQ7=1。 
闪存内部状态机设置了超时时间，以判断编
程或擦除是否因错误而超时。超时时，数据线上
的 DQ5 输出为 1，表示编程或擦除操作失败。在
超时之前，可以通过不断读取该地址的方式轮询
闪存的状态，直到 DQ7 翻转。由于在超时的瞬
间 DQ7 仍可能翻转，故可以在超时后最后读取
一次状态字，判断编程或者擦除操作是否失败。
编程或擦除失败后只能通过复位命令返回读存
储矩阵状态。 
从 NOR 闪存的编程和擦除方式可以看出闪
存与 RAM 的差异。闪存的存储单元在编程之前
需首先擦除；闪存发出编程命令也比 RAM 发出写命令复杂许多；闪存编程的速度远低于
RAM 的写入速度；闪存的读出速度也远低于 RAM。可见，虽然闪存具有非易失性并可在
线编程，但仍然属于 ROM，一般情况下闪存不能取代 RAM。 
 

CPU 和主存储器之间在速度上是不匹配的，这种情况成为限制高速计算机设计的主要
问题。为了提高 CPU 和主存之间的数据交换速率，可以在不同层次采用不同的技术加速存
储器访问速度： 
芯片技术  提高单个芯片的访问速度。可以选用更高速的半导体器件，或者改善存储
芯片内部结构和对外接口方式。例如，前述的突发传输技术、同步 DRAM 技术和 CDRAM
技术等。 
结构技术  为了解决存储器与 CPU 速度不匹配问题，需要改进存储器与 CPU 之间的
连接方式，加速 CPU 和存储器之间的有效传输。例如，采用并行技术的双口存储器甚至是
多口存储器，以及多体交叉存储器，都可以让 CPU 在一个周期中访问多个存储字。 
系统结构技术  这是从整个存储系统的角度采用分层存储结构解决访问速度问题。例
如，增加 cache，采用虚拟存储器等。 
本节讲授双端口存储器和多体交叉存储器，前者采用空间并行技术，后者采用时间并
行技术。 

早期的计算机系统以 CPU 为中心。机器内部各个部件之间的信息传递都受 CPU 控制，
I/O 设备与主存之间的信息交换也经过 CPU 的运算器。这种结构严重影响了 CPU 效能的发
挥，故以内存为中心的系统逐渐取代了以 CPU 为中心的结构。 
这种以内存为中心的结构要求不仅 CPU 可以访问主存，而且其他部件(如 I/O 设备)也
可不经 CPU 而直接与主存交换信息。这样，多个部件都可以与主存交换信息，使主存的访
问次数明显增多。而传统的存储器在任一时刻只能进行一个读或写操作，不能被多个部件
同时访问。为了进一步扩展主存的信息交换能力，提出了多口存储器结构。 
1. 双端口存储器的逻辑结构 
双端口存储器由于同一个存储器具有两组相互独立的读写控制电路而得名。由于进行
并行的独立操作，因而是一种高速工作的存储器，在科研和工程中非常有用。 
图 3.22 为双端口存储器 IDT7133 的逻辑框图。这是一个存储容量为 2K 字长 16 位的
SRAM，它提供了两个相互独立的端口，即左端口和右端口。它们分别具有各自的地址线
(A0～A10)、数据线(I/O0～I/O15)和控制线(R/ W 、 CE 、 OE 、 BUSY )，因而可以对存储
器中任何位置上的数据进行独立的存取操作。图中，字母符号下标中 L 表示左端口，R 表
示右端口，LB 表示低位字节，UB 表示高位字节。 
事实上双端口存储器也可以由 DRAM 构成。 
2. 无冲突读写控制 
当两个端口的地址不相同时，在两个端口上进行读写操作，一定不会发生冲突。当任
一端口被选中驱动时，就可对整个存储器进行存取，每一个端口都有自己的片选控制( CE )
和输出驱动控制( OE )。读操作时，端口的 OE (低电平有效)打开输出驱动器，由存储矩阵
读出的数据就出现在 I/O 线上。表 3.7 列出了无冲突的读写条件，表中符号 1 代表高电平，

0 为低电平，×为任意，Z 为高阻态。 
 

 
3. 有冲突的读写控制 
当两个端口同时存取存储器同一存储单元，而且至少有一个端口为写操作时，便发生
读写冲突。为解决此问题，特设置了 BUSY 标志。在这种情况下，片上的判断逻辑可以决定
对哪个端口优先进行写操作，而对另一个被延迟的端口置 BUSY 标志( BUSY 变为低电平)，
即暂时关闭此端口。换句话说，写操作对 BUSY 变为低电平的端口是不起作用的。一旦优
先端口完成写操作，才将被延迟端口的 BUSY 标志复位( BUSY 变为高电平)，开放此端口，
允许延迟端口进行写操作。 
总之，当两个端口均为开放状态( BUSY 为高电平)且存取地址相同时，发生写冲突。

此时仲裁逻辑可以根据两个端口的地址匹配或片选使能信号有效的时间决定对哪个端口进
行存取。判断方式有以下两种。 
(1)如果地址匹配且在 CE 之前有效，片上的控制逻辑在CEL 和 CER 之间进行判断来选
择端口( CE 判断)。 
(2)如果 CE 在地址匹配之前变低，片上的控制逻辑在左、右地址间进行判断来选择端
口(地址有效判断)。 
无论采用哪种判断方式，延迟端口的 BUSY 标志都将置位而关闭此端口，而当允许存
取的端口完成操作时，延迟端口 BUSY 标志才进行复位而打开此端口。表 3.8 列出了左、右
端口进行读写操作时的功能判断。 


1. 存储器的模块化组织 
一个由若干个模块组成的主存储器是线性编址的。这些地址在各模块中如何安排，有
两种方式：一种是顺序方式，一种是交叉方式。 
在常规主存储器设计中，访问地址采用顺序方式，如图 3.23(a)所示。为了说明原理，
设存储器容量为 32 字，分成 M0、M1、M2、M3 四个模块，每个模块存储 8 个字。访问地址
按顺序分配给一个模块后，接着又按顺序为下一个模块分配访问地址。这样，存储器的 32
个字可由 5 位地址寄存器指示，其中高 2 位选择 4 个模块中的一个，低 3 位选择每个模块
中的 8 个字。 
双端口存
储器读写
时序 

可以看出，在顺序方式中某个模块进行存取时，其他模块不工作。而某一模块出现故
障时，其他模块可以照常工作。另外通过增添模块来扩充存储器容量也比较方便。但顺序
方式的缺点是各模块一个接一个串行工作，因此存储器的带宽受到了限制。 
图 3.23(b)表示采用交叉方式寻址的存储器模块化组织示意图。存储器容量也是 32 个
字，也分成 4 个模块，每个模块 8 个字。但地址的分配方法与顺序方式不同：先将 4 个线
性地址 0、1、2、3 依次分配给 M0、M1、M2、M3 模块，再将线性地址 4、5、6、7 依次分
配给 M0、M1、M2、M3 模块……直到全部线性地址分配完毕为止。当存储器寻址时，用地
址寄存器的低 2 位选择 4 个模块中的一个，而用高 3 位选择模块中的 8 个字。 
可以看出，用地址码的低位字段经过译码选择不同的模块，而高位字段指向相应模块
内的存储字。这样，连续地址分布在相邻的不同模块内，而同一个模块内的地址都是不连
续的。因此，从定性分析，对连续字的成块传送，交叉方式的存储器可以实现多模块流水
式并行存取，大大提高存储器的带宽。由于 CPU 的速度比主存快，假如能同时从主存取出
n 条指令，这必然会提高机器的运行速度。多模块交叉存储器就是基于这种思想提出来的。 
 

2. 多模块交叉存储器的基本结构 
图 3.24 示出四模块交叉存储器结构框图。主存被分成 4 个相互独立、容量相同的模块
M0、M1、M2、M3，每个模块都有自己的读写控制电路、地址寄存器和数据寄存器，各自
以等同的方式与 CPU 交换信息。在理想情况下，如果程序段或数据块都是连续地在主存中
存取，那么将大大提高主存的访问速度。 
CPU 同时访问四个模块，由存储器控制部件控制它们分时使用数据总线进行信息传递。
这样，对每一个存储模块来说，从 CPU 给出访存命令直到读出信息仍然使用了一个存取周
期时间；而对 CPU 来说，它可以在一个存取周期内连续访问四个模块。各模块的读写过程
将重叠进行，所以多模块交叉存储器是一种并行存储器结构。 

下面进行定量分析。设模块字长等于数据总线宽度，又假设模块存取一个字的存储周
期为 T，总线传送周期为τ，存储器的交叉模块数为 m，那么为了实现流水线方式存取，应
当满足 
 
T≤mτ 
(3.1) 
即成块传送可按τ间隔流水方式进行，也就是每经 τ 时间延迟后启动下一个模块。图 3.25
示出了 m=4 的流水线方式存取示意图。 
m 的最小值 mmin=T/τ 称为交叉存取度。交叉存储器要求其模块数必须大于或等于 mmin，
以保证启动某模块后经 mτ 时间再次启动该模块时，它的上次存取操作已经完成。这样，连
续读取 m 个字所需的时间为 
 
t1=T+(m–1)τ 
(3.2) 
而顺序方式存储器连续读取 m 个字所需时间为 
 
t2=mT 
(3.3) 
      
 
         图 3.24  四模块交叉存储器结构框图           图 3.25  流水线方式存取示意图 
从以上定量分析可知，由于 t1＜t2，交叉存储器的带宽确实大大提高了。 

3. 二模块交叉存储器举例 
图 3.26 表示二模块交叉存储器方框图。每个模块的容量为 1MB(256K×32 位)，由 8
片 256K×4 位的 DRAM 芯片组成(位扩展)。二模块的总容量为 2MB(512K×32 位)。数据
总线宽度为 32 位，地址总线宽度为 24 位。为简化，将 2 片 DRAM 芯片用一个 256K×8 位
的长条框表示。 
DRAM 有读周期、写周期和刷新周期。存储器读/写周期时，在行选通信号 RAS 有效下
输入行地址，在列选通信号 CAS 有效下输入列地址，于是芯片中行列矩阵中的某一位组被
选中。如果是读周期，此位组内容被读出；如果是写周期，将总线上数据写入此位组。 
刷新周期是在 RAS 有效下输入刷新地址，此地址指示的一行所有存储元全部被再生。
刷新周期比读/写周期有高的优先权，当对同一行进行读/写与刷新操作时，存储控制器对读
/写请求予以暂存，延迟到此行刷新结束后再进行。 
由图 3.26 可看出：24 位的存储器物理地址指定的系统主存总容量可达 16MB，按“存
储体-块-字”进行寻址。其中高 3 位用于存储体选择(字扩展)，1 个存储体为 2MB，全系
统有 8 个 2MB 存储体。A20～A3 的 18 位地址用于模块中 256K 个存储字的选择。读/写周期
时，它们分为行、列地址两部分送至芯片的 9 位地址引脚。一个模块内所有芯片的 RAS 引
脚连接到一起，模块 0 由
0
RAS 驱动，模块 1 由
1
RAS 驱动。在读/写周期时，主存地址中 A2=0，
0
RAS 有效；A2=1，
1
RAS 有效。因此 A2 用于模块选择，连续的存储字(32 位)交错分布在两
个模块上，偶字地址在模块 0，奇字地址在模块 1。 
 
CPU 给出的主存地址中没有 A1 和 A0 位，替代的是 4 字节允许信号
3
BE ～
0
BE ，以允
许对 A23～A2 指定的存储字中的字节或字完成读/写访问。当
3
BE ～
0
BE 全有效时，即完成
字存取。图 3.27 中没给出译码逻辑，只暗示了
3
BE ～
0
BE 与
3
CAS ～
0
CAS 的对应关系。 
DRAM 需要逐行定时刷新，以使不因存储信息的电容漏电而造成信息丢失。另外，
DRAM 芯片的读出是一种破坏性读出，因此在读取之后要立即按读出信息予以充电再生。

这样，若 CPU 先后两次读取的存储字使用同一 RAS 选通信号，CPU 在接收到第一个存储
字之后必须插入等待状态，直至前一存储字再生完毕才开始第二个存储字的读取。为避免
这种情况，模块 0 由
0
RAS 驱动，模块 1 由
1
RAS 驱动。 
图 3.27 是无等待状态成块存取示意图。由于采用 m=2 的交叉存取度的成块传送，两个
连续地址字的读取之间不必插入等待状态，这称为零等待存取。 
 
1. cache 的功能 
cache 是一种高速缓冲存储器，是为了解决 CPU 和主存之间速度不匹配而采用的一项
重要技术。其原理基于程序运行中具有的空间局部性和时间局部性特征。 
如图 3.28 所示，cache 是介于 CPU 和主存 M2 之间的小容量存储器，但存取速度比主存
快，容量远小于主存。cache 能高速地向 CPU 提供指令和数据，从而加快了程序的执行速
度。从功能上看，它是主存的缓冲存储器，由高速的 SRAM 组成。为追求高速，包括管理
在内的全部功能由硬件实现，因而对程序员是透明的。 
 
 
当前，随着半导体器件集成度的进一步提高，可以将小容量的 cache 与 CPU 集成到同
一芯片中，其工作速度接近于 CPU 的速度，从而组成两级以上的 cache 系统。 
2. cache 的基本原理 
cache 除包含 SRAM 外，还要有控制逻辑。若 cache 在 CPU 芯片外，它的控制逻辑一
般与主存控制逻辑合成在一起，称为主存/chace 控制器；若 cache 在 CPU 内，则由 CPU 提
供它的控制逻辑。 

CPU 与 cache 之间的数据交换是以字为单位，
而 cache 与主存之间的数据交换是以块为单位。一
个块由若干字组成，是定长的。当 CPU 读取内存
中一个字时，便发出此字的内存地址到 cache 和主
存。此时 cache 控制逻辑依据地址判断此字当前是
否在 cache 中：若是，则 cache 命中，此字立即
传送给 CPU；若非，则 cache 缺失(未命中)，用
主存读周期把此字从主存读出送到 CPU，与此同
时，把含有这个字的整个数据块从主存读出送到
cache 中。 
图 3.29 示出了 cache 的原理图。假设 cache
读出时间为 50ns，主存读出时间为 250ns。存储系统是模块化的，主存中每个 8K 模块和容
量 16 字的 cache 相联系。cache 分为 4 行，每行 4 个字(W)。分配给 cache 的地址存放在一
个相联存储器 CAM 中，它是按内容寻址的存储器。当 CPU 执行访存指令时，就把所要访
问的字的地址送到 CAM；如果 W 不在 cache 中，则将 W 从主存传送到 CPU。与此同时，
把包含 W 的由前后相继的 4 个字所组成的一行数据送入 cache，替换原来 cache 中的一行数
据。在这里，由始终管理 cache 使用情况的硬件逻辑电路来实现替换算法。 
3. cache 的命中率 
从 CPU 来看，增加 cache 的目的，就是在性能上使主存的平均读出时间尽可能接近 cache
的读出时间。为了达到这个目的，在所有的存储器访问中由 cache 满足 CPU 需要的部分应
占很高的比例，即 cache 的命中率应接近于 1。由于程序访问的局部性，实现这个目标是可
能的。 
在一个程序执行期间，设 Nc 表示 cache 完成存取的总次数，Nm 表示主存完成存取的总
次数，h 定义为命中率，则有 
 
(3.4) 
若 tc 表示命中时的 cache 访问时间，tm 表示未命中时的主存访问时间，1–h 表示未命中
率(缺失率)，则 cache/主存系统的平均访问时间 ta 为 
 
ta=htc+(1–h)tm 
(3.5) 
我们追求的目标是，以较小的硬件代价使 cache/主存系统的平均访问时间 ta 越接近 tc
越好。设 r=tm/tc 表示主存与 cache 的访问时间之比，e 表示访问效率，则有 

由式(3.6)看出，为提高访问效率，命中率 h 越接近 1 越好。r 值以 5～10 为宜，不宜
太大。 
命中率 h 与程序的行为、cache 的容量、组织方式、块的大小有关。 

4. cache 结构设计必须解决的问题 
从 cache 的基本工作原理可以看出，cache 的设计需要遵循两个原则：一是希望 cache
的命中率尽可能高，实际应接近于 1；二是希望 cache 对 CPU 而言是透明的，即不论是否
有 cache，CPU 访存的方法都是一样的，软件不需增加任何指令就可以访问 cache。解决了
命中率和透明性问题，就 CPU 访存的角度而言，内存将具有主存的容量和接近 cache 的速
度。为此，必须增加一定的硬件电路完成控制功能，即 cache 控制器。 
在设计 cache 结构时，必须解决几个问题：①主存的内容调入 cache 时如何存放？②访
存时如何找到 cache 中的信息？③当 cache 空间不足时如何替换 cache 中已有的内容？④需
要写操作时如何改写 cache 的内容？ 
其中，前两个问题是相互关联的，即如何将主存信息定位在 cache 中，如何将主存地址
变换为 cache 地址。与主存容量相比，cache 的容量很小，它保存的内容只是主存内容的一
个子集，且 cache 与主存的数据交换是以块为单位。为了把主存块放到 cache 中，必须应用
某种方法把主存地址定位到 cache 中，称为地址映射。“映射”一词的物理含义是确定位置
的对应关系，并用硬件来实现。这样当 CPU 访问存储器时，它所给出的一个字的内存地址
会自动变换成 cache 的地址，即 cache 地址变换。 
cache 替换问题主要是选择和执行替换算法，以便在 cache 不命中时替换 cache 中的内容。 
最后一个问题涉及 cache 的写操作策略，重点是在更新时保持主存与 cache 的一致性。 

地址映射方式有全相联方式、直接方式和组相联方式三种，下面分别介绍。 
1. 全相联映射方式 
cache 的数据块大小称为行，用 Li 表示，其中 i=0,1,2,…,m–1，共有 m=2r 行。主存的数
据块大小称为块，用 Bj 表示，其中 j=0,1,2,…,n–1，共有 n=2s 块。行与块是等长的，每个块
(行)由 k=2w 个连续的字组成，字是 CPU 每次访问存储器时可存取的最小单位。 
在全相联映射中，将主存中一个块的地址(块号)与块的内容(字)一起存于 cache 的行
中，其中块地址存于 cache 行的标记(tag)部分中。这种带全部块地址一起保存的方法，可
使主存的一个块直接复制到 cache 中的任意一行上，非常灵活。图 3.30(a)是全相联映射的
多对一示意图，其中 cache 为 8 行，主存为 256 块，每块(行)中有同样多的字。 

图 3.30(b)表示全相联映射方式的检索过程。CPU 访存指令指定了一个主存地址，为了
快速检索，指令中的块号与 cache 中所有行的标记同时在比较器中进行比较。如果块号命中，
则按字地址从 cache 中读取一个字；如果块号未命中，则按主存地址从主存中读取这个字。
在全相联 cache 中，全部标记用一个相联存储器来实现，全部数据存储用一个普通 RAM 来
实现。全相联方式的主要缺点是高速比较器电路难于设计和实现，因此只适合于小容量
cache 采用。 
2. 直接映射方式 
直接映射方式也是一种多对一的映射关系，但一个主存块只能拷贝到 cache 的一个特定
行位置上去。cache 的行号 i 和主存的块号 j 有如下函数关系： 
 
i = j  mod   m 
(3.7) 
式中，m 为 cache 中的总行数。显然，主存的第 0 块，第 m 块，第 2m 块，…，第 2s–m 块
只能映射到 cache 的第 0 行；而主存的第 1 块，第 m+1 块，第 2m+1 块，…，第 2s–m+1 块

只能映射到 cache 的第 1 行。图 3.31(a)表示直接映射方式的示意图，cache 假设为 8 行，主
存假设为 256 块，故以 8 为模进行映射。这样，允许存于 cache 第 L0 行的主存块号是 B0, B8, 
B16, …, B248(共 32 块)。同样，映射到第 L7 的主存块号也是 32 块。此处 s=8，Y=3，s–Y=5。 
为了理解方便，可以把主存首先分区，每个区的块数与cache的行数m相等。如图3.31(a)
所示。所有区的第 0 块在调入 cache 时只能映射到 cache 的第 0 行，所有区的第 1 块在调入
cache 时只能映射到 cache 的第 1 行……所有区的第 m–1 块在调入 cache 时只能映射到 cache
的第 m–1 行。 
在直接映射方式中，将 s 位的主存块地址分成两部分：低 r 位主存区内块号作为 cache
的行地址，s–r 位区号作为标记(tag)与块数据一起保存在该行。当 CPU 以一个给定的内存
地址访问 cache 时，首先用 r 位区内块号找到 cache 中的特定一行，然后用地址中的 s–r 位
区号部分与此行的标记在比较器中做比较。若相符即命中，在 cache 中找到了所要求的块，
而后用地址中最低的 w 位读取所需求的字。若不符，则未命中，由主存读取所要求的字。

直接映射方式的优点是硬件简单，成本低，地址变换速度快。缺点是每个主存块只有
一个固定的行位置可存放。如果连续访问块号相距 m 整数倍的两个块，因两个块映射到同
一 cache 行时，就会发生冲突。发生冲突时就要将原先存入的行换出去，但很可能过一段时
间又要换入。频繁的置换会使 cache 效率下降。因此直接映射方式适合于需要大容量 cache
的场合，更多的行数可以减小冲突的机会。 
思考题  可否将第 0 区的所有页映射到 cache 第 0 行？可否将第 1 区的所有页映射到
cache 第 1 行？……请与上面的映射方式对比。 
3. 组相联映射方式 
全相联映射和直接映射两种方式的优缺点正好相反。从存放位置的灵活性和命中率来
看，前者为优；从比较器电路简单及硬件投资来说，后者为佳。而组相联映射方式是前两
种方式的折中方案，它适度地兼顾了二者的优点又尽量避免二者的缺点，因此被普遍采用。 
如图 3.32(a)所示，所有区的第 0 块在调入 cache 时只能映射到 cache 的第 0 组，所有
区的第 1 块在调入 cache 时只能映射到 cache 的第 1 组，所有区的第 u–1 块在调入 cache 时
只能映射到 cache 的第 u–1 组。在直接映射方式中，每个区第 i 块只能映射到 cache 唯一的
第 i 行，冲突的概率可能会很大。而在组相联映射方式中，每个区第 i 块可以映射到第 i 组
的 v 行中(图中 v=2)，而且在 v 行中可以自由选择空余的行。 
这种方式将 cache 分成 u 组，每组 v 行。主存块存放到哪个组是固定的，取决于主存块
在主存区中是第几块。至于存到该组哪一行是灵活的，即有如下函数关系： 

内存地址中，s 位块号划分成两部分：低 d 位(2d=u)主存区内块号用于表示 cache 组号(而
不是 cache 行号)，高 s–d 位区号作为标记与块数据一起存于此组的某行中。 

图 3.32(b)表示组相联映射的示意图。例中 cache 划分 u = 4 组，每组有 v = 2 行，即
m = u× v =8。主存容量为 256 块，其中 B0, B4, B9, …, B252 共 64 个主存块映射到 cache 第
S0 组；B1, B5, B10, …, B253 共 64 个主存块映射到 cache 的第 S1 组；以此类推。 
图 3.32(c)表示组相联 cache 的检索过程。注意 cache 的每一小框代表的不是“字”而
是“行”。当 CPU 给定一个内存地址访问 cache 时，首先用 d 位区内块号找到 cache 的相应
组，然后将主存地址高 s–d 位区号部分与该组 v 行中的所有标记同时进行比较。哪行的标
记与之相符，哪行即命中。此后再以内存地址的 w 位字地址部分检索此行的具体字，并完
成所需要求的存取操作。如果此组没有一行的标记与之相符，即 cache 未命中，此时需按主
存地址访问主存。 
 
 

组相联映射方式中的每组行数 v 一般取值较小，典型值是 2、4、8、16。这种规模的 v
路比较器容易设计和实现。而块在组中的排放又有一定的灵活性，使冲突减少。为强调比
较器的规模和存放的灵活程度，常称之为 v 路组相联 cache
①。 

cache 工作原理要求它尽量保存最新数据。当一个新的主存块需要拷贝到 cache，而允
许存放此块的行位置都被其他主存块占满时，就要产生替换。 
替换问题与 cache 的组织方式紧密相关。对直接映射的 cache 来说，因一个主存块只有
一个特定的行位置可存放，所以解决问题很简单，只要把此特定位置上的原主存块换出
cache 即可。对全相联和组相联 cache 来说，就要从允许存放新主存块的若干特定行中选
取一行换出。如何选取就涉及替换策略，又称替换算法。硬件实现的常用算法主要有以下
三种。 
1)最不经常使用(LFU)算法 
LFU 算法认为应将一段时间内被访问次数最少的那行数据换出。为此，每行设置一个
计数器。新行调入后从 0 开始计数，每访问一次，被访行的计数器增 1。当需要替换时，对
这些特定行的计数值进行比较，将计数值最小的行换出，同时将这些特定行的计数器都清
零。这种算法将计数周期限定在两次替换之间的间隔时间内，因而不能严格反映近期访问
情况。 
2)近期最少使用(LRU)算法 
LRU 算法将近期内长久未被访问过的行换出。为此，每行也设置一个计数器，但它们
是 cache 每命中一次，命中行计数器清零，其他各行计数器增 1。当需要替换时，比较各特
定行的计数值，将计数值最大的行换出。这种算法保护了刚复制到 cache 中的新数据行，符
合 cache 工作原理，因而使 cache 有较高的命中率。 
对 2 路组相联的 cache 来说，LRU 算法的硬件实现可以简化。因为一个主存块只能在
一个特定组的两行中来做存放选择，二选一完全不需要计数器，只需一个二进制位即可。
例如，规定一组中的 A 行复制进新数据可将此位置“1”，B 行复制进新数据可将此位置“0”。
当需要置换时，只需检查此二进制位状态即可：为 0 换出 A 行，为 1 换出 B 行，实现了保
护新行的原则。奔腾 CPU 内的数据 cache 是一个 2 路组相联结构，就采用这种简捷的 LRU
替换算法。 
3)随机替换 
随机替换策略实际上是不要什么算法，从特定的行位置中随机地选取一行换出即可。
这种策略在硬件上容易实现，且速度也比前两种策略快。缺点是随意换出的数据很可能马
上又要使用，从而降低命中率和 cache 工作效率。但这个不足随着 cache 容量增大而减小。
研究表明，随机替换策略的性能只是稍逊于前两种策略。 

由于 cache 的内容只是主存部分内容的副本，它应当与主存内容保持一致。而 CPU 对
cache 的写入更改了 cache 的内容。如何与主存内容保持一致，可选用如下三种写操作策略。 
1)写回法(write back, copy back) 
写回法要求：当 CPU 写 cache 命中时，只修改 cache 的内容，而不立即写入主存；只
有当此行被换出时才写回主存。这种方法使 cache 真正在 CPU-主存之间读/写两方面都起到
高速缓存作用。对一个 cache 行的多次写命中都在 cache 中快速完成，只是需要替换时才写
回速度较慢的主存，减少了访问主存的次数。实现这种方法时，每个 cache 行必须配置一个
修改位，以反映此行是否被 CPU 修改过。当某行被换出时，根据此行修改位是 1 还是 0，
来决定将该行内容写回主存还是简单弃去。 
如果 CPU 写 cache 未命中，为了包含欲写字的主存块在 cache 分配一行，将此块整个
复制到 cache 后对其进行修改。主存的写修改操作统一留到换出时再进行。显然，这种写
cache 与写主存异步进行的方式可显著减少写主存次数，但是存在不一致性的隐患。 
2)全写法(write through) 
全写法要求：当写 cache 命中时，cache 与主存同时发生写修改，因而较好地维护了 cache
与主存的内容的一致性。当写 cache 未命中时，只能直接向主存进行写入。但此时是否将修
改过的主存块取到 cache，有两种选择方法：一种称为 WTWA 法，取主存块到 cache 并为
它分配一个行位置；另一种称为 WTNWA 法，不取主存块到 cache。 
全写法是写 cache 与写主存同步进行，优点是 cache 中每行无须设置一个修改位，以及
相应的判断逻辑。缺点是，cache 对 CPU 向主存的写操作无高速缓冲功能，降低了 cache
的性能。 
3)写一次法(write once) 
写一次法是基于写回法并结合全写法的写策略：写命中与写未命中的处理方法和写回
法基本相同，只是第一次写命中时要同时写入主存。这是因为第一次写 cache 命中时，CPU
要在总线上启动一个存储写周期，其他 cache 监听到此主存块地址及写信号后，即可复制该
块或及时作废，以便维护系统全部 cache 的一致性。奔腾 CPU 的片内数据 cache 就采用了
写一次法。 

我们可以从 Intel 微处理器的演变中清楚地看到 cache 组织的演变。80386 不包含片内
cache。80486 包含 8KB 的片内 cache，它采用每行 16B 的 4 路组相联结构。所有的 Pentium
处理器包含两个片内 L1 cache，一个是 D-cache(数据 cache)，一个是 I-cache(指令 cache)。
Pentium 2 还包含一个 L2 cache，其容量是 256KB，每行 128B，采用 8 路组相联结构。Pentium 
3 增加了一个 L3 cache。到 Pentium 4，L3 cache 已移到处理器芯片中。图 3.33 示出了 Pentium 
4 的三级 cache 的布局。 
 
Pentium 4 处理器的核心由下列四个主要部件组成： 
取指/译码单元  按顺序从 L2 cache 中取程序指令，将它们译成一系列的微指令，并存
入 L1 指令 cache 中。 
乱序执行逻辑  依据数据相关性和资源可用性，调度微指令的执行，因而微指令可按
不同于所取机器指令流的顺序被调度执行。 
执行单元  它执行微指令，从 L1 数据 cache 中取所需数据，并在寄存器组中暂存运算
结果。 
存储器子系统  这部分包括 L2 cache、L3 cache 和系统总线。当 L1、L2 cache 未命中时，
使用系统总线访问主存。系统总线还用于访问 I/O 资源。 
不同于所有先前 Pentium 模式和大多数处理器所采用的结构，Pentium 4 的指令 cache
位于指令译码逻辑和执行部件之间。其设计理念是：Pentium 4 将机器指令译成由微指令组
成的简单 RISC 类指令，而使用简单定长的微指令可允许采用超标量流水线和调度技术，从
而增强机器的性能。关于流水线技术，将留在第五章中讨论。 
思考题  Pentium 4 中为什么设置 L1、L2、L3 三个 cache？L1cache 分成 I-cache 和 D-cache
有什么好处？ 

所有现代计算机都使用了 cache。大多数情况下，这些 cache 和组成 CPU 的微处理器集
成到一个芯片上。为进一步缩小现代处理器高时钟频率和访问 DRAM 相对较慢之间的差距，
高性能微处理器可支持附加一级的 cache。这种二级的 cache，位于处理器芯片内或是位于
处理器芯片外单独的一组 SRAM，当访问主 cache 缺失后就会访问它。如果二级 cache 包含
所请求的数据，缺失损失就是二级 cache 的访问时间，这要比主存的访问时间少得多。如果
第一级 cache、第二级 cache 都不包含这个数据，就需要访问主存储器，产生更大的缺失损
失。使用二级 cache 能使性能提高多少？下面通过例子来说明。 

1. 实地址与虚地址 
在早期的单用户单任务操作系统(如 DOS)中，每台计算机只有一个用户，每次运行一
      计算机组成原理 
104
个程序，且程序不是很大，单个程序完全可以存放在实际内存中。这时虚拟存储器(简称虚
存)并没有太大的用处。 
但随着程序占用存储器容量的增长和多用户多任务系统的出现，在程序设计时，程序
所需的存储器容量与计算机系统实际配备的主存储器的容量之间往往存在着矛盾。例如，
在某些低档的计算机中，物理内存的容量较小，而某些程序却需要很大的内存才能运行；
而在多用户多任务系统中，多个用户或多个任务共享全部主存，要求同时执行多道程序。
这些同时运行的程序到底占用实际内存中的哪一部分，在编制程序时是无法确定的，必须
等到程序运行时才动态分配。 
为此，希望在编制程序时独立编址，既不考虑程序是否能在物理存储器中存放得下(因
为这与程序运行时的系统配置和当时其他程序的运行情况有关，在编程时一般无法确定)，
也不考虑程序应该存放在什么物理位置。而在程序运行时，则分配给每个程序一定的运行
空间，由地址转换部件(硬件或软件)将编程时的地址转换成实际内存的物理地址。如果分
配的内存不够，则只调入当前正在运行的或将要运行的程序块(或数据块)，其余部分暂时
驻留在辅存中。 
这样，用户编制程序时使用的地址称为虚地址或逻辑地址，其对应的存储空间称为虚
存空间或逻辑地址空间；而计算机物理内存的访问地址则称为实地址或物理地址，其对应
的存储空间称为物理存储空间或主存空间。程序进行虚地址到实地址转换的过程称为程序
的再定位。 
2. 虚存的访问过程 
虚存空间的用户程序按照虚地址编程并存放在辅存中。程序运行时，由地址变换机构
依据当时分配给该程序的实地址空间把程序的一部分调入实存。 
每次访存时，首先判断该虚地址所对应的部分是否在实存中：如果是，则进行地址转
换并用实地址访问主存；否则按照某种算法将辅存中的部分程序调度进内存，再按同样的
方法访问主存。 
由此可见，每个程序的虚地址空间可以远大于实地址空间，也可以远小于实地址空间。
前一种情况以提高存储容量为目的，后一种情况则以地址变换为目的。后者通常出现在多
用户或多任务系统中：实存空间较大，而单个任务并不需要很大的地址空间，较小的虚存
空间则可以缩短指令中地址字段的长度。 
有了虚存机制后，应用程序就可以透明地使用整个虚存空间。对应用程序而言，如果
主存的命中率很高，虚存的访问时间就接近于主存访问时间，而虚存的大小仅仅依赖于辅
存的大小。 
这样，每个程序就可以拥有一个虚拟的存储器，它具有辅存的容量和接近主存的访问
速度。但这个虚存是由主存和辅存以及辅存管理部件构成的概念模型，不是实际的物理存
储器。 
虚存是在主存和辅存之外附加一些硬件和软件实现的。由于软件的介入，虚存对设计
存储管理软件的系统程序员而言是不透明的，但对应用程序员而言仍然是透明的。 
3. cache 与虚存的异同 
从虚存的概念可以看出，主存-辅存的访问机制与 cache-主存的访问机制是类似的。这
是由 cache 存储器、主存和辅存构成的三级存储体系中的两个层次。 

cache 和主存之间以及主存和辅存之间分别有辅助硬件和辅助软硬件负责地址变换与
管理，以便各级存储器能够组成有机的三级存储体系。cache 和主存构成了系统的内存，而
主存和辅存依靠辅助软硬件的支持支撑虚拟存储器工作。 
在三级存储体系中，cache-主存和主存-辅存这两个存储层次有许多相同点。 
(1)出发点相同  二者都是为了提高存储系统的性能价格比而构造的分层存储体系，都
力图使存储系统的性能接近高速存储器，而价格和容量接近低速存储器。 
(2)原理相同  都是利用了程序运行时的局部性原理把最近常用的信息块从相对慢速
而大容量的存储器调入相对高速而小容量的存储器。 
但 cache-主存和主存-辅存这两个存储层次也有许多不同之处。 
(1)侧重点不同  cache 主要解决主存与 CPU 的速度差异问题；而就性能价格比的提高
而言，虚存主要是解决存储容量问题，另外还包括存储管理、主存分配和存储保护等方面。 
(2)数据通路不同  CPU 与 cache 和主存之间均可以有直接访问通路，cache 不命中时
可直接访问主存；而虚存所依赖的辅存与 CPU 之间不存在直接的数据通路，当主存不命中
时只能通过调页解决，CPU 最终还是要访问主存。 
(3)透明性不同  cache 的管理完全由硬件完成，对系统程序员和应用程序员均透明；
而虚存管理由软件(操作系统)和硬件共同完成，由于软件的介入，虚存对实现存储管理的
系统程序员不透明，而只对应用程序员透明(段式和段页式管理对应用程序员“半透明”)。 
(4)未命中时的损失不同  由于主存的存取时间是 cache 的存取时间的 5～10 倍，而主
存的存取速度通常比辅存的存取速度快上千倍，故主存未命中时系统的性能损失要远大于
cache 未命中时的损失。 
4. 虚存机制要解决的关键问题 
虚存机制也要解决一些关键问题。 
(1)调度问题  决定哪些程序和数据应被调入主存。 
(2)地址映射问题  在访问主存时把虚地址变为主存物理地址(这一过程称为内地址变
换)；在访问辅存时把虚地址变成辅存的物理地址(这一过程称为外地址变换)，以便换页。
此外还要解决主存分配、存储保护与程序再定位等问题。 
(3)替换问题  决定哪些程序和数据应被调出主存。 
(4)更新问题  确保主存与辅存的一致性。 
在操作系统的控制下，硬件和系统软件为用户解决了上述问题，从而使应用程序的编
程大大简化。 

1. 页式虚存地址映射 
页式虚拟存储系统中，虚地址空间被分成等长的页，称为逻辑页；主存空间也被分成
同样大小的页，称为物理页。相应地，虚地址分为两个字段：高字段为逻辑页号，低字段
为页内地址(偏移量)；实存地址也分为两个字段：高字段为物理页号，低字段为页内地址。
通过页表可以把虚地址(逻辑地址)转换成物理地址。 
在大多数系统中，每个进程对应一个页表。页表中对应每一个虚存页面有一个表项，
表项的内容包含该虚存页面所在的主存页面的地址(物理页号)，以及指示该逻辑页是否已

调入主存的有效位。地址变换时，用逻辑页号作为页表内的偏移地址索引页表(将虚页号看
作页表数组下标)并找到相应物理页号，用物理页号作为实存地址的高字段，再与虚地址的
页内偏移量拼接，就构成了完整的物理地址。现代的中央处理器通常有专门的硬件支持地
址变换。图 3.34 显示了页式虚拟存储器的地址映射过程。 
 

每个进程所需的页数并不固定，所以页表的长度是可变的，因此通常的实现方法是把
页表的基地址保存在寄存器中，而页表本身则放在主存中。由于虚存地址空间可以很大，
因而每个进程的页表有可能非常长。例如，如果一个进程的虚地址空间为 2GB，每页的大
小为 512B，则总的虚页数为 231/29=222。 
为了节省页表本身占用的主存空间，一些系统把页表安排存储在虚存空间，因而页表
本身也要进行分页。当一个进程运行时，其页表中一部分在主存中，另一部分则在辅存中
保存。 
另一些系统采用二级页表结构。每个进程有一个页目录表，其中的每个表项指向一个
页表。因此，若页目录表的长度(表项数)是 m，每个页表的最大长度(表项数)为 n，则一个
进程最多可以有 m×n 个页。 
在页表长度较大的系统中，还可以采用反向页表(inverted page table)实现物理页号到逻
辑页号的反向映射。页表中对应每一个物理页号有一个表项，表项的内容包含该物理页所
对应的逻辑页号。访存时，通过逻辑页号在反向页表中逐一查找。如果找到匹配的页，则
用表项中的物理页号取代逻辑页号；如果没有匹配表项，则说明该页不在主存中。这种方
式的优点是页表所占空间大大缩小，但代价是需要对反向页表进行检索，查表的时间很长。
有些系统通过散列(哈希)表加以改进。 
2. 内页表和外页表 
上面所说的页表是虚地址到主存物理地址的变换表，通常称为内页表。与内页表对应
的还有外页表，用于虚地址与辅存地址之间的变换。当主存缺页时，调页操作首先要定位
辅存，而外页表的结构与辅存的寻址机制密切相关。例如，对磁盘而言，辅存地址包括磁
盘机号、磁头号、磁道号和扇区号等。 

外页表通常放在辅存中，在需要时可调入主存。当主存不命中时，由存储管理部件向
CPU 发出“缺页中断”，进行调页操作。 
3. 转换后援缓冲器(TLB) 
由于页表通常在主存中，因而即使逻辑页已经在主存中，也至少要访问两次物理存储
器才能实现一次访存，这将使虚拟存储器的存取时间加倍。为了避免对主存访问次数的增
多，可以对页表本身实行二级缓存，把页表中最活跃的部分存放在高速存储器中。这个专
用于页表缓存的高速存储部件通常称为转换后援缓冲器(TLB)，又称为快表。而保存在主
存中的完整页表则称为慢表。快表的作用是加快地址变换。 
TLB 的作用与主存和 CPU 之间的 cache 作用相似，通常由相联存储器实现，容量比慢
表小得多，存储慢表中部分信息的副本，可以完成硬件高速检索操作。地址变换时，根据
逻辑页号同时查快表和慢表，当在快表中有此逻辑页号时，就能很快地找到对应的物理页
号。根据程序的局部性原理，多数虚拟存储器访问都将通过 TLB 进行，从而有效降低访存
的时间延迟。图 3.35 显示了 TLB 的地址映射过程。 
 
由于 TLB 的缓冲过程与 cache 的缓冲过程是独立的，所以在每次存储器访问过程中有
可能要经历多次变换。存储管理部件首先用虚地址中的虚页号部分检索 TLB：匹配成功时
则通过实页号与偏移量拼接出物理地址；TLB 匹配不成功则需查询主存中的页表，然后通
过实页号与偏移量拼接出物理地址。而该物理地址所在的主存空间可能已经被调入 cache
中，也可能还在主存中，甚至还有可能在辅存中。对后一种情况，包含该地址的页必须被
调入主存，并将其所在的块装入 cache 中，修改相应的页表和 TLB 表项。可见虚拟存储器
的地址映射与地址变换过程是相当复杂的过程。 
4. 虚拟存储器、TLB 和 cache 的协同操作 
虚拟存储器和 cache 系统如同一个层次结构般一起工作。操作系统在管理该层次结构时
起到关键作用，当它决定要把某一页移到磁盘上去时，就迫使该页的全部内容从 cache 中删
除。同时，操作系统修改页表和 TLB，而试图访问该页上的任何数据可能将导致缺页。 
在最好的情况下，虚拟地址由 TLB 进行转换，然后被送到 cache，找到正确的数据并

取回处理器。在最坏的情况下，一次访问会在存储器层次结构的三个组成部分都产生缺失：
TLB、页表和 cache。 

1. 段式虚拟存储器 
页面是主存物理空间中划分出来的等长的固定区域。分页方式的优点是页长固定，因
而便于构造页表、易于管理，且不存在外碎片。但分页方式的缺点是页长与程序的逻辑大
小不相关。例如，某个时刻一个子程序可能有一部分在主存中，另一部分则在辅存中。这
不利于编程时的独立性，并给换入换出处理、存储保护和存储共享等操作造成麻烦。 
另一种划分可寻址的存储空间的方法称为分段。段是按照程序的自然分界划分的长度
可以动态改变的区域。通常，程序员把子程序、操作数和常数等不同类型的数据划分到不
同的段中，并且每个程序可以有多个相同类型的段。 
在段式虚拟存储系统中，虚地址由段号和段内地址(偏移量)组成。虚地址到实主存地
址的变换通过段表实现。每个程序设置一个段表，段表的每一个表项对应一个段。每个表
项至少包含下面三个字段： 
(1)有效位  指明该段是否已经调入实存。 
(2)段起址  指明在该段已经调入实存的情况下，该段在实存中的首地址。 
(3)段长  记录该段的实际长度。设置段长字段的目的是保证访问某段的地址空间时，
段内地址不会超出该段长度导致地址越界而破坏其他段。 
段表本身也是一个段，可以存在辅存中，但一般驻留在主存中。 
针对每个虚地址，存储管理部件首先以段号 s 为索引访问段表的第 s 个表项。若该表项
的有效位为 1，则将虚地址的段内偏移量 d 与该表项的段长字段比较：若偏移量较大则说明
地址越界，将产生地址越界中断；否则，将该表项的段起址与段内偏移量相加，求得主存
实地址并访存。如果该表项的有效位为 0，则产生缺段中断，从辅存中调入该段，并修改
段表。 
段式虚地址向实存地址的变换过程如图 3.36 所示。 

分页对程序员而言是不可见的，而分段通常对程序员而言是可见的，因而分段为组织
程序和数据提供了方便。与页式虚拟存储器相比，段式虚拟存储器有许多优点：①段的逻
辑独立性使其易于编译、管理、修改和保护，也便于多道程序共享。②段长可以根据需要
动态改变，允许自由调度，以便有效利用主存空间。 
因为段的长度不固定，段式虚拟存储器也有一些缺点：①主存空间分配比较麻烦。
②容易在段间留下许多外碎片，造成存储空间利用率降低。③由于段长不一定是 2 的整数
次幂，因而不能简单地像分页方式那样用虚地址和实地址的最低若干二进制位作为段内偏
移量，并与段号进行直接拼接，必须用加法操作通过段起址与段内偏移量的求和运算求得
物理地址。因此，段式存储管理比页式存储管理方式需要更多的硬件支持。 
2. 段页式虚拟存储器 
段页式虚拟存储器是段式虚拟存储器和页式虚拟存储器的结合。 
实存被等分成页。每个程序先按逻辑结构分段，每段再按照实存的页大小分页，程序
按页进行调入和调出操作，但可按段进行编程、保护和共享。 
在段页式虚拟存储系统中，每道程序均通过一个段表和多个页表进行两级再定位。段
表中的每个表项对应一个段，每个表项有一个指针指向该段的页表。页表则指明该段各页
在主存中的位置，以及是否已装入、是否已修改等状态信息。 
一个虚地址由段号、段内页号和页内偏移量构成。在多任务系统中，操作系统还会在
每个虚地址前面增加一个表明该程序在系统中的序号的基号。一个虚地址可以看作由四个
字段构成： 
(基号 N) 
段号 S 
段内逻辑页号 P 
页内地址偏移量 D 
【
在主存中，每道程序都有一张段表，A 程序有 4 段，C 程序有 3 段，每段应有一张页
表，段表的每行就表示相应页表的起始位置，而页表内的每行即为相应的物理页号。请说
明虚实地址变换过程。 
解  地址变换过程如下： 
(1)由存储管理部件根据基号 C 找到段表基址寄存器表第 c 个表项，获得程序 C 的段表
基址 SC。再根据段号 S(=1)找到程序 C 段表的第 S 个表项，得到段 S 的页表起始地址 b。 
(2)根据段内逻辑页号 P(=2)检索页表，得到物理页号(图中为 10)。 
(3)物理页号与页内地址偏移量拼接即得物理地址。 
假如计算机系统中只有一个基址寄存器，则基号可不要。多道程序切换时，由操作系
统修改基址寄存器内容。 
实际上，上述每个段表和页表的表项中都应设置一个有效位。只有在有效位为 1 时才
按照上述流程操作，否则需中断当前操作先进行建表或调页。 
可以看出，段页式虚拟存储器的缺点是在由虚地址向主存地址的映射过程中需要多次
查表，因而实现复杂度较高。 
当从辅存调页至主存而主存已满时，也需要进行主存页面的替换。虚拟存储器的替换
算法与 cache 的替换算法类似，有 FIFO 算法、LRU 算法、LFU 算法等。 
虚拟存储器的替换算法与 cache 的替换算法不同的是： 
(1)cache 的替换全部靠硬件实现，而虚拟存储器的替换有操作系统的支持。 
(2)虚存缺页对系统性能的影响比 cache 未命中要大得多，因为调页需要访问辅存，并
且要进行任务切换。 
(3)虚存页面替换的选择余地很大，属于一个进程的页面都可替换。 
为支持虚存的替换，通常在页表或段表的每一表项中设置一个修改位，标识该表项所
对应的主存页或段空间在被调入主存后是否被修改过。对于将被替换出去的空间，假如其
内容没有被修改过，就不必进行额外处理；否则就需把该空间存储的内容重新写入辅存，
以保证辅存中数据的正确性。 
在 FIFO 算法中，FIFO 队列中的页面始终按照从 a 到 c 的顺序依次推进，页面从 a 位
置进入队列，替换始终在页面 c 的位置进行。 
FIFO+LRU 算法是对 FIFO 算法的一种改进。但与 FIFO 算法不同的是，如果某个页面
命中，则将该页面移动到 FIFO 队列入口位置(页面 a 所在的位置)。因为根据程序的局部性
原理，刚被访问的页面在最近的将来被再次访问的概率较大，故将其被替换的时间延后。
上面的例子说明 FIFO+LRU 算法比 FIFO 算法的命中率高。 

存储管理部件(Memory Management Unit，MMU)是系统中进行虚实地址转换的核心部
件。MMU 的主要功能有：在 TLB 的协助下完成虚实地址转换；维护 TLB 的控制机制；负
责存储保护；在 TLB 失效或非法访问时向处理器发起中断；维护一个 TLB 失效后的再填充
机制(table walking)。 
MMU 的工作流程大致如下：CPU 发出访存的虚拟地址后，MMU 通过页表查找机制访
问主存页表，获得映射关系；如果主存命中，MMU 将虚页号变换为物理页号，产生物理地
址访存；如果主存缺页，CPU 将转到操作系统的页面失效程序入口，由操作系统进行调页操作。 

基于英特尔 IA-32 体系结构的奔腾系列机为存储管理提供了硬件支持。目前广泛使用
的奔腾处理机的存储管理机制与英特尔 80386 和 80486 基本相同。 
IA-32 体系结构微处理机的存储管理硬件支持三种存储器模型，如图 3.38 所示。 
平坦存储器模型(flat memory model)内存被组织成单一的、连续的地址空间，称为“线
性地址空间”。所有的代码、数据和堆栈均包含在该地址空间内，该空间的字节地址范围为
0～232–1。 
分段存储器模型(segmented memory model)每个程序均使用一组独立的地址空间，每个
地址空间就是一个段，段的最大长度为 232B。逻辑地址由段选择器和偏移量组成，处理机
将逻辑地址透明地转换为线性地址。 
实地址模式存储器模型(real-address mode memory model)是为保持与早期的 8086 处理
机兼容的存储器模式。线性地址空间被分为段，段的最大长度为 64KB。线性地址空间的最
大长度为 220B。 
IA-32 体系结构微处理机的虚拟存储器可以通过两种方式实现：分段和分页。存储管理
部件包括分段部件(SU)和分页部件(PU)两部分。分段部件将程序中使用的虚地址转换成线
性地址。而分页部件则将线性地址转换为物理地址。 
在分段部件和分页部件中，每一部分都可以独立地打开或关闭，因而可出现四种组合

方式： 
(1)不分段不分页模式  程序中使用的逻辑地址与物理地址相同。 
(2)分段不分页模式  相当于段式虚拟存储器。程序中使用的逻辑地址由一个 16 位段
选择器和一个 32 位偏移量组成。段选择器中的最低两位用于存储保护，其余 14 位选择一
个特定的段。因此，对于分段的存储器，用户的虚拟地址空间是 214+32=246=64TB。而物理
地址空间使用 32 位地址，最大 4GB。由分段部件将二维的虚拟地址转换为一维的线性地址。
在分页部件不工作的情况下，线性地址也就是主存物理地址。 
(3)不分段分页模式  相当于页式虚拟存储器。程序中使用的是 32 位线性地址，由分
页部件将其转换成 32 位物理地址。用户的虚拟地址空间是 232=4GB。 
(4)分段分页模式  在分段基础上增加分页存储管理的模式，即段页式虚拟存储器。程
序中使用的逻辑地址由一个 16 位段选择器和一个 32 位偏移量组成，由分段部件将二维的
虚拟地址转换为一维的线性地址，再由分页部件将其转换成 32 位物理地址。用户的虚拟地
址空间是 214+32=246=64TB。 
3.8.3  分页模式下的地址转换 
在分页模式下，有两种页大小，其地址映射方式不同：一种是兼容早期的 80386 和 80486
的 4KB 的页大小，使用页目录表和页表两级结构进行地址转换；另一种是从奔腾处理机开
始采用的 4MB 页大小，使用单级页表结构。 
4MB 分页方式的地址转换如图 3.39 所示。32 位线性地址分为高 10 位的页号和低 22
位的页内偏移量两个字段。 
系统中由一个有 1024 个表项的页表实现地址转换。控制寄存器 CR3 指向页表，页表的
每个表项为 32 位。其中： 

I 位指示页大小  I=1 为 4MB 页大小；I=0 为 4KB 大小。 
P 为出现位  P=1 表示此页已被装入主存；P=0 时访问此页将引起缺页中断。 
A 为已访问位  若在装入主存后此页被访问过，则 A 被置为 1；否则置 A 为 0。 
D 为脏位  若该页在调入主存后被修改过，则 D 被置为 1，表示在该页被换出主存时
应写回辅存。 
R/W 为读/写控制位  用于指明用户对该页的权限是只读还是可读写。 
U/S 为用户/管理员权限控制位  指明该页是只能被操作系统访问还是同时允许操作系
统和用户程序访问。 
本 章 小 结 
对存储器的要求是容量大、速度快、成本低。为了解决这三方面的矛盾，计算机采用
多级存储体系结构，即 cache、主存和外存。CPU 能直接访问内存(cache、主存)，但不能
直接访问外存。存储器的技术指标有存储容量、存取时间、存储周期、存储器带宽。 
广泛使用的 SRAM 和 DRAM 都是半导体随机读写存储器，前者速度比后者快，但集成
度不如后者高。二者的优点是体积小，可靠性高，价格低廉，缺点是断电后不能保存信息。 
只读存储器和闪速存储器正好弥补了 SRAM 和 DRAM 的缺点，即使断电也仍然保存原
先写入的数据。特别是闪速存储器能提供高性能、低功耗、高可靠性以及移动性，是一种
全新的存储器体系结构。 
双端口存储器和多模块交叉存储器属于并行存储器结构。前者采用空间并行技术，后
者采用时间并行技术。这两种类型的存储器在科研和工程中大量使用。 
cache 是一种高速缓冲存储器，是为了解决 CPU 和主存之间速度不匹配而采用的一项
重要的硬件技术，并且发展为多级 cache 体系，指令 cache 与数据 cache 分设体系。要求 cache
的命中率接近于 1。主存与 cache 的地址映射有全相联、直接、组相联三种方式。其中组相
联方式是前二者的折中方案，适度地兼顾了二者的优点又尽量避免其缺点，从灵活性、命
中率、硬件投资来说较为理想，因而得到了普遍采用。 
用户程序按照虚地址(逻辑地址)编程并存放在辅存中。程序运行时，由地址变换机构
依据当时分配给该程序的实地址空间把程序的一部分调入实存(物理存储空间或主存空
间)。由操作系统在硬件的支持下对程序进行虚地址到实地址的变换，这一过程称为程序的
再定位。每次访存时，首先判断该虚地址所对应的部分是否在实存中：如果是，则进行地
址转换并用实地址访问主存；否则，按照某种算法将辅存中的部分程序调度进内存，再按
同样的方法访问主存。对应用程序而言，如果主存的命中率很高，虚存的访问时间就接近
于主存访问时间，而虚存的大小仅仅依赖于辅存的大小。 
虚存机制也要解决一些关键问题，包括调度问题、地址映射问题和替换问题等。在操
作系统的控制下，硬件和系统软件为用户解决了上述问题，从而使应用程序的编程大大   
简化。 
页式虚拟存储系统中，虚地址空间和主存空间都被分成大小相等的页，通过页表可以
把虚地址转换成物理地址。为了避免对主存访问次数增多，可以对页表本身实行二级缓存，
把页表中的最活跃部分存放在转换后援缓冲器(TLB)中。 
分页方式的缺点是页长与程序的逻辑大小不相关，而分段方式则可按照程序的自然分
界将内存空间划分为长度可以动态改变的存储区域。在段式虚拟存储系统中，虚地址由段
号和段内地址(偏移量)组成。虚地址到实主存地址的变换通过段表实现。 
段页式虚拟存储器是段式虚拟存储器和页式虚拟存储器的结合，程序按页进行调入和
调出操作，但可按段进行编程、保护和共享。 
虚拟存储器还解决了存储保护等问题。在虚拟存储系统中，通常采用页表保护、段表保
护和键式保护方法实现存储区域保护。还可以结合对主存信息的使用方式实现访问方式保护。 

计算机的程序是由一系列的机器指令组成的。 
指令就是要计算机执行某种操作的命令。从计算机组成的层次结构来说，计算机的指
令有微指令、机器指令和宏指令之分。微指令是微程序级的命令，它属于硬件；宏指令是
由若干条机器指令组成的软件指令，它属于软件；而机器指令则介于微指令与宏指令之间，
通常简称为指令，每一条指令可完成一个独立的算术运算或逻辑运算操作。 
本章所讨论的指令，是机器指令。一台计算机中所有机器指令的集合，称为这台计算
机的指令系统(指令集)。指令系统是表征一台计算机性能的重要因素，它的格式与功能不仅
影响到机器的硬件结构，而且影响到系统软件。因为指令是设计一台计算机的硬件与低层
软件的接口。 
20 世纪 50 年代，由于受器件限制，计算机的硬件结构比较简单，所支持的指令系统只
有定点加减、逻辑运算、数据传送、转移等十几至几十条指令。60 年代后期，随着集成电路
的出现，硬件功能不断增强，指令系统越来越丰富，除以上基本指令外，还设置了乘除运算、
浮点运算、十进制运算、字符串处理等指令，指令数目多达一二百条，寻址方式也趋多样化。 
随着集成电路的发展和计算机应用领域的不断扩大，60 年代后期开始出现系列计算机。
所谓系列计算机，是指基本指令系统相同、基本体系结构相同的一系列计算机，如 Pentium
系列就是曾经流行的一种个人机系列。一个系列往往有多种型号，但由于推出时间不同，
采用器件不同，它们在结构和性能上有所差异。通常是新机种在性能和价格方面比旧机种
优越。系列机解决了各机种的软件兼容问题，其必要条件是同一系列的各机种有共同的指
令系统，而且新推出的机种指令系统一定包含所有旧机种的全部指令。因此旧机种上运行
的各种软件可以不加任何修改便可在新机种上运行，大大减少了软件开发费用。 
70 年代末期，计算机硬件结构随着 VLSI 技术的飞速发展而越来越复杂化，大多数计
算机的指令系统多达几百条。我们称这些计算机为复杂指令系统计算机，简称 CISC。但是
如此庞大的指令系统不但使计算机的研制周期变长，且由于采用了大量使用频率很低的复

杂指令而造成硬件资源浪费，产生指令系统所谓百分比 20∶80 的规律，即最常使用的简单
指令仅占指令总数的 20%，但在程序中出现的频率却占 80%。为此人们又提出了便于 VLSI
技术实现的精简指令系统计算机，简称 RISC。 

指令系统的性能如何，决定了计算机的基本功能，因而指令系统的设计是计算机系统
设计中的一个核心问题，它不仅与计算机的硬件结构紧密相关，而且直接关系到用户的使
用需要。一个完善的指令系统应满足如下四方面的要求： 
完备性  完备性是指用汇编语言编写各种程序时，指令系统直接提供的指令足够使用，
而不必用软件来实现。完备性要求指令系统丰富、功能齐全、使用方便。 
一台计算机中最基本、必不可少的指令是不多的。许多指令可用最基本的指令编程来
实现。例如，乘除运算指令、浮点运算指令可直接用硬件来实现，也可用基本指令编写的
程序来实现。采用硬件指令的目的是提高程序执行速度，便于用户编写程序。 
有效性  有效性是指利用该指令系统所编写的程序能够高效率地运行。高效率主要表
现在程序占据存储空间小、执行速度快。一般来说，一个功能更强、更完善的指令系统，
必定有更好的有效性。 
规整性  规整性包括指令系统的对称性、匀齐性、指令格式和数据格式的一致性。对
称性是指：在指令系统中所有的寄存器和存储器单元都可同等对待，所有的指令都可使用
各种寻址方式；匀齐性是指：一种操作性质的指令可以支持各种数据类型，如算术运算指
令可支持字节、字、双字整数的运算，十进制数运算和单、双精度浮点数运算等；指令格
式和数据格式的一致性是指：指令长度和数据长度有一定的关系，以方便处理和存取。例
如，指令长度和数据长度通常是字节长度的整数倍。 
兼容性  系列机各机种之间具有相同的基本结构和共同的基本指令系统，因而指令系
统是兼容的，即各机种上基本软件可以通用。但由于不同机种推出的时间不同，在结构和
性能上有差异，做到所有软件都完全兼容是不可能的，只能做到“向上兼容”，即低档机上
运行的软件可以在高档机上运行。 
计算机的程序，就是人们把需要用计算机解决的问题变换成计算机能够识别的一串指
令或语句。编写程序的过程，称为程序设计，而程序设计所使用的工具则是计算机语言。 
计算机语言有高级语言和低级语言之分。高级语言如 C，FORTRAN 等，其语句和用法
与具体机器的指令系统无关。低级语言分为机器语言(二进制语言)和汇编语言(符号语言)，
这两种语言都是面向机器的语言，它们和具体机器的指令系统密切相关。机器语言用指令
代码编写程序，而符号语言用指令助记符来编写程序。表 4.1 列出了高级语言与低级语言的
性能比较。 
计算机能够直接识别和执行的唯一语言是二进制机器语言，但人们用它来编写程序很
不方便。另一方面，人们采用符号语言或高级语言编写程序，虽然对人提供了方便，但是
机器却不懂这些语言。为此，必须借助汇编器(汇编程序)或编译器(编译程序)，把符号语
言或高级语言翻译成二进制码组成的机器语言。 
 
汇编语言依赖于计算机的硬件结构和指令系统。不同的机器有不同的指令，所以用汇
编语言编写的程序不能在其他类型的机器上运行。 
高级语言与计算机的硬件结构及指令系统无关，在编写程序方面比汇编语言优越。但
是高级语言程序“看不见”机器的硬件结构，因而不能用它来编写直接访问机器硬件资源(如
某个寄存器或存储器单元)的系统软件或设备控制软件。为了克服这一缺陷，一些高级语言
(如 C，FORTRAN 等)提供了与汇编语言之间的调用接口。用汇编语言编写的程序，可作为
高级语言的一个外部过程或函数，利用堆栈来传递参数或参数的地址。两者的源程序通过
编译或汇编生成目标(OBJ)文件后，利用连接程序(LINKER)把它们连接成可执行文件便可
运行。采用这种方法，用高级语言编写程序时，若用到硬件资源，则可用汇编程序来实现。 
机器语言程序员看到的计算机的属性就是指令系统体系结构，简称 ISA(Instruction Set 
Architecture)，是与程序设计有关的计算机架构。指令系统体系结构主要包括：寄存器组织，
存储器的组织和寻址方式，I/O 系统结构，数据类型及其表示，指令系统，中断机制，机器
工作状态的定义及切换，以及保护机制等。 
机器指令是用机器字来表示的。表示一条指令的机器字，就称为指令字，通常简称
指令。 
指令格式，则是指令字用二进制代码表示的结构形式，通常由操作码字段和地址码字
段组成。操作码字段表征指令的操作特性与功能，而地址码字段通常指定参与操作的操作
数的地址。因此，一条指令的结构可用如下形式来表示： 
操作码字段 OP 
地址码字段 A 
设计计算机时，对指令系统的每一条指令都要规定一个操作码。 
指令的操作码 OP 表示该指令应进行什么性质的操作，如进行加法、减法、乘法、除法、
取数、存数等。不同的指令用操作码字段的不同编码来表示，每一种编码代表一种指令。
例如，操作码 001 可以规定为加法操作；操作码 010 可以规定为减法操作；而操作码 110
可以规定为取数操作等。CPU 中的专门电路用来解释每个操作码，因此机器就能执行操作
码所表示的操作。 
组成操作码字段的位数一般取决于计算机指令系统的规模。较大的指令系统就需要更
多的位数来表示每条特定的指令。例如，一个指令系统只有 8 条指令，则有 3 位操作码就
够了(23=8)。如果有 32 条指令，那么就需要 5 位操作码(25=32)。一般来说，一个包含 n
位的操作码最多能够表示 2n 条指令。 
对于一个机器的指令系统，在指令字中操作码字段和地址码字段长度通常是固定的。
在单片机中，由于指令字较短，为了充分利用指令字长度，指令字的操作码字段和地址码
字段是不固定的，即不同类型的指令有不同的划分，以便尽可能用较短的指令字长来表示
越来越多的操作种类，并在越来越大的存储空间中寻址。 
根据一条指令中有几个操作数地址，可将该指令称为几操作数指令或几地址指令。一
般的操作数有被操作数、操作数及操作结果这三种数，因而就形成了三地址指令格式，这
是早期计算机指令的基本格式。在三地址指令格式的基础上，后来又发展成二地址格式、
一地址格式和零地址格式。各种不同操作数的指令格式如下所示： 
(1)零地址指令的指令字中只有操作码，而没有地址码。例如，停机指令就不需要地址
码，因为停机操作不需要操作数。 
(2)一地址指令只有一个地址码，它指定一个操作数，另一个操作数地址是隐含的。例
如，以运算器中累加寄存器 AC 中的数据为隐含的被操作数，指令字的地址码字段所指明
的数为操作数，操作结果又放回累加寄存器 AC 中，而累加寄存器中原来的数即被覆盖掉
了，其数学含义为 
AC←(AC)OP(A) 
式中，OP 表示操作性质，如加、减、乘、除等；(AC)表示累加寄存器 AC 中的数；(A)表
示内存中地址为 A 的存储单元中的数，或者是运算器中地址为 A 的通用寄存器中的数；←
表示把操作(运算)结果传送到指定的地方。 
注意：地址码字段 A 指明的是操作数的地址，而不是操作数本身。 
(3)二地址指令常称为双操作数指令，它有两个地址码字段 A1 和 A2，分别指明参与操
作的两个数在内存中或运算器中通用寄存器的地址，其中地址 A1兼作存放操作结果的地址。
      计算机组成原理 
122
其数学含义为 
A1←(A1)OP(A2) 
(4)三地址指令字中有三个操作数地址 A1，A2 和 A3，其数学含义为 
A3←(A1)OP(A2) 
式中，A1 为被操作数地址，也称源操作数地址；A2 为操作数地址，也称终点操作数地址；
A3 为存放操作结果的地址。 
三地址指令中 A1，A2，A3 通常指定为运算器中通用寄存器的地址，这是为了加快指令
执行速度。 
在二地址指令格式中，从操作数的物理位置来说，又可归结为三种类型： 
第一种是访问内存的指令格式，我们称这类指令为存储器存储器(SS)型指令。这种指
令操作时都是涉及内存单元，即参与操作的数都放在内存里。从内存某单元中取操作数，
操作结果存放至内存另一单元中，因此机器执行这种指令需要多次访问内存。 
第二种是访问寄存器的指令格式，我们称这类指令为寄存器寄存器(RR)型指令。机器
执行这类指令过程中，需要多个通用寄存器或个别专用寄存器，从寄存器中取操作数，把
操作结果放到另一寄存器。机器执行寄存器-寄存器型指令的速度很快，因为执行这类指令，
不需要访问内存。 
第三种类型为寄存器-存储器(RS)型指令，执行此类指令时，既要访问内存单元，又要
访问寄存器。 
在 CISC 计算机中，一个指令系统中指令字的长度和指令中的地址结构并不是单一的，
往往采用多种格式混合使用，这样可以增强指令的功能。 
4.2.3  指令字长度 
一个指令字中包含二进制代码的位数，称为指令字长度。而机器字长是指计算机能直
接处理的二进制数据的位数，它决定了计算机的运算精度。机器字长通常与主存单元的位
数一致。指令字长度等于机器字长度的指令，称为单字长指令；指令字长度等于半个机器
字长度的指令，称为半字长指令；指令字长度等于两个机器字长度的指令，称为双字长指令。
例如，IBM370 系列，它的指令格式有 16 位(半字)的，有 32 位(单字)的，还有 48 位(一个
半字)的。在 Pentium 系列机中，指令格式也是可变的：有 8 位、16 位、32 位、64 位不等。 
早期计算机使用多字长指令的目的，在于提供足够的地址位来解决访问内存任何单元
的寻址问题。但是使用多字长指令的缺点是必须两次或三次访问内存以取出一整条指令，
这就降低了 CPU 的运算速度，同时又占用了更多的存储空间。 
在一个指令系统中，如果各种指令字长度是相等的，称为等长指令字结构，它们可以
都是单字长指令或半字长指令。这种指令字结构简单，且指令字长度是不变的。如果各种
指令字长度随指令功能而异，如有的指令是单字长指令，有的指令是双字长指令，就称为
变长指令字结构。这种指令字结构灵活，能充分利用指令长度，但指令的控制较复杂。随
着技术发展，指令字长度逐渐变成多于 32 位的固定长度。 

由于硬件只能识别 1 和 0，所以采用二进制操作码是必要的，但是我们用二进制来书写
程序却非常麻烦。为了便于书写和阅读程序，每条指令通常用 3 个或 4 个英文缩写字母来
表示。这种缩写码称为指令助记符，如表 4.3 所示。这里我们假定指令系统只有 7 条指令，
所以操作码只需 3 位二进制。 

于一条存数指令，可以用助记符 STO 表示操作码 110。 
需要注意的是，在不同的计算机中，指令助记符的规定是不一样的。 
我们知道，硬件只能识别二进制语言。因此，指令助记符还必须转换成与它们相对应
的二进制操作码。这种转换借助汇编器可以自动完成，汇编器的作用相当于一个“翻译”。 

1. 八位微型计算机的指令格式 
早期的 8 位微型机字长只有 8 位。由于指令字较短，所以指令结构是一种可变字长形
式。指令格式包含单字长指令、双字长指令、三字长指令等多种。指令格式如下： 
 
单字长指令只有操作码，没有操作数地址。双字长或三字长指令包含操作码和地址码。由
于内存按字节编址，所以单字长指令每执行一条指令后，指令地址加 1。双字长指令或三字
长指令每执行一条指令时，必须从内存连续读出 2 字节或 3 字节代码，所以，指令地址要
加 2 或加 3，可见多字长的指令格式不利于提高机器速度。 
2. MIPS R4000 指令格式 
MIPS R4000 是 20 世纪 80 年代后期推出的 RISC 系统，字长 32 位，字节寻址。它的指
令格式简单，指令数量少，通用寄存器 32 个。其算术指令格式如下： 
 
指令格式中各个字段的含义如下： 
OP 字段——操作码，指定一条指令的基本操作。 
rs 字段——指定第 1 个源操作数寄存器，最多有 32 个寄存器。 
rt 字段——指定第 2 个源操作数寄存器，最多有 32 个寄存器。 
rd 字段——指定存放操作结果的目的数寄存器，最多有 32 个寄存器。 
shamt 字段——移位值，用于移位指令。 
funct 字段——函数码，指定 R 型指令的特定操作。 
在 MIPS 中，所有的算术运算，数据必须放在通用寄存器中。此时的指令格式称为 R
型(寄存器)指令。R 型指令格式就是上面所示的算术指令格式。 
在 MIPS 中，访问存储器(取数或存数)需要使用数据传送指令。此时的指令格式，称为
I 型(立即数)指令，其指令格式如下所示： 
 
16 位字段 address(地址)提供取字指令(IW)，存字指令(SW)访问存储器的基值地址码
(也称位移量)。 
保持指令格式基本一致可以降低硬件复杂程度。例如，R 型和 I 型格式的前 3 个字段长
度相等，并且名称也一样；I 型格式的第四个字段和 R 型后三个字段的长度相等。 
指令格式由第一个字段的值来区分：每种格式的第一个字段(OP)都被分配了一套不同
的值，因此计算机硬件可以根据 OP 来确定指令的后半部分是三个字段(R 型)还是一个字段
(I 型)。表 4.4 给出了 MIPS 指令的每一字段的值(十进制)。 

表中，reg 表示 0～31 中间的一个寄存器号，address 表示一个 16 位地址，而—表示该格式中这个字段没有出现。注意：加
法(add)指令和减法(sub)指令的 OP 字段值相同；硬件根据 funct 字段来确定操作类型：加法(32)或减法(34)。 
3. ARM 的指令格式 
ARM 是字长 32 位的嵌入式处理机，2008 年生产了 4 亿片，它具有世界上最流行的指
令系统。下面是 ARM 指令系统的一种指令格式： 
 
各字段的含义如下： 
opcode——指明指令的基本操作，称为操作码。 
Rd——指明目标寄存器地址(4 位)，共 16 个寄存器。 
Rn——指明源寄存器地址(4 位)，共 16 个寄存器。 
operand 2——指明第 2 个源操作数。 
I——指明立即数，如果 I=0，第 2 个源操作数在寄存器中；如果 I=1，第 2 个源操作数
是 12 位的立即数。 
S——设置状态，该字段涉及条件转移指令。 
cond——指明条件，该字段涉及条件转移指令。 
F——说明指令类型，当需要时该字段允许设置不同的指令。 
4. Pentium 指令格式 
Pentium 机的指令字长度是可变的：从 1B 到 12B，1B 表示 1 字节。指令格式如下所示。
这种非固定长度的指令格式是典型的 CISC 结构特征。之所以如此，一是为了与它的前身
80486 保持兼容，二是希望能给编译程序写作者以更多灵活的编程支持。 

指令本身由操作码字段、Mod-R/M 字段、SIB 字段、位移量字段、立即数字段组成。
除操作码字段外，其他四个字段都是可选字段(不选时取 0 字节)。 
Mod-R/M 字段规定了存储器操作数的寻址方式，给出了寄存器操作数的寄存器地址号。
除少数预先规定寻址方式的指令外，绝大多数指令都包含这个字段。 
SIB 字段由比例系数 S、变址寄存器号 I、基址寄存器号 B 组成。利用该字段，可和
Mod-R/M 字段一起，对操作数来源进行完整的说明。显然，Pentium 采用 RS 型指令，指令
格式中只有一个存储器操作数。 

机器指令对数据进行操作，数据通常分以下四类： 
地址数据  地址实际上也是一种形式的数据。多数情况下，对指令中操作数的引用必
须完成某种计算，才能确定它们在主存中的有效地址。此时，地址将被看作无符号整数。 
数值数据  计算机中普遍使用的三种类型的数值数据是：①定点整数或定点小数；
②浮点数；③压缩十进制数，1 字节用 2 位 BCD 码表示。 
字符数据  也称为文本数据或字符串，目前广泛使用 ASCII 码。以这种编码，每个字
符被表示成唯一的 7 位代码，共有 128 个可表示字符，加上最高位(b7)用作奇偶校验，因此
每个字符总是以 8 位的字节来存储和传送。 
逻辑数据  一个单元由若干二进制位项组成，每个位的值可以是 1 或 0。当数据以这种
方式看待时，称为逻辑性数据，它创造了对某个具体位进行布尔逻辑运算的机会。 

Pentium 能处理 8 位(字节)、16 位(字)、32 位(双字)、64 位(四字)各种长度的数据类
型。为求得数据结构最大的灵活性和最有效地使用存储器，单字不需要在偶数地址上对齐，
双字也不需要在 4 倍(字节)整数地址上对齐，四字不需要在 8 倍(字节)整数地址上对齐。
然而当经 32 位数据总线存取数据时，数据传送是以双字为单位进行的，双字的起始地址是
能被 4 整除的。表 4.7 列出了 Pentium 的数据类型。 

Power PC 是精简指令系统计算机，能处理 8 位(字节)、16 位(半字)、32 位(字)和 64
位(双字)各种长度的数据。处理器能识别如下数据类型： 
(1)无符号字节  用于逻辑和整数算术运算。它由存储器取出装入通用寄存器时，寄存
器左端以 0 填充。 
(2)无符号半字  同无符号字节，只是一个 16 位的量。 
(3)有符号半字  用于 16 位算术运算。由存储器取出装入通用寄存器时，要进行符号
位扩展，即所有空出位用符号位填充。 
(4)无符号字  用于 32 位逻辑运算，或作为地址指针。 
(5)有符号字  用于 32 位算术运算。 
(6)无符号双字  用作 64 位地址指针。 
(7)字节串  可从 0 到 128 字节长。 
(8)浮点数  支持 IEEE 754 中定义的单、双精度浮点数据类型。 

存储器既可用来存放数据，又可用来存放指令。因此，当某个操作数或某条指令存放
在某个存储单元时，其存储单元的编号，就是该操作数或指令在存储器中的地址。 
在存储器中，操作数或指令字写入或读出的方式，有地址指定方式、相联存储方式和
堆栈存取方式。几乎所有的计算机，在内存中都采用地址指定方式。当采用地址指定方式
时，形成操作数或指令地址的方式，称为寻址方式。寻址方式分为两类，即指令寻址方式
和数据寻址方式，前者比较简单，后者比较复杂。值得注意的是，在冯·诺依曼型结构的
计算机中，内存中指令的寻址与数据的寻址是交替进行的。而哈佛型计算机中指令寻址和
数据寻址是独立进行的。 

指令的寻址方式有两种，一种是顺序寻址方式，另一种是跳跃寻址方式。 
1. 顺序寻址方式 
由于指令地址在内存中按顺序安排，当执行一段程序时，通常是按一条指令接一条指
令的顺序进行。就是说，从存储器取出第一条指令，然后执行这条指令；接着从存储器取

出第二条指令，再执行第二条指令；接着再取出第三条指令……这种程序顺序执行的过程，
我们称为指令的顺序寻址方式。为此，必须使用程序计数器(又称指令指针寄存器)PC 来计
数指令的顺序号，该顺序号就是指令在内存中的地址。图 4.1(a)是指令顺序寻址方式的示
意图。 

2. 跳跃寻址方式 
当程序转移执行的顺序时，指令的寻址就采取跳跃寻址方式。所谓跳跃，是指下条指
令的地址码不是由程序计数器给出的，而是由本条指令给出。图 4.1(b)画出了指令跳跃寻
址方式的示意图。注意，程序跳跃后，按新的指令地址开始顺序执行。因此，指令计数器
的内容也必须相应改变，以便及时跟踪新的指令地址。 
采用指令跳跃寻址方式，可以实现程序转移或构成循环程序，从而能缩短程序长度，
或将某些程序作为公共程序引用。指令系统中的各种条件转移或无条件转移指令，就是为
了实现指令的跳跃寻址而设置的。 

在指令执行过程中，操作数的来源一般有三个：①由指令中的地址码部分直接给出操
作数，虽然简便快捷，但是操作数是固定不变的；②将操作数存放在 CPU 内的通用数据寄
存器中，这样可以很快获取操作数，但是可以存储的操作数的数量有限；③更一般化的方
式是将操作数存放在内存的数据区中。而对于内存寻址，既可以在指令中直接给出操作数
的实际访存地址(称为有效地址)，也可以在指令的地址字段给出所谓的形式地址，在指令
执行时，将形式地址依据某种方式变换为有效地址再取操作数。 
形成操作数的有效地址的方法，称为操作数的寻址方式。 
例如，一种单地址指令的结构如下所示，其中用 X、I、A 各字段组成该指令的操作数
地址。 
操作码 
OP 
变址 
X 
间址 
I 
形式地址 
A 

由于指令中操作数字段的地址码由形式地址和寻址方式特征位等组合形成，因此，一
般来说，指令中所给出的地址码，并不是操作数的有效地址。 
形式地址 A，也称偏移量，它是指令字结构中给定的地址量。寻址方式特征位，此处
由间址位和变址位组成。如果这条指令无间址和变址的要求，那么形式地址就是操作数的
有效地址。如果指令中指明要变址或间址变换，那么形式地址就不是操作数的有效地址，
而要经过指定方式的变换，才能形成有效地址。因此，寻址过程就是把操作数的形式地址，
变换为操作数的有效地址的过程。 
由于大型机、微型机和单片机结构不同，从而形成了各种不同的操作数寻址方式。
表 4.8 列出了比较典型而常用的寻址方式，而图 4.2 画出了它们形成有效地址的示意图。 

 
1. 隐含寻址 
这种类型的指令，不是明显地给出操作数的地址，而是在指令中隐含着操作数的地址，
如图 4.2(a)所示。例如，单地址的指令格式，就不是明显地在地址字段中指出第二操作数
的地址，而是规定累加寄存器 AC 作为第二操作数地址。指令格式明显指出的仅是第一操
作数的地址 D。因此，累加寄存器 AC 对单地址指令格式来说是隐含地址。 
2. 立即寻址 
指令的地址字段指出的不是操作数的地址，而是操作数本身，这种寻址方式称为立即
寻址，如图 4.2(b)所示。指令中的操作数称为立即数。立即寻址方式的特点是指令中包含
的操作数立即可用，节省了访问内存的时间。 
3. 直接寻址 
直接寻址是一种基本的寻址方法，其特点是：在指令格式的地址字段中直接指出操作
数在内存的地址 A。由于操作数的地址直接给出而不需要经过某种变换，所以称这种寻址
方式为直接寻址方式。图 4.2(c)是直接寻址方式的示意图。 
采用直接寻址方式时，指令字中的形式地址 A 就是操作数的有效地址 EA。因此通常把
形式地址 A 又称为直接地址。此时，由寻址模式给予指示，如 X1=0。 
如果用 D 表示操作数，那么直接寻址的表达式为 D=(A)。 

4. 间接寻址 
间接寻址是相对于直接寻址而言的，在间接寻址的情况下，指令地址字段中的形式地
址 A 不是操作数 D 的真正地址，而是操作数地址的指示器。图 4.2(d)画出了间接寻址方式
的示意图。通常，在间接寻址情况下，由寻址特征位给予指示。如果把直接寻址和间接寻
址结合起来，指令有如下形式： 
操作码 
I 
A 
若寻址特征位 I=0，表示直接寻址，这时有效地址 EA=A；若 I=1，则表示间接寻址，
这时有效地址 EA=(A)。 
间接寻址方式是早期计算机中经常采用的方式，但由于两次访存，影响指令执行速度，
现在较少使用。 
5. 寄存器寻址 
当操作数不在内存中，而是放在 CPU 的通用寄存器中时，可采用寄存器寻址方式，如
图 4.2(e)所示。显然，此时指令中给出的操作数地址不是内存的地址单元号，而是通用寄
存器的编号，EA=R。指令结构中的 RR 型指令，就是采用寄存器寻址方式的例子。 
6. 寄存器间接寻址 
寄存器间接寻址与寄存器寻址的区别在于：指令格式中的寄存器内容不是操作数，而

是操作数的地址，该地址指明的操作数在内存中，如图 4.2(f)所示。此时 EA=(R)。 
7. 偏移寻址 
一种强有力的寻址方式是直接寻址和寄存器间接寻址方式的结合，它有几种形式，我
们称它为偏移寻址，如图 4.2(g)所示。有效地址计算公式为 
EA=A+(R) 
它要求指令中有两个地址字段，至少其中一个是显示的。容纳在一个地址字段中的形式地
址 A 直接被使用；另一个地址字段，或基于操作码的一个隐含引用，指的是某个专用寄存
器。此寄存器的内容加上形式地址 A 就产生有效地址 EA。 
常用的三种偏移寻址是相对寻址、基址寻址、变址寻址。 
相对寻址  隐含引用的专用寄存器是程序计数器(PC)，即 EA=A+(PC)，它是当前 PC
的内容加上指令地址字段中 A 的值。一般来说，地址字段的值在这种操作下被看成 2 的补
码数的值。因此有效地址是对当前指令地址的一个上下范围的偏移，它基于程序的局部性
原理。使用相对寻址可节省指令中的地址位数，也便于程序在内存中成块搬动。 
基址寻址  被引用的专用寄存器含有一个存储器地址，地址字段含有一个相对于该地
址的偏移量(通常是无符号整数)。寄存器的引用可以是显式的，也可以是隐式的。基址寻
址也利用了存储器访问的局部性原理。后面讲到的段寻址方式中，就采用了段基址寄存器，
它提供了一个范围很大的存储空间。 
变址寻址  地址域引用一个主存地址，被引用的专用寄存器含有对那个地址的正偏移
量。这意味着主存地址位数大于寄存器中的偏移量位数，与基址寻址刚好相反。但是二者
有效地址的计算方法是相同的。变址的用途是为重复操作的完成提供一种高效机制。例如，
主存位置 A 处开始放一个数值列表，打算为表的每个元素加 1。我们需要取每个数位，对
它加 1，然后再存回，故需要的有效地址序列是 A, A+1, A+2, …直到最后一个位置。此时
值 A 存入指令地址字段，再用一个变址寄存器(初始化为 0)。每次操作之后，变址寄存器
内容增 1。此时，EA=A+(R)，R←(R+1)。 
8. 段寻址 
微型机中采用了段寻址方式，例如，它们可以给定一个
20 位的地址，从而有 220=1MB 存储空间的直接寻址能力。为
此将整个 1MB 空间存储器按照最大长度 64KB 划分成若干
段。在寻址一个内存具体单元时，由一个基地址再加上某些
寄存器提供的 16 位偏移量来形成实际的 20 位物理地址。这
个基地址就是 CPU 中的段寄存器。在形成 20 位物理地址时，
段寄存器中的 16 位数会自动左移 4 位，然后与 16 位偏移量
相加，即可形成所需的内存地址，如图 4.3 所示。这种寻址
方式的实质还是基址寻址。 
思考题  你能说出段寻址方式的创新点吗？ 
9. 堆栈寻址 
堆栈有寄存器堆栈和存储器堆栈两种形式，它们都以先进后出的原理存储数据，如   
图 4.2(h)所示。不论是寄存器堆栈，还是存储器堆栈，数据的存取都与栈顶地址打交通，
 

为此需要一个隐式或显式的堆栈指示器(寄存器)。数据进栈时使用 PUSH 指令，将数据压
入栈顶地址，堆栈指示器减 1；数据退栈时，使用 POP 指令，数据从栈顶地址弹出，堆栈
指示器加 1。从而保证了堆栈中数据先进后出的存取顺序。 
不同的指令系统采用不同的方式指定寻址方式。一般而言，有些指令固定使用某种寻
址方式；有些指令则允许使用多种寻址方式，或者在指令中加入寻址方式字段指明，或者
对不同的寻址方式分配不同的操作码而把它们看作不同的指令。有些指令系统会把常见的
寻址方式组合起来，构成更复杂的复合寻址方式。 
4.4.3  寻址方式举例 
1. Pentium 的寻址方式 
Pentium 的外部地址总线宽度是 36 位，但它也支持 32 位物理地址空间。 
在实地址模式下，逻辑地址形式为段寻址方式：将段名所指定的段寄存器内容(16 位)
左移 4 位，低 4 位补全 0，得到 20 位段基地址，再加上段内偏移，即得 20 位物理地址。 
在保护模式下，32 位段基地址加上段内偏移得到 32 位线性地址 LA。由存储管理部件
将其转换成 32 位的物理地址，如图 4.4 所示。这个转换过程对指令系统和程序员是透明的。
有 6 个用户可见的段寄存器，每个保存相应段的起始地址、段长和访问权限。 
 
图 4.4  Pentium 寻址方式的计算 
无论是实地址模式还是保护模式，段基地址的获取方式已是固定的方式。因此这里介
绍的寻址方式主要是指有效地址的获取方式，用字母 EA 表示。表 4.9 列出了 Pentium 机的
9 种寻址方式。 
 

 
下面对 32 位寻址方式作几点说明。 
(1)立即寻址：立即数可以是 8 位、16 位、32 位的操作数，包含在指令中。 
(2)寄存器寻址：一般指令或使用 8 位通用寄存器(AH，AL，BH，BL，CH，CL，DH，
DL)，或使用 16 位通用寄存器(AX，BX，CX，DX，SI，DI，SP，BP)，或使用 32 位通用
寄存器(EAX，EBX，ECX，EDX，ESI，EDI，ESP，EBP)。对 64 位浮点数操作，要使用
一对 32 位寄存器。有些指令用段寄存器(CS，DS，ES，SS，FS ，GS)来实施寄存器寻址
方式。 
以下的寻址方式引用的是存储器位置，通过指定包含此位置的段和离段起点的位移来
说明存储器位置。 
(3)偏移量寻址：也称直接寻址，偏移量就是操作数距段起点的位移。偏移量长度达 32
位，能用于访问全局。 
(4)基址寻址：基址寄存器 B 可以是上述通用寄存器中任何一个。基址寄存器 B 的内容
为有效地址。 
(5)基址+偏移量寻址：基址寄存器 B 是 32 位通用寄存器中任何一个。 
(6)比例变址+偏移量寻址：也称为变址寻址方式，变址寄存器 I 是 32 位通用寄存器中
除 ESP 外的任何一个，而且可将此变址寄存器内容乘以 1、2、4 或 8 的比例因子 S，然后
再加上偏移量而得到有效地址。 
(7)、(8)两种寻址方式是(4)、(6)两种寻址方式的组合，此时偏移量可有可无。 
(9)相对寻址：适用于转移控制类指令。用当前指令指针寄存器 EIP 或 IP 的内容(下一
条指令地址)加上一个有符号的偏移量，形成 CS 段的段内偏移。 
2. Power PC 寻址方式 
不像 Pentium 和大多数 CISC 机器，Power PC 是 RISC 机器，它采用了相当简单的一组
寻址方式。如表 4.10 所示，这些寻址方式按指令类型来分类。 
 

不同机器的指令系统是各不相同的。从指令的操作码功能来考虑，一个较完善的指令
系统，应当有数据处理、数据存储、数据传送、程序控制四大类指令，具体有数据传送类
指令、算术运算类指令、逻辑运算类指令、程序控制类指令、输入输出类指令、字符串类
指令、系统控制类指令。 
1. 数据传送指令 
数据传送指令主要包括取数指令、存数指令、传送指令、成组传送指令、字节交换指
令、清寄存器指令、堆栈操作指令等，这类指令主要用来实现主存和寄存器之间，或寄存
器和寄存器之间的数据传送。例如，通用寄存器 Ri 中的数存入主存；通用寄存器 Ri 中的数
送到另一通用寄存器 Rj；从主存中取数至通用寄存器 Ri；寄存器清零或主存单元清零等。 
2. 算术运算指令 
这类指令包括二进制定点加、减、乘、除指令，浮点加、减、乘、除指令，求反、求
补指令，算术移位指令，算术比较指令，十进制加、减运算指令等。这类指令主要用于定
点或浮点的算术运算，大型机中有向量运算指令，直接对整个向量或矩阵进行求和、求积
运算。 
3. 逻辑运算指令 
这类指令包括逻辑加、逻辑乘、按位加、逻辑移位等指令，主要用于无符号数的位操
作、代码的转换、判断及运算。 
移位指令用来对寄存器的内容实现左移、右移或循环移位。左移时，若寄存器的数看
作算术数，符号位不动，其他位左移，低位补零，右移时则高位补零，这种移位称算术移
位。移位时，若寄存器的数为逻辑数，则左移或右移时，所有位一起移位，这种移位称逻
辑移位。 
4. 程序控制指令 
程序控制指令也称转移指令。计算机在执行程序时，通常情况下按指令计数器的现行
地址顺序取指令。但有时会遇到特殊情况：机器执行到某条指令时，出现了几种不同结果，
这时机器必须执行一条转移指令，根据不同结果进行转移，从而改变程序原来执行的顺序。
这种转移指令称为条件转移指令。转移条件有进位标志(C)、结果为零标志(Z)、结果为负
标志(N)、结果溢出标志(V)和结果奇偶标志(P)等。 
除各种条件转移指令外，还有无条件转移指令、转子程序指令、返回主程序指令、中
断返回指令等。 
转移指令的转移地址一般采用直接寻址和相对寻址方式来确定。若采用直接寻址方式，
则称为绝对转移，转移地址由指令地址码部分直接给出。若采用相对寻址方式，则称为相
对转移，转移地址为当前指令地址(PC 的值)和指令地址部分给出的偏移量之和。 
5. 输入输出指令 
输入输出指令主要用来启动外围设备，检查测试外围设备的工作状态，并实现外部设
备和 CPU 之间，或外围设备与外围设备之间的信息传送。 
各种不同机器的输入输出指令差别很大。例如，有的机器指令系统中含有输入输出指
令，而有的机器指令系统中没有设置输入输出指令。这是因为后一种情况下外部设备的寄
存器和存储器单元统一编址，CPU 可以和访问内存一样去访问外部设备。换句话说，可以
使用取数、存数指令来代替输入输出指令。 
6. 字符串处理指令 
字符串处理指令是一种非数值处理指令，一般包括字符串传送、字符串转换(把一种编
码的字符串转换成另一种编码的字符串)、字符串比较、字符串查找(查找字符串中某一子
串)、字符串抽取(提取某一子串)、字符串替换(把某一字符串用另一字符串替换)等。这类
第 4 章  指 令 系 统       137 
指令在文字编辑中对大量字符串进行处理。 
7. 特权指令 
特权指令是指具有特殊权限的指令。由于指令的权限最大，若使用不当，会破坏系统
和其他用户信息。因此这类指令只用于操作系统或其他系统软件，一般不直接提供给用户
使用。 
在多用户、多任务的计算机系统中特权指令必不可少。它主要用于系统资源的分配和
管理，包括改变系统工作方式，检测用户的访问权限，修改虚拟存储器管理的段表、页表，
完成任务的创建和切换等。 
8. 其他指令 
除以上各类指令外，还有状态寄存器置位、复位指令、测试指令、暂停指令、空操作
指令，以及其他一些系统控制用的特殊指令。 

CISC 的指令系统一般多达二三百条，如 VAX11/780 计算机有 303 条指令，18 种寻址
方式。Pentium 机也有 191 条指令，9 种寻址方式。但是对 CISC 进行的测试表明，最常使
用的是一些最简单最基本的指令，仅占指令总数的 20%，但在程序中出现的频率却占 80%。
因此从教学目的考虑，下面给出一个基本指令系统的操作，如表 4.12 所示。从应用角度考
虑，这些指令的功能也具有普遍意义，几乎所有计算机的指令系统中都能找到这些指令。 

RISC 指令系统的最大特点是：①选取使用频率最高的一些简单指令，指令条数少；
②指令长度固定，指令格式种类少，寻址方式种类少；③只有取数/存数指令访问存储器，
其余指令的操作都在寄存器之间进行。表 4.13 列出了典型 RISC 指令系统的基本特征。 

表 4.14 比较了 RISC 和 CICS 的性能。设高级语言程序经编译后在机器上运行的机器
指令数为 I，每条机器指令执行时所需要的平均机器周期数是 C，每个机器周期的执行时间
为 T。表中 I、T 为比值，C 为实际周期数。由计算机执行程序的时间 P 的计算公式可以看
出两种类型的机器的性能差异： 
 
P=I×C×T 

 
下面以 Power PC 机为例来说明，该机是一个 32 位字长的计算机，共有 64 条指令。图 4.5
示出了它的指令类型与格式。 
 
Power PC 机有如下五种指令类型： 
(1)整数算术、逻辑、移位/旋转(循环移位)指令； 
(2)浮点算术指令； 
(3)取数/存数指令； 
(4)条件寄存器指令； 
(5)转移指令。 
      计算机组成原理 
140
所有的指令都是 32 位长，并有规整的格式。指令的前 6 位(网点表示)指定操作码部分。
在某些情况下在其他部分有此操作码的扩展，用于指定操作的细节(也用网点表示)。 
所有的取数/存数、算术、逻辑指令，在操作码之后是两个 5 位的寄存器字段，这表示
可以使用 32 个通用寄存器。 
转移指令包括了一个链接(L)位，它指示此转移指令之后的那条指令的有效地址是否放
入链接寄存器。两种转移指令格式还包含一个(A)位，它指示寻址方式是绝对寻址还是 PC
相对寻址。对于条件转移指令，CR 位字段指定条件寄存器中被测试的位，选项字段指向转
移发生的条件(如无条件转移；计数=0 转移；计数≠0 转移；条件是真转移；条件是假转移；
等等)。 
进行计算的大多数指令(算术、逻辑、浮点算术)都包含一个(R)位，它指示运算结果是
否应记录在条件寄存器中。这个特征对于转移预测处理是很有用的。 
浮点指令有三个源寄存器字段。多数情况下只使用两个源寄存器，少数指令涉及两个
源寄存器内容相乘，然后再加上或减去第三个源寄存器内容。这种复合指令经常用在矩阵
运算中，使得一部分内部积用“乘—加”来实现。 
思考题  你能说出 Power PC 机指令系统的特点吗？ 

汇编语言是计算机机器语言(二进制指令代码)进行符号化的一种表示方式，每一个基
本汇编语句对应一条机器指令。为了有一个完整概念，表 4.15 列出了嵌入式处理机 ARM
的汇编语言。其中操作数使用 16 个寄存器(r0，r1～r12，SP，Ir，PC)，230 个存储字(字节
编址，连续的字的地址之间相差 4)。 

 
在进行汇编语言程序设计时，可直接使用英文单词或其缩写表示指令，使用标识符表
示数据或地址，从而有效地避免了记忆二进制的指令代码，不再由程序设计人员为指令和
数据分配内存地址，直接调用操作系统的某些程序段完成输入输出及读写文件等操作功能。
用编辑程序建立好的汇编语言源程序，需要经过系统软件中的“汇编器”翻译为机器语言
程序之后，才能交付给计算机硬件系统去执行。 

 
本 章 小 结 
一台计算机中所有机器指令的集合，称为这台计算机的指令系统。指令系统是表征一
台计算机性能的重要因素，它的格式与功能不仅直接影响到机器的硬件结构，而且影响到
系统软件。 
指令格式是指令字用二进制代码表示的结构形式，通常由操作码字段和地址码字段组
成。操作码字段表征指令的操作特性与功能，而地址码字段指示操作数的地址。目前多采
用二地址、单地址、零地址混合方式的指令格式。指令字长度分为：单字长、半字长、双
字长三种形式。高档微机采用 32 位长度的单字长形式。 
形成指令地址的方式，称为指令寻址方式。有顺序寻址和跳跃寻址两种，由指令计数
器来跟踪。 
形成操作数地址的方式，称为数据寻址方式。操作数可放在专用寄存器、通用寄存器、
内存和指令中。数据寻址方式有隐含寻址、立即寻址、直接寻址、间接寻址、寄存器寻址、
寄存器间接寻址、相对寻址、基值寻址、变址寻址、块寻址、段寻址等多种。按操作数的
物理位置不同，有 RR 型和 RS 型。前者比后者执行的速度快。堆栈是一种特殊的数据寻址
方式，采用“先进后出”原理。按结构不同，分为寄存器堆栈和存储器堆栈。 
不同机器有不同的指令系统。一个较完善的指令系统应当包含数据传送类指令、算术
运算类指令、逻辑运算类指令、程序控制类指令、I/O 类指令、字符串类指令、系统控制类
指令。 
RISC 指令系统是目前计算机发展的主流，也是 CISC 指令系统的改进，它的最大特点
是：①指令条数少；②指令长度固定，指令格式和寻址方式种类少；③只有取数/存数指令
访问存储器，其余指令的操作均在寄存器之间进行。 
汇编语言与具体机器的依赖性很强。为了了解该语言的特点，列出了目前较流行的嵌
入式处理机 ARM 的汇编语言，以举一反三。 

当用计算机解决某个问题时，我们首先必须为它编写程序。程序是一个指令序列，这
个序列明确告诉计算机应该执行什么操作，在什么地方找到用来操作的数据。一旦把程序
装入内存储器，就可以由计算机部件来自动完成取指令和执行指令的任务。专门用来完成
此项工作的计算机部件称为中央处理器，通常简称 CPU。 
CPU 对整个计算机系统的运行是极其重要的，它具有如下四方面的基本功能。 
指令控制  程序的顺序控制，称为指令控制。由于程序是一个指令序列，这些指令的
相互顺序不能任意颠倒，必须严格按程序规定的顺序进行，因此，保证机器按顺序执行程
序是 CPU 的首要任务。 
操作控制  一条指令的功能往往是由若干个操作信号的组合来实现的，因此，CPU 管
理并产生由内存取出的每条指令的操作信号，把各种操作信号送往相应的部件，从而控制
这些部件按指令的要求进行动作。 
时间控制  对各种操作实施时间上的定时，称为时间控制。因为在计算机中，各种指
令的操作信号均受到时间的严格定时。另外，一条指令的整个执行过程也受到时间的严格
定时。只有这样，计算机才能有条不紊地自动工作。 
数据加工  所谓数据加工，就是对数据进行算术运算和逻辑运算处理。完成数据的加
工处理，是 CPU 的根本任务。因为，原始信息只有经过加工处理后才能对人们有用。 

运算器和控制器是组成 CPU 的两大核心部件。随着 VLSI 技术的发展，CPU 芯片外部
的一些逻辑功能部件，如浮点运算器、cache、总线仲裁器等往往集成到 CPU 芯片内部。 
从教学目的出发，本章以 CPU 执行指令为主线来组织教学内容。为便于读者建立计算
机的整机概念，突出主要矛盾，给出图 5.1 所示的 CPU 模型。 


控制器  由程序计数器、指令寄存器、指令译码器、时序产生器和操作控制器组成，
它是发布命令的“决策机构”，即完成协调和指挥整个计算机系统的操作。控制器的主要功
能有： 
(1)从指令 cache 中取出一条指令，并指出下一条指令在指令 cache 中的位置。 
(2)对指令进行译码或测试，并产生相应的操作控制信号，以便启动规定的动作。比如，
一次数据 cache 的读/写操作，一个算术逻辑运算操作，或一个输入/输出操作。 
(3)指挥并控制 CPU、数据 cache 和输入/输出设备之间数据流动的方向。 
运算器  由算术逻辑运算单元(ALU)、通用寄存器、数据缓冲寄存器(DR)和程序状态
字寄存器(状态条件寄存器，PSWR)组成，它是数据加工处理部件。相对控制器而言，运算
器接受控制器的命令而进行动作，即运算器所进行的全部操作都是由控制器发出的控制信
号来指挥的，所以它是执行部件。运算器有两个主要功能： 
(1)执行所有的算术运算。 
(2)执行所有的逻辑运算，并进行逻辑测试，如零值测试或两个值的比较。 
通常，一个算术操作产生一个运算结果，而一个逻辑操作则产生一个判决。 
鉴于第 2、3 章中已经详细讨论了运算器和存储器，所以本章重点放在控制器上。 

各种计算机的 CPU 可能有这样或那样的不同，但是在 CPU 中至少要有六类寄存器，
如图 5.1 所示。这些寄存器是：数据缓冲寄存器(DR)，指令寄存器(IR)，程序计数器(PC)，

数据地址寄存器(AR)，通用寄存器(R0～R3)，程序状态字寄存器(PSWR)。 
上述这些寄存器用来暂存一个计算机字。根据需要，可以扩充其数目。下面详细介绍
这些寄存器的功能与结构。 
(1)数据缓冲寄存器(DR)  数据缓冲寄存器用来暂时存放 ALU 的运算结果，或由数据
存储器读出的一个数据字，或来自外部接口的一个数据字。缓冲寄存器的作用是： 
① 作为 ALU 运算结果和通用寄存器之间信息传送中时间上的缓冲； 
② 补偿 CPU 和内存、外围设备之间在操作速度上的差别。 
(2)指令寄存器(IR)  指令寄存器用来保存当前正在执行的一条指令。当执行一条指令
时，先把它从指令存储器(简称指存)读出，然后再传送至指令寄存器。指令划分为操作码
和地址码字段，由二进制数字组成。为了执行任何给定的指令，必须对操作码进行测试，
以便识别所要求的操作。一个叫做指令译码器的部件就是做这项工作的。指令寄存器中操
作码字段 OP 的输出就是指令译码器的输入。操作码一经译码后，即可向操作控制器发出具
体操作的特定信号。 
(3)程序计数器(PC)  为了保证程序能够连续地执行下去，CPU 必须具有某些手段来确
定下一条指令的地址。而程序计数器(PC)正是起到这种作用，所以它又称为指令计数器。
在程序开始执行前，必须将它的起始地址，即程序的第一条指令所在的指存单元地址送入
PC，因此 PC 的内容即是从指存提取的第一条指令的地址。当执行指令时，CPU 将自动修
改 PC 的内容，以便使其保持的总是将要执行的下一条指令的地址。由于大多数指令都是按
顺序来执行的，所以修改的过程通常只是简单的对 PC 加 1。 
但是，当遇到转移指令如 JMP 指令时，那么后继指令的地址(即 PC 的内容)必须从指
令寄存器中的地址字段取得。在这种情况下，下一条从指存取出的指令将由转移指令来规
定，而不是像通常一样按顺序来取得。因此程序计数器的结构应当是具有寄存器和计数两
种功能的结构。 
(4)数据地址寄存器(AR)  数据地址寄存器用来保存当前 CPU 所访问的数据存储器(简
称数存)单元的地址。由于要对存储器阵列进行地址译码，所以必须使用地址寄存器来保持
地址信息，直到一次读/写操作完成。 
地址寄存器的结构和数据缓冲寄存器、指令寄存器一样，通常使用单纯的寄存器结构。
信息的存入一般采用电位-脉冲方式，即电位输入端对应数据信息位，脉冲输入端对应控制
信号，在控制信号作用下，瞬时将信息打入寄存器。 
(5)通用寄存器  在我们的模型中，通用寄存器有 4 个(R0～R3)，其功能是：当算术逻
辑单元(ALU)执行算术或逻辑运算时，为 ALU 提供一个工作区。例如，在执行一次加法运
算时，选择两个操作数(分别放在两个寄存器)相加，所得的结果送回其中一个寄存器(如
R2)中，而 R2 中原有的内容随即被替换。 
目前 CPU 中的通用寄存器，可多达 64 个，甚至更多。其中任何一个可存放源操作数，
也可存放结果操作数。在这种情况下，需要在指令格式中对寄存器号加以编址。从硬件结
构来讲，需要使用通用寄存器堆结构，以便选择输入信息源。通用寄存器还用作地址指示
器、变址寄存器、堆栈指示器等。 
(6)程序状态字寄存器(PSWR)  程序状态字寄存器又称为状态条件寄存器，保存由算
术运算指令和逻辑运算指令运算或测试结果建立的各种条件代码，如运算结果进位标志

(C)，运算结果溢出标志(V)，运算结果为零标志(Z)，运算结果为负标志(N)，等等。这些
标志位通常分别由 1 位触发器保存。 
除此之外，状态条件寄存器还保存中断和系统工作状态等信息，以便使 CPU 和系统能
及时了解机器运行状态和程序运行状态。因此，状态条件寄存器是一个由各种状态条件标
志拼凑而成的寄存器。 

从上面叙述可知，CPU 中的 6 类主要寄存器，每一类完成一种特定的功能。然而信息
怎样才能在各寄存器之间传送呢?也就是说，数据的流动是由什么部件控制的呢? 
通常把许多寄存器之间传送信息的通路，称为数据通路。信息从什么地方开始，中间
经过哪个寄存器或三态门，最后传送到哪个寄存器，都要加以控制。在各寄存器之间建立
数据通路的任务，是由称为操作控制器的部件来完成的。操作控制器的功能，就是根据指
令操作码和时序信号，产生各种操作控制信号，以便正确地选择数据通路，把有关数据打
入到一个寄存器，从而完成取指令和执行指令的控制。 
根据设计方法不同，操作控制器可分为时序逻辑型和存储逻辑型两种。第一种称为硬
布线控制器，它是采用时序逻辑技术来实现的；第二种称为微程序控制器，它是采用存储
逻辑来实现的。本书重点介绍微程序控制器。 
操作控制器产生的控制信号必须定时，为此必须有时序产生器。因为计算机高速地进
行工作，每一个动作的时间是非常严格的，不能太早也不能太迟。时序产生器的作用，就
是对各种操作信号实施时间上的控制。 
CPU 中除了上述组成部分外，还有中断系统、总线接口等其他功能部件，这些内容将
在以后各章中陆续展开。 

我们知道，指令和数据从形式上看都是二进制代码，所以人们很难区分出这些代码是
指令还是数据。然而 CPU 却能识别这些二进制代码：它能准确地判别出哪些是指令字，哪
些是数据字，并将它们送往相应的部件。本节我们将讨论在一些典型的指令周期中，CPU
的各部分是怎样工作的，从而能加深对这一问题的理解和体验。 
计算机之所以能自动地工作，是因为 CPU 能从存放程序的内存里取出一条指令并执行
这条指令；紧接着又是取指令，执行指令……如
此周而复始，构成了一个封闭的循环。除非遇到
停机指令，否则这个循环将一直继续下去，其过
程如图 5.2 所示。 
CPU 每取出一条指令并执行这条指令，都要
完成一系列的操作，这一系列操作所需的时间通
常叫做一个指令周期。换言之，指令周期是取出
图 5.2  取指令-执行指令序列 

一条指令并执行这条指令的时间。由于各种指令的操作功能不同，因此各种指令的指令周
期是不尽相同的。 
指令周期常常用若干个 CPU 周期数来表示，CPU 周期又称为机器周期。CPU 访问一
次内存所花的时间较长，因此通常用内存中读取一个指令字的最短时间来规定 CPU 周期。
这就是说，一条指令的取出阶段(通常称为取指)需要一个 CPU 周期时间。而一个 CPU 周期
时间又包含有若干个时钟周期(又称 T 周期或节拍脉冲，它是处理操作的最基本单位)。这些
Ti 周期的总和规定了一个 CPU 周期的时间宽度。 
图 5.3 示出了采用定长 CPU 周期的指令周期示意图。从这个例子知道，取出和执行任
何一条指令所需的最短时间为两个 CPU 周期。 
需要说明的是，不同的计算机系统中定义的术语未必相同。例如，在不采用三级时序
的系统中，机器周期就相当于时钟周期。 
 
单周期 CPU 和多周期 CPU  单周期 CPU 在一个时钟周期内完成从指令取出到得到结
果的所有工作，指令系统中所有指令执行时间都以最长时间的指令为准，因而效率低，当
前较少采用。多周期 CPU 把指令的执行分成多个阶段，每个阶段在一个时钟周期内完成，
因而时钟周期短，不同指令所用周期数可以不同。以下仅讨论多周期 CPU。 
表 5.1 列出了由 6 条指令组成的一个简单程序。这 6 条指令是有意安排的，因为它们是
非常典型的，既有 RR 型指令，又有 RS 型指令；既有算术逻辑指令，又有访存指令，还有
程序转移指令。我们将在下面通过 CPU 取出一条指令并执行这条指令的分解动作，来具体
认识每条指令的指令周期。 

MOV 是一条 RR 型指令，其指令周期如图 5.4 所示。它需要两个 CPU 周期，其中取
指周期需要一个 CPU 周期，执行周期需要一
个 CPU 周期。 
取指周期中 CPU 完成三件事：①从指存
取出指令；②对程序计数器 PC 加 1，以便为
取下一条指令做好准备；③对指令操作码进行
译码或测试，以便确定进行什么操作。 
执行周期中 CPU 根据对指令操作码的译
码或测试，进行指令所要求的操作。对 MOV
指令来说，执行周期中完成到两个通用寄存器
R0、R1 之间的数据传送操作。由于时间充足，
执行周期一般只需要一个 CPU 周期。 
1. 取指周期 
第一条指令的取指周期示于图 5.5。假定
表 5.1 的程序已装入指存中，因而在此阶段内，
CPU 的动作如下： 
(1)程序计数器 PC 中装入第一条指令地址 101(八进制)； 
(2)PC 的内容被放到指令地址总线 ABUS(I)上，对指存进行译码，并启动读命令； 
(3)从 101 号地址读出的 MOV 指令通过指令总线 IBUS 装入指令寄存器 IR； 
(4)程序计数器内容加 1，变成 102，为取下一条指令做好准备； 
(5)指令寄存器中的操作码(OP)被译码； 
(6)CPU 识别出是 MOV 指令。至此，取指周期结束。 
2. 执行指令阶段(执行周期) 
MOV 指令的执行周期示于图 5.6 中，在此阶段，CPU 的动作如下： 
(1)操作控制器(OC)送出控制信号到通用寄存器，选择 R1(10)作源寄存器，选择 R0 作
目标寄存器； 
(2)OC 送出控制信号到 ALU，指定 ALU 做传送操作； 

(3)OC 送出控制信号，打开 ALU 输出三态门，将 ALU 输出送到数据总线 DBUS 上。
注意，任何时候 DBUS 上只能有一个数据； 
(4)OC 送出控制信号，将 DBUS 上的数据打入到数据缓冲寄存器 DR(10)； 
(5)OC 送出控制信号，将 DR 中的数据 10 打入到目标寄存器 R0，R0 的内容由 00 变为
10。至此，MOV 指令执行结束。 
5.2.3  LAD 指令的指令周期 
LAD 指令是 RS 型指令，它先从指令存储器取出指令，然后从数据存储器 6 号单元取
出数据 100 装入通用寄存器 R1，原来 R1 中存放的数据 10 被更换成 100。由于一次访问指
存，一次访问数存，LAD 指令的指令周期需要 3 个 CPU 周期，如图 5.7 所示。 
 
1. LAD 指令的取指周期 
在 LAD 指令的取指周期中，CPU 的动作完全与 MOV 指令取指周期中一样(图 5.5)，
只是 PC 提供的指令地址为 102，按此地址从指令存储器读出“LDA R1, 6”指令放入 IR 中，
然后将 PC+1，使 PC 内容变成 103，为取下条 ADD 指令做好准备。 
以下 ADD、STO、JMP 三条指令的取指周期中，CPU 的动作完全与 MOV 指令一样，
不再细述。 
2. LAD 指令的执行周期 
LAD 指令的执行周期如图 5.8 所示。CPU 执行的动作如下： 
(1)操作控制器 OC 发出控制命令打开 IR 输出三态门，将指令中的直接地址码 6 放到
数据总线 DBUS 上； 
(2)OC 发出操作命令，将地址码 6 装入数存地址寄存器 AR； 
(3)OC 发出读命令，将数存 6 号单元中的数 100 读出到 DBUS 上； 
(4)OC 发出命令，将 DBUS 上的数据 100 装入缓冲寄存器 DR； 
(5)OC 发出命令，将 DR 中的数 100 装入通用寄存器 R1，原来 R1 中的数 10 被冲掉。
至此，LAD 指令执行周期结束。 
注意，数据总线 DBUS 上分时进行了地址传送和数据传送，所以需要 2 个 CPU 周期。 
 
ADD 指令是 RR 型指令，在运算器中用两个寄存器 R1 和 R2 的数据进行加法运算。指
令周期只需两个 CPU 周期，其中一个是取指周期，与图 5.5 相同。下面只讲执行周期，CPU
完成的动作如图 5.9 所示。 
(1)操作控制器 OC 送出控制命令到通用寄存器，选择 R1 做源寄存器，R2 做目标寄
存器； 
(2)OC 送出控制命令到 ALU，指定 ALU 做 R1(100)和 R2(20)的加法操作； 
(3)OC 送出控制命令，打开 ALU 输出三态门，运算结果 120 放到 DBUS 上； 
(4)OC 送出控制命令，将 DBUS 上数据打入缓冲寄存器 DR；ALU 产生的进位信号保
存在状态字寄存器 PSWR 中； 
(5)OC 送出控制命令，将 DR(120)装入 R2，R2 中原来的内容 20 被冲掉。至此，ADD
指令执行周期结束。 


STO 指令是 RS 型指令，它先访问指存取出 STO 指令，然后按(R3)=30 地址访问数存，
将(R2)=120 写入到 30 号单元。由于一次访问指存，一次访问数存，因此指令周期需 3 个
CPU 周期，其中执行周期为 2 个 CPU 周期，如图 5.10 所示。下面也只讲执行周期，CPU
完成的动作如图 5.11 所示。 

(1)操作控制器 OC 送出操作命令到通用寄存器，选择(R3)=30 做数据存储器的地址单元； 
(2)OC 发出操作命令，打开通用寄存器输出三态门(不经 ALU 以节省时间)，将地址
30 放到 DBUS 上； 
(3)OC 发出操作命令，将地址 30 打入 AR，并进行数存地址译码； 
(4)OC 发出操作命令到通用寄存器，选择(R2)=120，作为数存的写入数据； 
(5)OC 发出操作命令，打开通用寄存器输出三态门，将数据 120 放到 DBUS 上； 
(6)OC 发出操作命令，将数据 120 写入数存 30 号单元，它原先的数据 40 被冲掉。至
此，STO 指令执行周期结束。 
注意，DBUS 是单总线结构，先送地址(30)，后送数据(120)，必须分时传送。 

JMP 指令是一条无条件转移指令，用来改变程序的执行顺序。指令周期为两个 CPU 周
期，其中取指周期为 1 个 CPU 周期，执行周期为 1 个 CPU 周期(图 5.12)。下面也只讲执
行周期，CPU 完成的动作如图 5.13 所示。 
(1)OC 发生操作控制命令，打开指令寄存器 IR 的输出三态门，将 IR 中的地址码 101
发送到 DBUS 上； 
(2)OC 发出操作控制命令，将 DBUS 上的地址码 101 打入到程序计数器 PC 中，PC 中
的原先内容 106 被更换。于是下一条指令不是从 106 号单元取出，而是转移到 101 号单元
取出。至此，JMP 指令执行周期结束。 

应当指出，执行“JMP  101”指令时，我们此处所给的五条指令组成的程序进入了死
循环，除非人为停机，否则这个程序将无休止地运行下去。当然，我们此处所举的转移地
址 101 是随意的，仅仅用来说明转移指令能够改变程序的执行顺序而已。CPU 取指令与执
行指令的动态过程，请见 CAI 动画视频演示。 

在上面介绍了五条典型指令的指令周期，从而使我们对一条指令的取指过程和执行过
程有了一个较深刻的印象。然而我们是通过画示意图或数据通路图来解释这些过程的。这
样做的目的主要是为了教学。但是在进行计算机设计时，如果用这种办法来表示指令周期，
那就显得过于烦琐，而且也没有必要。 
在进行计算机设计时，可以采用方框图语言来表示指令的指令周期。一个方框代表一
个 CPU 周期，方框中的内容表示数据通路的操作或某种控制操作。除了方框，还需要一个
菱形符号，它通常用来表示某种判别或测试，不过时间上它依附于紧接它的前面一个方框
的 CPU 周期，而不单独占用一个 CPU 周期。 
我们把前面的五条典型指令加以归纳，用方框图语言表示的指令周期示于图 5.15。可
以明显地看到，所有指令的取指周期是完全相同的，而且是一个 CPU 周期。但是指令的执
行周期，由于各条指令的功能不同，所用的 CPU 周期是各不相同的，其中 MOV、ADD、
JMP 指令是一个 CPU 周期；LAD 和 STO 指令是两个 CPU 周期。框图中 DBUS 代表数据总
线，ABUS(D)代表数存地址总线，ABUS(I)代表指存地址总线，RD(D)代表数存读命令，
WE(D)代表数存写命令，RD(I)代表指存读命令。 
图 5.15 中，还有一个“～”符号，我们称它为公操作符号。这个符号表示一条指令已
经执行完毕，转入公操作。所谓公操作，就是一条指令执行完毕后，CPU 所开始进行的一
些操作，这些操作主要是 CPU 对外围设备请求的处理，如中断处理、通道处理等。如果外
围设备没有向 CPU 请求交换数据，那么 CPU 又转向指存取下一条指令。由于所有指令的
取指周期是完全一样的，因此，取指令也可认为是公操作。这是因为，一条指令执行结束
后，如果没有外设请求，CPU 一定转入“取指令”操作。 

在日常生活中，人们学习、工作和休息都有一个严格的作息时间。比如，早晨 6:00 起
床；8:00～12:00 上课，12:00～14:00 午休，……每个教师和学生都必须严格遵守这一规定，
在规定的时间里上课，在规定的时间里休息，不得各行其是，否则就难以保证正常的教学
秩序。 
CPU 中也有一个类似“作息时间”的东西，它称为时序信号。计算机所以能够准确、
迅速、有条不紊地工作，正是因为在 CPU 中有一个时序信号产生器。机器一旦被启动，即
CPU 开始取指令并执行指令时，操作控制器就利用定时脉冲的顺序和不同的脉冲间隔，有
条理、有节奏地指挥机器的动作，规定在这个脉冲到来时做什么，在那个脉冲到来时又做
什么，给计算机各部分提供工作所需的时间标志。为此，需要采用多级时序体制。 
再来考虑 5.2 节中提出的一个问题：用二进制码表示的指令和数据都放在内存里，那么
CPU 是怎样识别出它们是数据还是指令呢?事实上，通过 5.2 节讲述指令周期后，就自然会
得出如下结论：从时间上来说，取指令事件发生在指令周期的第一个 CPU 周期中，即发生
在“取指令”阶段，而取数据事件发生在“执行指令”阶段。从空间上来说，如果取出的
代码是指令，那么一定送往指令寄存器，如果取出的代码是数据，那么一定送往运算器。

由此可见，时间控制对计算机来说太重要了。 
不仅如此，在一个 CPU 周期中，又把时间分为若干个小段，以便规定在这一小段时间
中 CPU 干什么，在那一小段时间中 CPU 又干什么，这种时间约束对 CPU 来说是非常必要
的，否则就可能造成丢失信息或导致错误的结果。因为时间的约束是如此严格，以至于时
间进度既不能来得太早，也不能来得太晚。 
总之，计算机的协调动作需要时间标志，而时间标志则是用时序信号来体现的。一般
来说，操作控制器发出的各种控制信号都是时间因素(时序信号)和空间因素(部件位置)的
函数。如果忽略了时间因素，那么我们学习计算机硬件时往往就会感到困难，这一点务请
读者加以注意。 
组成计算机硬件的器件特性决定了时序信号最基本的体制是电位-脉冲制。这种体制最
明显的一个例子，就是当实现寄存器之间的数据传送时，数据加在触发器的电位输入端，
而打入数据的控制信号加在触发器的时钟输入端。电位的高低，表示数据是 1 还是 0，而且
要求打入数据的控制信号到来之前，电位信号必须已稳定。这是因为，只有电位信号先建
立，打入到寄存器中的数据才是可靠的。当然，计算机中有些部件，如算术逻辑运算单元
ALU 只用电位信号工作就可以了。但尽管如此，运算结果还是要送入通用寄存器，所以最
终还是需要脉冲信号来配合。 
硬布线控制器中，时序信号往往采用主状态周期-节拍电位-节拍脉冲三级体制。一个
节拍电位表示一个 CPU 周期的时间，它表示了一个较大的时间单位；在一个节拍电位中又
包含若干个节拍脉冲，以表示较小的时间单位；而主状态周期可包含若干个节拍电位，所
以它是最大的时间单位。主状态周期可以用一个触发器的状态持续时间来表示。 
在微程序控制器中，时序信号比较简单，一般采用节拍电位-节拍脉冲二级体制。就是
说，它只有一个节拍电位，在节拍电位中又包含若干个节拍脉冲(T 周期)。节拍电位表示一
个 CPU 周期的时间，而节拍脉冲把一个 CPU 周期划分成几个较小的时间间隔。根据需要，
这些时间间隔可以相等，也可以不相等。 

前面已分析了指令周期中需要的一些典型时序。时序信号产生器的功能是用逻辑电路
来实现这些时序。 
各种计算机的时序信号产生电路是不尽相同的。
一般来说，大型计算机的时序电路比较复杂，而微型
机的时序电路比较简单，这是因为前者涉及的操作动
作较多，后者涉及的操作动作较少。另一方面，从设
计操作控制器的方法来讲，硬布线控制器的时序电路
比较复杂，而微程序控制器的时序电路比较简单。然
而不管是哪一类，时序信号产生器最基本的构成是一
样的。 
图 5.18 示出了微程序控制器中使用的时序信号
产生器的结构图，它由时钟源、环形脉冲发生器、节
拍脉冲和读写时序译码、启停控制逻辑等部分组成。 
 

(1)时钟源  时钟源用来为环形脉冲发生器提供频率稳定且电平匹配的方波时钟脉冲
信号。它通常由石英晶体振荡器和与非门组成的正反馈振荡电路组成，其输出送至环形脉
冲发生器。 
(2)环形脉冲发生器  环形脉冲发生器的作用是产生一组有序的间隔相等或不等的脉
冲序列，以便通过译码电路来产生最后所需的节拍脉冲，其电路参见动画视频。 
(3)节拍脉冲和存储器读/写时序  我们假定在一个 CPU 周期中产生四个等间隔的节拍
脉冲
1T ～
4
T ，每个节拍脉冲的脉冲宽度均为 200ns，因此一个 CPU 周期便是 800ns，在下
一个 CPU 周期中，它们又按固定的时间关系重复。不过注意，图 5.19 中画出的节拍脉冲信
号是 T1～T4，它们在逻辑关系上与
1T ～
4
T 是完全一致的，是后者经过启停控制逻辑中与门
以后的输出，图中忽略了一级与门的时间延迟细节。 
 

存储器读/写时序信号 RD°、WE°用来进行存储器的读/写操作。 
在硬布线控制器中，节拍电位信号是由时序产生器本身通过逻辑电路产生的，一个节
拍电位持续时间正好包容若干个节拍脉冲。然而在微程序设计的计算机中，节拍电位信号
可由微程序控制器提供。一个节拍电位持续时间，通常也是一个 CPU 周期时间。例如，
图 5.20 中的 RD°，WE°信号持续时间均为 800ns，而一个 CPU 周期也正好是 800ns。关

于微程序控制器如何产生节拍电位信号，将留在 5.4 节介绍。 
(4)启停控制逻辑  机器一旦接通电源，就会自
动产生原始的节拍脉冲信号
1T ～
4
T ，然而，只有
在启动机器运行的情况下，才允许时序产生器发出
CPU 工作所需的节拍脉冲 T1～T4。为此需要由启停
控制逻辑来控制
1T ～
4
T 的发送。同样，对读/写时
序信号也需要由启停逻辑加以控制。图 5.20 给出作
者发明的启停控制逻辑，它是一个实用有效的工具
性电路。 
启停控制逻辑的核心是一个运行标志触发器
Cr。当运行触发器为“1”时，原始节拍脉冲
1T ～
4
T
和读/写时序信号 RD°，WE°通过门电路发送出去，
变成 CPU 真正需要的节拍脉冲信号 T1～T4 和读/写
时序 RD ， WE 。反之，当运行触发器“0”时，就
关闭时序产生器。 
由于启动计算机是随机的，停机也是随机的，为此必须要求：当计算机启动时，一定
要从第 1 个节拍脉冲前沿开始工作，而在停机时一定要在第 4 个节拍脉冲结束后关闭时序
产生器。只有这样，才能使发送出去的脉冲都是完整的脉冲。图 5.20 中，在 Cr(D 触发器)
下面加上一个 SR 触发器，且用
4
T 信号作 Cr 触发器的时钟控制端，那么就可以保证在 T1
的前沿开启时序产生器，而在 T4 的后沿关闭时序产生器。 
从 5.2 节知道，机器指令的指令周期是由数目不等的 CPU 周期数组成，CPU 周期数的
多少反映了指令动作的复杂程度，即操作控制信号的多少。对一个 CPU 周期而言，也有操
作控制信号的多少与出现的先后问题。这两种情况综合在一起，说明每条指令和每个操作
控制信号所需的时间各不相同。控制不同操作序列时序信号的方法，称为控制器的控制方
式。常用的有同步控制、异步控制、联合控制三种方式，其实质反映了时序信号的定时
方式。 
1. 同步控制方式 
在任何情况下，已定的指令在执行时所需的机器周期数和时钟周期数都是固定不变的，
称为同步控制方式。根据不同情况，同步控制方式可选取如下方案。 
(1)采用完全统一的机器周期执行各种不同的指令。这意味着所有指令周期具有相同的
节拍电位数和相同的节拍脉冲数。显然，对简单指令和简单的操作来说，将造成时间浪费。 
(2)采用不定长机器周期。将大多数操作安排在一个较短的机器周期内完成，对某些时
间紧张的操作，则采取延长机器周期的办法来解决。 
(3)中央控制与局部控制结合。将大部分指令安排在固定的机器周期完成，称为中央控
制，对少数复杂指令(乘、除、浮点运算)采用另外的时序进行定时，称为局部控制。 
2. 异步控制方式 
异步控制方式的特点是：每条指令、每个操作控制信号需要多少时间就占用多少时间。
这意味着每条指令的指令周期可由多少不等的机器周期数组成；也可以是当控制器发出某
一操作控制信号后，等待执行部件完成操作后发回“回答”信号，再开始新的操作。显然，
用这种方式形成的操作控制序列没有固定的 CPU 周期数(节拍电位)或严格的时钟周期(节
拍脉冲)与之同步。 
3. 联合控制方式 
此为同步控制和异步控制相结合的方式。一种情况是，大部分操作序列安排在固定的
机器周期中，对某些时间难以确定的操作则以执行部件的“回答”信号作为本次操作的结
束标志。例如，CPU 访问主存时，依靠其送来的“READY”信号作为读/写周期的结束标
志(半同步方式)。另一种情况是，机器周期的节拍脉冲数固定，但是各条指令周期的机器
周期数不固定。例如，5.4 节所讲的微程序控制就是这样。 
微程序控制器同硬布线控制器相比较，具有规整性、灵活性、可维护性等一系列优点，
因而在计算机设计中逐渐取代了早期采用的硬布线控制器，并已广泛地应用。在计算机系
统中，微程序设计技术是利用软件方法来设计硬件的一门技术。 
微程序控制的基本思想，就是仿照通常的解题程序的方法，把操作控制信号编成所谓
的“微指令”，存放到一个只读存储器里。当机器运行时，一条又一条地读出这些微指令，
从而产生全机所需要的各种操作控制信号，使相应部件执行所规定的操作。 
1. 微命令和微操作 
一台数字计算机基本上可以划分为两大部分——控制部件和执行部件。控制器就是控
制部件，而运算器、存储器、外围设备相对控制器来讲，就是执行部件。那么两者之间是
怎样进行联系的呢? 
控制部件与执行部件的一种联系，就是通过控制线。控制部件通过控制线向执行部件
发出各种控制命令，通常把这种控制命令称为微命令，而执行部件接受微命令后所进行的
操作，称为微操作。 
控制部件与执行部件之间的另一种联系是反馈信息。执行部件通过反馈线向控制部件
反映操作情况，以便使控制部件根据执行部件的“状态”来下达新的微命令，这也称为“状
态测试”。 
微操作在执行部件中是最基本的操作。由于数据通路的结构关系，微操作可分为相容
性和相斥性两种。所谓相容性的微操作，是指在同时或同一个 CPU 周期内可以并行执行的
微操作。所谓相斥性的微操作，是指不能在同时或不能在同一个 CPU 周期内并行执行的微
操作。 
图 5.21 示出了一个简单运算器模型，其中 ALU 为算术逻辑单元，R1、R2、R3 为三个
寄存器。三个寄存器的内容都可以通过多路开关从 ALU 的 X 输入端或 Y 输入端送至 ALU， 
而 ALU 的输出可以送往任何一个寄存器
或同时送往 R1，R2，R3 三个寄存器。在
我们给定的数据通路中，多路开关的每
个控制门仅是一个常闭的开关，它的一
个输入端代表来自寄存器的信息，而另
一个输入端则作为操作控制端。一旦两
个输入端都有输入信号时，它才产生一
个输出信号，从而在控制线能起作用的
一个时间宽度中来控制信息在部件中流
动。图中每个开关门由控制器中相应的
微命令来控制，例如，开关门 4 由控制
器中编号为 4 的微命令控制，开关门 6
由编号为 6 的微命令控制，如此等等。
三个寄存器 R1、R2、R3 的时钟输入端 1、
2、3 也需要加以控制，以便在 ALU 运算
完毕而输出公共总线上电平稳定时，将结果打入到某一寄存器。另外，我们假定 ALU 只有
+，–，M(传送)三种操作。Cy 为最高进位触发器，有进位时该触发器状态为“1”。 
ALU 的操作(加、减、传送)在同一个 CPU 周期中只能选择一种，不能并行，所以+，–，
M(传送)三个微操作是相斥性的微操作。类似地，4、6、8 三个微操作是相斥性的，5、7、
9 三个微操作也是相斥性的。ALU 的 X 输入微操作 4、6、8 与 Y 输入的 5、7、9 这两组信
号中，任意两个微操作也都是相容性的。 
2. 微指令和微程序 
在机器的一个 CPU 周期中，一组实现一定操作功能的微命令的组合，构成一条微指令。 
图 5.22 表示一个具体的微指令结构，微指令字长为 23 位，它由操作控制和顺序控制两
大部分组成。 

操作控制部分用来发出管理和指挥全机工作的控制信号。为了形象直观，在我们的例
子中，该字段为 17 位，每一位表示一个微命令。每个微命令的编号同图 5.21 所示的数据通
路相对应，具体功能示于微指令格式的左上部。当操作控制字段某一位信息为“1”时，表
示发出微命令；而某一位信息为“0”时，表示不发出微命令。例如，当微指令字第 1 位信
息为“1”时，表示发出 LDR′1 的微命令，那么运算器将执行 ALU→R1 的微操作，把公共

总线上的信息打入到寄存器 R1。同样，当微指令第 10 位信息为“1”时，表示向 ALU 发出
进行“+”的微命令，因而 ALU 就执行“+”的微操作。 
注意，图 5.22 中微指令给出的控制信号都是节拍电位信号，它们的持续时间都是一个
CPU 周期。如果要用来控制图 5.21 所示的运算器数据通路，势必会出现问题，因为前面的

这些微命令信号还要加入时间控制，例如同节拍脉冲 T4 相与而得到 LDR1～LDR3 信号，如
图 5.23(a)所示。在这种情况下，控制器最后发给运算器的 12 个控制信号中，3 个是节拍脉
冲信号(LDR1，LDR2，LDR3)，其他 9 个都是节拍电位信号，从而保证运算器在前 600ns
时间内进行运算。600ns 后运算完毕，公共总线上输出稳定的运算结果，由 LDR1(或 LDR2，
LDR3)信号打入到相应的寄存器，其时间关系如图 5.23 所示。 

微指令格式中的顺序控制部分用来决定产生下一条微指令的地址。下面我们将会知道，
一条机器指令的功能是用许多条微指令组成的序列来实现的，这个微指令序列通常称为微
程序。既然微程序是由微指令组成的，那么当执行当前一条微指令时，必须指出后继微指
令的地址，以便当前一条微指令执行完毕后，取出下一条微指令。 
决定后继微指令地址的方法不只一种。在我们所举的例子中，由微指令顺序控制字段
的 6 位信息来决定。其中 4 位(20～23)用来直接给出下一条微指令的地址。第 18、19 两位
作为判别测试标志。当此两位为“0”时，表示不进行测试，直接按顺序控制字段第 20～23
位给出的地址取下一条微指令；当第 18 位或第 19 位为“1”时，表示要进行 P1 或 P2 的判
别测试，根据测试结果，需要对第 20～23 位的某一位或几位进行修改，然后按修改后的地
址取下一条微指令。 
3. 微程序控制器原理框图 
微程序控制器原理框图如图 5.24 所示。它主要由控制存储器、微指令寄存器和地址转
移逻辑三大部分组成，其中微指令寄存器分为微地址寄存器和微命令寄存器两部分。 
(1)控制存储器  控制存储器用来存放实现全部指令系统的微程序，它是一种只读型存
储器。一旦微程序固化，机器运行时则只读不写。其工作过程是：每读出一条微指令，则
执行这条微指令；接着又读出下一条微指令，又执行这一条微指令……读出一条微指令并
执行微指令的时间总和称为一个微指令周期。通常，在串行方式的微程序控制器中，微指
令周期就是只读存储器的工作周期。控制存储器的字长就是微指令字的长度，其存储容量
视机器指令系统而定，即取决于微程序的数量。对控制存储器的要求是速度快，读出周期
要短。 

(2)微指令寄存器  微指令寄存器用来存放由控制存储器读出的一条微指令信息。其中
微地址寄存器决定将要访问的下一条微指令的地址，而微命令寄存器则保存一条微指令的
操作控制字段和判别测试字段的信息。 
(3)地址转移逻辑  在一般情况下，微指令由控制存储器读出后直接给出下一条微指令
的地址，通常我们简称微地址，这个微地址信息就存放在微地址寄存器中。如果微程序不
出现分支，那么下一条微指令的地址就直接由微地址寄存器给出。当微程序出现分支时，
意味着微程序出现条件转移。在这种情况下，通过判别测试字段 P 和执行部件的“状态条
件”反馈信息，去修改微地址寄存器的内容，并按改好的内容去读下一条微指令。地址转
移逻辑就承担自动完成修改微地址的任务。 
4. 微程序举例 
一条机器指令是由若干条微指令组成的序列来实现的。因
此，一条机器指令对应着一个微程序，而微程序的总和便可实现
整个的指令系统。 
现在我们举“十进制加法”指令为例，具体看一看微程序控
制的过程。 
“十进制加法”指令的功能是用 BCD 码来完成十进制数的加
法运算。在十进制运算时，当相加两数之和大于 9 时，便产生进
位。可是用 BCD 码完成十进制数运算时，当和数大于 9 时，必
须对和数进行加 6 修正。这是因为，采用 BCD 码后，在两数相
加的和数小于等于 9 时，十进制运算的结果是正确的；而当两数
相加的和数大于 9 时，结果不正确，必须加 6 修正后才能得出正
确结果。 
假定指令存放在指存中，数据 a、b 及常数 6 已存放在图 5.21
中的 R1、R2、R3 三寄存器中，因此，完成十进制加法的微程序
流程图示于图 5.25 中。执行周期要求先进行 a+b+6 运算，然后
判断结果有无进位：当进位标志 Cy=1，不减 6；当 Cy=0，减去 6，
从而获得正确结果。 
可以看到，十进制加法微程序流程图由四条微指令组成，每
一条微指令用一个长方框表示。第一条微指令为“取指”微指令，它是一条专门用来取机

器指令的微指令，任务有三：①从内存取出一条机器指令，并将指令放到指令寄存器 IR。
在我们的例子中，取出的是“十进制加法”指令。②对程序计数器加 1，做好取下一条机器
指令的准备。③对机器指令的操作码用 P1 进行判别测试，然后修改微地址寄存器内容，给
出下一条微指令的地址。在微程序流程图中，每一条微指令的地址用数字示于长方框的右
上角。注意，菱形符号代表判别测试，它的动作在时间上依附于第一条微指令。第二条微
指令完成 a+b 运算。第三条微指令完成 a+b+6 运算，同时又进行判别测试。不过这一次的
判别标志不是 P1 而是 P2，P2 用来测试进位标志 Cy。根据测试结果，微程序或者转向公操作，
或者转向第四条微指令。当微程序转向公操作(用符号～表示)时，如果没有外围设备请求
服务，那么又转向取下一条机器指令。与此相对应，第三条微指令和第四条微指令的下一
个微地址就又指向第一条微指令，即“取指”微指令。 
假设我们已经按微程序流程图编好了微程序，并已事先存放到控制存储器中。同时假
定用图 5.21 所示的运算器做执行部件。机器启动时，只要给出控制存储器的首地址，就可
以调出所需要的微程序。为此，首先给出第一条微指令的地址 0000，经地址译码，控制存
储器选中所对应的“取指”微指令，并将其读到微指令寄存器中。 
第一条微指令的二进制编码是 
000 
000 
000 
000 
11111 
10 
0000 
在这条微指令中，操作控制字段有五个微命令：第 16 位发出 PC→ABUS(I)，将 PC 内
容送到指存地址总线 ABUS(I)；第 13 位发出指存读命令 RD(I)，于是指存执行读操作，从
指存单元取出“十进制加法”指令放到指令总线 IBUS 上，其数据通路可参阅图 5.5。第 15
位发出 LDIR′，将 IBUS 上的“十进制加法”指令打入到指令寄存器 IR。假定“十进制加法”
指令的操作码为 1010，那么指令寄存器的 OP 字段现在是 1010。第 17 位发出 PC+1 微命令，
使程序计数器加 1，做好取下一条机器指令的准备。 
另一方面，微指令的顺序控制字段指明下一条微指令的地址是 0000，但是由于判别字
段中第 18 位为 1，表明是 P1 测试，因此 0000 不是下一条微指令的真正的地址。P1 测试的
“状态条件”是指令寄存器的操作码字段，即用 OP 字段作为形成下一条微指令的地址，于
是微地址寄存器的内容修改成 1010。 
在第二个 CPU 周期开始时，按照 1010 这个微地址读出第二条微指令，它的二进制编
码是 
010 
100 
100 
100 
00000 
00 
1001 
在这条微指令中，操作控制部分发出如下四个微命令：R1→X，R2→Y，+，LDR2′，于
是运算器完成 R1+R2→R2 的操作，其数据通路如图 5.21 所示。 
与此同时，这条微指令的顺序控制部分由于判别测试字段 P1 和 P2 均为 0，表示不进行
测试，于是直接给出下一条微指令的地址为 1001。 
在第三个 CPU 周期开始时，按照 1001 这个微地址读出第三条微指令，它的二进制编
码是 
010 
001 
001 
100 
00000 
01 
0000 
这条微指令的操作控制部分发出 R2→X，R3→Y，+，LDR2′的四个微命令，运算器完成
R2+R3→R2 的操作。 
顺序控制部分由于判别字段中 P2 为 1，表明进行 P2 测试，测试的“状态条件”为进位
标志 Cy。换句话说，此时微地址 0000 需要进行修改，我们假定用 Cy 的状态来修改微地址
寄存器的最后一位：当 Cy=0 时，下一条微指令的地址为 0001；当 Cy=1 时，下一条微指令
的地址为 0000。 
显然，在测试一个状态时，有两条微指令作为要执行的下一条微指令的“候选”微指
令。现在假设 Cy=0，则要执行的下一条微指令地址为 0001。 
在第四个 CPU 周期开始时，按微地址 0001 读出第四条微指令，其编码是 
010 
001 
001 
001 
00000 
00 
0000 
微指令发出 R2→X，R3→Y，–，LDR2′的微命令，运算器完成了 R2–R3→R2 的操作功能。
顺序控制部分直接给出下一条微指令的地址为 0000，按该地址取出的微指令是“取指”微
指令。 
如果第三条微指令进行测试时 Cy=1，那么微地址仍保持为 0000，将不执行第四条微指
令而直接由第三条微指令转向公操作。 
当下一个 CPU 周期开始时，“取指”微指令又从内存读出第二条机器指令。如果这条
机器指令是 STO 指令，那么经过 P1 测试，就转向执行 STO 指令的微程序。 
以上是由四条微指令序列组成的简单微程序。从这个简单的控制模型中，我们就可以
看到微程序控制的主要思想及大概过程。 
5. CPU 周期与微指令周期的关系 
在串行方式的微程序控制器中，微指令周期等于读出微指令的时间加上执行该条微指
令的时间。为了保证整个机器控制信号的同步，可以将一个微指令周期时间设计得恰好和
CPU 周期时间相等。图 5.26 示出了某计算机中 CPU 周期与微指令周期的时间关系。 
一个 CPU 周期为 0.8μs，它包含四个等间隔的节拍脉冲 T1～T4，每个脉冲宽度为 200ns。
用 T4 作为读取微指令的时间，用 T1+T2+T3 时间作为执行微指令的时间。例如，在前 600ns
时间内运算器进行运算，在 600ns 时间的末尾运算器已经运算完毕，可用 T4 上升沿将运算
结果打入某个寄存器。与此同时可用 T4 间隔读取下条微指令，经 200ns 时间延迟，下条微
指令又从只读存储器读出，并用 T1 上升沿打入到微指令寄存器。如忽略触发器的翻转延
迟，那么下条微指令的微命令信号就从 T1 上升沿起开始有效，直到下一条微指令读出后
打入微指令寄存器为止。因此一条微指令的保持时间恰好是 0.8μs，也就是一个 CPU 周
期的时间。 
6. 机器指令与微指令的关系 
经过上面的讲述，应该说，我们能够透彻地了解机器
指令与微指令的关系。也许读者会问：一会儿取机器指令，
一会儿取微指令，它们之间到底是什么关系? 
现在让我们把前面内容归纳一下，作为对此问题的
问答。 
(1)一条机器指令对应一个微程序，这个微程序是由若
干条微指令序列组成的。因此，一条机器指令的功能是由
若干条微指令组成的序列来实现的。简言之，一条机器指
令所完成的操作划分成若干条微指令来完成，由微指令进
行解释和执行。 
(2)从指令与微指令，程序与微程序，地址与微地址的
一一对应关系来看，前者与内存储器有关，后者与控制存
储器有关。与此相关，也有相对应的硬设备，如图 5.27
所示。 
(3)我们在讲述本章 5.2 节时，曾讲述了指令与机器周
期概念，并归纳了五条典型指令的指令周期(参见图 5.15)。
现在我们看到，图 5.15 就是这五条指令的微程序流程图，
每一个 CPU 周期就对应一条微指令。这就告诉我们如何设
计微程序，也将使我们进一步体验到机器指令与微指令的
关系。 
已经了解了微程序控制器的基本原理。这使我们认识到，如何确定微指令的结构，乃
是微程序设计的关键。 
设计微指令结构应当追求的目标是：①有利于缩短微指令字长度；②有利于减小控制
存储器的容量；③有利于提高微程序的执行速度；④有利于对微指令的修改；⑤有利于提
高微程序设计的灵活性。 
1. 微命令编码 
微命令编码，就是对微指令中的操作控制字段采用的表示方法。通常有以下三种方法。 
(1)直接表示法  采用直接表示法的微指令结构如图 5.22 所示，其特点是操作控制字段
中的每一位代表一个微命令。这种方法的优点是简单直观，其输出直接用于控制。缺点是
微指令字较长，因而使控制存储器容量较大。 
(2)编码表示法  编码表示法是把一组相斥性的微命令信号组成一个小组(即一个字
段)，然后通过小组(字段)译码器对每一个微命令信号进行译码，译码输出作为操作控制信
号，其微指令结构如图 5.28 所示。 
采用字段译码的编码方法，可以用较小的二进制信息位表示较多的微命令信号。例如，
3 位二进制位译码后可表示 7 个微命令，4 位二进制位译码后可表示 15 个微命令。与直接
控制法相比，字段译码控制法可使微指令字大大缩短，但由于增加译码电路，使微程序的
 
执行速度稍稍减慢。目前在微程序控制器设计中，字段直接译码法使用较普遍。 
(3)混合表示法  这种方法是把直接表示法与字段编码法混合使用，以便能综合考虑微
指令字长、灵活性、执行微程序速度等方面的要求。 
 
另外，在微指令中还可附设一个常数字段。该常数可作为操作数送入 ALU 运算，也可
作为计数器初值用来控制微程序循环次数。 
2. 微地址的形成方法 
微指令执行的顺序控制问题，实际上是如何确定下一条微指令的地址问题。通常，产
生后继微地址有两种方法。 
(1)计数器方式  这种方法同用程序器计数来产生机器指令地址的方法相类似。在顺序
执行微指令时，后继微地址由现行微地址加上一个增量来产生；在非顺序执行微指令时，
必须通过转移方式，使现行微指令执行后，转去执行指定后继微地址的下一条微指令。在
这种方法中，微地址寄存器通常改为计数器。为此，顺序执行的微指令序列就必须安排在
控制存储器的连续单元中。 
计数器方式的基本特点是：微指令的顺序控制字段较短，微地址产生机构简单。但是
多路并行转移功能较弱，速度较慢，灵活性较差。 
(2)多路转移方式  一条微指令具有多个转移分支的能力称为多路转移。例如，“取指”
微指令根据操作码 OP 产生多路微程序分支而形成多个微地址。在多路转移方式中，当微程
序不产生分支时，后继微地址直接由微指令的顺序控制字段给出；当微程序出现分支时，
有若干“后选”微地址可供选择：即按顺序控制字段的“判别测试”标志和“状态条件”
信息来选择其中一个微地址，其原理如图 5.23 所示。“状态条件”有 1 位标志，可实现微程
序两路转移，涉及微地址寄存器的一位；“状态条件”有 2 位标志，可实现微程序 4 路转移，
涉及微地址寄存器的两位。以此类推，“状态条件”有 n 位标志，可实现微程序 2n 路转移，
涉及微地址寄存器的 n 位。因此执行转移微指令时，根据状态条件可转移到 2n 个微地址中
的一个。 
多路转移方式的特点是，能以较短的顺序控制字段配合，实现多路并行转移，灵活性
好，速度较快，但转移地址逻辑需要用组合逻辑方法设计。 

3. 微指令格式 
微指令的编译方法是决定微指令格式的主要因素。考虑到速度、成本等原因，在设计
计算机时采用不同的编译法。因此微指令的格式大体分成两类：水平型微指令和垂直型微
指令。 
1)水平型微指令 
一次能定义并执行多个并行操作微命令的微指令，称为水平型微指令。例如 5.4 节中所
讲的微指令即为水平型微指令。 
水平型微指令的一般格式如下： 
 
按照控制字段的编码方法不同，水平型微指令又分为三种：第一种是全水平型(不译码
法)微指令，第二种是字段译码法水平型微指令，第三种是直接和译码相混合的水平型微
指令。 
2)垂直型微指令 
微指令中设置微操作码字段，采用微操作码编译法，由微操作码规定微指令的功能，
称为垂直型微指令。 
垂直型微指令的结构类似于机器指令的结构。它有微操作码，在一条微指令中只有 1～
2 个微操作命令，每条微指令的功能简单，因此，实现一条机器指令的微程序要比水平型微
指令编写的微程序长得多。它是采用较长的微程序结构去换取较短的微指令结构。 
下面用 4 条垂直型微指令的微指令格式加以说明。设微指令字长为 16 位，微操作码
3 位。 
(1)寄存器-寄存器传送型微指令。 
 
其功能是把源寄存器数据送目标寄存器。13～15 位为微操作码，源寄存器和目标寄存器编
址各 5 位，可指定 31 个寄存器。 
 

(2)运算控制型微指令。 
 
其功能是选择 ALU 的左、右两输入源信息，按 ALU 字段所指定的运算功能(8 种操作)进行
处理，并将结果送入暂存器中。左、右输入源编址可指定 31 种信息源之一。 
(3)访问主存微指令。 
 
其功能是将主存中一个单元的信息送入寄存器或者将寄存器的数据送往主存。存储器编址
是指按规定的寻址方式进行编址。第 1、2 位指定读操作或写操作(取其之一)。 
(4)条件转移微指令。 
 
其功能是根据测试对象的状态决定是转移到 D 所指定的微地址单元，还是顺序执行下一条
微指令。9 位 D 字段不足以表示一个完整的微地址，但可以用来替代现行 μPC 的低位地址。
测试条件字段有 4 位，可规定 16 种测试条件。 
3)水平型微指令与垂直型微指令的比较 
(1)水平型微指令并行操作能力强，效率高，灵活性强，垂直型微指令则较差。 
在一条水平型微指令中，设置有控制信息传送通路(门)以及进行所有操作的微命令，
因此在进行微程序设计时，可以同时定义比较多的并行操作的微命令，来控制尽可能多的
并行信息传送，从而使水平型微指令具有效率高及灵活性强的优点。 
在一条垂直型微指令中，一般只能完成一个操作，控制一两个信息传送通路，因此微
指令的并行操作能力低，效率低。 
(2)水平型微指令执行一条指令的时间短，垂直型微指令执行时间长。 
因为水平型微指令的并行操作能力强，所以与垂直型微指令相比，可以用较少的微指
令数来实现一条指令的功能，从而缩短了指令的执行时间。而且当执行一条微指令时，水
平型微指令的微命令一般直接控制对象，而垂直型微指令要经过译码，会影响速度。 
(3)由水平型微指令解释指令的微程序，有微指令字较长而微程序短的特点。垂直型微
指令则相反，微指令字较短而微程序长。 
(4)水平型微指令用户难以掌握，而垂直型微指令与指令比较相似，相对来说，比较容
易掌握。 
水平型微指令与机器指令差别很大，一般需要对机器的结构、数据通路、时序系统以
及微命令很精通才能设计。 
垂直型微指令的设计思想在 Pentium 4、安腾系列机中得到了应用。 
4. 动态微程序设计 
微程序设计技术还有静态微程序设计和动态微程序设计之分。对应于一台计算机的机
器指令只有一组微程序，而且这一组微程序设计好之后，一般无须改变而且也不好改变，
这种微程序设计技术称为静态微程序设计。本节前面讲述的内容基本上属于静态微程序设
计的概念。 
当采用 E2PROM 作为控制存储器时，还可以通过改变微指令和微程序来改变机器的指
令系统，这种微程序设计技术称为动态微程序设计。采用动态微程序设计时，微指令和微
程序可以根据需要加以改变，因而可在一台机器上实现不同类型的指令系统。这种技术又
可用于仿真其他机器指令系统，以便扩大机器的功能。 

1. 基本思想 
硬布线控制器是早期设计计算机的一种方法。这种方法是把控制部件看作产生专门固
定时序控制信号的逻辑电路，而此逻辑电路以使用最少元件和取得最高操作速度为设计目
标。一旦控制部件构成后，除非重新设计和物理上对它重新布线，否则要想增加新的控制
功能是不可能的。这种逻辑电路是一种由门电路和触发器构成的复杂树形逻辑网络，故称
之为硬布线控制器。 
硬布线控制器是计算机中最复杂的逻辑部件之一。当执行不同的机器指令时，通过激
活一系列彼此很不相同的控制信号来实现对指令的解释，其结果使得控制器往往很少有明
确的结构而变得杂乱无章。结构上的这种缺陷使得硬布线控制器的设计和调试非常复杂且
代价很大。正因为如此，硬布线控制器被微程序控制器所取代。但是随着新一代机器及 VLSI
技术的发展，硬布线逻辑设计思想又得到了重视。 
图 5.29 示出了硬布线控制器的结构方框图。逻
辑网络的输入信号来源有三个：①来自指令操作码
译码器的输出 Im；②来自执行部件的反馈信息 Bj；
③来自时序产生器的时序信号，包括节拍电位信号
M 和节拍脉冲信号 T。其中节拍电位信号就是 5.3
节规定的机器周期(CPU 周期)信号，节拍脉冲信号
是时钟周期信号。 
逻辑网络 N 的输出信号就是微操作控制信号，
它用来对执行部件进行控制。另有一些信号则根据
条件变量来改变时序发生器的计数顺序，以便跳过
某些状态，从而可以缩短指令周期。显然，硬布线
控制器的基本原理，归纳起来可叙述为：某一微操
作控制信号 C 是指令操作码译码器输出 Im、时序信
号(节拍电位 Mi，节拍脉冲 Tk)和状态条件信号 Bj 的逻辑函数，即 
C = f (Im, Mi, Tk, Bj) 
这个控制信号是用门电路、触发器等许多器件采用布尔代数方法来设计实现的。当机
器加电工作时，某一操作控制信号 C 在某条特定指令和状态条件下，在某一序号的特定节
拍电位和节拍脉冲时间间隔中起作用，从而激活这条控制信号线，对执行部件实施控制。
显然，从指令流程图出发，就可以一个不漏地确定在指令周期中各个时刻必须激活的所有
 
操作控制信号。例如，对引起一次主存读操作的控制信号 C3 来说，当节拍电位 M1=1，取
指令时被激活；而节拍电位 M4=1，三条指令(LAD，ADD，AND)取操作数时也被激活，
此时指令译码器的 LAD，ADD，AND 输出均为 1，因此 C3 的逻辑表达式可由下式确定： 
C3=M1+M4(LAD+ADD+AND) 
一般来说，还要考虑节拍脉冲和状态条件的约束，所以每一控制信号 Cn 可以由以下形
式的布尔代数表达式来确定： 

与微程序控制相比，硬布线控制的速度较快。其原因是微程序控制中每条微指令都要
从控存中读取一次，影响了速度，而硬布线控制主要取决于电路延迟。因此在某些超高速
新型计算机结构中，又选用了硬布线控制器，或与微程序控制器混合使用。 
2. 指令执行流程 
前面在介绍微程序控制器时曾提到，一个机器指令对应一个微程序，而一个微指令周
期则对应一个节拍电位时间。一条机器指令用多少条微指令来实现，则该条指令的指令周
期就包含了多少个节拍电位时间，因而对时间的利用是十分经济的。由于节拍电位是用微
指令周期来体现的，因而时序信号比较简单，时序计数器及其译码电路只需产生若干节拍
脉冲信号即可。 
在用硬布线实现的操作控制器中，通常，时序产生器除了产生节拍脉冲信号外，还应
当产生节拍电位信号。这是因为，在一个指令周期中要顺序执行一系列微操作，需要设置
若干节拍电位来定时。如图 5.15 所示五条指令的指令周期，其指令流程可用图 5.30 来表示。 
 
由图 5.30 可知，所有指令的取指周期放在 M1 节拍。在此节拍中，操作控制器发出微
操作控制信号，完成从指令存储器取出一条机器指令。 
指令的执行周期由 M2、M3 两个节拍来完成。MOV、ADD 和 JMP 指令只需一个节拍
(M2)即可完成。LAD 和 STO 指令需要两个节拍(M2、M3)。为了简化节拍控制，指令的执
行过程可采用同步工作方式，即各条指令的执行阶段均用最长节拍数 M3 来考虑。这样，对
MOV、ADD、JMP 三条指令来讲，在 M3 节拍中没有什么操作。 

显然，由于采用同步工作方式，长指令和短指令对节拍时间的利用都是一样的。这对
短指令来讲，在时间的利用上是浪费的，因而也降低了 CPU 的指令执行速度，影响到机器
的速度指标。为了改变这种情况，在设计短指令流程时可以跳过某些节拍，如 MOV 指令、
ADD 指令和 JMP 指令执行 M2 节拍后跳过 M3 节拍而返回 M1 节拍。当然在这种情况下，节
拍信号发生器的电路相应就要复杂一些。 
节拍电位信号的产生电路与节拍脉冲产生电路十分类似，它可以在节拍脉冲信号时序
器的基础上产生，运行中以循环方式工作，并与节拍脉冲保持同步。 
3. 微操作控制信号的产生 
在微程序控制器中，微操作控制信号由微指令产生，并且可以重复使用。 
在硬布线控制器中，某一微操作控制信号由布尔代数表达式描述的输出函数产生。 
设计微操作控制信号的方法和过程是，根据所有的机器指令流程图，寻找出产生同一
个微操作信号的所有条件，并与适当的节拍电位和节拍脉冲组合，从而写出其布尔代数表
达式并进行简化，然后用门电路或可编程器件来实现。 
为了防止遗漏，设计时可按信号出现在指令流程图中的先后次序来书写，然后进行归
纳和简化。要特别注意控制信号是电位有效还是脉冲有效，如果是脉冲有效，必须加入节
拍脉冲信号进行相“与”。 

计算机自诞生到现在，人们追求的目标之一是很高的运算速度，因此并行处理技术便
成为计算机发展的主流。 
早期的计算机基于冯·诺伊曼的体系结构，采用的是串行处理。这种计算机的主要特
征是：计算机的各个操作(如读/写存储器，算术或逻辑运算，I/O 操作)只能串行地完成，即
任一时刻只能进行一个操作。而并行处理则使得以上各个操作能同时进行，从而大大提高
了计算机的速度。 
广义地讲，并行性有着两种含义：一是同时性，指两个以上事件在同一时刻发生；二
是并发性，指两个以上事件在同一时间间隔内发生。计算机的并行处理技术可贯穿于信息
加工的各个步骤和阶段，概括起来，主要有三种形式：① 时间并行；②空间并行；③时间
并行+空间并行。 
时间并行  指时间重叠，在并行性概念中引入时间因素，让多个处理过程在时间上相
互错开，轮流重叠地使用同一套硬件设备的各个部分，以加快硬件周转而赢得速度。 
时间并行性概念的实现方式就是采用流水处理部件。这是一种非常经济而实用的并行
技术，能保证计算机系统具有较高的性能价格比。目前的高性能微型机几乎无一例外地使
用了流水技术。 
空间并行  指资源重复，在并行性概念中引入空间因素，以“数量取胜”为原则来大
幅度提高计算机的处理速度。大规模和超大规模集成电路的迅速发展为空间并行技术带来
了巨大生机，因而成为目前实现并行处理的一个主要途径。空间并行技术主要体现在多处
理器系统和多计算机系统。但是在单处理器系统中也得到了广泛应用。 
时间并行+空间并行  指时间重叠和资源重复的综合应用，既采用时间并行性又采用空
间并行性。例如，奔腾 CPU 采用了超标量流水技术，在一个机器周期中同时执行两条指令，
因而既具有时间并行性，又具有空间并行性。显然，第三种并行技术带来的高速效益是最好的。 

图 5.31 为现代流水计算机的系统组成原理示意
图。其中 CPU 按流水线方式组织，通常由三大部分
组成：指令部件、指令队列、执行部件。这三个功能
部件可以组成一个 3 级流水线。 
程序和数据存储在主存中，主存通常采用多体交
叉存储器，以提高访问速度。cache 是一个高速缓冲
存储器，用以弥补主存和 CPU 速度上的差异。 
指令部件本身又构成一个流水线，即指令流水
线，它由取指令、指令译码、计算操作数地址、取操
作数等几个过程段组成。 
指令队列是一个先进先出(FIFO)的寄存器栈，
用于存放经过译码的指令和取来的操作数。它也是由
若干个过程段组成的流水线。 
执行部件可以具有多个算术逻辑运算部件，这些
部件本身又用流水线方式构成。 
由图可见，当执行部件正在执行第 I 条指令时，
指令队列中存放着 I+1, I+2, …, I+k 条指令，而与此
同时，指令部件正在取第 I+k+1 条指令。 
为了使存储器的存取时间能与流水线的其他各过程段的速度相匹配，一般都采用多体
交叉存储器。例如，IBM 360/91 计算机，根据一个机器周期输出一条指令的要求、存储器
的存取周期、CPU 访问存储器的频率，采用了模 8 交叉存储器。在现有的流水线计算机中，
存储器几乎都是采用交叉存取的方式工作。 
执行段的速度匹配问题，通常采用并行的运算部件以及部件流水线的工作方式来解决。
一般采用的方法包括：①将执行部件分为定点执行部件和浮点执行部件两个可并行执行的
部分，分别处理定点运算指令和浮点运算指令；②在浮点执行部件中，又有浮点加法部件
和浮点乘/除部件，它们也可以同时执行不同的指令；③浮点运算部件都以流水线方式工作。 
2. 流水 CPU 的时空图 
计算机的流水处理过程非常类似于工厂中的流水装配线。为了实现流水，首先把输入
的任务(或过程)分割为一系列子任务，并使各子任务能在流水线的各个阶段并发地执行。
当任务连续不断地输入流水线时，在流水线的输出端便连续不断地吐出执行结果，从而实
现了子任务级的并行性。 
下面通过时空图来证明这个结论。图 5.32(a)
表示流水 CPU 中一个指令周期的任务分解。假
设指令周期包含四个子过程：取指令(IF)、指令
译码(ID)、执行运算(EX)、结果写回(WB)，每
个子过程称为过程段(Si)，这样，一个流水线由
一系列串联的过程段组成。各个过程段之间设有
高速缓冲寄存器，以暂时保存上一过程段子任务
处理的结果。在统一的时钟信号控制下，数据从
一个过程段流向相邻的过程段。 
图 5.32(b)表示非流水计算机的时空图。对非
流水计算机来说，上一条指令的四个子过程全部
执行完毕后才能开始下一条指令。因此，每隔 4
个机器时钟周期才有一个输出结果。 
图 5.32(c)表示流水计算机的时空图。对流水
计算机来说，上一条指令与下一条指令的四个子
过程在时间上可以重叠执行。因此，当流水线满
载时，每一个时钟周期就可以输出一个结果。 
图 5.32(d)表示超标量流水计算机的时空图。
一般的流水计算机因只有一条指令流水线，所以
称为标量流水计算机。所谓超标量流水，是指它
具有两条以上的指令流水线。如图所示，当流水
线满载时，每一个时钟周期可以执行 2 条指令。
显然，超标量流水计算机是时间并行技术和空间
并行技术的综合应用。Pentium 微型机就是一个
超标量流水计算机。 
直观比较后发现：标量流水计算机在 8 个单
位时间中执行了 5 条指令，超标量流水计算机在 8 个单位时间中执行了 10 条指令，而非流
水计算机在 8 个单位时间中仅执行了 2 条指令。显然，流水技术的应用，使计算机的速度
大大提高了。 
3. 流水线分类 
一个计算机系统可以在不同的并行等级上采用流水线技术。常见的流水线形式有： 
指令流水线  指指令步骤的并行。将指令流的处理过程划分为取指令、译码、取操作
数、执行、写回等几个并行处理的过程段。目前，几乎所有的高性能计算机都采用了指令
流水线。 
算术流水线  指运算操作步骤的并行。如流水加法器、流水乘法器、流水除法器等。 
现代计算机中已广泛采用了流水的算术运算器。例如，STAR-100 为 4 级流水运算器，
TI-ASC 为 8 级流水运算器，CRAY-1 为 14 级流水运算器，等等。 
处理机流水线  又称为宏流水线，是指程序步骤的并行。由一串级联的处理机构成流
水线的各个过程段，每台处理机负责某一特定的任务。数据流从第一台处理机输入，经处
理后被送入与第二台处理机相联的缓冲存储器中。第二台处理机从该存储器中取出数据进
行处理，然后传送给第三台处理机，如此串联下去。随着高档微处理器芯片的出现，构造
处理机流水线将变得容易了。处理机流水线应用在多机系统中。 
要使流水线具有良好的性能，必须使流水线畅通流动，不发生断流。但由于流水过程
中会出现以下三种相关冲突，实现流水线的不断流是困难的，这三种相关是资源相关、数
据相关和控制相关。 
1. 资源相关 
所谓资源相关，是指多条指令进入流水线后在同一机器时钟周期内争用同一个功能部
件所发生的冲突。假定一条指令流水线由五段组成，分别为取指令(IF)、指令译码(ID)、
计算有效地址或执行(EX)、访存取数(MEM)、结果写寄存器堆(WB)。由表 5.2 看出，在
时钟 4 时，第 I1 条的 MEM 段与第 I4 条的 IF 段都要访问存储器。当数据和指令放在同一个
存储器且只有一个访问口时，便发生两条指令争用存储器资源的相关冲突。解决冲突的办
法，一是第 I4 条指令停顿一拍后再启动，二是增设一个存储器，将指令和数据分别放在两
个存储器中。 

2. 数据相关 
在一个程序中，如果必须等前一条指令执行完毕后，才能执行后一条指令，那么这两
条指令就是数据相关的。 
第 5 章  中央处理器       179 
在流水计算机中，指令的处理是重叠进行的，前一条指令还没有结束，第二、三条指
令就陆续地开始工作。由于多条指令的重叠处理，当后继指令所需的操作数，刚好是前一
指令的运算结果时，便发生“先读后写”的数据相关冲突。例如： 
ADD   R1, R2, R3        ；(R2)+(R3)→R1 
SUB    R4, R1, R5        ；(R1)–(R5)→R4 
AND   R6, R1, R7        ；(R1)·(R7)→R6 
如表 5.3 所示，ADD 指令在时钟 5 时将运算结果写入寄存器堆(R1)，但 SUB 指令在时
钟 4 时读寄存器堆(R1)到 ALU 运算，AND 指令在时钟 5 时读寄存器堆(R1)到 ALU 运算。
本来 ADD 指令应该先写 R1，SUB 指令后读 R1，结果变成 SUB 指令先读 R1，ADD 指令后
写 R1。因而发生了 SUB、ADD 两条指令间先读后写的数据相关冲突；AND、ADD 两条指
令间发生了同时读写数据的相关冲突。 

为了解决数据相关冲突，流水 CPU 的运算器中特意设置若干运算结果缓冲寄存器，暂
时保留运算结果，以便于后继指令直接使用，这称为“向前”或定向传送技术。 
3. 控制相关 
控制相关冲突是由转移指令引起的。当执行转移指令时，依据转移条件的产生结果，
可能为顺序取下条指令；也可能转移到新的目标地址取指令，从而使流水线发生断流。 
为了减小转移指令对流水线性能的影响，常用以下两种转移处理技术。 
延迟转移法  由编译程序重排指令序列来实现。基本思想是“先执行再转移”，即发生
转移取时并不排空指令流水线，而是让紧跟在转移指令 Ib 之后已进入流水线的少数几条指
令继续完成。如果这些指令是与 Ib 结果无关的有用指令，那么延迟损失时间片正好得到了
有效的利用。 
转移预测法  硬件方法来实现，依据指令过去的行为来预测将来的行为。通过使用转
移取和顺序取两路指令预取队列器以及目标指令 cache，可将转移预测提前到取指阶段进
行，以获得良好的效果。 

第一台 RISC(精简指令系统计算机)于 1981 年在美国加州大学伯克利分校问世。它是
在继承了 CISC(复杂指令系统计算机)的成功技术，并在克服了 CISC 机器缺点的基础上发
展起来的。 
尽管众多厂家生产的 RISC 处理器实现手段有所不同，但是 RISC 概括的三个基本要素
是普遍认同的。这三个要素是：①一个有限的简单的指令系统；②CPU 配备大量的通用寄
存器；③强调对指令流水线的优化。 
RISC 的目标绝不是简单的缩减指令系统，而是使处理器的结构更简单，更合理，具有
更高的性能和执行效率，并降低处理器的开发成本。基于三要素的 RISC 机器的特征如下。 
(1)使用等长指令，目前的典型长度是 4B。 
(2)寻址方式少且简单，一般为二三种，最多不超过 4 种，绝不出现存储器间接寻址    
方式。 
(3)只有取数指令、存数指令访问存储器。指令中最多出现 RS 型指令，绝不出现 SS
型指令。 
(4)指令系统中的指令数目一般少于 100 种，指令格式一般少于 4 种。 
(5)指令功能简单，控制器多采用硬布线方式，以期更快的执行速度。 
(6)平均而言，所有指令的执行时间为一个处理时钟周期。 
(7)指令格式中，用于指派整数寄存器的个数不少于 32 个，用于指派浮点数寄存器的
个数不少于 16 个。 
(8)强调通用寄存器资源的优化使用。 
(9)支持指令流水并强调指令流水的优化使用。 
(10)RISC 技术的复杂性在它的编译程序，因此软件系统开发时间比 CISC 机器长。 
表 5.4 中列出了 RISC 与 CISC 的主要特征对比。 

1. MC88110 CPU 结构框图 
MC 88110 CPU 是 Motorola 公司的产品，其目标是以较好的性能价格比作为 PC 和工作
站的通用微处理器。它是一个 RISC 处理器。处理器有 12 个执行功能部件，三个 cache 和
一个控制部件。其结构框图如图 5.33 所示。 
 
在三个 cache 中，一个是指令 cache，一个是数据 cache，它们能同时完成取指令和取数
据，还有一个是目标指令 cache(TIC)，它用于保存转移目标指令。 
两个寄存器堆：一个是通用寄存器堆，用于整数和地址指针，其中有 R0～R31 共 32 个
寄存器(32 位长)；另一个是扩展寄存器堆，用于浮点数，其中有 X0～X31 共 32 个寄存器(长
度可以是 32 位、64 位或 80 位)。 
12 个执行功能部件是：取数/存数(读写)部件、整数运算部件(2 个)、浮点加法部件、

乘法部件、除法部件、图形处理部件(2 个)、位处理部件、用于管理流水线的超标量指令派
遣/转移部件。 
所有这些 cache、寄存器堆、功能部件，在处理器中通过六条 80 位宽的内部总线相连
接。其中 2 条源 1 总线，2 条源 2 总线，2 条目标总线。 
2. MC88110 的指令流水线 
由于 MC88110 是超标量流水 CPU，所以指令流水线在每个机器时钟周期完成两条
指令。流水线分为三段：取指和译码(F＆D)段、执行(EX)段、写回(WB)段，如图 5.34
所示。 

F＆D 段需要一个时钟周期，完成由指令 cache 取一对指令并译码，并从寄存器堆取
操作数，然后判断是否把指令发射到 EX 段。如果所要求的资源(操作数寄存器、目标寄
存器、功能部件)发生资源使用冲突，或与先前指令发生数据相关冲突，或转移指令将转
向新的目标指令地址，则 F＆D 段不再向 EX 段发射指令，或不发射紧接转移指令之后的
指令。 
EX 段对于大多数指令只需一个时钟周期，某些指令可能多于一个时钟周期。EX 段
执行的结果在 WB 段写回寄存器堆，WB 段只需时钟周期的一半。为了解决数据相关冲突，
EX 段执行的结果一方面在 WB 段写回寄存器堆，另一方面经定向传送电路提前传送到
ALU，可直接被当前进入 EX 的指令所使用。图 5.34(a)表示 MC88110 CPU 超标量流水线
正常运行情况。 

3. 指令动态调度策略 
88110 采用按序发射、按序完成的指令动态调度策略。指令派遣单元总是发出单一地址，
然后从指令 cache 取出此地址及下一地址的两条指令。译码后总是力图同一时间发射这两条
指令到 EX 段。若这对指令的第一条指令由于资源冲突或数据相关冲突，则这一对指令都不
发射，两条指令在 F＆D 段停顿，等待资源的可用或数据相关的消除。若第一条指令能发射
而第二条指令不能发射，则只发射第一条指令，而第二条指令停顿并与新取的指令之一进
行配对等待发射，此时原第二条指令作为配对的第一条指令对待。可见，这样实现的方式
是按序发射，图 5.34(b)示出了指令配对情况。 
为了判定能否发射指令，88110 使用了计分牌方法。计分牌是一个位向量，寄存器堆中
每个寄存器都有一个相应位。每当一条指令发射时，它预约的目的寄存器在位向量中的相
应位上置“1”，表示该寄存器“忙”。当指令执行完毕并将结果写回此目的寄存器时，该位
被清除。于是，每当判定是否发射一条指令(STO 存数指令和转移指令除外)时，一个必须
满足的条件是：该指令的所有目的寄存器、源寄存器在位向量中的相应位都已被清除。否
则，指令必须停顿等待这些位被清除。为了减少经常出现的数据相关，流水线采用了如前
面所述的定向传送技术，将前面指令执行的结果直接送给后面指令所需此源操作数的功能
部件，并同时将位向量中的相应位清除。因此，指令发射和定向传送是同时进行的。 
如何实现按序完成呢?因为执行段有多个功能部件，很可能出现无序完成的情况。为此，
88110 提供了一个 FIFO 指令执行队列，称为历史缓冲器。每当一条指令发射出去，它的副
本就被送到 FIFO 队尾。队列最多能保存 12 条指令。只有前面的所有指令执行完，这条指
令才到达队首。当它到达队首并执行完毕后才离开队列。 
对于转移处理，88110 使用了延迟转移法和目标指令 cache(TIC)法。延迟转移是个选项
(.n)。如果采用这个选项(指令如 bcnd.n)，则跟随在转移指令后的指令将被发射。如果不采
用这个选项，则在转移指令发射之后的转移延迟时间片内没有任何指令被发射。延迟转移
通过编译程序来调度。 
TIC 是一个 32 项的全相联 cache，每项能保存转移目标路径的前两条指令。当一条转
移指令译码并命中 cache 时，能同时由 TIC 取来它的目标路径的前面两条指令。 

所谓动态流水线调度，是对指令进行重新排序以避免处理器阻塞的硬件支持。图 5.36
描述了动态流水线调度模型。通常流水线分为 3 个主要单元：一个取指令发射单元，多个
功能单元(10 个或更多)，一个指令完成单元。第一个单元用于取指令，将指令译码，并将
它们送到相应的功能单元执行。每个功能单元都有自己的缓冲器，称为保留站，它用于暂
存操作数和操作指令。当缓冲器中包含了所有的操作数，并且功能单元已经就绪，结果就
被计算出来。当完成结果时，它就被发送到等待特殊结果的储存站及指令完成单元。而指
令完成单元确定何时能够安全地将结果放入到寄存器堆或内存中。 
指令完成单元中的缓冲器通常称为重排序缓冲器，它也可以用来提供操作数，其工作
方式类似于旁路逻辑在静态调度流水线中的工作方式。一且结果写回寄存器堆，便可以从
寄存器堆中直接取得操作数，就像一般流水线取得操作数的方式一样。 

本 章 小 结 
CPU 是计算机的中央处理部件，具有指令控制、操作控制、时间控制、数据加工等基
本功能。 
早期的 CPU 由运算器和控制器组成。随着集成电路技术的发展，当今的 CPU 芯片变
成运算器、cache 和控制器三大部分，CPU 中至少有六类寄存器：指令寄存器、程序计数器、
地址寄存器、数据缓冲寄存器、通用寄存器、状态条件寄存器。 
CPU 从存储器取出一条指令并执行这条指令的时间和称为指令周期。CISC 中，由于各
种指令的操作功能不同，各种指令的指令周期是不尽相同的。划分指令周期，是设计操作
控制器的重要依据。RISC 中，由于流水执行，大部分指令在一个机器周期完成。 
时序信号产生器提供 CPU 周期(也称机器周期)所需的时序信号。操作控制器利用这些
时序信号进行定时，有条不紊地取出一条指令并执行这条指令。 
微程序设计技术是利用软件方法设计操作控制器的一门技术，具有规整性、灵活性、
可维护性等一系列优点，因而在计算机设计中得到了广泛应用。但是随着 ULSI 技术的发展

和对机器速度的要求，硬布线逻辑设计思想又得到了重视。硬布线控制器的基本思想是：
某一微操作控制信号是指令操作码译码输出、时序信号和状态条件信号的逻辑函数，即用
布尔代数写出逻辑表达式，然后用门电路、触发器等器件实现。 
从简单到复杂，举出一个 CPU 模型，目的在于使读者由浅入深地理解教学内容，这对
于建立整机概念是十分重要的。 
不论微型机还是超级计算机，并行处理技术已成为计算机技术发展的主流。并行处理
技术可贯穿于信息加工的各个步骤和阶段。概括起来，主要有三种形式：①时间并行；
②空间并行；③时间并行+空间并行。 
流水 CPU 是以时间并行性为原理构造的处理机，是一种非常经济而实用的并行技术。
目前的高性能微处理机几乎无一例外地使用了流水技术。流水技术中的主要问题是资源相
关、数据相关和控制相关，为此需要采取相应的技术对策，才能保证流水线畅通而不断流。 
RISC CPU 是继承 CISC 的成功技术，并在克服 CISC 机器缺点的基础上发展起来的。
RISC 机器的三个基本要素是：①一个有限的简单指令系统；②CPU 配备大量的通用寄存器；
③强调指令流水线的优化。RISC 机器一定是流水 CPU，但流水 CPU 不一定是 RISC 机器。
如奔腾 CPU 是流水 CPU，但奔腾机是 CISC 机器。 

数字计算机是由若干系统功能部件构成的，这些系统功能部件在一起工作才能形成一
个完整的计算机系统。 
总线是构成计算机系统的互联机构，是多个系统功能部件之间进行数据传送的公共通
路。借助于总线连接，计算机在各系统功能部件之间实现地址、数据和控制信息的交换，
并在争用资源的基础上进行工作。 
一个单处理器系统中的总线，大致分为三类： 
(1)CPU 内部连接各寄存器及运算部件之间的总线，称为内部总线。 
(2)CPU 同计算机系统的其他高速功能部件，如存储器、通道等互相连接的总线，称为
系统总线。 
(3)中、低速 I/O 设备之间互相连接的总线，称为 I/O 总线。 
1. 总线的特性 
物理特性  总线的物理特性是指总线的物理连接方式，包括总线的根数，总线的插头、
插座的形状，引脚线的排列方式等。 
功能特性  功能特性描述总线中每一根线的功能。如地址总线的宽度指明了总线能够
直接访问存储器的地址空间范围；数据总线的宽度指明了访问一次存储器或外设时能够交
换数据的位数；控制总线包括 CPU 发出的各种控制命令(如存储器读/写、I/O 读/写)，请求
信号与仲裁信号，外设与 CPU 的时序同步信号，中断信号，DMA 控制信号等。 
电气特性  电气特性定义每一根线上信号的传递方向及有效电平范围。一般规定送入
CPU 的信号叫输入(IN)信号，从 CPU 发出的信号叫输出(OUT)信号。例如，地址总线是输

出线，数据总线是双向传送的信号线，这两类信号线都是高电平有效。控制总线中各条线
一般是单向的，有 CPU 发出的，也有进入 CPU 的，有高电平有效的，也有低电平有效的。
总线的电平都符合相应电平规范的定义。 
时间特性  时间特性定义了每根线在什么时间有效。也就是说，只有规定了总线上各
信号有效的时序关系，CPU 才能正确无误地使用。 
2. 总线的标准化 
相同的指令系统，相同的功能，不同厂家生产的各功能部件在实现方法上几乎没有相
同的，但各厂家生产的相同功能部件却可以互换使用，其原因何在呢?就是因为它们都遵守
了相同的系统总线的要求，这就是系统总线的标准化问题。 
例如，微型计算机系统中采用的标准总线，从 ISA 总线(16 位，带宽 8MB/s)发展到 EISA
总线(32 位，带宽 33.3MB/s)，又发展到 VESA 总线(32 位，带宽 132MB/s)，而 PCI 总线又
进一步过渡到 64 位，100MHz。 
衡量总线性能的重要指标是总线带宽，它定义为总线本身所能达到的最高传输速率，
单位是兆字节每秒(MB/s)。实际带宽会受到总线布线长度、总线驱动器/接收器性能、连接
在总线上的模块数等因素的影响。这些因素将造成信号在总线上的畸变和延时，使总线最
高传输速率受到限制。 

任何数字计算机的用途很大程度上取决于它所能连接的外围设备的范围。遗憾的是，
由于外围设备种类繁多，速度各异，不可能简单地把外围设备直接连接在 CPU 上。因此必
须寻找一种方法，以便将外围设备同某种计算机连接起来，使它们在一起可以正常工作。
通常，这项任务用适配器部件来完成。通过适配器可以实现高速 CPU 与低速外设之间工作
速度上的匹配和同步，并完成计算机和外设之间的所有数据传送和控制。适配器通常简称
为接口。 
大多数总线都是以相同方式构成的，其不同之处仅在于总线中数据线和地址线的宽度，
以及控制线的多少及其功能。然而，总线的排列布置与其他各类部件的连接方式对计算机
系统的性能来说，将起着十分重要的作用。根据连接方式不同，单机系统中采用的总线结
构有两种基本类型：①单总线结构；②多总线结构。 
1. 单总线结构 
在许多单处理器的计算机中，使用单一的系统总线来连接 CPU、主存和 I/O 设备，称
为单总线结构，如图 6.1 所示。 

在单总线结构中，要求连接到总线上的逻辑部件必须高速运行，以便在某些设备需要
使用总线时，能迅速获得总线控制权；而当不再使用总线时，能迅速放弃总线控制权。否
则，由于一条总线由多种功能部件共用，可能导致很大的时间延迟。 
在单总线系统中，当 CPU 取一条指令时，首先把程序计数器 PC 中的地址同控制信息
一起送至总线上。该地址不仅加至主存，同时也加至总线上的所有外围设备。然而，只有
与出现在总线上的地址相对应的设备，才执行数据传送操作。我们知道，在“取指令”情
况下的地址是主存地址，所以，此时该地址所指定的主存单元的内容一定是一条指令，而
且将被传送给 CPU。取出指令之后，CPU 将检查操作码。操作码规定了对数据要执行什么
操作，以及数据是流进 CPU 还是流出 CPU。 
在单总线系统中，对输入/输出设备的操作，完全和主存的操作方法一样来处理。这样，
当 CPU 把指令的地址字段送到总线上时：①如果该地址字段对应的地址是主存地址，则主
存予以响应，从而在 CPU 和主存之间发生数据传送，而数据传送的方向由指令操作码决定。 
②如果该指令地址字段对应的是外围设备地址，则外围设备译码器予以响应，从而在 CPU
和与该地址相对应的外围设备之间发生数据传送，而数据传送的方向由指令操作码决定。 
在单总线系统中，某些外围设备也可以指定地址。此时，外围设备通过与 CPU 中的总
线控制部件交换控制信号的方式占有总线。一旦外围设备得到总线控制权后，就可向总线
发送地址信号，使总线上的地址线置为适当的代码状态，以便指定它将要与哪一个设备进
行信息交换。如果一个由外围设备指定的地址对应于一个主存单元，则主存予以响应，于
是在主存和外设之间将进行直接存储器传送。 
我们发现，单总线结构容易扩展成多 CPU 系统。 
2. 多总线结构 
单总线系统中，由于所有的高速设备和低速设备都挂在同一总线上，且总线只能分时
工作，即某一时间只能允许在一对儿设备之间传送数据，这就使信息传送的效率和吞吐量
受到极大限制。为此出现了图 6.2 所示的多总线系统结构。 
图 6.2 中，CPU、存储器控制器和两个 PCI-E 桥通过接口与高速的前端总线(FSB)相连。
总线桥是一种具有缓冲、转换、控制功能的逻辑电路。不同类型的桥扩展出不同层次的总
线，并分别连接高速、中速和低速设备。图中的两个 PCI-E 桥分别连接图形处理器(GPU)
和其他高速 I/O 设备。连接 I/O 设备的 PCI-E 总线又分别连接以太网设备控制器接口(DCI)、
USB 主机控制器接口、SATA(串行高级技术附件)桥、VGA(视频图形阵列)桥、DMA 控制 

器和 PCI 总线扩展桥。SATA 总线用于与 SATA 硬盘和光盘驱动器连接，PCI 总线上连接的
第二个 USB 主机控制器接口用于与 USB 键盘和 USB 鼠标相连。 
多总线结构确保高速、中速、低速设备连接到不同的总线上同时工作，以提高总线的
效率和吞吐量，而且处理器结构的变化不影响高速总线。 
思考题  你能说出多总线结构比单总线结构的创新点吗？ 

早期总线的内部结构如图 6.3 所示，它实际上是处理器芯片引脚的延伸，是处理器与 I/O
设备适配器的通道。这种简单的总线一般也由 50～100 条信号线组成，这些信号线按其功
能可分为三类：地址线、数据线和控制线。地址线是单向的，用来传送主存与设备的地址；
数据线是双向的，用来传送数据；控制线一般而言对每一根线是单向的(CPU 发向接口，或
接口发向 CPU)，用来指明数据传送的方向(存储器读、存储器写、I/O 读、I/O 写)、中断控
制(请求、识别)和定时控制等。 

早期总线结构的不足之处在于：①CPU 是总线上唯一的主控者。即使后来增加了具有
简单仲裁逻辑的 DMA 控制器以支持 DMA 传送，但仍不能满足多 CPU 环境的要求。②总
线信号是 CPU 引脚信号的延伸，故总线结构紧密与 CPU 相关，通用性较差。 
图 6.4 示出了当代流行的总线内部结构，它是一些标准总线，追求与结构、CPU、技术
无关的开发标准，并满足包括多个 CPU 在内的主控者环境需求。 

在当代总线结构中，CPU 和它私有的 cache 一起作为一个模块与总线相连。系统中允
许有多个这样的处理器模块。而总线控制器完成多个总线请求者之间的协调与仲裁。整个
总线分成如下四部分。 
数据传送总线  由地址线、数据线、控制线组成。其结构与图 6.3 中的简单总线相似，
但一般信号条数较多，如 32 条地址线，32 或 64 条数据线。为了减少引脚数量，64 位数据
的低 32 位数据线常常和地址线采用多路复用方式。 
仲裁总线  包括总线请求线和总线授权线。 
中断和同步总线  用于处理带优先级的中断操作，包括中断请求线和中断认可线。 

公用线  包括时钟信号线、电源线、地线、系统复位线以及加电或断电的时序信号线等。 

大多数计算机采用了分层次的多总线结构。在这种结构中，速度差异较大的设备模块
使用不同速度的总线，而速度相近的设备模块使用同一类总线。显然，这种结构的优点在
于不仅解决了总线负载过重的问题，而且使总线设计简单，并能充分发挥每类总线的效能。 
图 6.5 是 Pentium 计算机主板的总线结构框图。可以看出，它是一个三层次的多总线结
构，即有 CPU 总线、PCI 总线和 ISA 总线。 
CPU 总线  也称 CPU-存储器总线，它是包含 64 位数据线和 32 位地址线的同步总线。
总线时钟频率为 66.6MHz(或 60MHz)，CPU 内部时钟是此时钟频率的倍频。此总线可连接
4～128MB 的主存。主存扩充容量是以内存条形式插入主板有关插座来实现的。CPU 总线
还接有 L2 级 cache。主存控制器和 cache 控制器芯片用来管理 CPU 对主存和 cache 的存取
操作。CPU 是这条总线的主控者，但必要时可放弃总线控制权。从传统的观点看，可以把
CPU 总线看成是 CPU 引脚信号的延伸。 

PCI 总线  用于连接高速的 I/O 设备模块，如图形显示器适配器、网络接口控制器、硬
盘控制器等。通过“桥”芯片，上面与更高速的 CPU 总线相连，下面与低速的 ISA 总线相
接。PCI 总线是一个 32(或 64)位的同步总线，32 位(或 64 位)数据/地址线是同一组线，分
时复用。总线时钟频率为 33.3MHz，总线带宽是 132MB/s。PCI 总线采用集中式仲裁方式，
有专用的 PCI 总线仲裁器。主板上一般有 3 个 PCI 总线扩充槽。 
ISA 总线  Pentium 机使用该总线与低速 I/O 设备连接。主板上一般留有 3～4 个 ISA
总线扩充槽，以便使用各种 16 位/8 位适配器卡。该总线支持 7 个 DMA 通道和 15 级可屏蔽
硬件中断。另外，ISA 总线控制逻辑还通过主板上的片级总线与实时钟/日历、ROM、键盘
和鼠标控制器(8042 微处理器)等芯片相连接。 
我们看到，CPU 总线、PCI 总线、ISA 总线通过两个“桥”芯片连成整体。桥芯片在此
起到了信号速度缓冲、电平转换和控制协议的转换作用。有的资料将 CPU 总线-PCI 总线的
桥称为北桥，将 PCI 总线-ISA 总线的桥称为南桥。通过桥将两类不同的总线“粘合”在一
起的技术特别适合于系统的升级换代。这样，每当 CPU 芯片升级时只需改变 CPU 总线和
北桥芯片，全部原有的外围设备可自动继续工作。 
Pentium 机总线系统中有一个核心逻辑芯片组，简称 PCI 芯片组，它包括主存控制器和
cache 控制器芯片、北桥芯片和南桥芯片。 

数字计算机使用二进制数，它们或用电位的高、低来表示，或用脉冲的有、无来表示。
在前一种情况下，如果电位高时表示数字“1”，那么电位低时则表示数字“0”。在后一种
情况下，如果有脉冲时表示数字“1”，那么无脉冲时就表示数字“0”。 
计算机系统中，传输信息一般采用串行传送或并行传送两种方式之一。但是出于速度
和效率上的考虑，系统总线上传送的信息必须采用并行传送方式。 
1. 串行传送 
当信息以串行方式传送时，只有一条传输线，且采用脉冲传送。在串行传送时，按顺
序来传送表示一个数码的所有二进制位(bit)的脉冲信号，每次一位，通常以第一个脉冲信
号表示数码的最低有效位，最后一个脉冲信号表示数码的最高有效位。图 6.6(a)示出了串
行传送的示意图。 
当串行传送时，有可能按顺序连续传送若干个“0”或若干个“1”。如果在编码时用有
脉冲表示二进制数“1”，无脉冲表示二进制数“0”，那么当连续出现几个“0”时，表示某
段时间间隔内传输线上没有脉冲信号。为了要确定传送了多少个“0”，必须采用某种时序
格式，以便使接收设备能加以识别。通常采用的方法是指定位时间，即指定一个二进制位
在传输线上占用的时间长度。显然，位时间是由同步脉冲来体现的。 
假定串行数据是由位时间组成的，那么传送 8 比特需要 8 个位时间。例如，如果接收设
备在第一个位时间和第三个位时间接收到一个脉冲，而其余的 6 个位时间没有收到脉冲，那
么就会知道所收到的二进制信息是 00000101。注意，串行传送时低位在前，高位在后。 

在串行传送时，被传送的数据需要在发送部件进行并-串变换，这称为拆卸；而在接收
部件又需要进行串-并变换，这称为装配。 
串行传送的主要优点是只需要一条传输线，这一点对长距离传输显得特别重要，不管
传送的数据量有多少，只需要一条传输线，成本比较低廉。 
2. 并行传送 
用并行方式传送二进制信息时，对每个数据位都需要单独一条传输线。信息由多少二
进制位组成，就需要多少条传输线，从而使得二进制数“0”或“1”在不同的线上同时进
行传送。 
并行传送的过程示于图 6.6(b)。如果要传送的数据由 8 位二进制位组成(1 字节)，那么
就使用 8 条线组成的扁平电缆。每一条线分别代表了二进制数的不同位值。例如，最上面
的线代表最高有效位，最下面的线代表最低有效位，因而图中正在传送的二进制数是
10101100。 
并行传送一般采用电位传送。由于所有的位同时被传送，所以并行数据传送比串行数
据传送快得多，例如，使用 32 条单独的地址线，可以从 CPU 的地址寄存器同时传送 32 位
地址信息给主存。 

I/O 功能模块通常简称为 I/O 接口，也叫适配器。广义地讲，I/O 接口是指 CPU、主存
和外围设备之间通过系统总线进行连接的标准化逻辑部件。I/O 接口在它动态连接的两个部
件之间起着“转换器”的作用，以便实现彼此之间的信息传送。 

图 6.7 示出了 CPU、I/O 接口和外围设备之
间的连接关系。外围设备本身带有自己的设备控
制器，它是控制外围设备进行操作的控制部件。
它通过 I/O 接口接收来自 CPU 传送的各种信息，
并根据设备的不同要求把这些信息传送到设备，
或者从设备中读出信息传送到 I/O 接口，然后送
给 CPU。由于外围设备种类繁多且速度不同，因
而每种设备都有适应它自己工作特点的设备控制器。图 6.7 中将外围设备本体与它自己的控
制电路画在一起，统称为外围设备。 
为了使所有的外围设备能在一起正确地工作，CPU 规定了不同的信息传送控制方法。
不管什么样的外围设备，只要选用某种数据传送控制方法，并按它的规定通过总线和主机
连接，就可进行信息交换。通常在总线和每个外围设备的设备控制器之间使用一个适配器
(接口)电路来解决这个问题，以保证外围设备用计算机系统特性所要求的形式发送和接收
信息。因此接口逻辑必须标准化。 
一个标准 I/O 接口可能连接一个设备，也可能连接多个设备。图 6.8 是 I/O 接口模块的
一般结构框图。 
 

它通常具有如下功能。 
控制  接口模块靠指令信息来控制外围设备的动作，如启动、关闭设备等。 
缓冲  接口模块在外围设备和计算机系统其他部件之间用作为一个缓冲器，以补偿各
种设备在速度上的差异。 
状态  接口模块监视外围设备的工作状态并保存状态信息。状态信息包括数据“准备
就绪”“忙”“错误”等，供 CPU 询问外围设备时进行分析之用。 
转换  接口模块可以完成任何要求的数据转换，如并-串转换或串-并转换，因此数据
能在外围设备和 CPU 之间正确地进行传送。 
整理  接口模块可以完成一些特别的功能，例如，在需要时可以修改字计数器或当前
内存地址寄存器。 
程序中断  每当外围设备向 CPU 请求某种动作时，接口模块即发生一个中断请求信号
到 CPU。例如，如果设备完成了一个操作或设备中存在着一个错误状态，接口即发出中断。 
事实上，一个 I/O 接口模块有两个接口：一是和系统总线的接口。CPU 和 I/O 接口模块

的数据交换一定是并行方式；二是和外设的接口。I/O 接口模块和外设的数据交换可能是并
行方式，也可能是串行方式。因此，根据外围设备供求串行数据或并行数据的方式不同，I/O
接口模块分为串行数据接口和并行数据接口两大类。 


连接到总线上的功能模块有主动和被动两种形态。如 CPU 模块，它在不同的时间可以
用作主方，也可用作从方；而存储器模块只能用作从方。主方(主设备)可以启动一个总线
周期，而从方(从设备)只能响应主方的请求。每次总线操作，只能有一个主方占用总线控
制权，但同一时间里可以有一个或多个从方。 
我们知道，除 CPU 模块外，I/O 模块也可提出总线请求。为了解决多个主设备同时竞
争总线控制权的问题，必须具有总线仲裁部件，以某种方式选择其中一个主设备作为总线
的下一次主方。 
对多个主设备提出的占用总线请求，一般采用优先级或公平策略进行仲裁。例如，在
多处理器系统中对各 CPU 模块的总线请求采用公平的原则来处理，而对 I/O 模块的总线请
求采用优先级策略。被授权的主方在当前总线业务一结束，即接管总线控制权，开始新的
信息传送。主方持续控制总线的时间称为总线占用期。 
按照总线仲裁电路的位置不同，仲裁方式分为集中式仲裁和分布式仲裁两类。 

集中式仲裁中每个功能模块有两条线连到总线控制器：一条是送往仲裁器的总线请求
信号线 BR，一条是仲裁器送出的总线授权信号线 BG。 
链式查询方式  为减少总线授权线数量，采用了图 6.10(a)所示的菊花链查询方式，其
中 A 表示地址线，D 表示数据线。BS 线为 1，表示总线正被某外设使用。 

链式查询方式的主要特点是，总线授权信号 BG 串行地从一个 I/O 接口传送到下一个 I/O
接口。假如 BG 到达的接口无总线请求，则继续往下查询；假如 BG 到达的接口有总线请求，
BG 信号便不再往下查询。这意味着该 I/O 接口就获得了总线控制权。作为思考题，读者不
妨画出链式查询电路的逻辑结构图。 
显然，在查询链中离总线仲裁器最近的设备具有最高优先级，离总线仲裁器越远，优
先级越低。因此，链式查询是通过接口的优先级排队电路来实现的。 
链式查询方式的优点是，只用很少几根线就能按一定优先次序实现总线仲裁，并且这
种链式结构很容易扩充设备。 
链式查询方式的缺点是对询问链的电路故障很敏感，如果第 i 个设备的接口中有关链的
电路有故障，那么第 i 个以后的设备都不能进行工作。另外查询链的优先级是固定的，如果
优先级高的设备出现频繁的请求，那么优先级较低的设备可能长期不能使用总线。 
计数器定时查询方式  计数器定时查询方式原理示于图 6.10(b)。总线上的任一设备要
求使用总线时，通过 BR 线发出总线请求。总线仲裁器接到请求信号以后，在 BS 线为“0”
的情况下让计数器开始计数，计数值通过一组地址线发向各设备。每个设备接口都有一个
设备地址判别电路，当地址线上的计数值与请求总线的设备地址相一致时，该设备置“1”
BS 线，获得了总线使用权，此时中止计数查询。 

每次计数可以从“0”开始，也可以从中止点开始。如果从“0”开始，各设备的优先
次序与链式查询法相同，优先级的顺序是固定的。如果从中止点开始，则每个设备使用总
线的优先级相等。计数器的初值也可用程序来设置，这就可以方便地改变优先次序，显然
这种灵活性是以增加线数为代价的。 
独立请求方式  独立请求方式原理示于图 6.10(c)。在独立请求方式中，每一个共享总
线的设备均有一对总线请求线 BRi 和总线授权线 BGi。当设备要求使用总线时，便发出该设
备的请求信号。总线仲裁器中有一个排队电路，它根据一定的优先次序决定首先响应哪个
设备的请求，给设备以授权信号 BGi。 
独立请求方式的一个优点是响应时间快，即确定优先响应的设备所花费的时间少，用
不着一个设备接一个设备地查询。另一个优点是对优先次序的控制相当灵活。它可以预先
固定，如 BR0 优先级最高，BR1 次之……BRn 最低；也可以通过程序来改变优先次序；还可
以用屏蔽(禁止)某个请求的办法，不响应来自无效设备的请求。因此当代总线标准普遍采
用独立请求方式。 
对于单处理器系统总线而言，总线仲裁器又称为总线控制器，它是 CPU 的一部分，一
般是一个单独的功能模块，如图 6.4 所示。 
思考题  三种集中式仲裁方式中，哪种方式效率最高？为什么？ 

分布式仲裁不需要集中的总线仲裁器，每个潜在的主方功能模块都有自己的仲裁号和
仲裁器。当它们有总线请求时，把它们唯一的仲裁号发送到共享的仲裁总线上，每个仲裁
器将仲裁总线上得到的号与自己的号进行比较。如果仲裁总线上的号大，则它的总线请求
不予响应，并撤销它的仲裁号。最后，获胜者的仲裁号保留在仲裁总线上。显然，分布式
仲裁是以优先级仲裁策略为基础的。 

(1)所有参与本次竞争的各主设备(本例中共 8 个)将设备竞争号 CN 取反后打到仲裁总
线 AB 上，以实现“线或”逻辑。AB 线低电平时表示至少有一个主设备的 CNi 为 1，AB
线高电平时表示所有主设备的 CNi 为 0。 
(2)竞争时 CN 与 AB 逐位比较，从最高位(b7)至最低位(b0)以一维菊花链方式进行，
只有上一位竞争得胜者 Wi+1 位为 1。当 CNi=1，或 CNi =0 且 ABi 为高电平时，才使 Wi 位为
1。若 Wi=0 时，将一直向下传递，使其竞争号后面的低位不能送上 AB 线。 
(3)竞争不到的设备自动撤除其竞争号。在竞争期间，由于 W 位输入的作用，各设备
在其内部的 CN 线上保留其竞争号并不破坏 AB 线上的信息。 
(4)由于参加竞争的各设备速度不一致，这个比较过程反复(自动)进行，才有最后稳定
的结果。竞争期的时间要足够，保证最慢的设备也能参与竞争。 

总线的一次信息传送过程，大致可分为如下五个阶段：请求总线，总线仲裁，寻址(目
的地址)，信息传送，状态返回(或错误报告)。为了同步主方、从方的操作，必须制订定时
协定。所谓定时，是指事件出现在总线上的时序关系。下面介绍数据传送过程中采用的几
种定时协定：同步定时协定、异步定时协定、半同步定时协定和周期分裂式总线协定。 
1. 同步总线定时协定 
在同步定时协议中，事件出现在总线上的时刻由总线时钟信号来确定，所以总线中包
含时钟信号线。一次 I/O 传送被称为时钟周期或总线周期。图 6.12 表示读数据的同步时序
例子，所有事件都出现在时钟信号的前沿，大多数事件只占据单一时钟周期。例如，在总
线读周期，CPU 首先将存储器地址放到地址线上，它亦可发出一个启动信号，指明控制信
息和地址信息已出现在总线上。第 2 个时钟周期发出一个读命令。存储器模块识别地址码，
经一个时钟周期延迟(存取时间)后，将数据和认可信息放到总线上，被 CPU 读取。如果是
总线写周期，CPU 在第 2 个时钟周期开始将数据放到数据线上，待数据稳定后 CPU 发出一
个写命令，存储器模块在第 3 个时钟周期存入数据。 

由于采用了公共时钟，每个功能模块什么时候发送或接收信息都由统一时钟规定，因
此，同步定时具有较高的传输频率。 
同步定时适用于总线长度较短、各功能模块存取时间比较接近的情况。这是因为同步
方式对任何两个功能模块的通信都给予同样的时间安排。由于同步总线必须按最慢的模块
来设计公共时钟，当各功能模块存取时间相差很大时，会大大损失总线效率。 
2. 异步总线定时协定 
在异步定时协议中，后一事件出现在总线上的时刻取决于前一事件的出现时刻，即建
立在应答式或互锁机制基础上。在这种系统中，不需要统一的公共时钟信号。总线周期的
长度是可变的。 
图 6.13(a)表示系统总线读周期时序图。CPU 发送地址信号和读状态信号到总线上。待
这些信号稳定后，它发出读命令，指示有效地址和控制信号的出现。存储器模块进行地址
译码并将数据放到数据线上。一旦数据线上的信号稳定，则存储器模块使确认线有效，通
知 CPU 数据可用。CPU 由数据线上读取数据后，立即撤销读状态信号，从而引起存储器模
块撤销数据和确认信号。最后，确认信号的撤销又使 CPU 撤销地址信息。 
 

图 6.13(b)表示系统总线写周期时序图。CPU 将数据放到数据线上，与此同时启动状态
线和地址线。存储器模块接受写命令从数据线上写入数据，并使确认线上信号有效。然后，
CPU 撤销写命令，存储器模块撤销确认信号。 
异步定时的优点是总线周期长度可变，不把响应时间强加到功能模块上，因而允许快
速和慢速的功能模块都能连接到同一总线上。但这以增加总线的复杂性和成本为代价。 
思考题  你能说出同步定时与异步定时各自的应用环境吗？ 

3. 半同步总线定时协定 
同步总线的优点是控制简单，传输速率通常较高，但不适用于速度差异较大的设备。
如果在总线上传输的大部分设备的速度相当，仅有很少的设备需要较长的传输时间，则可
以在同步总线定时协定的基础上稍加改动，扩展为半同步总线定时协定。 
半同步总线整体上仍然采用同步操作方式，其总线周期是时钟周期的整数倍。不同之
处在于增加一根联络信号线，如高电平有效的准备好信号 READY(或者低电平有效的等待
信号 nWAIT))，由此信号决定是否需要增加时钟周期。图 6.15 为某种半同步总线的操作时
序图。从图中可以看出，基本的总线传输周期由 T1 到 T4 四个时钟周期构成，但如果某个设
备来不及在四个时钟周期内完成总线操作，可以使 READY 信号无效(或者 nWAIT 信号有
效)以增加时钟周期数。总线控制逻辑在 T3 的前沿检测 READY 引脚是否有效：如果 READY
有效，则在 T3 时钟周期后进入 T4 时钟周期；如果 READY 无效，则在 T3 和 T4 之间插入一
个等待周期 Tw，并在 Tw 前沿再次检测 READY 引脚是否有效，直到 READY 有效后才进入
T4 时钟周期。 
半同步总线协定在同步总线协定的基础上仅仅增加了一点点成本，但适应能力却大大
提升。因此，现代的许多同步总线都已扩展为半同步总线。 

4. 周期分裂式总线定时协定 
分析图 6.12 中的同步总线读操作时序可以看出，在第一个时钟周期 CPU 送出地址信息
和最后一个时钟周期存储器送出数据之间，通常有若干个时钟周期的延迟时间。这是存储
器内部准备数据的操作时间，占用的时钟周期数取决于存储器自身的速度。但是这部分时
间实际上并不需要占用总线传输数据，因而宝贵的总线资源被浪费了。 
故在对总线性能要求非常高的系统中，可以将每个读周期分为三步：①主方通过总线
向从方发送地址和读命令；②从方根据命令进行内部读操作，这是从方执行读命令的数据
准备时间；③从方通过数据总线向主方提供数据。相应地，将一个读周期分解成两个分离
的传输子周期：第一个子周期，主方发送地址和命令及有关信息后，立即和总线断开，供
其他设备使用；第二个子周期，被读出的设备重新申请总线使用权后将数据通过总线发向
请求数据的设备。而写周期只需要第一个子周期即可完成。 
在分离式总线定时协定中，由于每个设备都要申请总线使用权，故读数据的双方都是
总线主方。分离式总线定时协定以硬件复杂度的提高换取总线性能的提升。 

当代的总线标准大都能支持以下四类模式的数据传送，如图 6.16 所示。 
读、写操作  读操作是由从方到主方的数据传送；
写操作是由主方到从方的数据传送。一般，主方先以一
个总线周期发出命令和从方地址，经过一定的延时再开
始数据传送总线周期。为了提高总线利用率，减少延时
损失，主方完成寻址总线周期后可让出总线控制权，以
使其他主方完成更紧迫的操作。然后再重新竞争总线，
完成数据传送总线周期。 
块传送操作  只需给出块的起始地址，然后对固定
块长度的数据一个接一个地读出或写入。对于 CPU(主
方)-存储器(从方)而言的块传送，常称为突发(猝发)式传
送，其块长一般固定为数据线宽度(存储器字长)的 4 倍。
例如，一个 64 位数据线的总线，一次猝发式传送可达 256
位。这在超标量流水中十分有用。 
写后读、读修改写操作  这是两种组合操作。只给
出地址一次(表示同一地址)，或进行先写后读操作，或进行先读后写操作。前者用于校验
目的，后者用于多道程序系统中对共享存储资源的保护。这两种操作和猝发式操作一样，

主方掌管总线直到整个操作完成。 
广播、广集操作  一般而言，数据传送只在一个主方和一个从方之间进行。但有的总
线允许一个主方对多个从方进行写操作，这种操作称为广播。与广播相反的操作称为广集，
它将选定的多个从方数据在总线上完成 AND 或 OR 操作，用以检测多个中断源。 

图 6.17 示出了典型的多总线结构框图。实际上，这也是 PC 机和服务器的主板总线的
经典结构。 
 
如图 6.17 所示，整个系统有如下三种不同的总线。 
HOST 总线  该总线有 CPU 总线、系统总线、主存总线、前端总线等多种名称，各自
反映了总线功能的一个方面。这里称“宿主”总线，也许更全面，因为 HOST 总线不仅连
接主存，还可以连接多个 CPU。 
HOST 总线是连接“北桥”芯片与 CPU 之间的信息通路，它是一个 64 位数据线和 32
位地址线的同步总线。32 位的地址线可支持处理器 4GB 的存储寻址空间。总线上还接有
L2 级 cache，主存与 cache 控制器芯片。后者用来管理 CPU 对主存和 cache 的存取操作。CPU
拥有 HOST 总线的控制权，但在必要情况下可放弃总线控制权。 
PCI 总线  连接各种高速的 PCI 设备。PCI 是一个与处理器无关的高速外围总线，又是
至关重要的层间总线。它采用同步时序协议和集中式仲裁策略，并具有自动配置能力。PCI
设备可以是主设备，也可以是从设备，或兼而有之。在 PCI 设备中不存在 DMA(直接存储
器传送)的概念，这是因为 PCI 总线支持无限的猝发式传送。这样，传统总线上用 DMA 方
式工作的设备移植到 PCI 总线上时，采用主设备工作方式即可。系统中允许有多条 PCI 总
线，它们可以使用 HOST 桥与 HOST 总线相连，也可使用 PCI/PCI 桥与已和 HOST 总线相

连的 PCI 总线相连，从而得以扩充整个系统的 PCI 总线负载能力。 
LEGACY 总线  可以是 ISA、EISA、MCA 等这类性能较低的传统总线，以便充分利
用市场上丰富的适配器卡，支持中、低速 I/O 设备。 
在 PCI 总线体系结构中有三种桥。其中 HOST 桥又是 PCI 总线控制器，含有中央仲裁
器。桥起着重要的作用，它连接两条总线，使彼此间相互通信。桥又是一个总线转换部件，
可以把一条总线的地址空间映射到另一条总线的地址空间上，从而使系统中任意一个总线
主设备都能看到同样的一份地址表。桥本身的结构可以十分简单，如只有信号缓冲能力和
信号电平转换逻辑，也可以相当复杂，如有规程转换、数据快存、装拆数据等。 
PCI 总线的基本传输机制是猝发式传送，利用桥可以实现总线间的猝发式传送。写操作
时，桥把上层总线的写周期先缓存起来，以后的时间再在下层总线上生成写周期，即延迟
写。读操作时，桥可早于上层总线，直接在下层总线上进行预读。无论延迟写和预读，桥
的作用可使所有的存取都按 CPU 的需要出现在总线上。 
由上可见，以桥连接实现的 PCI 总线结构具有很好的扩充性和兼容性，允许多条总线
并行工作。它与处理器无关，不论 HOST 总线上是单 CPU 还是多 CPU，也不论 CPU 是什
么型号，只要有相应的 HOST 桥芯片(组)，就可与 PCI 总线相连。 
思考题  多总线结构中“桥”起着何种作用？你怎样看待北桥和南桥？ 

表 6.1 列出了 PCI 标准 2.0 版的必有类信号名称及其功能描述。它采用 32～64 位数据
线和 32 位地址线，数据线和地址线是一组线，分时复用。使用同步时序协议，总线时钟为
方波信号，频率为 33.3MHz。总线所有事件都出现在时钟信号的下跳沿，正好是时钟周期
的中间。采样发生在时钟信号的上跳沿。PCI 采用集中式仲裁方式，每个 PCI 主设备都有总
线请求 REQ#和授权 GNT#两条信号线与中央仲裁器相连。符号#表示信号低电平有效，in
表示输入线，out 表示输出线，t/s 表示双向三态信号线，s/t/s 表示一次只被一个拥有者驱动
的抑制三态信号线，o/d 表示开路驱动，允许多个设备以线或方式共享此线。 

总 线 周 期 类 型 由 C/nBE 线 上 的 总 线 命 令 给 出 。 总 线 周 期 长 度 由 周 期 类 型 和
nFRAME(帧)、nIRDY(主就绪)、nTRDY(目标就绪)、nSTOP(停止)等信号控制。一个总
线周期由一个地址期和一个或多个数据期组成。启动此总线周期的主设备，在地址期送出
总线命令和目标设备地址，而目标设备以 nDEVSEL(设备选择)信号予以响应。还有一个
IDSEL(初始化设备选择)信号，用以配置读写期间的芯片选择。 
除必有类信号外，还有 16 种可选类信号线。除一组信号线用于扩充到 64 位传送外，
其他三组信号分别用于 cache 一致性支持、中断请求、测试与边界扫描。其中，中断请求信
号线是开路驱动，允许多个设备共享一条中断请求信号线。有关中断的概念留在第 7 章    
介绍。 
电源线和地线未列入表中。2.0 版定义了 5V 和 3.3V 两种信号环境，更新的版本均使用
3.3V 工作电压。 

PCI 总线周期由当前被授权的主设备发起。PCI 支持任何主设备和从设备之间点到点的
对等访问，也支持某些主设备的广播读写。 
PCI 总线周期类型由主设备在 C/BE[3—0]线上送出的 4 位总线命令代码指明，被目标
设备译码确认，然后主从双方协调配合完成指定的总线周期操作。4 位代码组合可指定 16
种总线命令，但实际给出 12 种。PCI 总线命令类型如表 6.2 所示。 

存储器读/写总线周期  以猝发式传送为基本机制，一次猝发式传送总线周期通常由一
个地址期和一个或几个数据周期组成。存储器读/写周期的解释，取决于 PCI 总线上的存储
器控制器是否支持存储器/cache 之间的 PCI 传输协议。如果支持，则存储器读/写一般是通
过 cache 来进行；否则，是以数据块非缓存方式来传输。 
存储器写和使无效周期  与存储器写周期的区别在于，前者不仅保证一个完整的 cache
行被写入，而且在总线上广播“无效”信息，命令其他 cache 中的同一行地址变为无效。关
于存储器读的三个总线周期的说明示于表 6.3 中。 

特殊周期  用于主设备将其信息(如状态信息)广播到多个目标方。它是一个特殊的写
操作，不需要目标方以 nDEVSEL 信号响应。但各目标方须立即使用此信息，无权中止此写
操作过程。 
配置读/写周期  是 PCI 具有自动配置能力的体现。PCI 有三个相互独立的物理地址空
间，即存储器、I/O、配置空间。所有 PCI 设备必须提供配置空间，而多功能设备要为每一
实现功能提供一个配置空间。配置空间是 256 个内部寄存器，用于保存系统初始化期间设
置的配置参数。CPU 通过 HOST 桥的两个 32 位专用寄存器(配置地址、配置数据)来访问
PCI 设备的配置空间。即 HOST 桥根据 CPU 提供给这两个寄存器的值，生成 PCI 总线的配
置读/写周期，完成配置数据的读出或写入操作。 
双地址周期  用于主方指示它正在使用 64 位地址。 

下面以数据传送类的总线周期为代表，说明 PCI 总线周期的操作过程。为了深化概念，
图 6.18 中给出了一个读操作总线周期时序示例。图中的环形箭头符号表示某信号线由一个
设备驱动转换成另一设备驱动的过渡期，以此过渡期避免两个设备同时驱动一条信号线的
冲突。 
我们看到，PCI 总线周期的操作过程有如下特点。 
(1)采用同步时序协议。总线时钟周期以上跳沿开始，半个周期高电平，半个周期低电
平。总线上所有事件，即信号电平转换出现在时钟信号的下跳沿时刻，而对信号的采样出
现在时钟信号的上跳沿时刻。 
(2)总线周期由被授权的主方启动，以帧 nFRAME 信号变为有效来指示一个总线周期
的开始。 
(3)一个总线周期由一个地址期和一个或多个数据期组成。在地址期内除给出目标地址
外，还在 C/BE#线上给出总线命令以指明总线周期类型。 

(4)地址期为一个总线时钟周期，一个数据期在没有等待状态下也是一个时钟周期。一
次数据传送是在挂钩信号 nIRDY 和 nTRDY 都有效情况下完成，任一信号无效(在时钟上跳
沿被对方采样到)，都将加入等待状态。 
(5)总线周期长度由主方确定。在总线周期期间 nFRAME 持续有效，但在最后一个数
据期开始前撤除。即以 nFRAME 无效后，nIRDY 也变为无效的时刻表明一个总线周期结束。
由此可见，PCI 的数据传送以猝发式传送为基本机制，单一数据传送反而成为猝发式传送的
一个特例。并且 PCI 具有无限制的猝发能力，猝发长度由主方确定，没有对猝发长度加以
固定限制。 
(6)主方启动一个总线周期时要求目标方确认。即在 nFRAME 变为有效和目标地址送
上 AD 线后，目标方在延迟一个时钟周期后必须以 nDEVSEL 信号有效予以响应。否则，主
设备中止总线周期。 
(7)主方结束一个总线周期时不要求目标方确认。目标方采样到 nFRAME 信号已变为
无效时，即知道下一数据传送是最后一个数据期。目标方传输速度跟不上主方速度，可用
nTRDY 无效通知主方加入等待状态时钟周期。当目标方出现故障不能进行传输时，以
nSTOP 信号有效通知主方中止总线周期。 

PCI 总线采用集中式仲裁方式，每个 PCI 主设备都有独立的 nREQ(总线请求)和
nGNT(总线授权)两条信号线与中央仲裁器相连。由中央仲裁器根据一定的算法对各主设备
的申请进行仲裁，决定把总线使用权授予谁。但 PCI 标准并没有规定仲裁算法。 
中央仲裁器不仅采样每个设备的 nREQ 信号线，而且采样公共的 nFRAME 和 nIRDY 信
号线。因此，仲裁器清楚当前总线的使用状态：是处于空闲状态还是一个有效的总线周期。 
PCI 总线支持隐藏式仲裁。即在主设备 A 正在占用总线期间，中央仲裁器根据指定的
算法裁决下一次总线的主方应为主设备 B 时，它可以使 nGNT-A 无效而使 nGNT-B 有效。

此时，设备 A 应在数据传送完成后立即释放 nFRAME 和 nIRDY 信号线，由设备 B 掌管后
开始一个新的总线周期。隐藏式仲裁使裁决过程或在总线空闲期进行或在当前总线周期内
进行，不需要单独的仲裁总线周期，提高了总线利用率。中央仲裁器使 nGNT-A 无效与
nGNT-B 有效之间至少有 1 个时钟周期的延迟，以保证信号线由 A 驱动变为 B 驱动时在临
界情况下也不产生冲突，即上述的交换期。 
一个提出申请并被授权的主设备，应在 nFRAME、nIRDY 线已释放的条件下尽快开始
新的总线周期操作。自 nFRAME、nIRDY 信号变为无效开始，16 个时钟周期内信号仍不变
为有效，中央仲裁器认为被授权的主设备为“死设备”，并收回授权，以后也不再授权给该
设备。 

相比早期的 ISA 和 EISA 等第一代总线，PCI 总线的传输速度有明显提升。但是计算机
系统对传输性能的要求仍在不断提升中，PCI 总线逐渐难以满足高速显卡等高性能传输模块
的性能要求。于是，第三代的 PCIe 总线逐渐取代了 PCI 总线。 
PCIe 总线全称为 PCI-Express，是基于 PCI 总线技术发展起来的总线标准，对 PCI 总
线有良好的继承性，在软件和应用上兼容 PCI 总线。与 PCI 总线相比，PCIe 总线的主要改
进有如下几点。 
(1)高速差分传输。与 PCI 总线使用的单端信号对地传输方式相比，PCIe 总线改用差分
信号进行数据传送，一个信号由 D+和 D–两根信号线传输，信号接收端通过比较这两个信
号的差值判断发送端发送的是逻辑“1”还是逻辑“0”。由于外部干扰噪声将同时附加到
D+和 D–两根信号上，因而在理论上并不影响二者的差值，对外界的电磁干扰也比较小。因
此差分信号抗干扰的能力更强，可以使用更高的总线频率。PCIe 总线还引入了嵌入时钟技
术，发送端不向接收端传输时钟信号，而是通过 8b/10b 或 128b/130b 编码将时钟信息嵌入
数据信号中，接收端可以从数据中恢复出时钟。 
(2)串行传输。由于并行传输方式使用更多的信号线进行传输，因而理论上并行传输的
速率比串行传输更高。但是并行总线通常需要在系统底板上进行复杂的走线，随着信号传
输速度的提高，不同长度或在 PCB 板不同层布放的导线引起的定时偏差的影响和并行导线
之间存在的相互干扰变得越来越严重，限制了信号传输的最高速率。而串行传输方式在每
个方向只有一个差分信号，且时钟信息嵌入在数据信号中，故不会出现定时偏移。因此，
串行信号在有些情况下传输速度反而更高。与 USB 总线和 SATA 接口类似，PCIe 总线也采
用串行传输方式替代 PCI 总线的并行传输方式。 
(3)全双工端到端连接。与 PCI 的共享总线模式不同，PCIe 链路使用端到端的数据传送
方式，每一通道(Lane)只能连接两个设备，设备之间通过双向的链路相连接，每个传输通
道独享带宽。如图 6.19 所示，PCIe 总线的物理链路的一个通道由两组差分信号组成，发送
端的发送器与接收端的接收器通过一对儿差分信号连接，接收端的发送器与发送端的接收
器通过另外一对儿差分信号连接。PCIe 支持全双工通信，允许在同一时刻同时进行数据发
送和接收。 

(4)基于多通道的数据传递方式。一个 PCIe 链路可以由多条通道组成，目前可支持×1、
×2、×4、×8、×12、×16 和×32 宽度的 PCIe 链路。不同的 PCIe 总线规范所定义的总
线频率和链路编码方式并不相同，PCIe 1.0 规范中，×1 单通道单向传输带宽可达到
250MB/s。多通道设计增加了灵活性，较慢的设备可以分配较少的通道。 
(5)基于数据包的传输。作为串行通信总线，PCIe 所有的数据都是以数据包为单位进行
传输的。一个完整的 PCIe 体系结构由上到下包括应用层、事务层、数据链路层和物理层，
如图 6.20 所示。 
 

图 6.21 为 PCIe 总线的拓扑结构实例。可以看出，PCIe 总线上包括四类实体：根复合
体、交换器、PCIe 桥和端点。根复合体(Root Complex)是 PCIe 的根控制器，将处理器/内
存子系统连接到 PCIe 交换结构。一个根复合体可能包含多个 PCIe 端口，可将多个交换器
连接到根复合体或级联的端口。PCIe 总线采用基于交换的技术，交换器(Switch)可以扩展
PCIe 总线，PCIe 总线系统可以通过交换器连接多个 PCIe 设备。PCIe 桥(PCIe brige)负责

PCIe 和其他总线之间的转换，PCIe 总线系统可以通过 PCIe 桥扩展出传统的 PCI 总线或
PCI-X 总线。在 PCIe 总线中，基于 PCIe 总线的设备称为端点(Endpoint)，如 PCIe 接口网
卡、串口卡、存储卡等。端点处于 PCIe 总线系统拓扑结构中的最末端，一般作为总线操作
的发起者或者终结者，老旧端点(Legacy Endpoint)则是指那些原本准备设计用于 PCI-X 总
线但却被改为 PCIe 接口的设备。 
此外，电源管理、服务质量(QoS)、热插拔支持、数据完整性、错误处理机制等也是
PCIe 总线所支持的高级特征。 

本 章 小 结 
总线是构成计算机系统的互联机构，是多个系统功能部件之间进行数据传送的公共通
道，并在争用资源的基础上进行工作。 
总线有物理特性、功能特性、电气特性、机械特性，因此必须标准化。微型计算机系
统的标准总线从 ISA 总线(16 位，带宽 8MB/s)发展到 EISA 总线(32 位，带宽 33.3MB/s)和
VESA 总线(32 位，带宽 132MB/s)，又进一步发展到 PCI 总线(64 位，带宽 264MB/s)。衡
量总线性能的重要指标是总线带宽，它定义为总线本身所能达到的最高传输速率。 
当代流行的标准总线追求与结构、CPU、技术无关的开发标准。其总线内部结构包含：
①数据传送总线(由地址线、数据线、控制线组成)；②仲裁总线；③中断和同步总线；
④公用线(电源、地线、时钟、复位等信号线)。 
计算机系统中，根据应用条件和硬件资源不同，信息的传输方式可采用：①并行传送；
②串行传送；③复用传送。 
各种外围设备必须通过 I/O 接口与总线相连。I/O 接口是指 CPU、主存、外围设备之间
通过总线进行连接的逻辑部件。接口部件在它动态联结的两个功能部件间起着缓冲器和转
换器的作用，以便实现彼此之间的信息传送。 
 
InfiniBand
标准 
InfiniBand
通信协 
议栈 

总线仲裁是总线系统的核心问题之一。为了解决多个主设备同时竞争总线控制权的问
题，必须具有总线仲裁部件。它通过采用优先级策略或公平策略，选择其中一个主设备作
为总线的下一次主方，接管总线控制权。按照总线仲裁电路的位置不同，总线仲裁分为集
中式仲裁和分布式仲裁。集中式仲裁方式必有一个中央仲裁器，它受理所有功能模块的总
线请求，按优先原则或公平原则进行排队，然后仅给一个功能模块发出授权信号。分布式
仲裁不需要中央仲裁器，每个功能模块都有自己的仲裁号和仲裁器。 
总线定时是总线系统的核心问题之一。为了同步主方、从方的操作，必须制订定时协
议，通常采用同步定时与异步定时两种方式。在同步定时协议中，事件出现在总线上的时
刻由总线时钟信号来确定，总线周期的长度是固定的。在异步定时协议中，后一事件出现
在总线上的时刻取决于前一事件的出现时刻，即建立在应答式或互锁机制基础上，不需要
统一的公共时钟信号。在异步定时中，总线周期的长度是可变的。 
当代的总线标准大都能支持以下数据传送模式：①读/写操作；②块传送操作；③写后
读、读修改写操作；④广播、广集操作。 
PCI 总线是当前实用的总线，是一个高带宽且与处理器无关的标准总线，又是重要的层
次总线。它采用同步定时协议和集中式仲裁策略，并具有自动配置能力。PCI 适合于低成本
的小系统，因此在微型机系统中得到了广泛的应用。PCI 总线的升级版 PCIe 总线在许多方
面进行了改进，其性能得到大幅度提升。 

外围设备这个术语涉及相当广泛的计算机部件。事实上，除了 CPU 和主存外，计算机
系统的每一部分都可作为一个外围设备来看待。 
20 世纪末，主机与外围设备的价格比为 1∶6。这种情况表明：一方面，在计算机的发
展中，外围设备的发展占有重要地位；另一方面，外围设备的发展同主机的发展还不相适
应。尽管如此，外围设备还是得到了较快的发展。在指标上，外围设备不断采用新技术，
向低成本、小体积、高速、大容量、低功耗等方面发展。在结构上，由初级的串行操作输
入/输出方式，发展到有通道连接的多种外设并行操作方式。在种类上，由简单的输入/输出
装置，发展到多种输入/输出装置、随机存取大容量外存、多种终端设备，等等。在性能上，
信息交换速度大大提高，输入输出形态不仅有数字形式，还有直观的图像和声音等形式。 
外围设备的功能是在计算机和其他机器之间，以及计算机与用户之间提供联系。没有
外围设备的计算机就像缺乏五官四肢的人一样，既不能从外界接收信息，又不能对处理的
结果做出表达和反应。随着计算机系统的飞速发展和应用的扩大，系统要求外围设备类型
越来越多，外围设备智能化的趋势越来越明显，特别是出现多媒体技术以后。毫无疑问，
随着科学技术的发展，提供人-机联系的外围设备将会变成计算机真正的“五官四肢”。 
一般说来，外围设备由三个基本部分组成。 
(1)存储介质，具有保存信息的物理特征。例如，磁盘，用记录在盘上的磁化元表示    
信息。 
(2)驱动装置，用于移动存储介质。例如，磁盘设备中，驱动装置用于转动磁盘并进行
定位。 
(3)控制电路，向存储介质发送数据或从存储介质接收数据。例如，磁盘读出时，控制
电路把盘上用磁化元形式表示的信息转换成计算机所需要的电信号，并把这些信号用电缆


一个计算机系统配备什么样的外围设备，是根据实际需要来决定的。图 7.1 示出了计算
机的五大类外围设备，这只是一个典型化了的计算机环境。 
如图 7.1 所示，中央部分是 CPU 和主存，通过系统总线与第二层的适配器(接口)部件
相连，第三层是各种外围设备控制器，最外层则是外围设备。 

外围设备可分为输入设备、输出设备、外存设备、数据通信设备和过程控制设备几大
类。表 7.1 列出了各种 I/O 设备名称、功能及数据传输速率。 

每一种外围设备，都是在它自己的设备控制器控制下进行工作的，而设备控制器则通
过 I/O 接口和主机连接，并受主机控制。 

计算机的外存储器又称磁表面存储设备。所谓磁表面存储，是用某些磁性材料薄薄地涂
在金属铝或塑料表面作载磁体来存储信息。磁盘存储器、磁带
存储器均属于磁表面存储器。 
磁表面存储器的优点：①存储容量大，位价格低；②记
录介质可以重复使用；③记录信息可以长期保存而不丢失，
甚至可以脱机存档；④非破坏性读出，读出时不需要再生信
息。当然，磁表面存储器也有缺点，主要是存取速度较慢，
机械结构复杂，对工作环境要求较高。 
磁表面存储器由于存储容量大，位成本低，在计算机系
统中作为辅助大容量存储器使用，用以存放系统软件、大型
文件、数据库等大量程序与数据信息。 
1. 磁性材料的物理特性 
在计算机中，用于存储设备的磁性材料，是一种具有矩
形磁滞回线的磁性材料。这种磁性材料在外加磁场的作用下，
其磁感应强度 B 与外加磁场 H 的关系，可用矩形磁滞回线来
描述，如图 7.2 所示。 
从磁滞回线可以看出，磁性材料被磁化以后，工作点总

是在磁滞回线上。只要外加的正向脉冲电流(即外加磁场)幅度足够大，那么在电流消失后
磁感应强度 B 并不等于零，而是处在+Br 状态(正剩磁状态)。反之，当外加负向脉冲电流时，
磁感应强度 B 将处在–Br 状态(负剩磁状态)。这就是说，当磁性材料被磁化后，会形成两个
稳定的剩磁状态，就像触发器电路有两个稳定的状态一样。利用这两个稳定的剩磁状态，
可以表示二进制代码 1 和 0。如果规定用+Br 状态表示代码“1”，–Br 状态表示代码“0”，
那么要使磁性材料记忆“1”，就要加正向脉冲电流，使磁性材料正向磁化；要使磁性材料
记忆“0”，则要加负向脉冲电流，使磁性材料反向磁化。磁性材料上呈现剩磁状态的地方
形成了一个磁化元或存储元，它是记录一个二进制信息位的最小单位。 
2. 磁表面存储器的读写原理 
在磁表面存储器中，利用一种称为“磁头”的装置来形成和判别磁层中的不同磁化状
态。换句话说，写入时，利用磁头使载磁体(盘片)具有不同的磁化状态，而在读出时又利
用磁头来判别这些不同的磁化状态。磁头实际上是由软磁材料做铁芯绕有读写线圈的电磁
铁，如图 7.3 所示。 
写操作  当写线圈中通过一定方向的脉
冲电流时，铁芯内就产生一定方向的磁通。由
于铁芯是高导磁率材料，而铁芯空隙处为非磁
性材料，故在铁芯空隙处集中很强的磁场。如
图 7.3 所示，在这个磁场作用下，载磁体就被
磁化成相应极性的磁化位或磁化元。若在写线
圈里通入相反方向的脉冲电流，就可得到相反
极性的磁化元。如果我们规定按图中所示电流
方向为写“1”，那么写线圈里通以相反方向的
电流时即为写“0”。上述过程称为“写入”。
显然，一个磁化元就是一个存储元，一个磁化元中存储一位二进制信息。当载磁体相对于
磁头运动时，就可以连续写入一连串的二进制信息。 
读操作  如何读出记录在磁表面上的二进制代码信息呢?也就是说，如何判断载磁体上
信息的不同剩磁状态呢? 
当磁头经过载磁体的磁化元时，由于磁头铁芯是良好的导磁材料，磁化元的磁力线很
容易通过磁头而形成闭合磁通回路。不同极性的磁化元在铁芯里的方向是不同的。当磁头
对载磁体作相对运动时，由于磁头铁芯中磁通的变化，使读出线圈中感应出相应的电动势 e，
其值为 
 
d
d
e
k
t

 
 
(7.1) 
负号表示感应电势的方向与磁通的变化方向相反。不同的磁化状态，所产生的感应电
势方向不同。这样，不同方向的感应电势经读出放大器放大鉴别，就可判知读出的信息是
“1”还是“0”。图 7.4 示出了记录方式的写读过程波形图。 
归纳起来，通过电-磁变换，利用磁头写线圈中的脉冲电流，可把一位二进制代码转换成
载磁体存储元的不同剩磁状态；反之，通过磁-电变换，利用磁头读出线圈，可将由存储元的
不同剩磁状态表示的二进制代码转换成电信号输出。这就是磁表面存储器存取信息的原理。 

磁层上的存储元被磁化后，它可以供多次读出而不被破坏。当不需要这批信息时，可
通过磁头把磁层上所记录的信息全部抹去，称为写“0”。通常，写入和读出是合用一个磁
头，故称为读写磁头。每个读写磁头对应着一个信息记录磁道。 

硬磁盘是指记录介质为硬质圆形盘片的磁表面存储器。其逻辑结构如图 7.5 所示。此图
中未反映出寻址机构，而仅仅表示了存取功能的逻辑结构，它主要由磁记录介质、磁盘控
制器、磁盘驱动器三大部分组成。磁盘控制器包括控制逻辑与时序、数据并-串变换电路和
串-并变换电路。磁盘驱动器包括写入电路与读出电路、读写转换开关、读写磁头与磁头定
位伺服系统等。 
 

写入时，将计算机并行送来的数据取至并-串变换寄存器，变为串行数据，然后一位
一位地由写电流驱动器作功率放大并加到写磁头线圈上产生电流，从而在盘片磁层上形成
按位的磁化存储元。读出时，当记录介质相对磁头运动时，位磁化存储元形成的空间磁场
在读磁头线圈中产生感应电势，此读出信息经放大检测就可还原成原来存入的数据。由于
数据是一位一位串行读出的，故要送至串-并变换寄存器变换为并行数据，再并行送至计
算机。 
硬磁盘按盘片结构，分成可换盘片式与固定盘片式两种；磁头也分为可移动磁头和固
定磁头两种。 
可移动磁头固定盘片的磁盘机  特点是一片或一组盘片固定在主轴上，盘片不可更换。
盘片每面只有一个磁头，存取数据时磁头沿盘面径向移动。 
固定磁头磁盘机  特点是磁头位置固定，磁盘的每一个磁道对应一个磁头，盘片不可
更换。优点是存取速度快，省去磁头找道时间，缺点是结构复杂。 
可移动磁头可换盘片的磁盘机  盘片可以更换，磁头可沿盘面径向移动。优点是盘片
可以脱机保存，同种型号的盘片具有互换性。 
温彻斯特磁盘机  温彻斯特磁盘简称温盘，是一种采用先进技术研制的可移动磁头固
定盘片的磁盘机。它是一种密封组合式的硬磁盘，即磁头、盘片、电机等驱动部件乃至读
写电路等组装成一个不可随意拆卸的整体。工作时，高速旋转在盘面上形成的气垫将磁头
平稳浮起。优点是防尘性能好，可靠性高，对使用环境要求不高，成为最有代表性的硬磁
盘存储器。而普通的硬磁盘要求具有超净环境，只能用于大型计算机中。 
常用的温盘盘片直径有 5.25 英寸、3.5 英寸、2.5 英寸、1.75 英寸等几种。 
思考题  温盘的发明具有划时代意义，你能说说为什么吗？ 

磁盘驱动器  它是一种精密的电子和机械装置，因此各部件的加工安装有严格的技术
要求。对温盘驱动器，还要求在超净环境下组装。各类磁盘驱动器的具体结构虽然有差别，
但基本结构相同，主要由定位驱动系统、主轴系统和数据转换系统组成。图 7.6 是磁盘驱动
器外形和结构示意图。 
 

在可移动磁头的磁盘驱动器中，驱动磁头沿盘面径向位置运动以寻找目标磁道位置的
机构称为磁头定位驱动系统，它由驱动部件、传动部件、运载部件(磁头小车)组成。当磁
盘存取数据时，磁头小车的平移运动驱动磁头进入指定磁道的中心位置，并精确地跟踪该
磁道。目前磁头小车的驱动方式主要采用步进电机和音圈电机两种。步进电机靠脉冲信号
驱动，控制简单，整个驱动定位系统是开环控制，因此定位精度较低，一般用于道密度不
高的硬磁盘驱动器。音圈电机是线性电机，可以直接驱动磁头作直线运动，整个驱动定位
系统是一个带有速度和位置反馈的闭环控制系统，驱动速度快，定位精度高，因此用于较
先进的磁盘驱动器。 
主轴系统的作用是安装盘片，并驱动它们以额定转速稳定旋转。其主要部件是主轴电
机和有关控制电路。 
数据转换系统的作用是控制数据的写入和读出，包括磁头、磁头选择电路、读写电路
以及索引、区标电路等。 
磁盘控制器  它是主机与磁盘驱动器之间的接口，电路板实物如图 7.7(a)所示。由于
磁盘存储器是高速外存设备，故与主机之间采用成批交换数据方式。作为主机与驱动器之
间的控制器，它需要有两个方面的接口：一个是与主机的接口，控制外存与主机总线之间
交换数据；另一个是与设备的接口，根据主机命令控制设备的操作。前者称为系统级接口，
后者称为设备级接口。 
主机与磁盘驱动器交换数据的控制逻辑见图 7.7(b)。磁盘上的信息经读磁头读出以后
送读出放大器，然后进行数据与时钟的分离，再进行串-并变换、格式变换，最后送入数据
缓冲器，经 DMA(直接存储器传送)控制将数据传送到主机总线。 
 

我们看到，磁盘控制器的功能全部转移到设备中，主机与设备之间采用标准的通用接
口，如 SCSI 接口(小型计算机系统接口)，从而使设备相对独立。 

盘片的上下两面都能记录信息，通常把磁盘片表面称为记录面。记录面上一系列同心
圆称为磁道。每个盘片表面通常有几百到几千个磁道，每个磁道又分为若干个扇区，如
图 7.8 所示。从图中看出，外面扇区比里面扇区面积要大。磁盘上的这种磁道和扇区的排列
称为格式。 

磁道的编址是从外向内依次编号，最外一个同心圆称为 0 磁道，最里面的一个同心圆
称为 n 磁道，n 磁道里面的圆面积并不用来记录信息。扇区的编号有多种方法，可以连续编
号，也可间隔编号。磁盘记录面经这样编址后，就可用 n 磁道 m 扇区的磁盘地址找到实际
磁盘上与之相对应的记录区。除了磁道号和扇区号，还有记录面的面号，以说明本次处理
是在哪一个记录面上。例如，对活动头磁盘组来说，磁盘地址是由记录面号(也称磁头号)、
磁道号和扇区号三部分组成的。 
在磁道上，信息是按区存放的，每个区中存放一定数量的字或字节，各个区存放的字
或字节数是相同的。为进行读/写操作，要求定出磁道的起始位置，这个起始位置称为索引。
索引标志在传感器检索下可产生脉冲信号，再通过磁盘控制器处理，便可定出磁道起始   
位置。 
磁盘存储器的每个扇区记录定长的数据，因此读/写操作是以扇区为单位一位一位串行
进行的。每一个扇区记录一个记录块。数据在磁盘上的记录格式如图 7.9 所示。 

每个扇区开始时由磁盘控制器产生一个扇标脉冲。扇标脉冲的出现即标志一个扇区的
开始。两个扇标脉冲之间的一段磁道区域即为一个扇区(一个记录块)。每个记录块由头部
空白段、序标段、数据段、校验字段及尾部空白段组成。其中空白段用来留出一定的时间

作为磁盘控制器的读写准备时间，序标被用来作为磁盘控制器的同步定时信号。序标之后
即为本扇区所记录的数据。数据之后是校验字，它用来校验磁盘读出的数据是否正确。 
7.2.5  磁盘存储器的技术指标 
磁盘存储器的主要技术指标包括存储密度、存储容量、存取时间及数据传输率。 
存储密度  存储密度分道密度、位密度和面密度。道密度是沿磁盘半径方向单位长度
上的磁道数，单位为道/英寸。位密度是磁道单位长度上能记录的二进制代码位数，单位为
位/英寸。面密度是位密度和道密度的乘积，单位为位/英寸 2。 
存储容量  一个磁盘存储器所能存储的字节总数，称为磁盘存储器的存储容量。存储
容量有格式化容量和非格式化容量之分。格式化容量是指按照某种特定的记录格式所能存
储信息的总量，也就是用户可以真正使用的容量。非格式化容量是磁记录表面可以利用的
磁化单元总数。将磁盘存储器用于某计算机系统中，必须首先进行格式化操作，然后才能
供用户记录信息。格式化容量一般是非格式化容量的 60%～70%，3.5 英寸的硬盘容量可达
数十 TB。 
平均寻址时间  寻址时间是指从读写命令发出后，磁头从某一起始位置移动至新的记
录位置，再到磁道上需要访问的扇区移动到磁头下方所需的时间。这段时间包括寻道时间
和等待时间。磁盘接到读/写指令后将磁头定位至所要访问的磁道上所需的时间，称为寻道
时间或找道时间、定位时间。寻道完成后，磁道上需要访问的扇区移动到磁头下方所需的
时间，称为等待时间或寻区时间、潜伏期、旋转延迟。这两个时间都是随机变化的，因此
往往使用平均值来表示。平均寻道时间是最大寻道时间与最小寻道时间的平均值，一般由
厂家给出，目前典型的平均寻道时间小于 10ms。平均等待时间和磁盘转速有关，它用磁盘
旋转一周所需时间的一半来表示。若 r 表示磁盘旋转速率，单位是转/秒，则平均等待时间
为 1/(2r)。转速为 7200 转/分的磁盘的平均等待时间约为 4.16ms。 
平均存取时间  存取(访问)时间是从读/写指令发出到开始第一笔数据读/写时所用的
平均时间，包括寻道时间、等待时间及相关的内务操作时间。内务操作时间一般很短(一般
在 0.2ms 左右)，可忽略不计。故平均访问时间近似等于平均寻道时间+平均等待时间，即
平均寻址时间。 
因此，总的平均读写操作时间 Ta 可表示为 
 
式中，Ts 表示平均寻道时间，b 表示传送的字节数，N 表示每磁道字节数，b/(rN)表示数据
传输时间。 
数据传输率  磁盘存储器在单位时间内向主机传送数据的字节数，称为数据传输率。
现代磁盘设备通常会配置磁盘 cache，单位时间内从硬盘 cache 向主机传送的数据信息量称
为外部数据传输率，与磁盘的接口类型和磁盘缓存大小有关。从主机接口逻辑考虑，应有
足够快的传送速度向设备发送或从设备接收信息。在磁盘存储器盘片上读写数据的速率则
称为内部数据传输率，即磁头找到要访问的位置后，单位时间读/写的字节数，等于每个磁
道上的字节数/磁盘旋转一周的时间。设磁盘旋转速度为 n 转/秒，每条磁道容量为 N 字节，
则内部数据传输率为 
      计算机组成原理 
224
 
Dr=nN(字节/秒)    或   Dr=D·v(字节/秒) 
(7.3) 
其中，D 为位密度，v 为磁盘旋转的线速度。 
磁盘存储器的数据传输率可达几十兆字节/秒。 

1)磁盘 cache 的概念 
随着微电子技术的飞速发展，CPU 的速度每年增长 1 倍左右，主存芯片容量和磁盘驱
动器的容量每 1.5 年增长 1 倍左右。但磁盘驱动器的存取时间没有出现相应的下降，仍停留
在毫秒(ms)级。而主存的存取时间为纳秒(ns)级，两者速度差别十分突出，因此磁盘 I/O
系统成为整个系统的瓶颈。为了减少存取时间，可采取的措施有：提高磁盘机主轴转速，
提高 I/O 总线速度，采用磁盘 cache(磁盘缓存)等。 
主存和 CPU 之间设置高速缓存 cache 是为了弥补主存和 CPU 之间速度上的差异。同样，
磁盘 cache 是为了弥补慢速磁盘和主存之间速度上的差异。 
2)磁盘 cache 的原理 
在磁盘 cache 中，由一些数据块组成的一个基本单位称为 cache 行。当一个 I/O 请求送
到磁盘驱动时，首先搜索驱动器上的高速缓冲行是否已写上数据？如果是读操作，且要读
的数据已在 cache 中，则为命中，可从 cache 行中读出数据，否则需从磁盘介质上读出。写
入操作和 CPU 中的 cache 类似，有“直写”和“写回”两种方法。 
磁盘 cache 利用了被访问数据的空间局部性和时间局部性原理。空间局部性是指当某些
数据被存取时，该数据附近的其他数据可能也将很快被存取；时间局部性是指当一些数据
被存取后，不久这些数据还可能再次存取。因此现在大多数磁盘驱动器中都使用了预读策
略，而根据局部性原理预取一些不久将可能读入的数据放到磁盘 cache 中。 
CPU 的 cache 存取时间一般小于 10ns，命中率 95%以上，全用硬件来实现。磁盘 cache
一次存取的数量大，数据集中，速度要求较 CPU 的 cache 低，管理工作较复杂，因此一般
由硬件和软件共同完成。其中 cache 采用 SRAM 或 DRAM。 
RAID 最早称为廉价冗余磁盘阵列，后来改为独立冗余磁盘阵列，它是用多台磁盘存储
器组成的大容量外存系统。其构造基础是利用数据分块技术和并行处理技术，在多个磁盘
上交错存放数据，使之可以并行存取。在 RAID 控制器的组织管理下，可实现数据的并行
存储、交叉存储、单独存储。由于阵列中的一部分磁盘存有冗余信息，一旦系统中某一磁盘
失效，可以利用冗余信息重建用户信息。 
RAID 是 1988 年由美国加州大学伯克利分校一个研究小组提出的，它的设计理念是用
多个小容量磁盘代替一个大容量磁盘，并用分布数据的方法能够同时从多个磁盘中存取数
据，因而改善了 I/O 性能，增加了存储容量，现已在超级或大型计算机中使用。 
工业上制定了一个称为 RAID 的标准，它分为 7 级(RAID 0～RAID 6)。这些级别不是
表示层次关系，而是指出了不同存储容量、可靠性、数据传输能力、I/O 请求速率等方面的
应用需求。 
下面以 RAID 0 级为例来说明。考虑到低成本比可靠性更重要，RAID 0 未采用奇偶校
验等冗余技术。RAID 0 用于高速数据传输和高速 I/O 请求。 
对 RAID 0，用户和系统数据分布在阵列中的所有磁盘上。与单个大容量磁盘相比，其
优点是：如果两个 I/O 请求正在等待两个不同的数据块，则被请求的块有可能在不同的盘上。
因此，两个请求能够并行发出，减少了 I/O 排队的时间。 
图 7.10 表示使用磁盘阵列管理软件在逻辑磁盘和物理磁盘间进行映射。此软件可在磁
盘子系统或主机上运行。 
所有的用户数据和系统数据都被看成是逻辑条带，存储在一个逻辑磁盘上。而实际物
理磁盘也以条带形式划分，每个条带是一些物理的块、扇区或其他单位。数据条带以轮转
方式映射到连续的阵列磁盘中。每个磁盘映射一条带，一组逻辑连续条带称为条带集。在
一个有 n 个磁盘的阵列中，第 1 组的 n 个逻辑条带依次物理地存储在 n 个磁盘的第 1 个条

带上，构成第 1 个条带集；第 2 组的 n 个逻辑条带分布在每个磁盘的第 2 个条带上；依次
类推。这种布局的优点是，如果单个 I/O 请求由多个逻辑相邻的条带组成，则对多达 n 个条
带的请求可以并行处理，从而大大减少了 I/O 的传输时间。 
 
磁带机的记录原理与磁盘机基本相同，只是它的载磁体是一种带状塑料，称为磁带。
写入时可通过磁头把信息代码记录在磁带上。当记录有代码的磁带在磁头下移动时，就可
在磁头线圈上感应出电动势，即读出信息代码。磁带存储设备由磁带机和磁带两部分组成，
它通常用作为海量存储设备的数据备份。 
磁带速度比磁盘速度慢，原因是磁带上的数据采用顺序访问方式，而磁盘则采用随机
访问方式。 
目前的磁带技术有如下几种类型。 
1)1/4 英寸磁带(QIC) 
1/4 英寸磁带看起来像家用录音带一样，内部有供带轮和收带轮。不同的是，QIC 标准
有 36～72 条磁道，数据并行记录，存储容量为 80MB～1.2GB。最新技术通过增加磁带的
长度和宽度，使磁带的存储容量达到 4GB。 
QIC 磁带驱动器使用 3 个磁头，即一个读磁头两侧各有一个写磁头，如图 7.11 所示。
这种设计使磁带驱动器能在磁带往两个方向上运动时，都可以确认刚写入的数据。在规定
的记录方式下，磁带以 100 英寸/秒的速度移动。 
 

磁带机的数据传输率 D 可用下式表示： 
 
D=d·v 
(7.4) 
其中，d 表示记录密度(单位长度上的存储信息量)，v 表示走带速度。 
2)数码音频磁带(DAT) 
DAT 是数码音频磁带的英文缩写，它采用旋转扫描技术。DAT 的存储容量最大达到
12GB。与 QIC 相比，价格上比较昂贵。 
3)8mm 磁带 
8mm 磁带最初为视频行业设计，现已被计算机行业采用，被认为是存储大量计算机数
据的可靠方式。8mm 磁带与 DAT 磁带在结构上类似，但是最大存储容量可达 25GB。 
4)数码线性磁带(DLT) 
DLT 是数码线性磁带的英文缩写，它是半英寸宽的磁带，比 8mm 磁带宽 60%，比 QIC
磁带宽 2 倍。因此 DLT 磁带提供所有磁带类型的存储容量，最大可以达到 35GB。 

目前的光盘有 CD-ROM、WORM、CD-R、CD-RW、DVD-ROM 等类型。 
1. CD-ROM 光盘 
CD-ROM 是只读型光盘，一张光盘容量为 680MB。光盘是直径为 120mm、厚度为 1.2mm
的单面记录盘片。盘片的膜层结构如图 7.12(a)所示，盘基为聚碳酸酯，反射层多为铝质，
保护层为聚丙烯酸酯。最上层为印刷的盘标。 
 
所有的只读型光盘系统都基于一个共同原理，即光盘上的信息以坑点形式分布，有坑
点表示为“1”，无坑点表示为“0”，一系列的坑点(存储元)形成信息记录道，见图 7.12(b)。
对数据存储用的 CD-ROM 光盘来讲，这种坑点分布作为数字“1”“0”代码的写入或读出
标志。为此必须采用激光作为光源，并采用良好的光学系统才能实现。 
光盘的记录信息以凹坑方式永久性存储。读出时，当激光束聚焦点照射在凹坑上时将
发生衍射，反射率低；而聚焦点照射在凸面上时大部分光将返回。根据反射光的光强变化
并进行光-电转换，即可读出记录信息。 
信息记录的轨迹称为光道。光道上划分出一个个扇区，它是光盘的最小可寻址单位。

光盘扇区分为 4 个区域。2 个全 0 字节和 10 个全 1 字节组成同步(SYNC)区，标志着
扇区的开始。4 字节的扇区标识(ID)区用于说明此扇区的地址和工作模式。光盘的扇区地址
编码不同于磁盘，它是以分(MN)、 秒(SC)和分数秒(FR，1/75s)时间值作为地址。由于光
盘的恒定线速度是每秒钟读出 75 个扇区，故 FR 的值实际上就是秒内的扇区号(0～74)。 
ID 区的 MD 为模式控制，用于控制数据区和校验区的使用。共有三种模式：模式 0 规
定数据区和校验区的全部 2336 字节都是 0，这种扇区不用于记录数据，而是用于光盘的导
入区和导出区；模式 1 规定 288 字节的校验区为 4 字节的检测码(EDC)、8 字节的保留域(未
定义)和 276 字节的纠错码(ECC)，这种扇区模式有 2048 字节的数据并有很强的检测和纠
错能力，适合于保存计算机的程序和数据；模式 2 规定 288 字节的校验区也用于存放数据，
用于保存声音、图像等对误码率要求不高的数据。 

2. WORM、CD-R 光盘 
WORM  表示一次写多次读，它是一种只能写一次的光盘。数据写到光盘后不可擦除
但可多次读。记录信息时，低功率激光束在光盘表面灼烧形成微小的凹陷区。被灼烧的部
分和未被灼烧的部分分别表示 1 和 0。 
CD-R  实质上是 WORM 的一种，区别在于 CD-R 允许多次分段写数据。CD-R 光盘有
与 CD-ROM 的相似的圆形轨道，但不再是机械的在盘面上烧印凹痕来表示数据。CD-R 使
用激光将微型斑点烧在有机燃料表层。读取数据时，在超过标准温度的激光束的照射下，
这些烧过的斑点颜色发生变化，呈现出比未被灼烧的地方较暗的亮度。因此，CD-R 光盘通
过激光烧和不烧斑点表示 1 和 0，而 CD-ROM 则通过凹凸区来表示。CD-R 光盘的数据一
旦写上也不能擦除。 
3. CD-RW 光盘 
CD-RW 表示可重复写光盘，用于反复读写数据。与 CD-R 所使的基于染料的记录表层
不同，CD-RW 光盘采用一种特殊的水晶复合物作为记录介质。当加热到一个确定的温度后，
冷却时它即呈现出水晶状；但如果一开始把它加热到一个更高的温度，它会被熔化，随即

冷却成一种非晶形的固态。写数据时，用激光束将待写区域加热至高温，使之熔化冷却成
非晶形物质。由于非晶形区域比水晶形区域反射的光线强度弱，这样读数据时就可以区分
出是 1 还是 0。这种光盘允许多次写，重写数据时只需将被写过的呈非晶形的区域重新加热，
温度在可结晶温度和熔化温度之间，使之重新转化为水晶态即可。 
4. DVD-ROM 光盘 
最初 DVD 的全称是数字化视频光盘，但后来逐渐演变成数字化通用光盘的简称。
DVD-ROM 的数据也是事先存储在光盘上，这与 CD-ROM 是相同的。不过，凹陷区的大小
相对更小一些，使得圆形光道上存储的数据总量更大。CD-ROM 和 DVD-ROM 的主要区别
是：CD 光盘是单面使用，而 DVD 光盘两面都可以写数据。另外，除了有两面可写的 DVD
光盘，还有多层可写的光盘，在主数据层上还放置着多层透明的可写层，这种光盘的容量
可以达到数十 GB。读写这种多层数据光盘时，激光头每次都需要在层与层之间重新定位。 

顾名思义，磁光盘(MO)存储设备是采用磁场技术和激光技术相结合的产物。磁光盘和磁
盘一样，由磁道和扇区组成。磁光盘是重写型光盘，可以进行随机写入、擦除或重写信息。 
MO 盘和纯磁盘的基本区别是：磁光盘的磁表面需要高温来改变磁极。因此，MO 盘在
常温下是非常稳定的，数据不会改变。 

磁光盘的基本工作原理是：利用热磁效应写入数据：当激光束将磁光介质上的记录点
加热到居里点温度以上时，外加磁场作用改变记录点的磁化方向，而不同的磁化方向可表
示数字“0”和“1”。利用磁光克尔效应读出数据：当激光束照射到记录点时，记录点的磁
化方向不同，会引起反射光的偏振面发生不同结果，从而检测出所记录的数据“1”或“0”。 
图 7.14 示出了磁光盘操作的四种情况。 
图 7.14(a)表示未编码的磁盘，如所有磁化点均存“0”。 
图 7.14(b)表示写操作：高功率激光束照射加热点(记录点)，磁头线圈中外加电流后产
生的磁场使其对应的记录点产生相反的磁性微粒，从而写入“1”。 
图 7.14(c)表示读操作：低功率的激光束反射掉相反极性的磁性粒子且使它的极性变化。
如果这些粒子没有被反射掉，则反射激光束的极性是不变化的。 
图 7.14(d)表示擦除操作：高功率激光束照射记录点，外加磁场改变方向，使磁性粒子
恢复到原始极性。 
总之，MO 盘介质材料发生的物理特性改变是可逆变化，因此信息是可重写的。 

以可见光的形式传递和处理信息的设备称为显示设备，它是目前计算机系统中应用最
广泛的人-机界面设备。 
显示设备种类繁多。按显示设备所用的显示器件分类，有阴极射线管(CRT)显示器、
液晶显示器(LCD)、等离子显示器等。按所显示的信息内容分类，有字符/图形显示器、图
像显示器等。 
在 CRT 显示设备中，以扫描方式不同，分成光栅扫描和随机扫描两种显示器；以分辨
率不同，分成高分辨率显示器和低分辨率显示器；以显示的颜色分类，有单色(黑白)显示
器和彩色显示器。以 CRT 荧光屏对角线的长度分类，有 14 英寸、16 英寸、19 英寸等多种。 
1. 分辨率和灰度级 
分辨率是指显示器所能表示的像素个数。像素越密，分辨率越高，图像越清晰。分辨
率取决于显像管荧光粉的粒度、荧光屏的尺寸和 CRT 电子束的聚焦能力。同时刷新存储器
要有与显示像素数相对应的存储空间，用来存储每个像素的信息。例如，12 英寸彩色 CRT
的分辨率为 640×480 像素。每个像素的间距为 0.31mm，水平方向的 640 像素所占显示长
度为 198.4mm，垂直方向 480 像素是按 4∶3 的长宽比例分配(640×3/4=480)。按这个分辨
率表示的图像具有较好的水平线性和垂直线性，否则看起来会失真变形，同样 16 英寸的
CRT 显示 1024×768 像素也满足 4∶3 的比例。某些专用的方形 CRT 显示分辨率为 1024×
1024 像素，甚至更多。 
灰度级是指黑白显示器中所显示的像素点的亮暗差别，在彩色显示器中则表现为颜色
的不同。灰度级越多，图像层次越清楚逼真。灰度级取决于每个像素对应刷新存储器单元
的位数和 CRT 本身的性能。如果用 4 位表示一像素，则只有 16 级灰度或颜色；如果用 8

位表示一像素，则有 256 级灰度或颜色。字符显示器只用“0”，“1”两级灰度就可表示字
符的有无，故这种只有两级灰度的显示器称为单色显示器。具有多种灰度级的黑白显示器
称为多灰度级黑白显示器。图像显示器的灰度级一般在 256 级以上。 
2. 刷新和刷新存储器 
CRT 发光是由电子束打在荧光粉上引起的。电子束扫过之后其发光亮度只能维持几十
毫秒便消失。为了使人眼能看到稳定的图像显示，必须使电子束不断地重复扫描整个屏幕，
这个过程称为刷新。按人的视觉生理，刷新频率大于 30 次/秒时才不会感到闪烁。 
为了不断提供刷新图像的信号，必须把一帧图像信息存储在刷新存储器，也称视频存
储器。其存储容量 M 由图像分辨率和灰度级决定。 
 
M=r×C 
(7.5) 
分辨率 r 越高，颜色深度 C 越多，刷新存储器容量越大。如分辨率为 1024×1024，256
级颜色深度的图像，存储容量 M=1024×1024×8bit=1MB。刷新存储器的存取周期必须满足
刷新频率的要求。刷存容量和存取周期是刷新存储器的重要技术指标。 
不同的计算机系统，显示器的组成方式也不同。在大型计算机中，显示器作为终端设
备独立存在，即键盘输入和 CRT 显示输出是一个整体，通过标准的串行接口与主机相连。
在微型机系统中，CRT 显示输出和键盘输入是两个独立的设备，显示系统由插在主机槽中
的显示适配器卡和显示器两部分组成，而且将字符显示与图形显示结合为一体。 
1. 字符显示 
显示字符的方法以点阵为基础。点阵是由 m×n 个点组成的阵列，并以此来构造字符。
将点阵存入由 ROM 构成的字符发生器中，在 CRT 进行光栅扫描的过程中，从字符发生器
中依次读出某个字符的点阵，按照点阵中 0 和 1 代码不同控制扫描电子束的开或关，从而
在屏幕上显示出字符，如图 7.15(a)所示。 

点阵的多少取决于显示字符的质量和字符窗口的大小。字符窗口是指每个字符在屏幕
上所占的点数，它包括字符显示点阵和字符间隔。在 IBM/PC 系统中，屏幕上共显示 80 列×
25 行=2000 个字符，故字符窗口数目为 2000。在单色字符方式下，每个字符窗口为 9×

14 点阵，字符为 7×9 点阵。 
对应于每个字符窗口，所需显示字符的 ASCII 代码被存放在视频存储器 VRAM 中，以
备刷新，故 VRAM 应有 2000 个单元存放被显示的字符信息。字符发生器 ROM 的高位地址
来自 VRAM 的 ASCII 代码，低位地址来自光栅地址计数器的输出 RA3～RA0，它具体指向
这个字形点阵中的某字节。在显示过程中，按照 VRAM 中的 ASCII 码和光栅地址计数器访
问 ROM，依次取出字形点阵，就可以完成一个字符的输出，见图 7.15(b)。 
2. 图形显示 
图形显示是指用计算机手段表示现实世界的各种事物，并形象逼真地加以显示。根据
产生图形的方法，分随机扫描图形显示器和光栅扫描图形显示器。 
随机扫描图形显示器  工作原理是将所显示图形的一组坐标点和绘图命令组成显示文
件存放在缓冲存储器，缓存中的显示文件送矢量(线段)产生器，产生相应的模拟电压，直
接控制电子束在屏幕上的移动。为了在屏幕上保留持久稳定的图像，需要按一定的频率对
屏幕反复刷新。这种显示器的优点是分辨率高(可达 4096×4096 像素)，显示的曲线平滑。
目前高质量图形显示器采用这种随机扫描方式。 
光栅扫描图形显示器  产生图形的方法称为相邻像素串接法，即曲线是由相邻像素串
接而成。因此光栅扫描图形显示器的原理是：把对应于屏幕上每个像素的信息都用刷新存
储器存起来，然后按地址顺序逐个地刷新显示在屏幕上。 
刷新存储器中存放一帧图形的形状信息，它的地址和屏幕上的地址一一对应，例如，
屏幕的分辨率为 1024×1024 像素，刷存就要有 1024×1024 单元；屏幕上像素的灰度为 256
级，刷存每个单元的字长就是 8 位。因此刷存的容量直接取决于显示器的分辨率和灰度级。
换言之，此时需要有 1MB 的刷存与之对应。 
光栅扫描图形显示器的优点是通用性强，灰度层次多，色调丰富，显示复杂图形时无
闪烁现象；所产生的图形有阴影效应、隐藏面消除、涂色等功能。它的出现使图形学的研
究从简单的线条图扩展到丰富多彩、形象逼真的各种立体及平面图形，从而成为目前流行
的显示器。 

图像的概念与图形的概念不同。图形是用计算机表示和生成的图，称为主观图像。在
计算机中表示图形，只需存储绘图命令和坐标点，没有必要存储每个像素点。而图像所处
理的对象多半来自客观世界，即由摄像机摄取下来存入计算机的数字图像，这种图像称为
客观图像。由于数字化以后逐点存储，因此图像处理需要占用非常庞大的主存空间。 
图像显示器采用光栅扫描方式，其分辨率在 256 像素×256 像素或 512 像素×512 像素，
与图形显示兼容的图像显示器已达 1024 像素×1024 像素，灰度级在 64～256 级。 
图像显示器有两种类型。一种是图 7.17 所示的简单图像显示器，它仅仅显示由计算机
送来的数字图像。图像处理操作在计算机中完成，显示器不做任何处理。虚线框中的 I/O 接
口、图像存储器(刷新存储器)、A/D 与 D/A 变换等组成单独的一个部分，称为图像输入控
制板或视频数字化仪。图像输入控制板的功能是实现连续的视频信号与离散的数字量之间
的转换。图像输入控制板接收摄像机模拟视频输入信号，经 A/D 变换为数字量存入刷新存
储器用于显示，并可传送到计算机进行图像处理操作。处理后的结果送回刷存，经 D/A 变
换成模拟视频输出，由监视器进行显示输出。监视器只包括扫描、视频放大等与显示有关
的电路及显像管。也可以接入电视机的视频输入端来代替监视器。数字照相机的出现，更
容易组成一个图像处理系统。 
 
另一种是图形处理子系统，其硬件结构较前一种复杂得多。它本身就是一个具有并行
处理功能的专用计算机，不仅能完成显示操作，同时由于子系统内部有容量很大的存储器
和高速处理器。可以快速执行许多图像处理算法，减轻主计算机系统的运算量。这种子系
统可以单独使用，也可以联到通用计算机系统。目前流行的图形工作站就属于图形处理子
系统。 
由于新一代多媒体计算机的发展，图像的处理与显示技术越来越受到人们的重视。 

不同的显示标准所支持的最大分辨率和颜色数目是不同的。随着 IBM PC 系列机的升级
发展，PC 机采用的显示标准经历了很多变化。 
MDA 是 PC 机最早使用的显示标准。MDA 是单色字符显示适配器，采用 9×14 点阵
的字符窗口，满屏显示 80 列×25 行字符，对应分辨率为 720×350 像素。 
VGA 显示标准可兼容字符和图形两种显示方式。字符窗口为 9×16 点阵，图形方式下
分辨率为 640×480 像素，16 种颜色。 
自 IBM 公司推出 VGA 后，VESA(美国视频电子标准协会)定义了一个 VGA 扩展集，
将显示方式标准化，从而成为著名的 Super-VGA 模式。该模式除兼容 VGA 的显示方式外，
还支持 1280×1024 像素光栅，每像素点 24 位颜色深度，刷新频率可达 75MHz。 
当今的显示适配器为支持视窗的 API 应用程序界面，几乎都安装图形加速器硬件，这
样的适配器称为 AVGA。它在显示方式上除遵循 VESA 的 Super-VGA 模式外，并没有提出
新的显示方式。但由于有了图形加速器硬件，并在视窗驱动程序的支持下，系统的图形显
示性能得到显著改善。 
表 7.1 中列出了 VESA 扩充的标准显示模式。早期的 MDA 等显示方式是由 BIOS 的一
组功能调用(INT 10h)来设置和管理的，使用 7 位的方式码。VESA 保留了这种方式，将 VGA
类显示器及适配器所能支持的新的显示方式进行定义，并为新的显示方式指定了 15 位的方
式码。方式码的 b8位为 VESA 标志位，b14～b9为保留位，故 VESA 的显示方式号为 1××h。
表 7.2 中括号内的数字，如 5∶5∶5，指的是三原色 R∶G∶B 每色所占的位数，有的还在
前面有 1，表示 I(加亮)占 1 位。 

图 7.18 是显示适配器的结构框图，它由刷新存储器、显示控制器、ROM BIOS 三部分
组成。 

在 Pentium 系列中显示适配器大多作成插卡形式，插入一个 PCI(或 VESA VL)总线槽。
它一方面与 32 位或 64 位的系统总线相接，另一方面通过一个 15 针 D 形插口与显示器电缆
连接，将水平、垂直同步信号(VSYNC、HSYNC)和红(R)、绿(G)、蓝(B)三色模拟信号送至
显示器。显示适配器的顶部另有一个 VFC 插头，通过一个 24 芯扁平电缆与视频卡相连，
通过传送像素的电平信号，还可以实现视频图像与 PC 图形的合成。 
刷新存储器  存放显示图案的点阵数据。其存储容量取决于设定的显示工作方式。例
如，设定 VESA 显示模式中的方式码为 118h 时，其分辨率为 1024×768 像素，颜色深度为
24 位(3 字节)，则显示一屏画面需要 2304KB 的存储器容量。因此当前的刷存容量一般在 2～
4MB，由高速的 DRAM 组成。刷存通过适配器内部的 32 位或 64 位总线与显示控制器连接。 
ROM BIOS  含有少量的固化软件，用于支持显示控制器建立所要求的显示环境。此
BIOS 软件主要用于 DOS 操作系统。在视窗环境下，它的大部分功能不被使用，而由后者
的设备驱动程序建立操作系统与适配器硬件的衔接。 
显示控制器  是适配器的心脏。它依据设定的显示工作方式，自主地、反复不断地读
取显存中的图像点阵(包括图形、字符文本)数据，将它们转换成 R、G、B 三色信号，并配
以同步信号送至显示器刷新屏幕。显示控制器还要提供一个由系统总线至刷存总线的通路，
以支持 CPU 将主存中已修改好的点阵数据写入到刷存，以更新屏幕。这些修改数据一般利
用扫描回程的消隐时间写入到刷存中，因此显示屏幕不会出现凌乱。 
先进的显示控制器具有图形加速能力，这样的控制器芯片称为 AVGA 芯片。典型的图
形加速功能有：①位和块传送，用于生成和移动一个矩形块(如窗口)数据；②画线，由硬
件在屏上任意两点间画一向量；③填域，以预先指定的颜色或花样填满一个任意多边形；

④颜色扩充，将一个单色的图像放到屏上某一位置后，给它加上指定的前景颜色和背景颜色。 
思考题  显示适配器中为什么一定要具有显示存储器？ 

1. 图形输入设备 
图形输入方法较多，特别是交互式图形系统要求具有人-机对话功能：计算机将结果显
示给人，人根据看到的显示决定下一步操作，并通过输入设备告诉计算机。如此反复多次，
直到显示结果满意。为此必须具有方便灵活的输入手段，才能体现“交互式”的优越性。 
键盘输入  键盘是字符和数字的输入装置，无论字符输入还是图形输入，键盘是一种
最基本的常用设备。当需要输入坐标数据建立显示文件时，要利用键盘。另外，利用键盘
上指定的字符与屏幕上的光标结合，可用来移动光标，拾取图形坐标，指定绘图命令等。 
鼠标器输入  鼠标器是一种手持的坐标定位部件，有两种类型。一种是机械式的，在
底座上装有一个金属球，在光滑的表面上摩擦，使金属球转动，球与四个方向的电位器接
触，就可以测量出上下左右四个方向的相对位移量。另一种是光电式的鼠标器，需要一块
画满小方格的长方形金属板配合使用。当鼠标器在板上移动时，安装在鼠标器底部的光电
转换装置可以定位坐标点。光电式鼠标器比机械式鼠标器可靠性高，但需要附带一块金属
板。另外，用相对坐标定位，必须和 CRT 显示的光标配合，计算机先要给定光标初始位置，
然后用读取的相对位移移动光标。 
2. 图像输入设备 
最理想的图像输入设备是数字摄像机。它可以摄取任何地点、任何环境的自然景物和
物体，直接将数字图像存入磁盘。 
当图像已经记录到某种介质上时，要利用读出装置读出图像。例如，记录在录像带上
的图像要用录像机读出，再将视频信号经图像板量化后输入计算机。记录在数字磁带上的
遥感图像可以直接在磁带机上输入。如果想把纸上的图像输入计算机，一种方法是用摄像

机对着纸上的图像摄像输入，另一种方法是利用装有 CCD(电荷耦合器件)的图文扫描仪或
图文传真机。还有一种叫“扫描仪”的专用设备，可以直接将纸上的图像转换成数字图像。 
由于一帧数字图像要占很大的存储空间，图像数据的传输与存储问题将是一个十分重
要的研究课题，目前普遍采用的方法是压缩-恢复技术。 
3. 语音输入设备 
利用人的自然语音实现人-机对话是新一代多媒体计算机的重要标志之一。图 7.19 示出
了一种语音输入/输出设备的原理方框图。语音识别器作为输入设备，可以将人的语言声音
转换成计算机能够识别的信息，并将这些信息送入计算机。而计算机处理的结果又可以通
过语音合成器变成声音输出，以实现真正的“人机对话”。通常语音识别器与语言合成器放
在一起做成语音输入/输出设备。图 7.19 中声音通过话筒进入语音识别器，然后送入计算机；
计算机输出数据送入语音合成器变为声音，然后由喇叭输出。 
 

打印输出是计算机最基本的输出形式。与显示器输出相比，打印输出可产生永久性记
录，因此打印设备又称为硬拷贝设备。 
1. 打印设备的分类 
打印设备种类繁多，有多种分类方法。按印字原理分，分为击打式和非击打式两大类。
击打式是利用机械作用使印字机构与色带和纸相撞击而打印字符。因此习惯上将属于击打
式打印方式的机种称为“打印机”。击打式设备的成本低，缺点是噪声大，速度慢。非击打
式是采用电、磁、光、喷墨等物理、化学方法印刷字符，因此习惯上将这类非击打式的机
种称为“印字机”，如激光印字机、喷墨印字机等。非击打式的设备速度快，噪声低，印字
质量高，但价格较贵，有的设备还需要专用纸张。目前的发展趋势是机械化的击打式设备
逐步转向电子化的非击打式设备。 
另外，还有能够输出图形/图像的打印机，具有彩色效果的彩色打印机等。 
2. 激光印字机 
激光印字机是激光技术和电子照相技术结合的产物，其基本原理与静电复印机相似。 
激光印字机的结构见图 7.20。激光器输出的激光束经光学透镜系统被聚焦成一个很细
7.19 

小的光点，沿着圆周运动的滚筒进行横向重复扫描。滚筒是记录装置，表面镀有一层具有
光敏特性的感光材料，通常是硒，因此又将滚筒称为硒鼓。硒鼓在未被激光束扫描之前，
首先在黑暗中充电，使鼓表面均匀地沉积一层电荷。此后根据控制电路输出的字符或图形，
变换成数字信号来驱动激光器的打开与关闭。扫描时激光器将对鼓表面有选择地曝光，曝
光部分产生放电现象，未曝光部分仍保留充电时的电荷，从而形成静电潜像。随着鼓的转
动，潜像部分将通过装有碳粉盒的显影器，使得具有字符信息的区域吸附上碳粉，达到显
影的目的。当鼓上的字符信息区和普通纸接触时，由于在纸的背面施以反向的静电电荷，
鼓表面上的碳粉就会被吸附到纸上来，这个过程称为转印。最后，当记录有信息的纸经过
定影辊高温加热，碳粉被溶化，永久性地黏附在纸上，达到定影的效果。 
 
另外，转印后的鼓面还留有残余的碳粉。因此先要除去鼓表面的电荷，然后经清扫刷，
将残余的碳粉全部清除。清除以后的鼓表面又继续重复上述的充电、曝光、显影、转印、
定影等一系列过程。 
激光印字机是非击打式硬拷贝输出设备，输出速度快，印字质量高，可使用普通纸张。
其印字分辨率达到每英寸 300 个点以上，缓冲存储器容量一般在 1MB 以上，对汉字或图形/
图像输出，是理想的输出设备，因而在办公自动化及轻印刷系统中得到了广泛的应用。 
本 章 小 结 
外围设备大体分为输入设备、输出设备、外存设备、数据通信设备、过程控制设备五
大类。每一种设备，都是在它自己的设备控制器控制下进行工作的，而设备控制器则通过
I/O 接口模块和主机相连，并受主机控制。 
磁盘、磁带属于磁表面存储器，特点是存储容量大，位价格低，记录信息永久保存，
但存取速度较慢，因此在计算机系统中作为辅助大容量存储器使用。 
硬磁盘按盘片结构分为可换盘片式、固定盘片式两种，磁头也分为可移动磁头和固定
磁头两种。温彻斯特磁盘是一种采用先进技术研制的可移动磁头、固定盘片的磁盘机，组
装成一个不可拆卸的机电一体化整体，防尘性能好，可靠性高，因而得到了广泛的应用，
成为最有代表性的硬磁盘存储器。磁盘存储器的主要技术指标有存储密度、存储容量、平
均存取时间、数据传输速率。 
磁盘阵列 RAID 是多台磁盘存储器组成的大容量外存系统，它实现数据的并行存储、
交叉存储，单独存储，改善了 I/O 性能，增加了存储容量，是一种先进的硬磁盘体系结构。
各种可移动硬盘的诞生，是磁盘先进技术的又一个重要进展。 
光盘和磁光盘是近年发展起来的一种外存设备，是多媒体计算机不可缺少的设备。按
读写性质分类有：①只读型：记录的信息只能读出，不能被修改。②一次型：用户可在这
种盘上记录信息，但只能写一次，写后的信息不能再改变，只能读。③重写型：用户可对
这类光盘进行随机写入、擦除或重写信息。光盘由于存储容量大、耐用、易保存等优点，
成为计算机大型软件的传播载体和电子出版物的媒体。 
不同的 CRT 显示标准所支持的最大分辨率和颜色数目是不同的。VESA 标准，是一个
可扩展的标准，它除兼容传统的 VGA 等显示方式外，还支持 1280 像素×1024 像素光栅，
每像素点 24 位颜色深度，刷新频率可达 75MHz。显示适配器作为 CRT 与 CPU 的接口，由
刷新存储器、显示控制器、ROM BIOS 三部分组成。先进的显示控制器具有图形加速能力。 
常用的计算机输入设备有图形输入设备(键盘、鼠标)、图像输入设备、语音输入设备。
常用的打印设备有激光打印机、彩色喷墨打印机等，它们都属于硬拷贝输出设备。 

外围设备的种类繁多，有机械式和电动式，也有电子式和其他形式。其输入信号，可
以是数字式的电压，也可以是模拟式的电压和电流。从信息传输速率来讲，相差也很悬殊。
例如，当用手动的键盘输入时，每个字符输入的间隔可达数秒钟。又如，磁盘输入的情况
下，在找到磁道以后，磁盘能以大于 30000B/s 的速率输入数据。 
在计算机系统中，为了保证高速的主机和不同速度的外设之间的高效和可靠的交互，
CPU 必须通过 I/O 接口与外设连接。因此，CPU 的输入/输出操作实际上分为两个传输阶段：
I/O 接口与外设间的数据传送，以及 CPU 与 I/O 接口之间的数据传送如图 8.1 所示。显然，
这两个阶段是相互关联的。 

I/O 接口是由半导体介质构成的逻辑电路，它作为一个转换器，保证外部设备用计算机
系统特性所要求的形式发送或接收信息。为了与 CPU 交互信息的方便，在接口内部一般要
设置一些可以被 CPU 直接访问的寄存器。这些寄存器称为端口(Port)。例如，接口内用于

接收来自 CPU 等主控设备的控制命令的寄存器称为命令端口，简称命令口，接口内向 CPU
报告 I/O 设备的工作状态的寄存器称为状态端口或状态口，接口内在外设和总线间交换数据
的缓冲寄存器称为数据端口或数据口。 
为便于 CPU 访问端口，也需对端口安排地址。通常有两种不同的编址方式。一种是统
一编址方式：输入/输出设备接口中的控制寄存器、数据寄存器、状态寄存器等和内存单元
一样看待，它们和内存单元联合在一起编排地址。这样就可用访问内存的指令(读、写指令)
去访问 I/O 设备接口内的某个寄存器，因而不需要专门的 I/O 指令组。另一种是 I/O 独立编
址方式：内存地址和 I/O 设备地址是分开的，访问内存和访问 I/O 设备使用不同的指令，即
访问 I/O 设备有专门的 I/O 指令组。 
8.1.2  输入/输出操作的一般过程 
由于接口与 CPU 的速度大致相当，仅从 CPU 读写接口内寄存器的角度看，CPU 读写
端口的方式与 CPU 读写内存单元是相似的。但是，内存单元的功能是存储数据，而端口的
功能则是辅助 CPU 与外设交互，故端口中的数据并不是静态的，而是动态变化的。CPU 写
入控制口的信息要由接口内的逻辑电路转换成相关控制信号发送给外设，外设的状态信息
则由接口的逻辑电路转换成状态字存入状态口供 CPU 读取。CPU 写入输出数据口的信息要
由外设取走。外设发送给 CPU 的数据则通过输入数据口缓冲。外设状态信息可能是时刻变
化的，给外设的控制命令也往往会不断改变，CPU 与外设交互数据一般情况下也是成批连
续进行的。因此，对端口的连续访问必须确保信息的有效性。 
首先我们看看输入/输出设备同 CPU 交换数据的一般过程。 
如果是输入过程，一般需要以下三个步骤： 
(1)CPU 把一个地址值放在地址总线上，选择某一输入设备； 
(2)CPU 等候输入设备的数据成为有效； 
(3)CPU 从数据总线读入数据，并放在一个相应的寄存器中。 
如果是输出过程，一般需要以下三个步骤： 
(1)CPU 把一个地址值放在地址总线上，选择一个输出设备； 
(2)CPU 把数据放在数据总线上； 
(3)输出设备认为数据有效，从而把数据取走。 
从上述输入/输出过程看出，问题的关键就在于：究竟什么时候数据才成为有效? 事实
上，各种外围设备的数据传输速率相差甚大。如果把高速工作的处理器同按照不同速度工
作的外围设备相连接，那么首先遇到的一个问题，就是如何保证处理器与外围设备在时间
上同步? 这就是我们要讨论的外围设备的定时问题。很显然，由于输入/输出设备本身的速
度差异很大，因此，对于不同速度的外围设备，需要有不同的定时方式。 
一个计算机系统，即使 CPU 有极高的速度，如果忽略 I/O 速度的提升，对整个系统的
性能仍然影响极大。下面通过一个例子说明 I/O 对系统性能的影响。 

根据外设工作速度的不同，I/O 接口与外设间的数据传送方式有以下三种。 
1. 速度极慢或简单的外围设备：无条件传送方式 
对这类设备，如机械开关、发光二极管等，在任何一次数据交换之前，外设无需进行
准备操作。换句话说，对机械开关来讲，可以认为输入的数据一直有效，因为机械开关的
动作相对主机的速度来讲是非常慢的。对发光二极管来讲，可以认为主机输出时外设一定
准备就绪，因为只要给出数据，发光二极管就能进行显示。所以，对于简单的慢速设备，
接口与外设之间只需要数据信号线，无需握手联络信号线，接口只需实现数据缓冲和寻址
功能，故称为无条件传送方式或零线握手联络方式。 
2. 慢速或中速的外围设备：应答方式(异步传送方式) 
由于这类设备的速度和主机的速度并不在一个数量级，或者由于设备(如键盘)本身是
在不规则时间间隔下操作的，因此，主机与这类设备之间的数据交换通常采用异步定时方
式，接口与外设之间在数据传送信号线之外安排若干条握手(联络、挂钩)信号线，用以在
收发双方之间传递控制信息，指明何时能够交换数据。例如，最常见的双线握手方式设置
两条联络握手信号线：一条发方向收方发出的选通信号或请求信号，指明数据是否有效；
一条收方向发方发出的应答信号，指明数据是否已经被取走。 
3. 高速的外围设备：同步传送方式 
对于中等以上数据传送速率并按规则间隔工作的外部设备，接口以某一确定的时钟速
率和外设交换信息。因此，这种方式称为同步定时方式。一旦接口和外设确认同步，它们
之间的数据交换便靠时钟脉冲控制来进行。例如，若外设是一条传送 2400 位/秒的同步通信
线路，那么接口即每隔 1/2400 秒执行一次串行的输入/输出操作。 
为便于理解，先讲一个例子，假设幼儿园一个阿姨带 10 个孩子，要给每个孩子分 2 块
水果糖。假设孩子们把 2 块糖都吃完，那么她采用什么方法呢? 
第一种方法：她先给孩子甲一块糖，盯着甲吃完，然后再给第二块。接着给孩子乙，
其过程与孩子甲完全一样。以此类推，直至到第 10 个孩子发完 2 块糖。看来这种方法效率
太低，重要之点还在于孩子们吃糖时她一直在守候，什么事也不能干。于是她想了第二种

方法：每人发一块糖各自去吃，并约定谁吃完后就向她举手报告，再发第二块。看来这种
新方法提高了工作效率，而且在未接到孩子们吃完糖的报告以前，她还可以腾出时间给孩
子们批改作业。但是这种方法还可以改进，于是她想了第三种方法，进行批处理：每人拿 2
块糖各自去吃，吃完 2 块糖后再向她报告。显然这种方法工作效率大大提高，她可以腾出
更多的时间批改作业。还有没有更好的方法呢? 我们假定她给孩子们改作业是她的主要任
务，那么她还可以采用第四种方法：权力下放，把发糖的事交给另一个人分管，只是必要
时她才过问一下。 

在计算机系统中，CPU 管理外围设备也有几种类似的方式。 
1. 无条件传送方式(简单 I/O 方式) 
无条件传送方式假设外设始终处于就绪状态，数据传送时，CPU 不必通过接口查询外
设的状态，而直接执行 I/O 指令进行数据传输。显然，只有当接口与外设之间采用无条件传
送方式时，CPU 与接口之间才能采用无条件传送方式。这种方式下，CPU 在端口读、写操
作之前对目标设备的状态不作任何检测。当简单外设作为输入设备时，可使用三态缓冲器
与数据总线相连；当简单外设作为输出设备时，输出一般采用锁存器。 
2. 程序查询(轮询)方式 
多数外设每传送完一次数据总要进行一段时间的处理或准备才能传送下一个数据，因
此在数据传送之前，CPU 需要通过接口对目标设备的状态进行查询：如果外设已准备好传
送数据则进行数据传送；如果外设未准备好传送数据，则 CPU 不断地查询并等待，直到外
设准备好信息交互。其定时过程如下：如果 CPU 希望从外设接收一个字，则它首先通过状
态口询问外设的状态，如果该外设的状态标志表明设备已“准备就绪”，那么 CPU 就从总
线上接收数据。CPU 在接收数据以后，通过接口发出输入响应信号，告诉外设已经把数据
总线上的数据取走。然后，外设把“准备就绪”的状态标志复位，并准备下一个字的交换。如
果外设没有“准备就绪”，那么它就发出“忙”的标志。于是，CPU 将进入一个循环程序中等
待，并在每次循环中询问外设的状态，一直到外设发出“准备就绪”信号以后，才从外设接收
数据。 
CPU 发送数据的情况也与上述情况相似，外设先通过接口发出请求输出信号，而后 CPU
询问外设是否准备就绪。如果外设已准备就绪，CPU 便发出准备就绪信号，并送出数据。
外设接收数据以后，将向 CPU 发出“数据已经取走”的通知。 
程序查询方式是一种简单的输入输出方式，数据在 CPU 和外围设备之间的传送完全靠
计算机程序控制。这种方式的优点是 CPU 的操作和外围设备的操作能够同步，而且软硬件
结构都比较简单。但问题是，外围设备通常动作很慢，程序进入查询循环时将白白消耗掉
CPU 很多时间。这种情况类似于上述例子中第一种方法。即使 CPU 采用定期地由主程序转
向查询设备状态的子程序进行扫描轮询(polling)的办法，CPU 时间的消耗也是可观的。因
此程序查询方式只适用于连接低速外设或者 CPU 任务不繁忙的情况。 
3. 程序中断方式 
中断是外围设备用来“主动”通知 CPU，准备送出输入数据或接收输出数据的一种方
法。通常，当一个中断发生时，CPU 暂停其现行程序，而转向中断处理程序，从而可以输
入或输出一个数据。当中断处理完毕后，CPU 又返回到原来执行的任务，并从其停止的地
方开始执行程序。这种方式和我们前述例子的第二种方法类似。可以看出，它节省了 CPU
宝贵的时间，是管理 I/O 操作的一个比较有效的方法。中断方式一般适用于随机出现的服务
请求，并且一旦提出要求，能使服务请求立即得到响应，因而适合于计算机工作量十分饱
满、而 I/O 处理的实时性要求又很高的系统。同程序查询方式相比，中断方式硬件结构相对
复杂，软件复杂度也提高了，服务开销时间较大。 
4. 直接内存访问(DMA)方式 
用中断方式交换数据，是通过 CPU 执行程序来实现数据传送的。每进行一次传送，CPU
必须执行一遍中断处理程序，完成一系列取指令、分析指令、执行指令的过程。而且，每
进入一次中断处理程序，CPU 都要保护被打断的程序的下一条指令地址(断点)和状态条件
寄存器的当前值；在中断处理程序中，通常还要保护及恢复通用数据寄存器。因此，每处
理一次 I/O 交换，需几十微秒到几百微秒的时间。在指令流水方式中，中断发生或从中断返
回时，指令队列预取的指令会全部作废。因此，在高速、成批传送数据时，中断方式难以
满足速度要求。 
直接内存访问(DMA)方式是一种完全由硬件执行 I/O 交换的工作方式。这种方式既能
够响应随机发生的服务请求，同时又可以省去中断处理的开销。此时，DMA 控制器从 CPU
完全接管对总线的控制，数据交换不经过 CPU，而直接在内存和外围设备之间进行，以高
速传送数据。这种方式和前述例子的第三种方法相仿，主要的优点是数据传送速度很高，
传送速率仅受到内存访问时间的限制。与中断方式相比，需要更多的硬件。DMA 方式适用
于内存和高速外围设备之间大批数据交换的场合。 
5. 通道和输入/输出处理器 
DMA 方式的出现已经减轻了 CPU 执行 I/O 操作的压力，使得 CPU 的效率有显著的提
高，而通道的出现则进一步提高了 CPU 的效率。这是因为，CPU 将部分权力下放给通道。
通道是一个具有特殊功能的简化版处理器，它可以实现对外围设备的统一管理和外围设备
与内存之间的数据传送控制。更进一步，现代的很多高性能计算机系统为输入/输出操作配
置专用的处理器，称为输入输出处理器(IOP)或者外围处理器。这种方式与前述例子的 DMA
方式相仿，大大提高了 CPU 的工作效率。然而这种提高 CPU 效率的方式是以耗费更多硬
件为代价的。 
综上所述，外围设备的输入/输出方式可用图 8.2 表示。 

程序查询方式和程序中断方式适用于数据传输率比较低的外围设备，而 DMA 方式、通
道方式和 IOP 方式适用于数据传输率比较高的设备。 

程序查询方式又称为程序控制 I/O 方式。在这种方式中，数据在 CPU 和外围设备之间
的传送完全靠计算机程序控制，是在 CPU 主动控制下进行的。当需要输入/输出时，CPU
暂停执行主程序，转去执行设备输入/输出的服务程序，根据服务程序中的 I/O 指令进行数
据传送。这是一种最简单、最经济的输入/输出方式，只需要很少的硬件。 
1. 输入/输出指令 
当用程序实现输入/输出传送时，I/O 指令一般具有如下功能： 
(1)置“1”或置“0”I/O 接口的某些控制触发器，用于控制设备进行某些动作，如启
动、关闭设备等。 
(2)测试设备的某些状态，如“忙”“准备就绪”等，以便决定下一步的操作。 
(3)传送数据，当输入数据时，将 I/O 接口中数据寄存器的内容送到 CPU 某一寄存器；
当输出数据时，将 CPU 中某一寄存器的内容送到 I/O 接口的数据寄存器。 
不同的机器，所采用的 I/O 指令格式和操作也不相同。例如，某机的 I/O 指令格式如下： 

其中第 0～1 位 01 表示 I/O 指令；OP 表示操作码，用以指定 I/O 指令的 8 种操作类型；
DMs 表示 64 个外部设备的设备地址，每个设备地址中可含有 A、B、C 三个数据寄存器；8、
9 位表示控制功能，如 01 启动设备(S)、10 关闭设备(C)等；R0～R7 表示 CPU 中的 8 个通
用寄存器。 
上述 I/O 指令如用汇编语言写出，指令“DOAS  2,13”表示把 CPU 中 R2 的内容输出
到 13 号设备的 A 数据缓冲寄存器中，同时启动 13 号设备工作。指令“DICC  3,12”表示
把 12 号设备中 C 寄存器的数据送入 CPU 中通用寄存器 R3，并关闭 12 号设备。 
输入/输出指令不仅用于传送数据和控制设备的启动与关闭，而且也用于测试设备的状
态。如 SKP 指令是测试跳步指令，它是程序查询方式中常用的指令，其功能是测试外部设
备的状态标志(如“就绪”触发器)：若状态标志为“1”，则顺序执行下一条指令；若状态
标志为“0”，则跳过下一条指令。 
2. 程序查询方式的接口 
由于主机和外部设备之间进行数据传送的方式不同，因而接口的逻辑结构也相应有所
不同。程序查询方式的接口是最简单的，如图 8.3 所示。 
程序查询方式的接口电路包括如下部分。 
(1)设备选择电路  接到总线上的每个设备预先都给定了设备地址码。CPU 执行 I/O 指
令时需要把指令中的设备地址送到地址总线上，用以指示 CPU 要选择的设备。每个设备接
口电路都包含一个设备选择电路，用它判别地址总线上呼叫的设备是不是本设备。如果是，
本设备就进入工作状态，否则不予理睬。设备选择电路实际上是设备地址的译码器。 

(2)数据缓冲寄存器  当输入操作时，用数据缓冲寄存器来存放从外部设备读出的数
据，然后送往 CPU；当输出操作时，用数据缓冲寄存器来存放 CPU 送来的数据，以便送给
外部设备输出。 
(3)设备状态标志  是接口中的标志触发器，如“忙”“准备就绪”“错误”等，用来标
志设备的工作状态，以便接口对外设动作进行监视。一旦 CPU 用程序询问外部设备时，将
状态标志信息取至 CPU 进行分析。 
3. 程序查询输入/输出方式 
程序查询方式是利用程序控制实现 CPU 和外部设备之间的数据传送。程序执行的动作
如下： 
(1)先向 I/O 设备发出命令字，请求进行数据传送。 
(2)从 I/O 接口读入状态字。 
(3)检查状态字中的标志，看看数据交换是否可以进行。 
(4)假如这个设备没有准备就绪，则第(2)、第(3)步重复进行，一直到这个设备准备好
交换数据，发出准备就绪信号“Ready”。 
(5)CPU 从 I/O 接口的数据缓冲寄存器输入数据，或者将数据从 CPU 输出至接口的数
据缓冲寄存器。与此同时，CPU 将接口中的状态标志复位。 
图 8.3 中用①～⑥表示了 CPU 从外设输入一个字的过程。 
按上述步骤执行时 CPU 资源浪费严重，故实际应用中做如下改进：CPU 在执行主程序
的过程中可周期性地调用各外部设备询问子程序，而询问子程序依次测试各 I/O 设备的状态
触发器“Ready”。如果某设备的 Ready 为“1”，则转去执行该设备的服务子程序；如该设
备的 Ready 为“0”，则依次测试下一个设备。 
图 8.4 示出了典型的程序查询流程图。图的右边列出了汇编语言所写的查询程序，其中
使用了跳步指令 SKP 和无条件转移指令 JMP。第 1 条指令“SKP DZ 1”的含义是，检查 1
号设备的 Ready 标志是否为“1”? 如果是，接着执行第 2 条指令，即执行 1 号设备的设备
服务子程序 PTRSV；如果 Ready 标志为“0”，则跳过第 2 条指令，转去执行第 3 条指令。
依次类推。最后一条指令返回主程序断点 m。 
设备服务子程序的主要功能是：①实现数据传送。输入时，由 I/O 指令将设备的数据传
送到 CPU 某寄存器，再由访内指令把寄存器中的数据存入内存；输出时，其过程正好相反。

②修改内存地址，为下一次数据传送做准备。③修改传送字节数，以便修改传送长度。
④进行状态分析或其他控制功能。 

某设备的服务子程序执行完以后，接着查询下一个设备。被查询设备的先后次序由查
询程序决定，图 8.4 中以 1、2、3、4 为序。也可以用改变程序的办法来改变询问次序。一
般来说，总是先询问数据传输率高的设备，后询问数据传输率低的设备，因而后询问的设
备要等待更长的时间。 
中断是一种程序随机切换的方式，有时也统称为异常。当外部发生某些随机的事件需
要及时处理时，无论 CPU 正在执行哪一条指令，都可以通过中断响应的方式暂停正在执行
的主程序的执行，转而执行另外一段中断服务程序。在高优先级的中断服务程序执行完毕
后，可以返回被打断的主程序“断点”继续执行。 
中断方式的典型应用包括： 
(1)实现 CPU 与外界进行信息交换的握手联络。一方面，中断可以实现 CPU 与外设的
并行工作；另一方面，对于慢速 I/O 设备，使用中断方式可以有效提高 CPU 的效率。 
(2)故障处理。中断可以用于处理常见的硬件故障，如掉电、校验错、运算出错等；也
可以处理常见的软件故障，如溢出、地址越界、非法指令等。 

(3)实时处理。中断可以保证在事件出现的实际时间内及时地进行处理。 
(4)程序调度。中断是操作系统进行多任务调度的手段。 
(5)软中断(程序自愿中断)。软中断不是随机发生的，而是与子程序调用功能相似，但
其调用接口简单，不依赖于程序入口地址，便于软件的升级维护和调用。 
中断概念的出现，是计算机系统结构设计中的一个重大变革。8.1 节中曾经提到，在程
序中断方式中，某一外设的数据准备就绪后，它“主动”向 CPU 发出请求中断的信号，请
求 CPU 暂时中断目前正在执行的程序而进行数据交换。当 CPU 响应这个中断请求时，便
暂停运行主程序，并自动转移到该设备的中断服务程序。当中断服务程序结束以后，CPU
又回到原来的主程序。这种原理和调用子程序相仿，不过，这里要求转移到中断服务程序
的请求是由外部设备发出的。中断方式特别适合于随机出现的服务。 
图 8.5 示出了中断处理示意图。主程序只是在设备 A、B、C 数据准备就绪时，才去与
设备 A、B、C 进行数据交换。在速度较慢的外围设备准备自己的数据时，CPU 照常执行自
己的主程序。在这个意义上说，CPU 和外围设备的一些操作是并行地进行的，因而同串行
进行的程序查询方式相比，计算机系统的效率大大提高了。 

实际的中断过程还要复杂一些，图 8.6 示出了一个典型的向量中断处理过程的详细流程
图。当 CPU 执行完一条现行指令时，如果外设向 CPU 发出中断请求，那么 CPU 在满足响
应条件的情况下，将发出中断响应信号，与此同时关闭中断(“中断屏蔽”触发器置“1”)，
表示 CPU 不再受理另外一个设备的中断请求。这时，CPU 将寻找中断请求源是哪一个设备，
并保存 CPU 自己的程序计数器(PC)的内容。然后，它将转移到处理该中断源的中断服务程
序。CPU 在保存现场信息，设备服务(如交换数据)以后，将恢复现场信息。在这些动作完
成以后，开放中断(“中断屏蔽”触发器清“0”)，并返回到原来被中断的主程序的下一条
指令。 
以上是中断处理的大致过程，但是有一些问题需要进一步加以说明。 
第一个问题，尽管外界中断请求是随机的，但 CPU 只有在当前一条指令执行完毕后，
即转入公操作时才受理设备的中断请求，这样才不至于使当前指令的执行受到干扰。所谓
公操作，是指一条指令执行结束后 CPU 所进行的操作，如中断处理、取下条指令等。外界
中断请求信号通常存放在接口中的中断源锁存器里，并通过中断请求线连至 CPU，每当一
条指令执行到末尾，CPU 便检查中断请求信号。若中断请求信号为“1”且允许响应该中断
请求，则 CPU 转入“中断周期”，受理外界中断。 
第二个问题，为了在中断服务程序执行完毕以后，能够正确地返回到原来主程序被中
断的断点而继续执行主程序，必须把程序计数器 PC 的内容，以及当前指令执行结束后 CPU
的状态(包括寄存器的内容和一些状态标志位)都保存到堆栈中。这些操作称为保存现场。 

第三个问题，当 CPU 响应中断后，正要去执行中断服务程序时，可能有另一个新的中
断源向它发出中断请求。为了不致造成混乱，在 CPU 的中断管理部件中必须有一个“中断
屏蔽”触发器，它可以在程序的控制下置“1”(关中断)，或清“0”(开中断)。只有在“中
断屏蔽”标志为“0”时，CPU 才可以受理中断。当一条指令执行完毕 CPU 接受中断请求
并作出响应时，它一方面发出中断响应信号 INTA，另一方面把“中断屏蔽”标志置“1”，
即关闭中断。这样，CPU 不能再受理另外的新的中断源发来的中断请求。只有在 CPU 把中
断服务程序执行完毕以后，它才重新使“中断屏蔽”标志置“0”，即开放中断，并返回主
程序。因此，中断服务程序的最后必须有两条指令，即开中断指令和中断返回指令，同时
在硬件上要保证中断返回指令执行以后才受理新的中断请求。 
第四个问题，中断处理过程是由硬件和软件结合来完成的。如在图 8.6 中，“中断周期”
由硬件实现，而中断服务子程序由机器指令序列实现。后者除执行保存现场、恢复现场、
开放中断并返回主程序任务外，需对请求中断的设备进行服务，使其同 CPU 交换一个字的
数据，或作其他服务。至于在中断周期中如何转移到各个设备的中断服务程序，将在稍后
介绍。在中断周期中由硬件实现的响应中断、关中断等操作由于在主程序和中断服务程序
的代码中都看不到，因而被称为“中断处理的隐操作”。 
第五个问题，中断分为内中断和外中断。机器内部原因导致出错引起的中断叫内中断，
也叫异常。外部设备请求服务的中断叫外中断。  

现代计算机系统中，中断是频繁发生的，这些引起中断的事件被称为中断源。CPU 在
中断响应的过程中必须首先确认应该为哪个中断源服务。当有多个中断源同时提出中断申
请时，还需对中断源进行优先级判别和排队，以确定应该首先响应哪个中断源的服务请求。
然后，CPU 需要获取应被服务的中断源的中断服务程序入口地址，并转到相应的中断服务
程序执行。获取中断服务程序入口地址一般有两种方式：向量中断方式和查询中断方式，
选择哪种方式通常在处理器的中断机构设计时就已经确定。 
向量中断  向量中断是指 CPU 响应中断后，由中断机构
自动将相应中断源的中断向量地址送入 CPU，由其指明中断
服务程序入口地址并实现程序切换的中断方式。在向量中断
方式中，每个中断源都对应一个中断服务程序，而中断服务
程序的入口地址被称为中断向量。在有的系统中，中断向量
还包括中断服务程序开始执行时的程序状态字 PSW 的初始
值。一般而言，系统中所有的中断向量都按顺序存放在内存
指定位置的一张中断向量表中，当 CPU 识别出某中断源时，
由硬件直接产生一个与该中断源对应的中断向量地址，以便
能快速在中断向量表中找到并转入中断服务程序入口。 
图 8.7 给出了一个中断向量表实例。图中，A1、A2 到 An
为 n 个中断向量的向量地址；PC1、PC2 到 PCn 为各个中断
服务程序的入口地址，在中断响应时由硬件自动加载到程序
计数器 PC 中；PSW1、PSW2 到 PSWn 为各个中断服务程序
开始执行时的初始程序状态字，在中断响应时由硬件自动加
载到程序状态字寄存器 PSWR 中。 
在有些计算机中，由硬件产生的向量地址不是直接地址，而是一个“位移量”，这个位
移量加上 CPU 某寄存器里存放的基地址，最后得到中断服务程序的入口地址。 
还有的计算机在中断向量表中存放的不是中断服务程序入口地址，而是一条转移到中
断服务程序入口地址的转移指令的指令字。在中断切换过程中，由硬件直接执行这条转移
指令，从而跳转到相应的中断服务程序执行。 
查询中断  在查询中断方式中，硬件不直接提供中断服务程序的入口地址，而是为
所有中断服务程序安排一个公共的中断服务程序。在中断响应时，由公共的中断服务程
序软件查询中断源，并跳转至相应中断服务子程序入口执行。图 8.8 给出了查询中断程
序实例。 
在向量中断方式中，查找中断源、中断排队与判优、获取中断服务程序入口地址都是
由硬件在中断周期中自动完成的。但在查询中断方式中，查找中断源和获取中断服务程序
入口地址都是由软件实现的，而中断优先级则与软件查询中断源的顺序相关，因此可以更
灵活地调整中断优先级。 
程序中断方式的基本接口示意图如图 8.9 所示。接口电路中有一个工作标志触发器
(BS)，就绪标志触发器(RD)，还有一个控制触发器，称为允许中断触发器(EI)。 
 

程序中断由外设接口的状态和 CPU 两方面来控制。在接口方面，有决定是否向 CPU
发出中断请求的机构，主要是接口中的“准备就绪”标志(RD)和“允许中断”标志(EI)两
个触发器。在 CPU 方面，有决定是否受理中断请求的机构，主要是“中断请求”标志(IR)
和“中断屏蔽”标志(IM)两个触发器。上述四个标志触发器的具体功能如下。 
准备就绪触发器(RD)  一旦设备做好一次数据的接收或发送，便发出一个设备动作完
毕信号，使 RD 标志置“1”。在中断方式中，该标志用作中断源触发器，简称中断触发器。 
允许中断触发器(EI)  可以用程序指令来置位。EI 为“1”时，某设备可以向 CPU 发
出中断请求；EI 为“0”时，不能向 CPU 发出中断请求，这意味着某中断源的中断请求被
禁止。设置 EI 标志的目的，就是通过软件来控制是否允许某设备发出中断请求。 
中断请求触发器(IR)  它暂存中断请求线上由设备发出的中断请求信号。当 IR 标志为
“1”时，表示设备发出了中断请求。 
中断屏蔽触发器(IM)  是 CPU 是否受理中断或批准中断的标志。IM 标志为“0”时，
CPU 可以受理外界的中断请求，反之，IM 标志为“1”时，CPU 不受理外界的中断请求。 
图 8.9 中，标号①～⑧表示由某一外设输入数据的控制过程。①表示由程序启动外设，
将该外设接口的“忙”标志 BS 置“1”，“准备就绪”标志 RD 清“0”；②表示接口向外设
发出启动信号；③表示数据由外设传送到接口的缓冲寄存器；④表示当设备动作结束或缓
冲寄存器数据填满时，设备向接口送出一控制信号，将数据“准备就绪”标志 RD 置“1”；
⑤表示允许中断标志 EI 为“1”时，接口向 CPU 发出中断请求信号；⑥表示在一条指令执
行末尾 CPU 检查中断请求线，将中断请求线的请求信号接收到“中断请求”标志 IR；⑦表
示如果“中断屏蔽”标志 IM 为“0”时，CPU 在一条指令执行结束后受理外设的中断请求，
向外设发出响应中断信号并关闭中断；⑧表示转向该设备的中断服务程序入口；⑨表示在
中断服务程序通过输入指令把接口中数据缓冲寄存器的数据读至 CPU 中的寄存器；⑩表示
CPU 发出控制信号 C 将接口中的 BS 和 RD 标志复位。 
1. 单级中断的概念 
根据计算机系统对中断处理的策略不同，可分为单级中断系统和多级中断系统。单级
中断系统是中断结构中最基本的形式。在单级中断系统中，所有的中断源都属于同一级，
所有中断源触发器排成一行，其优先次序是离 CPU 近的优先权高。当响应某一中断请求时，
执行该中断源的中断服务程序。在此过程中，不允许其他中断源再打断中断服务程序，即
使优先权比它高的中断源也不能再打断。只有该中断服务程序执行完毕之后，才能响应其
他中断。图 8.10 示出了单级中断示意图(a)和单级中断系统结构图(b)。图 8.10(b)中所有的
I/O 设备通过一条线向 CPU 发出中断请求信号。CPU 响应中断请求后，发出中断响应信号
INTA，以链式查询方式识别中断源。这种中断结构与第 6 章讲的链式总线仲裁相对应，中
断请求信号 IR 相当于总线请求信号 BR。 
2. 单级中断源的识别 
如何确定中断源，并转入被响应的中断服务程序入口地址，是中断处理首先要解决的
问题。 
在单级中断中，采用串行排队链法来实现具有公共请求线的中断源判优识别。其逻辑
电路见图 8.11。 

图中下面的虚线部分是一个串行的优先链，称作中断优先级排队链。IRi 是从各中断源
设备来的中断请求信号，优先顺序从高到低是 IR1、IR2、IR3。而 IS1、IS2、IS3 是与 IR1、IR2、
IR3 相对应的中断排队选中信号，若 ISi=1，即表示该中断源被选中。INTI 为中断排队输入，
INTO 中断排队输出。若没有更高优先级的中断请求时，INTI =0，门 1 输出高电平，即 IS1=1，
若此时中断请求 IR1=1(有中断请求)，当 CPU 发来中断识别信号 INTA=1 时，发出 IR1 请求
的中断源被选中，选中信号经门 7 送入编码电路，产生一个唯一对应的设备地址，并经数
据总线送往 CPU 的主存地址寄存器，然后执行该中断源设备的中断服务程序。 
另一方面，由于此时
1
IR 为 0，封锁门 2，使 IS2、IS3 全为低电平，即排队识别工作不
再向下进行。 
若 IR1 无请求，则 IR1=0，门 7 被封锁，不会向编码电路送入选中信号。与此同时，因
1
IR =1，经门 2 和门 3，使 IS2=1，如果 IR2=1，则被选中。否则查询链继续向下查询，直至

找到发出中断请求信号 IRi 的中断源设备。 
3. 中断向量的产生 
当 CPU 识别出某中断源时，由硬件直接产生一个与该中断源对应的向量地址，很快便
引入中断服务程序。向量中断要求在硬件设计时考虑所有中断源的向量地址，而实际中断
时只能产生一个向量地址。图 8.11 中上面部分即为中断向量产生逻辑，它是由编码电路实
现的。 

1. 多级中断的概念 
多级中断系统是指计算机系统中有相当多的中断源，根据各中断事件的轻重缓急程度
不同而分成若干级别，每一中断级分配给一个优先权。一般说来，优先权高的中断级可以
打断优先权低的中断服务程序，以程序嵌套方式进行工作。如图 8.12(a)所示，三级中断优
先权高于二级，而二级中断优先权又高于一级。 
 

根据系统的配置不同，多级中断又可分为一维多级中断和二维多级中断，如图 8.12(b)
所示。一维多级中断是指每一级中断中只有一个中断源，而二维多级中断是指每一级中断
中有多个中断源。图中虚线左边结构为一维多级中断，如果去掉虚线则成为二维多级中断
结构。 
对多级中断，着重说明如下几点。 
(1)一个系统若有 n 级中断，在 CPU 中就有 n 个中断请求触发器，总称为中断请求寄
存器；与之对应的有 n 个中断屏蔽触发器，总称为中断屏蔽寄存器。与单级中断不同，在
多级中断中，中断屏蔽寄存器的内容是一个很重要的程序现场，因此在响应中断时，需要
把中断屏蔽寄存器的内容保存起来，并设置新的中断屏蔽状态。一般在某一级中断被响应
后，要置“1”(关闭)本级和优先权低于本级的中断屏蔽触发器，清“0”(开放)更高级的
中断屏蔽触发器，以此来实现正常的中断嵌套。 
(2)多级中断中的每一级可以只有一个中断源，也可以有多个中断源。在多级中断之间
可以实现中断嵌套，但是同一级内有不同中断源的中断是不能嵌套的，必须是处理完一个
中断后再响应和处理同一级内其他中断源。 
(3)设置多级中断的系统一般都希望有较快的中断响应时间，因此首先响应哪一级中断
和哪一个中断源，由硬件逻辑实现，而不是用程序实现。图 8.12 中的中断优先级排队电路，
就是用于决定优先响应中断级的硬件逻辑。另外，在二维中断结构中，除了有中断优先级
排队电路确定优先响应中断级外，还要确定优先响应的中断源，一般通过链式查询的硬件
逻辑来实现。显然，这里采用了独立请求方式与链式查询方式相结合的方法决定首先响应
哪个中断源。 
(4)和单级中断情况类似，在多级中断中也使用中断堆栈保存现场信息。使用堆栈保存
现场的好处是：①控制逻辑简单，保存和恢复现场的过程按先进后出顺序进行。②每一级
中断不必单独设置现场保护区，各级中断现场可按其顺序放在同一个栈里。 
2. 多级中断源的识别 
在多级中断中，每一级均有一根中断请求线送往 CPU 的中断优先级排队电路，对每一
级赋予了不同的优先级。显然这种结构就是独立请求方式的逻辑结构。 
图 8.13 示出了独立请求方式的中断优先级排队与中断向量产生的逻辑结构。每个中断
请求信号保存在“中断请求”触发器中，经“中断屏蔽”触发器控制后，可能有若干个中
断请求信号 IR′i 进入虚线框所示的排队电路。排队电路在若干中断源中决定首先响应哪个中
断源，并在其对应的输出线 IRi 上给出“1”信号，而其他各线为“0”信号(IR1～IR4 中只
有一个信号有效)。之后，编码电路根据排上队的中断源输出信号 IRi，产生一个预定的地
址码，转向中断服务程序入口地址。 
例如，假设图 8.13 中请求源 1 的优先级最高，请求源 4 的优先级最低。又假定中断请
求寄存器的内容为 1111，中断屏蔽寄存器的内容为 0010，那么进入排队器的中断请求是
1101。根据优先次序，排队器输出为 1000。然后由编码器产生中断源 1 所对应的向量地址。 
在多级中断中，如果每一级请求线上还连接有多个中断源设备，那么在识别中断源时，
还需要进一步用串行链式方式查询。这意味着要用二维方式来设计中断排队逻辑。 
【
1. 中断类型 
Pentium 有两类中断源，即中断和异常。 
中断  通常称为外部中断，它是由 CPU 的外部硬件信号引发的。有两种情况：①可屏
蔽中断：CPU 的 INTR 引脚收到中断请求信号，如果 CPU 中标志寄存器 IF=1 时，可引发
中断；IF=0 时，中断请求信号在 CPU 内部被禁止。②非屏蔽中断：CPU 的 NMI 引脚收到
的中断请求信号而引发的中断，这类中断不能被禁止。 
异常  通常称为异常中断，它是由指令执行引发的。有两种情况：①执行异常：CPU
执行一条指令过程中出现错误、故障等不正常条件引发的中断。②执行软件中断指令：如
执行 INT 0，INT 3，INT n 等指令，执行时产生异常中断。 
如果详细分类，Pentium 共有 256 种中断和异常。每种中断给予一个编号，称为中断向
量号(0～255)，以便发生中断时，程序转向相应的中断服务子程序入口地址。 
当有一个以上的异常或中断发生时，CPU 以一个预先确定的优先顺序为它们先后进行
服务。中断优先级分为 5 级。异常中断的优先级高于外部中断的优先级，这是因为异常中
断发生在取一条指令或译码一条指令或执行一条指令时出现故障的情况下，情况更为紧急。 
2. 中断服务子程序进入过程 
中断服务子程序的入口地址信息存于中断向量号检索表内。实模式为中断向量表 IVT，
保护模式为中断描述符表 IDT。 
CPU 识别中断类型取得中断向量号的途径有三种：①指令给出，如软件中断指令 INT n
中的 n 即为中断向量号。②外部提供，可屏蔽中断是在 CPU 接收到 INTR 信号时产生一个
中断识别周期，接收外部中断控制器由数据总线送来的中断向量号；非屏蔽中断是在接收
到 NMI 信号时中断向量号固定为 2。③CPU 识别错误、故障现象，根据异常和中断产生的
条件自动指定向量号。 
CPU 依据中断向量号获取中断服务子程序入口地址，但在实模式下和保护模式下采用
不同的途径。 
实模式下使用中断向量表  中断向量表 IVT 位于内存地址 0 开始的 1KB 空间。实模式
是 16 位寻址，中断服务子程序入口地址(段，偏移)的段寄存器和段内偏移量各为 16 位。
它们直接登记在 IVT 表中，每个中断向量号对应一个中断服务子程序入口地址。每个入口
地址占 4 字节。256 个中断向量号共占 1KB。CPU 取得向量号后自动乘以 4，作为访问 IVT
的偏移，读取 IVT 相应表项，将段地址和偏移量设置到 CS 和 IP 寄存器，从而进入相应的
中断控 
制器 
8259 中断 
控制器 

中断服务子程序。进入过程如图 8.14(a)所示。 
保护模式下使用中断描述符表  保护模式为 32 位寻址。中断描述符表 IDT 每一表项对
应一个中断向量号，表项称为中断门描述符、陷阱门描述符。这些门描述符为 8 字节长，
对应 256 个中断向量号，IDT 表长为 2KB。由中断描述符表寄存器 IDTR 来指示 IDT 的内
存地址。 
以中断向量号乘以 8 作为访问 IDT 的偏移，读取相应的中断门/陷阱门描述符表项。门
描述符给出中断服务子程序入口地址(段，偏移)，其中 32 位偏移量装入 EIP 寄存器，16
位的段值装入 CS 寄存器。由于此段值是选择符，还必须访问 GDT 或 LDT，才得到段的基
地址。保护模式下进入中断服务子程序的过程如图 8.14(b)所示。 
 

3. 中断处理过程 
上面说明了中断向量号的获取方式，也说明了实模式与保护模式下进入中断服务子程
序的途径。现将 Pentium 机的中断处理过程叙述如下： 
(1)当中断处理的 CPU 控制权转移涉及特权级改变时，必须把当前的 SS 和 ESP 两个寄
存器的内容压入系统堆栈予以保存。 
(2)标志寄存器 EFLAGS 的内容也压入堆栈。 
(3)清除标志触发器 TF 和 IF。 
(4)当前的代码段寄存器 CS 和指令指针 EIP 也压入此堆栈。 
(5)如果中断发生伴随有错误码，则错误码也压入此堆栈。 

(6)完成上述中断现场保护后，从中断向量号获取的中断服务子程序入口地址(段，偏
移)分别装入 CS 和 EIP，开始执行中断服务子程序。 
(7)中断服务子程序最后的 IRET 指令使中断返回。保存在堆栈中的中断现场信息被恢
复，并由中断点继续执行原程序。 
 

直接内存访问(DMA)，是一种完全由硬件执行 I/O 交换的工作方式。在这种方式中，
DMA 控制器从 CPU 完全接管对总线的控制，数据交换不经过 CPU，而直接在内存和 I/O
设备之间进行。DMA 方式一般用于高速传送成组数据。DMA 控制器将向内存发出地址和
控制信号，修改地址，对传送的字的个数计数，并且以中断方式向 CPU 报告传送操作的
结束。 
DMA 方式的主要优点是速度快。由于 CPU 根本不参加传送操作，因此就省去了 CPU
取指令、取数、送数等操作。在数据传送过程中，没有保存现场、恢复现场之类的工作。
内存地址修改、传送字个数的计数等，也不是由软件实现，而是用硬件线路直接实现的。
所以 DMA 方式能满足高速 I/O 设备的要求，也有利于 CPU 效率的发挥。正因为如此，包
括微型机在内，DMA 方式在计算机中被广泛采用。 
目前由于大规模集成电路工艺的发展，很多厂家直接生产大规模集成电路的 DMA 控制
器。虽然 DMA 控制器复杂程度差不多接近于 CPU，但使用起来非常方便。 
DMA 方式的特点如下。 
DMA 方式以响应随机请求的方式，实现主存与 I/O 设备间的快速数据传送。DMA 方
式并不影响 CPU 的程序执行状态，只要不存在访存冲突，CPU 就可以继续执行自己的程序。
但是 DMA 只能处理简单的数据传送，不能在传送数据的同时进行判断和计算。 
与查询方式相比，在 DMA 方式中 CPU 不必等待查询，可以执行自身的程序，而且直
接由硬件(DMA 控制器)控制传输过程，CPU 不必执行指令。与中断方式相比，DMA 方式
仅需占用系统总线，不切换程序，因而 CPU 可与 DMA 传送并行工作；DMA 可以实现简单
的数据传送，难以识别和处理复杂事态。 
由于 DMA 传送开始的时间是随机的，但开始传送后需要进行连续批量的数据交换，因
此 DMA 方式非常适合主存与高速 I/O 设备间的简单数据传送。例如，以数据块为单位的磁
盘读/写操作；以数据帧为单位的外部通信；以及大批量数据采集等场景。 
DMA 的种类很多，但多种 DMA 至少能执行以下一些基本操作。 
(1)从外围设备发出 DMA 请求。 
(2)CPU 响应请求，把 CPU 工作改成 DMA 操作方式，DMA 控制器从 CPU 接管总线
的控制。 
(3)由 DMA 控制器对内存寻址，即决定数据传送的内存单元地址及数据传送个数的计
数，并执行数据传送的操作。 
第 8 章  输入/输出系统       261 
(4)向 CPU 报告 DMA 操作的结束。 
注意，在 DMA 方式中，一批数据传送前的准备工作，以及传送结束后的处理工作，均
由管理程序承担，而 DMA 控制器仅负责数据传送的工作。 

DMA 技术的出现，使得外围设备可以通过 DMA 控制器直接访问内存，与此同时，CPU
可以继续执行程序。那么 DMA 控制器与 CPU 怎样分时使用内存呢? 根据每提出一次 DMA
请求，DMA 控制器将占用多少个总线周期，可以将 DMA 传送分成以下几种方式：① 成
组连续传送方式(停止 CPU 访存)；② 周期挪用方式(单字传送方式，周期窃取方式)；③ 透
明 DMA 方式(DMA 与 CPU 交替操作方式，总线周期分时方式)。 
1. 成组连续传送方式 
当外围设备要求传送一批数据时，由 DMA 控制器发一个停止信号给 CPU，要求 CPU
放弃对地址总线、数据总线和有关控制总线的使用权。DMA 控制器获得总线控制权以后，
开始进行数据传送。在一批数据传送完毕后，DMA 控制器通知 CPU 可以使用内存，并把
总线控制权交还给 CPU。图 8.15(a)是这种传送方式的时间图。很显然，在这种 DMA 传送
过程中，CPU 基本处于不工作状态或者说保持状态。 

这种传送方法的优点是控制简单，它适用于数据传输率很高的设备进行成组传送。缺

点是在 DMA 控制器访内阶段，内存的效能没有充分发挥，相当一部分内存工作周期是空闲
的。这是因为，外围设备传送两个数据之间的间隔一般总是大于内存存储周期，即使高速
I/O 设备也是如此。例如，软盘读出一个 8 位二进制数大约需要 32μs，而半导体内存的存储
周期小于 0.2μs，因此许多空闲的存储周期不能被 CPU 利用。 
2. 周期挪用方式 
在这种 DMA 传送方法中，当 I/O 设备没有 DMA 请求时，CPU 按程序要求访问内存；
一旦 I/O 设备有 DMA 请求，则由 I/O 设备挪用一个或几个内存周期。 
I/O 设备要求 DMA 传送时可能遇到两种情况：一种是此时 CPU 不需要访内，如 CPU
正在执行乘法指令。由于乘法指令执行时间较长，此时 I/O 访内与 CPU 访内没有冲突，即
I/O 设备挪用一两个内存周期对 CPU 执行程序没有任何影响。另一种是 I/O 设备要求访内时
CPU 也要求访内，这就产生了访内冲突，在这种情况下 I/O 设备访内优先，因为 I/O 访内有
时间要求，前一个 I/O 数据必须在下一个访内请求到来之前存取完毕。显然，在这种情况下
I/O 设备挪用一两个内存周期，意味着 CPU 延缓了对指令的执行，或者更明确地说，在 CPU
执行访内指令的过程中插入 DMA 请求，挪用了一两个内存周期。图 8.15(b)是周期挪用的
DMA 方式示意图。 
与停止 CPU 访内的 DMA 方法比较，周期挪用的方法既实现了 I/O 传送，又较好地发
挥了内存和 CPU 的效率，是一种广泛采用的方法。但是 I/O 设备每一次周期挪用都有申请
总线控制权、建立总线控制权和归还总线控制权的过程，所以传送一个字对内存来说要占
用一个周期，但对 DMA 控制器来说一般要 2～5 个内存周期(视逻辑线路的延迟而定)。因
此，周期挪用的方法适用于 I/O 设备读写周期大于内存存储周期的情况。 
3. 透明 DMA 方式 
如果 CPU 的工作周期比内存存取周期长很多，则采用交替访内的方法可以使 DMA 传
送和 CPU 同时发挥最高的效率，其原理示意图如图 8.15(c)所示。假设 CPU 工作周期为
1.2μs，内存存取周期小于 0.6μs，那么一个 CPU 周期可分为 C1 和 C2 两个分周期，其中 C1
专供 DMA 控制器访内，C2 专供 CPU 访内。 
这种方式不需要总线使用权的申请、建立和归还过程，总线使用权是通过 C1 和 C2 分时
控制的。CPU 和 DMA 控制器各自有自己的访内地址寄存器、数据寄存器和读/写信号等控
制寄存器。在 C1 周期中，如果 DMA 控制器有访内请求，可将地址、数据等信号送到总线
上。在 C2 周期中，如 CPU 有访内请求，同样传送地址、数据等信号。事实上，对于总线，
这是用 C1 和 C2 控制的一个多路转换器，这种总线控制权的转移几乎不需要什么时间，所以
对 DMA 传送来讲效率是很高的。 
这种传送方式称为“透明的 DMA”方式，其来由是这种 DMA 传送对 CPU 来说，如同
透明的玻璃一般，没有任何感觉或影响。在透明的 DMA 方式下工作，CPU 既不停止主程
序的运行，也不进入等待状态，是一种高效率的工作方式。当然，相应的硬件逻辑也就更
加复杂。 

1. DMA 控制器的基本组成 
一个 DMA 控制器，实际上是采用 DMA 方式的外围设备与系统总线之间的接口电路。
第 8 章  输入/输出系统       263 
这个接口电路是在中断接口的基础上再加 DMA 机构组成的。 
图 8.16 示出了一个最简单的 DMA 控制器组成示意图，它由以下逻辑部件组成。 

内存地址计数器  用于存放内存中要交换的数据的地址。在 DMA 传送前，须通过程序
将数据在内存中的起始位置(首地址)送到内存地址计数器。而当 DMA 传送时，每交换一次
数据，将地址计数器加“1”，从而以增量方式给出内存中要交换的一批数据的地址。 
字计数器  用于记录传送数据块的长度(多少字数)。其内容也是在数据传送之前由程
序预置，交换的字数通常以补码形式表示。在 DMA 传送时，每传送一个字，字计数器就加
“1”，当计数器溢出即最高位产生进位时，表示这批数据传送完毕，于是引起 DMA 控制器
向 CPU 发中断信号。 
数据缓冲寄存器  用于暂存每次传送的数据(一个字)。当输入时，由设备(如磁盘)送
往数据缓冲寄存器，再由缓冲寄存器通过数据总线送到内存。反之，输出时，由内存通过
数据总线送到数据缓冲寄存器，然后再送到设备。 
DMA 请求标志  每当设备准备好一个数据字后给出一个控制信号，使“DMA 请求”
标志置“1”。该标志置位后向“控制/状态”逻辑发出 DMA 请求，后者又向 CPU 发出总线
使用权的请求(HOLD)，CPU 响应此请求后发回响应信号 HLDA，“控制/状态”逻辑接收此
信号后发出 DMA 响应信号，使“DMA 请求”标志复位，为交换下一个字做好准备。 
控制/状态逻辑  由控制和时序电路以及状态标志等组成，用于修改内存地址计数器和
字计数器，指定传送类型(输入或输出)，并对“DMA 请求”信号和 CPU 响应信号进行协
调和同步。 
中断机构  当字计数器溢出时(全 0)，意味着一组数据交换完毕，由溢出信号触发中断
机构，向 CPU 提出中断报告。这里的中断与 8.3 节介绍的 I/O 中断所采用的技术相同，但
中断的目的不同，前面是为了数据的输入或输出，而这里是为了报告一组数据传送结束。
因此它们是 I/O 系统中不同的中断事件。 

2. DMA 数据传送过程 
DMA 的数据块传送过程可分为三个阶段：传送前预处理；正式传送；传送后处理。 
预处理阶段由 CPU 执行几条输入输出指令，测试设备状态，向 DMA 控制器的设备地
址寄存器中送入设备号并启动设备，向内存地址计数器中送入起始地址，向字计数器中送
入交换的数据字个数。在这些工作完成后，CPU 继续执行原来的主程序。 
当外设准备好发送数据(输入)或接受数据(输出)时，它发出 DMA 请求，由 DMA 控制
器向 CPU 发出总线使用权的请求(HOLD)。图 8.17 示出了成组连续传送方式的 DMA 传送
数据的流程图。当外围设备发出 DMA 请求时，CPU 在指令周期执行结束后响应该请求，
并使 CPU 的总线驱动器处于第三态(高阻状态)。之后，CPU 与系统总线相脱离，而 DMA
控制器接管数据总线与地址总线的控制，并向内存提供地址，于是，在内存和外围设备之
间进行数据交换。每交换一个字，则地址计数器和字计数器加“1”，当计数值到达零时，
DMA 操作结束，DMA 控制器向 CPU 提出中断报告。 
DMA 的数据传送是以数据块为基本单位进行的，因此，每次 DMA 控制器占用总线后，
无论是数据输入操作，还是输出操作，都是通过循环来实现的。当进行输入操作时，外围
设备的数据(一次一个字或一字节)传向内存；当进行输出操作时，内存的数据传向外围设备。 
DMA 的后处理进行的工作是，一旦 DMA 的中断请求得到响应，CPU 停止主程序的执
行，转去执行中断服务程序做一些 DMA 的结束处理工作。这些工作包括校验送入内存的数
据是否正确；决定继续用 DMA 方式传送下去，还是结束传送；测试在传送过程中是否发生
了错误等。 
基本 DMA 控制器与系统的连接可采用两种方式：一种是公用的 DMA 请求方式，另一
种是独立的 DMA 请求方式，这与中断方式类似。 
思考题  说出 DMA 方式的创新点，其意义何在？ 

前面介绍的是最简单的 DMA 控制器，一个控制器只控制一个 I/O 设备。实际中经常采
用的是选择型 DMA 控制器和多路型 DMA 控制器，它们已经被做成集成电路片子。 
1. 选择型 DMA 控制器 
图 8.18 是选择型 DMA 控制器的逻辑框图，它在物理上可以连接多个设备，而在逻辑
上只允许连接一个设备。换句话说，在某一段时间内只能为一个设备服务。 

选择型 DMA 控制器工作原理与前面的简单 DMA 控制器基本相同。除了前面讲到的基
本逻辑部件外，还有一个设备号寄存器。数据传送是以数据块为单位进行的，在每个数据
块传送之前的预置阶段，除了用程序中 I/O 指令给出数据块的传送个数、起始地址、操作命
令外，还要给出所选择的设备号。从预置开始，一直到这个数据块传送结束，DMA 控制器
只为所选设备服务。下一次预置再根据 I/O 指令指出的设备号，为另一选择的设备服务。显
然，选择型 DMA 控制器相当于一个逻辑开关，根据 I/O 指令来控制此开关与某个设备连接。 
选择型 DMA 控制器只增加少量硬件达到了为多个外围设备服务的目的，它特别适合数
据传输率很高以至于接近内存存取速度的设备。在很快地传送完一个数据块后，控制器又
可为其他设备服务。 
2. 多路型 DMA 控制器 
选择型 DMA 控制器不适用于慢速设备。但是多路型 DMA 控制器却适合于同时为多个
慢速外围设备服务。图 8.19 表示独立请求方式的多路型 DMA 控制器的原理图。 
 
多路型 DMA 不仅在物理上可以连接多个外围设备，而且在逻辑上也允许这些外围设备
同时工作，各设备以字节交叉方式通过 DMA 控制器进行数据传送。 

由于多路型 DMA 同时要为多个设备服务，因此对应多少个 DMA 通路(设备)，在控制
器内部就有多少组寄存器用于存放各自的传送参数。 
图 8.20 是一个多路型 DMA 控制器的芯片内部逻辑结构，通过配合使用 I/O 通用接口
片子，它可以对 8 个独立的 DMA 通路(CH)进行控制，使外围设备以周期挪用方式对内存
进行存取。 

8 条独立的 DMA 请求线或响应线能在外围设备与 DMA 控制器之间进行双向通信。一
条线上进行双向通信是通过分时和脉冲编码技术实现的。也可以分别设立 DMA 请求线和响
应线实现双向通信。每条 DMA 线在优先权结构中具有固定位置，一般 DMA0 线具有最高
优先权，DMA7 线具有最低优先权。 
控制器中有 8 个 8 位的控制传送长度的寄存器，8 个 16 位的地址寄存器。每个长度寄
存器和地址寄存器对应一个设备。每个寄存器都可以用程序中的 I/O 指令从 CPU 送入控制
数据。每一寄存器组各有一个计数器，用于修改内存地址和传送长度。 
当某个外围设备请求 DMA 服务时，操作过程如下： 

(1)DMA 控制器接到设备发出的 DMA 请求时，将请求转送到 CPU。 
(2)CPU 在适当的时刻响应 DMA 请求。若 CPU 不需要占用总线则继续执行指令；若
CPU 需要占用总线，则 CPU 进入等待状态。 
(3)DMA 控制器接到 CPU 的响应信号后，进行以下工作：①对现有 DMA 请求中优先
权最高的请求给予 DMA 响应；②选择相应的地址寄存器的内容驱动地址总线；③根据所选
设备操作寄存器的内容，向总线发读、写信号；④外围设备向数据总线传送数据，或从数
据总线接收数据；⑤每字节传送完毕后，DMA 控制器使相应的地址寄存器和长度寄存器加
“1”或减“1”。 
以上是一个 DMA 请求的过程，在一批数据传送过程中，要多次重复上述过程，直到外
围设备表示一个数据块已传送完毕，或该设备的长度控制器判定传送长度已满。 

1)通道的功能 
DMA 控制器的出现已经减轻了 CPU 对数据输入输出的控制，使得 CPU 的效率有显著
的提高。而通道的出现则进一步提高了 CPU 的效率。这是因为通道是一个特殊功能的处理
器，它有自己的指令和程序专门负责数据输入输出的传输控制，而 CPU 将“传输控制”的
功能下放给通道后只负责“数据处理”功能。这样，通道与 CPU 分时使用存储器，实现了
CPU 内部运算与 I/O 设备的并行工作。 
图 8.22 是典型的具有通道的计算
机系统结构图。它具有两种类型的总
线，一种是系统总线，它承担通道与
存储器、CPU 与存储器之间的数据传
输任务。另一种是通道总线，即 I/O
总线，它承担外围设备与通道之间的
数据传送任务。这两类总线可以分别
按照各自的时序同时进行工作。 
由图 8.20 看出，通道总线可以接
若干个 I/O 模块，一个 I/O 模块可以接
一个或多个设备。因此，从逻辑结构
上讲，I/O 系统一般具有四级连接：
CPU 与存储器  通道  I/O 模块 
外围设备。为了便于通道对各设备的
统一管理，通道与 I/O 模块之间用统
一的标准接口，I/O 模块与设备之间则
根据设备要求不同而采用专用接口。 
具有通道的机器一般是大型计算机和服务器，数据流量很大。如果所有的外设都接在
一个通道上，那么通道将成为限制系统效能的瓶颈。因此大型计算机的 I/O 系统一般接有多
个通道。显然，设立多个通道的另一好处是，对不同类型的外设可以进行分类管理。 
存储管理部件是存储器的控制部件，它的主要任务是根据事先确定的优先次序，决定
下一周期由哪个部件使用系统总线访问存储器。由于大多数 I/O 设备是旋转性的设备，读写
信号具有实时性，不及时处理会丢失数据，所以通道与 CPU 同时要求访存储器时，通道优
先权高于 CPU。在多个通道有访存请求时，选择通道的优先权高于多路通道，因为前者一
般连接高速设备。 
通道的基本功能是执行通道指令，组织外围设备和内存进行数据传输，按 I/O 指令要求
启动外围设备，向 CPU 报告中断等，具体有以下五项任务。 
(1)接受 CPU 的 I/O 指令，按指令要求与指定的外围设备进行通信。 
(2)从存储器选取属于该通道程序的通道指令，经译码后向 I/O 控制器模块发送各种
命令。 
(3)组织外设和存储器之间进行数据传送，并根据需要提供数据缓存的空间，以及提供

数据存入存储器的地址和传送的数据量。 
(4)从外围设备得到设备的状态信息，形成并保存通道本身的状态信息，根据要求将这
些状态信息送到存储器的指定单元，供 CPU 使用。 
(5)将外设的中断请求和通道本身的中断请求，按次序及时报告 CPU。 
2)CPU 对通道的管理 
CPU 是通过执行 I/O 指令以及处理来自通道的中断，实现对通道的管理。来自通道的
中断有两种，一种是数据传送结束中断，另一种是故障中断。 
通常把 CPU 运行操作系统的管理程序的状态称为管态，而把 CPU 执行目的程序时的
状态称为目态。大型计算机的 I/O 指令都是管态指令，只有当 CPU 处于管态时，才能运行
I/O 指令，目态时不能运行 I/O 指令。这是因为大型计算机的软、硬件资源为多个用户所共
享，而不是分给某个用户专用。 
3)通道对设备控制器的管理 
通道通过使用通道指令来控制 I/O 模块进行数据传送操作，并以通道状态字接收 I/O 模
块反映的外围设备的状态。因此，I/O 模块是通道对 I/O 设备实现传输控制的执行机构。I/O
模块的具体任务如下： 
(1)从通道接受通道指令，控制外围设备完成所要求的操作。 
(2)向通道反映外围设备的状态。 
(3)将各种外围设备的不同信号转换成通道能够识别的标准信号。 
思考题  通道的设计理念，在技术上有什么创新？ 

根据通道的工作方式，通道分为选择通道、多路通道两种类型。 
一个系统可以兼有两种类型的通道，也可以只有其中一种。 
1)选择通道 
选择通道又称高速通道，在物理上它可以连接多个设备，但是这些设备不能同时工作，
在某一段时间内通道只能选择一个设备进行工作。选择通道很像一个单道程序的处理器，
在一段时间内只允许执行一个设备的通道程序，只有当这个设备的通道程序全部执行完毕
后，才能执行其他设备的通道程序。 
选择通道主要用于连接高速外围设备，如磁盘、磁带等，信息以数据块方式高速传输。
由于数据传输率很高，所以在数据传送期间只为一台设备服务是合理的。但是这类设备的
辅助操作时间很长，如磁盘机平均找道时间是 10ms，磁带机走带时间可以长达几分钟。在
这样长的时间里通道处于等待状态，因此整个通道的利用率不是很高。 
2)多路通道 
多路通道又称多路转换通道，在同一时间能处理多个 I/O 设备的数据传输。它又分为数
组多路通道和字节多路通道。 
数组多路通道是对选择通道的一种改进，它的基本思想是当某设备进行数据传送时，
通道只为该设备服务；当设备在执行寻址等控制性动作时，通道暂时断开与这个设备的连
接，挂起该设备的通道程序，去为其他设备服务，即执行其他设备的通道程序。所以数组
多路通道很像一个多道程序的处理器。 

数组多路通道不仅在物理上可以连接多个设备，而且在一段时间内能交替执行多个设
备的通道程序，换句话说在逻辑上可以连接多个设备，这些设备应是高速设备。 
由于数组多路通道既保留了选择通道高速传送数据的优点，又充分利用了控制性操作
的时间间隔为其他设备服务，使通道效率得到充分发挥，因此数组多路通道在大型系统中
得到较多应用。 
字节多路通道主要用于连接大量的低速设备，如键盘、打印机等，这些设备的数据传
输率很低。例如，数据传输率是 1000B/s，即传送 1 字节的时间是 1ms，而通道从设备接收
或发送 1 字节只需要几百纳秒，因此通道在传送 2 字节之间有很多空闲时间，字节多路通
道正是利用这个空闲时间为其他设备服务。 
字节多路通道和数组多路通道有共同之处，即它们都是多路通道，在一段时间内能交
替执行多个设备的通道程序，使这些设备同时工作。 
字节多路通道和数组多路通道也有不同之处，主要是：①数组多路通道允许多个设备
同时工作，但只允许一个设备进行传输型操作，其他设备进行控制型操作。而字节多路通
道不仅允许多个设备同时操作，而且也允许它们同时进行传输型操作。②数组多路通道与
设备之间数据传送的基本单位是数据块，通道必须为一个设备传送完一个数据块以后，才
能为别的设备传送数据块。而字节多路通道与设备之间数据传送的基本单位是字节，通道
为一个设备传送完 1 字节后，又可以为另一个设备传送 1 字节，因此各设备与通道之间的
数据传送是以字节为单位交替进行。 

通道结构的进一步发展，出现了两种计算机 I/O 系统结构。 
一种是通道结构的 I/O 处理器，通常称为输入输出处理器(IOP)。IOP 可以和 CPU 并行
工作，提供高速的 DMA 处理能力，实现数据的高速传送。但是它不是独立于 CPU 工作的，
而是主机的一个部件。有些 IOP 如 Intel 8089 IOP，还提供数据的变换、搜索以及字装配/
拆卸能力。这种 IOP 可应用于服务器及微型计算机中。 
另一种是外围处理机(PPU)。PPU 基本上是独立于主机工作的，它有自己的指令系统，
完成算术/逻辑运算，读/写主存储器，与外设交换信息等。有的外围处理机干脆就选用已有
的通用机。外围处理机 I/O 方式一般应用于大型高效率的计算机系统中。 
思考题  你对通道技术的未来发展有什么见解？ 

SCSI 是小型计算机系统接口的简称，其设计思想来源于 IBM 大型机系统的 I/O 通道结
构，目的是使 CPU 摆脱对各种设备的繁杂控制。它是一个高速智能接口，可以混接各种磁
盘、光盘、磁带机、打印机、扫描仪、条码阅读器以及通信设备。它首先应用于 Macintosh
和 Sun 平台上，后来发展到工作站、网络服务器和 Pentium 系统中，并成为 ANSI(美国国
家标准局)标准。SCSI 有如下性能特点。 

(1)SCSI 接口总线由 8 条数据线、一条奇偶校验线、9 条控制线组成。使用 50 芯电缆，
规定了两种电气条件：单端驱动，电缆长 6m；差分驱动，电缆最长 25m。 
(2)总线时钟频率为 5MHz，异步方式数据传输率是 2.5MB/s，同步方式数据传输率是
5MB/s。 
(3)SCSI 接口总线以菊花链形式最多可连接 8 台设备。在 Pentium 中通常是：由一个主
适配器 HBA 与最多 7 台外围设备相接，HBA 也算作一个 SCSI 设备，由 HBA 经系统总线(如
PCI)与 CPU 相连，如图 8.23 所示。 

(4)每个 SCSI 设备有自己的唯一设备号 ID0～7。ID=7 的设备具有最高优先权，ID=0
的设备优先权最低。SCSI 采用分布式总线仲裁策略。在仲裁阶段，竞争的设备以自己的设
备号驱动数据线中相应的位线(如 ID=7 的设备驱动 DB7 线)，并与数据线上的值进行比较。
因此仲裁逻辑比较简单，而且在 SCSI 的总线选择阶段，启动设备和目标设备的设备号能同
时出现在数据线上。 
(5)所谓 SCSI 设备是指连接在 SCSI 总线上的智能设备，即除主适配器 HBA 外，其他
SCSI 设备实际是外围设备的适配器或控制器。每个适配器或控制器通过各自的设备级 I/O
线可连接一台或几台同类型的外围设备(如一个 SCSI 磁盘控制器接 2 台硬盘驱动器)。标准
允许每个 SCSI 设备最多有 8 个逻辑单元，每个逻辑单元可以是物理设备也可以是虚拟设备。
每个逻辑单元有一个逻辑单元号(LUN0～LUN7)。 
(6)由于 SCSI 设备是智能设备，对 SCSI 总线以至主机屏蔽了实际外设的固有物理属性
(如磁盘柱面数、磁头数等参数)，各 SCSI 设备之间就可用一套标准的命令进行数据传送，
也为设备的升级或系统的系列化提供了灵活的处理手段。 
(7)SCSI 设备之间是一种对等关系，而不是主从关系。SCSI 设备分为启动设备(发命令
的设备)和目标设备(接受并响应命令的设备)。但启动设备和目标设备是依当时总线运行状
态来划分的，而不是预先规定的。 
总之，SCSI 是系统级接口，是处于主适配器和智能设备控制器之间的并行 I/O 接口。
一块主适配器可以接 7 台具有 SCSI 接口的设备，这些设备可以是类型完全不同的设备，主
适配器却只占主机的一个槽口。这对于缓解计算机挂接外设的数量和类型越来越多、主机
槽口日益紧张的状况很有吸引力。 
为提高数据传输率和改善接口的兼容性，20 世纪 90 年代又陆续推出了 SCSI-2 和 SCSI-3
标准。SCSI-2 扩充了 SCSI 的命令集，通过提高时钟速率和数据线宽度，最高数据传输率可
达 40MB/s，采用 68 芯电缆，且对电缆采用有源终端器。SCSI-3 标准允许 SCSI 总线上连接
的设备由 8 个提高到 16 个，可支持 16 位数据传输。另一个变化是发展串行 SCSI，使串行

数据传输率达到 640Mb/s(电缆)或 1Gb/s(光纤)，从而使串行 SCSI 成为 IEEE1394 标准的
基础。 

1. 1394 性能特点 
随着 CPU 速度达到上百兆赫，存储器容量达到 GB 级，以及 PC、工作站、服务器对快
速 I/O 的强烈需求，工业界期望能有一种更高速、连接更方便的 I/O 接口。1993 年 Apple
公司公布了一种高速串行接口，希望能取代并行的 SCSI 接口。IEEE 接管了这项工作，在
此基础上制定了 IEEE 1394-FireWire 标准，它是一个通用的串行 I/O 接口。 
IEEE 1394 串行接口与 SCSI 等并行接口相比，有如下三个显著特点。 
(1)数据传送的高速性。 
1394 的数据传输率分为 100Mb/s、200Mb/s、400Mb/s 三档。而 SCSI-2 也只有 40MB/s(相
当于 320Mb/s)。这样的高速特性特别适合于新型高速硬盘及多媒体数据传送。 
1394 之所以达到高速，一是因为串行传送比并行传送容易提高数据传送时钟速率；二
是因为采用了 DS-Link 编码技术，把时钟信号的变化转变为选通信号的变化，即使在高的
时钟速率下也不易引起信号失真。 
(2)数据传送的实时性。 
实时性可保证图像和声音不会出现时断时续的现象，因此对多媒体数据传送特别重要。 
1394 之所以做到实时性，原因有二：一是它除了异步传送外，还提供了一种等步传送
方式，数据以一系列的固定长度的包规整间隔地连续发送，端到端既有最大延时限制而又
有最小延时限制；二是总线仲裁除优先权仲裁之外，还有均等仲裁和紧急仲裁方式。 
(3)体积小易安装，连接方便。 
1394 使用 6 芯电缆，直径约为 6mm，插座也小。而 SCSI 使用 50 芯或 68 芯电缆，插
座体积也大。在当前 PC 机要连接的设备越来越多，主机箱的体积越显窄小的情况下，电缆
细、插座小的 1394 是很有吸引力的，尤其对笔记本电脑一类机器。 
1394 的电缆不需要与电缆阻抗匹配的终端，而且电缆上的设备随时可从插座拔出或插
入，即具有热插入能力。这对用户安装和使用 1394 设备很有利。 
2. 1394 配置 
1394 采用菊花链式配置，但也允许树形结构配置。事实上，菊花链结构是树形结构的
一种特殊情况。 
1394 接口也需要一个主适配器和系统总线相连。这个主适配器的功能逻辑在高档的
Pentium 机中是集成在主板的核心芯片组的 PCI 总线到 ISA 总线的桥芯片中。机箱的背面只
看到主适配器的外接端口插座。 
在这里将主适配器及其端口称为主端口。主端口是 1394 接口树形配置结构的根节点。
一个主端口最多可连接 63 台设备，这些设备称为节点，它们构成亲子关系。两个相邻节点
之间的电缆最长为 4.5m，但两个节点之间进行通信时中间最多可经过 15 个节点的转接再驱
动，因此通信的最大距离是 72m。电缆不需要终端器。图 8.24 给出一个 IEEE 1394 配置的
实例，其中右侧是线性链接方式，左侧是亲子层次链接方式。整体是一个树形结构。 

1394 采用集中式总线仲裁方式。中央仲裁逻辑在主端口内，并以先到先服务方法来处
理节点提出的总线访问请求。在 n 个节点同时提出使用总线请求时，按照优先权进行仲裁。
最靠近根节点的竞争节点有高的优先权；同样靠近根节点的竞争节点，其设备标识号 ID 大
的有更高优先权。1394 具有 PnP(即插即用)功能，设备标识号是系统自动指定的，而不是
用户设定的。 
为了保证总线设备的对等性和数据传送的实时性，1394 的总线仲裁还增加了均等仲裁
和紧急仲裁功能。均等仲裁是将总线时间分成均等的间隔，当间隔期间开始时，竞争的每
个节点置位自己的仲裁允许标志，在间隔期内各节点可竞争总线的使用权。一旦某节点获
得总线访问权，则它的仲裁允许标志被复位，在此期间它不能再去竞争总线，以此来防止
具有高优先权的忙设备独占总线。紧急仲裁是指对某些高优先权的节点可为其指派紧急优
先权。具有紧急优先权的节点可在一个间隔期内多次获得总线控制权，允许它控制 75%的
总线可用时间。 
3. 1394 协议集 
1394 的一个重要特色是，它规范了一个三层协议集，将串行总线与各外围设备的交互
动作标准化。图 8.25 表示 IEEE 1394 的协议集。 
业务层  定义了一个完整的请求-响应协议实现总线传输，包括读操作、写操作和锁定
操作。 
链路层  可为应用程序直接提供等步数据传送服务。它支持异步和等步的包发送和接
收。异步包传送是，一个可变总量的数据及业务层的几个信息字节作为一个包传送到显式
地址的目标方，并要求返回一个认可包。等步包传送是，一个可变总量的数据以一串固定
大小的包按照规整间隔来发送，使用简化寻址方式，不要求目标方认可。1394 把完成一个
包的递交过程称为子动作。 
物理层  将链路层的逻辑信号根据不同的串行总线介质转换成相应的电信号，也为串
行总线的接口定义了电气和机械特性。实际上，1394 串行接口的物理拓扑结构分成“底板
环境”和“电气环境”两部分。总线规范并未要求特别的环境设定。所有节点可严格限定

在单一底板上，也可直接连在电缆上。 

串行总线管理  它提供总线节点所需的标准控制、状态寄存器服务和基本控制功能。 
总之，IEEE 1394 是一种高速串行 I/O 标准接口。英特尔、微软等公司联手将 1394 列
为 1998 年以后的新一代 PC 机新标准。另一个重大特点是，各被连接装置的关系是平等的，
不用 PC 机介入也能自成系统。例如，利用数字相机直接进行印刷的打印机便可利用这一特
点。这意味着 1394 在家电等消费类设备的连接应用方面有很好的前景。 
I/O 系统设计要考虑两种主要规范：时延约束和带宽约束。在这两种情况下，对通信模
式的认知将影响整个系统的分析和设计。 
时延约束  时延约束确保完成一次 I/O 操作的延迟时间被限制在某个数量范围内。一种
简单的情况是认为系统是无负载的，设计者必须保证满足某些时延约束，这是因为这种限
制对应用程序非常重要，或者设备为了防止某种错误必须接受某些有保证的服务。同样，
在一个无负载系统中计算延迟时间相对比较容易，因为只用跟踪 I/O 操作的路径并累加单个
延迟时间即可。 
在有负载的情况下，得到平均时延是一个复杂的问题。这些问题可以通过排队理论(当
工作量请求的行为和 I/O 服务次数能够通过简单的分布来近似时)或模拟(当 I/O 事件的行为
很复杂时)的方法解决。 
带宽约束  给定一个工作负载，设计一个满足一组带宽约束的 I/O 系统是设计者需要面
对的另一个典型问题。或者，给定一个部分配置好的 I/O 系统，要求设计者平衡系统，以维
持该系统预配置部分规定的可能达到的最大带宽。 
设计这样一个系统的一般方法如下。 
(1)找出 I/O 系统中效率最低的连接，它是 I/O 路径中约束设计的部件。依赖于不同的
工作负载，该部件可以存在于任何地方，包括 CPU、内存系统、底板总线、I/O 控制器或 I/O
设计。工作负载和配置限制会决定这个效率最低的部件到底在哪儿。 

(2)配置这个部件以保持所需的带宽。 
(3)研究系统中其他部分的需求，配置它们以支持这个带宽。 


各种外围设备的数据传输速率相差很大。如何保证主机与外围设备在时间上同步，则
涉及外围设备的定时问题。一个计算机系统的性能，不仅取决于 CPU，还取决于 I/O 速度。 

在计算机系统中，CPU 对外围设备的管理方式有：①程序查询方式；②程序中断方式；
③DMA 方式；④通道方式。每种方式都需要硬件和软件结合起来进行。 
程序查询方式是 CPU 管理 I/O 设备的最简单方式，CPU 定期执行设备服务程序，主动
来了解设备的工作状态。这种方式浪费 CPU 的宝贵资源。 
程序中断方式是各类计算机中广泛使用的一种数据交换方式。当某一外设的数据准备
就绪后，它“主动”向 CPU 发出请求信号。CPU 响应中断请求后，暂停运行主程序，自动
转移到该设备的中断服务子程序，为该设备进行服务，结束时返回主程序。中断处理过程
可以嵌套进行，优先级高的设备可以中断优先级低的中断服务程序。 
DMA 技术的出现，使得外围设备可以通过 DMA 控制器直接访问内存，与此同时，CPU
可以继续程序。DMA 方式采用以下三种方法：①停止 CPU 访内；②周期挪用；③DMA 与
CPU 交替访内。DMA 控制器按其组成结构，分为选择型和多路型两类。 
通道是一个特殊功能的处理器。它有自己的指令和程序专门负责数据输入输出的传输
控制，从而使 CPU 将“传输控制”的功能下放给通道，CPU 只负责“数据处理”功能。这
样，通道与 CPU 分时使用内存，实现了 CPU 内部的数据处理与 I/O 设备的平行工作。通道
有两种类型：①选择通道；②多路通道。 
标准化是建立开放式系统的基础。CPU、系统总线、I/O 总线及标准接口技术近年来取
得了重大进步。其中并行 I/O 接口 SCSI 与串行 I/O 接口 IEEE 1394 是两个最具权威性和发
展前景的标准接口技术。 
SCSI 是系统级接口，是处于主适配器和智能设备控制器之间的并行 I/O 接口，改进的
SCSI 可允许连接 1～15 台不同类型的高速外围设备。SCSI 的不足处在于硬件较昂贵，并需
要通用设备驱动程序和各类设备的驱动程序模块的支持。 
IEEE 1394 是串行 I/O 标准接口。与 SCSI 并行 I/O 接口相比，它具有更高的数据传输
速率和数据传送的实时性，具有更小的体积和连接的方便性。IEEE 1394 的一个重大特点是，
各被连接的设备的关系是平等的，不用 PC 介入也能自成系统。因此 IEEE 1394 已成为 Intel、
Microsoft 等公司联手制定的新标准。 

计算机系统中的并行性有不同的等级。 
所谓并行性，是指计算机系统具有可以同时进行运算或操作的特性，它包括同时性与
并发性两种含义。 
同时性  两个或两个以上的事件在同一时刻发生。 
并发性  两个或两个以上的事件在同一时间间隔内发生。 
(1)从处理数据的角度看，并行性等级从低到高可分为： 
· 字串位串：同时只对一个字的一位进行处理。这是最基本的串行处理方式，不存在
并行性。 
· 字串位并：同时对一个字的全部位进行处理，不同字之间是串行的。这里已开始出
现并行性。 
· 字并位串：同时对许多字的同一位进行处理。这种方式有较高的并行性。 
· 全并行：同时对许多字的全部位进行处理。这是最高一级的并行。 
(2)从执行程序的角度看，并行性等级从低到高可分为： 
· 指令内部并行：一条指令执行时各微操作之间的并行。 
· 指令级并行：并行执行两条或多条指令。 
· 任务级或过程级并行：并行执行两个以上过程或任务(程序段)。 
· 作业或程序级并行：并行执行两个以上作业或程序。 
在计算机系统中，可以采取多种并行性措施。既可以有处理数据方面的并行性，又可
以有执行程序方面的并行性。当并行性提高到一定级别时，则进入并行处理领域。 
并行处理着重挖掘计算过程中的并行事件，使并行性达到较高的级别。因此，并行处
理是体系结构、硬件、软件、算法、编程语言等多方面综合的领域。 

计算机系统中提高并行性的措施多种多样，就其基本思想而言，可归纳成如下四条
途径。 
(1)时间重叠。 
时间重叠即时间并行。在并行性概念中引入时间因素，让多个处理过程在时间上相互
错开，轮流重叠地使用同一套硬件设备的各个部分，以加快硬件周转而赢得速度。时间重
叠的实质就是把一件工作按功能分割为若干个相互联系的部分，每一部分指定专门的部件
完成，各部分执行过程在时间上重叠起来，使所有部件依次分工合作完成完整的工作。时
间重叠的典型应用就是流水线技术。 
(2)资源重复。 
资源重复即空间并行。在并行性概念中引入空间因素，以数量取胜的原则，通过重复
设置硬件资源，大幅度提高计算机系统的性能。随着硬件价格的降低，资源重复在单处理
机中通过部件冗余、多存储体等方式被广泛应用，而多处理机本身就是实施“资源重复”
原理的结果。 
(3)时间重叠+资源重复。 
在计算机系统中同时运用时间并行和空间并行技术，这种方式在计算机系统中得到广
泛应用，成为并行性主流技术。 
(4)资源共享。 
资源共享是一种软件方法的并行，它使多个任务按一定时间顺序轮流使用同一套硬件
设备。多道程序、分时系统就是资源共享的具体应用。资源共享既降低了成本，又提高了
计算机硬件的利用率。 

早期单处理机的发展过程中，起着主导作用的是时间并行(流水线)技术。实现时间并
行的物质基础是“部件功能专用化”，即把一件工作按功能分割为若干相互联系的部分，把
每一部分指定给专门的部件完成；然后按时间重叠原理把各部分执行过程在时间上重叠起
来，使所有部件依次分工完成一组同样的工作。例如，指令执行的 5 个子过程分别需要 5 个
专用部件，即取指令部件(IF)、指令译码部件(ID)、指令执行部件(EX)、访问存储器部件
(M)、结果写回部件(WB)。将它们按流水方式连接起来，就满足时间重叠原理，从而使得
处理机内部能同时处理多条指令，提高了处理机的速度。显然，时间并行技术开发了计算
机系统中的指令级并行。 
在单处理机中，空间并行技术的运用也已经十分普遍。例如，不论是非流水线处理机，
还是流水线处理机，多体存储器和多操作部件都是成功应用的结构形式。在多操作部件处
理机中，通用部件被分解成若干个专用操作部件，如加法部件、乘法部件、除法部件、逻
辑运算部件等。一条指令所需的操作部件只要空闲，就可以开始执行这条指令，这就是指
令级并行。 
在单处理机中，资源共享的概念实质上是用单处理机模拟多处理机的功能，形成所谓
虚拟机的概念。例如，分时系统，在多终端情况下，每个终端上的用户感到好像自己独占

一台处理机一样。 
单处理机并行性发展的代表作有奔腾系列机和安腾系列机。 

多处理机系统也遵循时间重叠、资源重复、资源共享原理，向着不同体系结构的多处
理机方向发展。但在采取的技术措施上与单处理机系统有些差别。 
为了反映多处理机系统各机器之间物理连接的紧密程度与交互作用能力的强弱，通常
使用耦合度这一术语。多处理机系统的耦合度，分为紧耦合系统和松耦合系统两大类。 
紧耦合系统又称直接耦合系统，指处理机之间物理连接的频带较高，一般是通过总线
或高速开关实现互连，可以共享主存。由于具有较高的信息传输率，因而可以快速并行处
理作业或任务。 
松耦合系统又称间接耦合系统，一般是通过通道或通信线路实现处理机之间的互连，
可以共享外存设备(磁盘、磁带等)。机器之间的相互作用是在文件或数据集一级上进行。
松耦合系统表现为两种形式：一种是多台计算机和共享的外存设备连接，不同机器之间实
现功能上的分工(功能专用化)，机器处理的结果以文件或数据集的形式送到共享外存设备，
供其他机器继续处理。另一种是计算机网，机器通过通信线路连接，以求得更大范围的资
源共享。 
多处理机中为了实现时间重叠，将处理功能分散给各专用处理机去完成，即功能专用
化，各处理机之间则按时间重叠原理工作。如输入/输出功能的分离，导致由通道向专用外
围处理机发展。许多主要功能，如数组运算、高级语言编译、数据库管理等，也逐渐分离
出来，交由专用处理机完成，机间的耦合程度逐渐加强，从而发展成为异构多处理机系统。 
随着硬件价格的降低，系统设计的目标聚焦到通过多处理机的并行处理来提高整个系
统的速度。为此，对计算机间互联网络的性能提出了更高要求。高带宽、低延迟、低开销
的机间互联网络，是高效实现程序段或任务一级并行处理的前提条件。为了使并行处理的
任务能在处理机之间随机地进行调度，就必须使各处理机具有同等的功能，从而成为同构
多处理机系统。 
20 世纪 70 年代以来，芯片技术的飞速发展，为多处理机系统的研究和设计提供了强大
的物质基础，各种类型的并行计算机系统纷纷问世。 
20 世纪 80 年代，我国研制了向量处理机 YH-1/2 和 757。它们都是流水线单机内部并
行的机器。进入 90 年代以来，我国又研制了多种类型的并行计算机系统，打破了国外在高
性能计算机领域对我国的封锁。表 9.1 列出了我国 90 年代以来自行研制的几种并行计算机
系统。 

2000 年，超级计算机浮点最高运算速度达到每秒 10000 亿次。我国的神威号计算机运
算速度达到每秒 3480 亿次，使我国成为继美国、日本之后世界上第三个拥有高速计算机的
国家。 
2004 年 6 月曙光 4000A 被评为世界超级计算机五百强的第十名，并作为中国国家网格
最大主节点安装在上海超级计算中心。 
龙芯 2F 是中国科学院计算技术研究所研制的采用 90nm 设计技术的 64 位高性能通用
CPU 芯片。2007 年中国科学技术大学第一个用国产龙芯 2F CPU 设计出了万亿次的高性能
机器，这是值得称道和令人鼓舞的，是中国人用自己的 CPU 做超级计算机的开始。 
2010 年 11 月，世界超级计算机五百强排行榜中，第一名是中国国防科技大学的“天河
1A”(2500 万亿次/秒)，第二名是美国 Cray 公司的 Jaguar(美洲虎)，第三名是中国曙光信息
产业有限公司的“星云”，第七名是美国 IBM 公司的 Roadrunner(走鹃)。表 9.2 是这四台超级
计算机的列表。 

1966 年，M. J. Flynn 从计算机体系结构的并行性出发，按照指令流和数据流的不同组
织方式，把计算机系统结构分为如下四种类型，如图 9.1 所示。 
· 单指令流单数据流(SISD)，其代表机型是单处理机。 
· 单指令流多数据流(SIMD)，其代表机型是向量处理机。 
· 多指令流单数据流(MISD)，这种结构从来没有实现过。 
· 多指令流多数据流(MIMD)，其代表机型是多处理机和机群系统。前者为紧耦合系统，
后者为松耦合系统。 

 

图 9.2 进一步说明了上述分类的组成方式。其中，图 9.2(a)表示一个 SISD 的结构，CU
代表控制单元，PU 代表处理单元，MU 代表存储单元，IS 代表单一指令流，DS 代表单一
数据流。这是单处理机系统进行取指令和执行指令的过程。 
图 9.2(b)表示 SIMD 的结构，仍是一个单一控制单元 CU，但现在是向多个处理单元
(PUl～PUn)提供单一指令流，每个处理单元可有自己的专用存储器(局部存储器 LMl～
LMn)。这些专用存储器组成分布式存储器。 
图 9.2(c)和图 9.2(d)表示 MIMD 的结构，两者均有多个控制单元(CUl～CUn)，每个控
制单元向自己的处理部件(PUl～PUn)提供一个独立的指令流。不同的是，图 9.2(c)是共享
存储器多处理机，而图 9.2(d)是分布式存储器多处理机。 
 

计算机体系结构可以采用不同方式的并行机制。 
1. 超标量处理机和超长指令字处理机 
在计算机系统的最底层，流水线技术将时间并行性引入处理机，而多发射处理机则把
空间并行性引入处理机。超标量(superscalar)设计采用多发射技术，在处理机内部设置多条

并行执行的指令流水线，通过在每个时钟周期内向执行单元发射多条指令实现指令级并行。
超长指令字技术(very long instruction word，VLIW)则由编译器在编译时找出指令间潜在的
并行性，进行适当的调度安排，把多个能够并行执行的操作组合在一起，控制处理机中的
多个相互独立的功能部件，相当于同时执行多条指令，从而提高处理机的并行性。 
2. 多处理机和多计算机 
在单个处理机的性能一定的情况下，进一步提高计算机系统处理能力的简单方法就是
让多个处理机协同工作，共同完成任务。广义而言，使用多台计算机协同工作来完成所要
求的任务的计算机系统称为多处理机(multiprocessor)系统。具体而言，多处理机系统由多
台独立的处理机组成，每台处理机都能够独立执行自己的程序和指令流，相互之间通过专
门的网络连接，实现数据的交换和通信，共同完成某项大的计算或处理任务。多处理机系
统中的各台处理机由操作系统管理，实现作业级或任务级并行。 
与广义多处理机系统不同，狭义多处理机系统仅指在同一计算机内处理机之间通过共
享存储器方式通信的并行计算机系统。运行在狭义多处理机上的所有进程能够共享映射到
公共内存的单一虚拟地址空间。任何进程都能通过执行 LOAD 或 STORE 指令来读写一个
内存字。 
与狭义多处理机相对应，由不共享公共内存的多个处理机系统构成的并行系统又称为
多计算机(multicomputers)系统。每个系统都有自己的私有内存，通过消息传递的方式进行
互相通信。 
多计算机系统有各种不同的形状和规模。机群(cluster，也称集群)系统就是一种常见的
多计算机系统。机群系统是由一组完整计算机通过高性能的网络或局域网互连而成的系统，
这组计算机作为统一的计算机资源一起工作，并能产生一台机器的印象。术语“完整计算
机”意指一台计算机离开机群系统仍能运行自己的任务。机群系统中的每台计算机一般称
为节点。 
3. 多线程处理机 
当通过简单提高处理机主频从而提升单处理机的性能的传统方法受到制约时，处理机
厂商被迫转向处理机片内并行技术。除了传统的指令级并行技术之外，多线程技术和多核
技术也是提高单芯片处理能力的片内并行技术。 
由于现代处理机广泛采用指令流水线技术，因而处理机必须面对一个固有的问题：如
果处理机访存时 cache 缺失(不命中)，则必须访问主存，这会导致执行部件长时间的等待，
直到相关的 cache 块被加载到 cache 中。解决指令流水线因此必须暂停的一种方法就是片上
多线程(on-chip multithreading)技术。该技术允许 CPU 同时运行多个硬件线程，如果某个
线程被迫暂停，其他线程仍可以执行，这样能保证硬件资源被充分利用。 
4. 多核处理机(片上多处理机) 
多线程技术能够屏蔽线程的存储器访问延迟，增加系统吞吐率，但并未提高每个单线
程的执行速度。而多核(multicore)技术通过开发程序内的线程级或进程级并行性提高性能。
多核处理机是指在一颗处理机芯片内集成两个或两个以上完整且并行工作的计算引擎
(核)，也称为片上多处理机(chip multi-processor，CMP)。核(core，又称内核或核心)是指
包含指令部件、算术/逻辑部件、寄存器堆和一级或两级 cache 的处理单元，这些核通过某
种方式互联后，能够相互交换数据，对外呈现为一个统一的多核处理机。 

多核技术的兴起一方面是由于单核技术面临继续发展的瓶颈，另一方面也是由于大规
模集成电路技术的发展使单芯片容量增长到足够大，能够把原来大规模并行处理机结构中
的多处理机和多计算机节点集成到同一芯片内，让各个处理机核实现片内并行运行。因此，
多核处理机是一种特殊的多处理机架构。所有的处理机都在同一块芯片上，不同的核执行
不同的线程，在内存的不同部分操作。多核也是一个共享内存的多处理机：所有的核共享
同一个内存空间。多个核在一个芯片内直接连接，多线程和多进程可以并行运行。 
不同于多核结构，在传统的多处理机结构中，分布于不同芯片上的多个处理机通过片
外系统总线连接，因此需要占用更大的芯片尺寸，消耗更多的热量，并需要额外的软件支
持。多个处理机可以分布于不同的主板上，也可以构建在同一块电路板上，处理机之间通
过高速通信接口连接。 
图 9.3(a)～9.3(f)显示了不同结构的处理机形态。图 9.3(a)是单核处理机结构，由执行
单元、CPU 状态、中断逻辑和片上 cache 组成。图 9.3(b)是多处理机结构，由两个完全独
立的单核处理机构成双处理机系统。图 9.3(c)是多线程处理机结构，在一个物理处理机芯
片内集成两个逻辑处理机，二者共享执行单元和片上 cache，但各自有自己的 CPU 状态和
中断逻辑。图 9.3(d)是多核处理机结构，两个完全独立的单处理机核集成在同一个芯片内，
构成双核处理机，每个核都有自己私有的片上 cache。图 9.3(e)同样是多核处理机结构，但
与图 9.3(d)显示的多核处理机结构的差别在于两个核共享片内 cache。图 9.3(f)显示的是多
核多线程处理机结构，这是多核与多线程相结合的片上并行技术。两个完全独立的处理机
核集成在同一个芯片内，每个核又是双线程的，故该处理机为双核四线程结构。 

硬件多线程技术是提高处理机并行度的有效手段，以往常被应用于高性能计算机的处
理机。2002 年秋，英特尔公司推出一款采用超线程(hyper threading，HT)技术的 Pentium 4
处理机，使多线程技术进入桌面应用环境。超线程技术是同时多线程技术在英特尔处理机
上的具体实现。在经过特殊设计的处理机中，原有的单个物理内核经过简单扩展后被模拟
成两个逻辑内核，并能够同时执行两个相互独立的程序，从而减少了处理机的闲置时间，
充分利用了中央处理机的执行资源。 

1. 超标量处理机的水平浪费和垂直浪费 
超标量技术和超长指令字技术都是针对单一的指令流中的若干指令来提高并行处理能
力的，当单一的指令流出现 cache 缺失等现象时，指令流水线就会断流；而指令之间的相关
性也会严重影响执行单元的利用率。例如，资源冲突会导致处理机流水线不能继续执行新
的指令而造成垂直浪费，而指令相关会导致多条流水线中部分流水线被闲置，造成水平浪
费。图 9.4 显示了一个有四条流水线的超标量处理机的指令执
行实例。图中，每个方框代表一个可用的指令发射时间，水平
方向表示并行执行指令的 4 条指令流水线(指令发射槽)，垂直
方向表示时钟周期，“A”表示某指令流 A 占用的周期，白框
为浪费的周期。显然，水平浪费和垂直浪费造成了处理机执行
部件的空闲。 
因此，如何减少处理机执行部件的空闲时间成为提升处理
机性能的关键。而线程级并行(thread-level parallelism，TLP)
技术正是针对这一问题而引入的。 
2. 硬件线程的概念 
多任务系统必须解决的首要问题就是如何分配宝贵的处理机时间，这通常是由操作系
统负责的。操作系统除了负责管理用户程序的执行外，也需要处理各种系统任务。在操作
系统中，通常使用进程(process)这一概念描述程序的动态执行过程。通俗地讲，程序是静
态实体，而进程是动态实体，是执行中的程序。进程不仅仅包含程序代码，也包含了当前
的状态(这由程序计数器和处理机中的相关寄存器表示)和资源。因此，如果两个用户用同
样一段代码分别执行相同功能的程序，那么其中的每一个都是一个独立的进程。虽然其代
码是相同的，但是数据却未必相同。 
传统的计算机系统把进程当作系统中的一个基本单位，操作系统将内存空间、I/O 设备
和文件等资源分配给每个进程，调度和代码执行也以进程作为基本单位。但进程调度是频
繁进行的，因而在处理机从一个进程切换到另一个进程的过程中，系统要不断地进行资源
的分配与回收、现场的保存与恢复等工作，为此付出了较大的时间与空间的开销。 
因此，在现代操作系统中，大都引入线程作为进程概念的延伸，线程是在操作系统中
描述能被独立执行的程序代码的基本单位。进程只作为资源分配的单位，不再是调度和执
 

行的基本单位；而每个进程又拥有若干线程，线程则是调度和执行的基本单位。除了拥有
一点儿在运行中必不可少的独立资源(如程序计数器、一组寄存器和栈)之外，线程与属于
同一个进程的其他线程共享进程所拥有的全部资源。由于线程调度时不进行资源的分配与
回收等操作，因而线程切换的开销比进程切换少得多。 
在处理机设计中引入硬件线程(hardware thread)的概念，其原理与操作系统中的软件多
线程并行技术相似。硬件线程用来描述一个独立的指令流，而多个指令流能共享同一个支
持多线程的处理机。当一个指令流因故暂时不能执行时，可以转向执行另一个线程的指令
流。由于各个线程相互独立，因而大大降低了因单线程指令流中各条指令之间的相互依赖
导致的指令流水线冲突现象，从而有效提高处理机执行单元的利用率。因此，并行的概念
就从指令级并行扩展至线程级并行。 
图 9.5 显示了一个支持两个线程的超标量处理
机的指令执行实例。其中，“A”表示线程 A(指令
流 A)占用的周期，“B”表示线程 B(指令流 B)占
用的周期。在每个时钟周期内，所有的流水线都用
于执行同一线程的指令，但在下一个时钟周期则可
以选择另一个线程的指令并行执行。 
3. 细粒度多线程和粗粒度多线程 
根据多线程处理机的具体实现方法差异，又可
以分为细粒度多线程(交错多线程)处理机和粗粒
度多线程(阻塞多线程)处理机。 
细粒度多线程如图 9.5(a)所示，处理机交替执行 A、B 两个线程的指令，在每个时钟周
期都进行线程切换。由于多个线程交替执行，并且处于阻塞状态的线程在切换时被跳过，
故在一定程度上降低了指令阻塞造成的处理机吞吐率损失。当然，每个线程的执行速度降
低了，因为就绪状态的线程会因为其他线程的执行而延迟。 
粗粒度多线程如图 9.5(b)所示，只有在遇到代价较高的长延迟操作(如因 cache 缺失需
要访问主存)时才由处理机硬件进行线程切换，否则一直执行同一个线程的指令。因此，粗
粒度多线程比细粒度多线程有更低的线程切换开销，且每个线程的执行速度几乎不会降低。
但是粗粒度多线程也有弱点，就是在线程切换的过程中需要排空或填充指令流水线。只有
当长延迟操作导致线程被阻塞的时间远长于指令流水线排空或填充的时间时，粗粒度多线
程才是有意义的。 
多线程处理机通常为每个线程维护独立的程序计数器和数据寄存器。处理机硬件能够
快速实现线程间的切换。由于多个相互独立的线程共享执行单元的处理机时间，并且能够
进行快速的线程切换，因而多线程处理机能够有效地减少垂直浪费情况，从而利用线程级
并行来提高处理机资源的利用率。 

从图 9.5 可以看出，多线程处理机虽然可以减少长延迟操作和资源冲突造成的处理机执
行单元浪费，但并不能完全利用处理机中的所有资源。这是因为每个时钟周期执行的指令
都来自同一个线程，因而不能有效地消除水平浪费。为了最大限度地利用处理机资源，同
 

时多线程(simultaneous multi-threading，SMT)技术被引入现代处理机中。 
同时多线程技术结合了超标量技术和细粒度多线程技术的优点，允许在一个时钟周期
内发射来自不同线程的多条指令，因而可以同时减少水平浪费和垂直浪费。 
图 9.6 显示了一个支持两个线程的同时多线程处理机的指令执
行实例。在一个时钟周期内，处理机可以执行来自不同线程的多条
指令。当其中某个线程由于长延迟操作或资源冲突而没有指令可以
执行时，另一个线程甚至能够使用所有的指令发射时间。因此，同
时多线程技术既能够利用线程级并行减少垂直浪费，又能够在一个
时钟周期内同时利用线程级并行和指令级并行来减少水平浪费，从
而大大提高处理机的整体性能。 
同时多线程技术是一种简单、低成本的并行技术。与单线程处
理机相比，同时多线程处理机只花费很小的代价，而性能得到很大
改善。在原有的单线程处理机内部为多个线程提供各自的程序计数器、相关寄存器以及其
他运行状态信息，一个“物理”处理机被模拟成多个“逻辑”处理机，以便多个线程同步
执行并共享处理机的执行资源。应用程序无须做任何修改就可以使用多个逻辑处理机。 
由于多个逻辑处理机共享处理机内核的执行单元、高速缓存和系统总线接口等资源，
因而在实现多线程时多个逻辑处理机需要交替工作。如果多个线程同时需要某一个共享资
源，只有一个线程能够使用该资源，其他线程要暂停并等待资源空闲时才能继续执行。因
此，同时多线程技术就性能提升而言远不能等同于多个相同时钟频率处理机核组合而成的
多核处理机，但从性能-价格比的角度看，同时多线程技术是一种对单线程处理机执行资源
的有效而经济的优化手段。 
由于同时运行的多个线程需要共享执行资源，因而处理机的实时调度机制非常复杂。
就调度策略而言，取指部件要在单线程执行时间延迟与系统整体性能之间取得平衡。与单
线程处理机相比，并发执行的多个线程必然拉长单个线程的执行时间，但处理机可以通过
指定一个线程为最高优先级而减小其执行延迟，只有当优先线程阻塞时才考虑其他线程。
为了最大限度地提高处理机整体性能，同时多线程处理机也可以采用另外一种策略，即处
理机的取指部件可以选择那些可以带来最大性能好处的线程优先取指并执行，代价是牺牲
单个线程的执行时间延迟。 
为了实现同时多线程，处理机需要解决一系列问题。例如，处理机内需要设置大量寄
存器保存每个线程的现场信息，需要保证由于并发执行多个线程带来的 cache 冲突不会导致
显著的性能下降，确保线程切换的开销尽可能小。 

超线程技术是同时多线程技术在英特尔系列处理机产品中的具体实现。 
自 2002 年起，英特尔公司先后在其奔腾 4 处理机和至强(XEON)处理机等产品中采用
超线程技术。奔腾 4 处理机和至强处理机基于同样的 Intel NetBurst 微体系结构
(micro-architecture，处理机体系结构在硅芯片上的具体实现)。 
图 9.7 显示了支持超线程技术的 NetBurst 微体系结构的流水线结构。每条指令的执行过
程都需要经过 10 个功能段组成的流水线。 
图 9.7  支持超线程技术的 NetBurst 微体系结构的流水线结构 
原有的流水线只支持单线程运行。统计表明，单线程的 NetBurst 微体系结构的流水线
在执行典型的指令序列时仅仅利用了大约 35％的流水线资源。 
为了支持两个硬件线程同时运行，需要对流水线进行改造。改造的方式是让每级流水
线中的资源通过三种方式之一复用于两个线程：复制、分区或共享。 
其中，复制方式是在处理机设计时分别为两个线程设置独立的部件。被复制的资源包
括所有的处理机状态、指令指针 IP(程序计数器)寄存器、寄存器重命名部件和一些简单资
源(如指令 TLB 等)。复制这些资源仅仅会少许提高处理机的成本，而每个线程使用这些资
源的方式与单线程相同。 
分区方式则是在处理机设计时把原有的用于单线程的独立资源分割成两部分，分别供
两个线程使用。采用分区方式的主要是各种缓冲区和队列，如重排序缓冲区、取数/存数缓
冲区和各级队列等。与单线程相比，每个线程使用的缓冲区或队列的容量减半，而处理机
成本并没有增加。 
共享方式则是由处理机在执行指令的过程中根据使用资源的需要在两个线程之间动态
分享资源。乱序执行部件和 cache 采用共享方式复用。这种方式同样不增加处理机成本，但
单线程运行时存在的资源闲置得到有效改善。 
由于不同的资源采用不同的复用方式，因此当指令在不同的资源之间转移时，处理机
需在图中箭头和多路开关标识的选择点根据需要动态选择能够使用下级资源的线程。 
多线程技术只对传统的单线程超标量处理机结构做了很少改动，但却获得很大的性能
提升。启用超线程技术的内核比禁用超线程技术的内核吞吐率要高出 30%。当然，超线程
技术需要解决一系列复杂的技术问题。例如，作业调度策略、取指和发射策略、寄存器回
收机制、存储系统层次设计等比单线程处理机复杂许多。 

多处理机系统由多个独立的处理机组成，每个处理机能够独立执行自己的程序。 
现有的多处理机系统分为如下四种类型：并行向量处理机(PVP)、对称多处理机(SMP)、
大规模并行处理机(MPP)、分布共享存储器多处理机(DSM)，如图 9.8 所示。 
并行向量处理机见图 9.8(a)。它是由少数几台巨型向量处理机采用共享存储器方式互
连而成，在这种类型中，处理机数目不可能很多。 
对称多处理机见图 9.8(b)。它由一组处理机和一组存储器模块经过互联网络连接而成。
有多个处理机且是对称的，每台处理机的能力都完全相同。每次访问存储器时，数据在处
理机和存储器模块间的传送都要经过互联网络。由于是紧耦合系统，不管访问的数据在哪
一个存储器模块中，访问存储器所需的延迟时间都是一样的。 
分布共享存储器多处理机见图 9.8(c)。同 PVP 和 SMP 一样，它也属于紧耦合系统。它
的共享存储器分布在各台处理机中，每台处理机都带有自己的本地存储器，组成一个处理
机-存储器单元。但是这些分布在各台处理机中的实际存储器又合在一起统一编址，在逻辑
上组成一个共享存储器。这些处理机-存储器单元通过互联网络连接在一起，每台处理机除
了能访问本地存储器外，还能通过互联网络直接访问在其他处理机-存储器单元中的“远程
存储器”。处理机在访问远程存储器时所需的延迟时间与访问本地存储器时所需的延迟时间
是不一样的，访问本地存储器要快得多。 
 
大规模并行处理机见图 9.8(d)。它属于松耦合多处理机系统。每个计算机模块称为一
个结点。每个结点有一台处理机和它的局部存储器(LM)、结点接口(NIC)，有的还有本身
的 I/O 设备，这几部分通过结点内的总线连在一起。计算机模块又通过结点接口连接到互联
网络上。由于 VLSI 技术的发展，整个结点上的计算机已可以做在一个芯片上。 
第 9 章  并行组织与结构       291 
在这种松耦合的多计算机系统中，各台计算机间传送数据的速度低，延迟时间长，且
各结点间的距离是不相等的，因此把经常要在结点间传送数据的任务放在相邻的结点中执
行。由于松耦合的多计算机系统的互联网络的成本低得多，故同紧耦合多处理机系统相比，
其优点是可以组成计算机数目很多的大规模并行处理系统。也就是说，可以比较经济合理
地用微处理机构成几百台乃至几千台的多计算机系统。 
鉴于当前并行处理系统的发展趋势，下面重点讲授对称多处理机 SMP。 
不久前，所有的单用户个人计算机和大多数工作站还只含有单一通用微处理机。随着
性能需求的增长和微处理机价格的持续下跌，计算机制造商推出了 SMP 系统。SMP 既指计
算机硬件体系结构，也指反映此体系结构的操作系统行为。SMP 定义为具有如下特征的独
立计算机系统。 
(1)有两个以上功能相似的处理机。 
(2)这些处理机共享同一主存和 I/O 设施，以总线或其他内部连接机制互连在一起；这
样，存储器存取时间对每个处理机都是大致相同的。 
(3)所有处理机共享对 I/O 设备的访问，或通过同一通道，或通过提供到同一设备路径
的不同通道。 
(4)所有处理机能完成同样的功能。 
(5)系统被一个集中式操作系统(OS)控制。OS 提供各处理机及其程序之间的作业级、
任务级、文件级和数据元素级的交互。 
其中，(1)～(4)是十分明显的。(5)表示了 SMP 与机群系统之类的松耦合多处理系统
的对照。后者的交互物理单位通常是消息或整个文件；而在 SMP 中，个别的数据元素能成
为交互级别，于是处理机间能够有高度的相互协作。 
SMP 的操作系统能跨越所有处理机来调度进程或线程。SMP 有如下几个超过单处理机
的优点。 
性能  如果可以对一台计算机完成的工作进行组织，使得某些工作部分能够并行完成；
则具有多个处理机的系统与具有同样类型的单处理机的系统相比，将产生更高的性能。 
可用性  在一个对称多处理机系统中，所有处理机都能完成同样的功能，故单个处理
机的故障不会造成系统的停机，系统在性能降低的情况下继续运行。 
增量式增长  用户可以通过在系统中添加处理机来提高系统性能。 
可扩展性  厂商能提供一个产品范围，它们基于系统中配置的处理机数目不同而有不
同的价格和性能特征。 
SMP 的一个有吸引力的特点是：多个处理机的存在对用户是透明的；由操作系统实际
关注各个处理机上进程或线程的调度，以及处理机间的同步。 

对个人计算机、工作站和服务器而言，互连机构使用分时共享总线。分时共享总线是构
成一个多处理机系统的最简单机构。结构和界面基本上同于使用总线互连的单处理机系统。

总线由控制、地址和数据线组成。为便利来自 I/O 处理器的 DMA 传送，应具备如下特征。 
(1)寻址  必须能区别总线上各模块，以
确定数据的源和目标。 
(2)仲裁  任何I/O模块都能临时具备主
控器(master)功能。要提供一种机制来对总
线控制的竞争请求进行仲裁，可使用某种类
型的优先级策略。 
(3)分时共享  当一个模块正在控制总
线时，其他模块是被锁住的，而且如果需
要，应能挂起它的操作直到当前的总线访
问完成。 
这些单处理机特征在对称多处理机配置
中是直接可用的，但可能会出现多个处理机
以及多个 I/O 适配器都试图掌管总线，并对一个或多个存储器模块进行存取操作的更为复
杂的情况。 
与其他方法比较，总线组织方式有如下几个优点。 
简易性  这是多处理机系统组成的最简单方式。物理接口以及每个处理机的寻址、仲
裁和分时逻辑保持与单处理机系统相同。 
灵活性  以附加更多处理机到总线的方法来扩充系统，一般来说也是容易的。 
可靠性  本质上来说，总线是一个被动介质，并且总线上任一设备的故障不会引起整
个系统的失败。 
总线组织的主要缺点在于性能。所有的存储器访问都要通过公共总线，于是系统速度
受限于总线周期。为改善性能，就要求为每个处理机配置 cache，这将急剧地减少总线访问
次数。一般来说，工作站和个人机 SMP 都有两级 cache，L1 cache 是内部的(与处理机同一
芯片)，L2 cache 或是内部的，或是外部的。现在，某些处理机还使用了 L3 cache。 
cache 的使用导致某些新的设计考虑，因为每个局部 cache 只保存部分存储器的映像，
如果在某个 cache 中修改了一个字，可想象出其他 cache 中的此字将会是无效的。为防止这
个问题，必须通知其他处理机：已经发生了修改。这个问题称为 cache 一致性问题，并且一
般是以硬件解决。 

从单处理机到多核处理机的变化并不是处理机设计厂商根据客户需求和市场趋势做出
的主动选择，而是在物理规律限制下的无奈之举。多核解决方案可以利用新工艺带来的集
成电路集成度的提高，将几个处理机核心集成在一块芯片内。 

与传统的单核技术相比，多核技术是应对芯片物理规律限制的相对简单的办法。与提

高处理机主频相比，在一个芯片内集成多个相对简单而主频稍低的处理机核既可以充分利
用摩尔定律带来的芯片面积提升，又可以更容易地解决功耗、芯片内部互联延迟和设计复
杂度等问题。 
(1)高并行性：每个处理机核都不必提高晶体管的翻转速度，而多核处理机可同时执行
的线程数或任务数是单处理机的数倍，极大地提升了处理机的并行性，带来了更强的并行
处理能力和更高的计算密度。 
(2)高通信效率：多个核集成在片内，各个处理机核只需要在核内部的相对较小的区域
内交换数据，不需要很长的互联线，通信延迟变低，提高了通信效率，数据传输带宽也得
到提高。 
(3)高资源利用率：多核结构可以有效支持片内资源共享，片上资源的利用率得到了
提高。 
(4)低功耗：处理机的功耗增长随着内核数目的增加呈线性增长，而不是随着频率的增
加呈指数级增长。由于不再依靠提高主频改善性能，内核的工作频率不需要达到上限，多
个简单低速核的功耗远低于一个高速复杂处理机的功耗。如果进一步采用动态管理各处理
机核功耗的方法，针对不同的任务，每个核可以被降频或关闭，多核在功耗控制上会更有
优势。 
(5)低设计复杂度：多核处理机中的每个核的结构相对简单，易于优化设计，扩展性强。
设计高速而复杂的单处理机往往要采用超标量处理机结构和超长指令字结构，控制逻辑复
杂。而在芯片内复制多个低速简单内核的设计难度显然更低，设计和验证周期更短，出现
错误的机会也更小。 
(6)较低的成本：多核处理机内的各个核共享器件芯片封装和芯片 I/O 资源，也使占单
核处理机成本 25%～50%的芯片封装和 I/O 成本的比重大大下降，生产成本得以降低。设计
复杂度的降低也会使处理机设计开发的成本降低。 
这些优势最终推动多核的发展并使多核逐渐取代单核处理机成为主流技术。 
多核技术是在超线程、超标量和多处理机等技术的基础上发展起来的，也充分吸收了
其他技术的优势。 
超线程技术是通过隐藏潜在访存延迟的方法提高处理机的性能，其主要目的是充分利
用空闲的处理机资源，本质上仍然是多个线程共享一个处理机核。因此，采用超线程技术
是否能获得性能的提升依赖于应用程序以及硬件平台。多核处理机则是将多个独立的处理
机核嵌入到一个处理机芯片内部，每个线程都具有完整的硬件执行环境，故各线程之间可
以实现真正意义上的并行。当然，多核架构中灵活性的提升是以牺牲资源利用率为代价的。
不管是超线程处理机还是多核处理机，性能的提升都需要软件的配合，性能提升的程度取
决于并行性的大小。 
多处理机系统是利用任务级并行的方式提高系统性能的，即把任务并行化并分配到多
个处理机中去执行。由于多处理机之间的耦合度较低，不适合实现细粒度并行，而功耗也
较高。而多核处理机由于在一个芯片内集成多个核心，核间耦合度高，核间互连延迟更小，
功耗更低，故可以在任务级、线程级和指令级等多个层次充分发挥程序的并行性，灵活
度高。 

1. 同构多核处理机与异构多核处理机 
与多处理机的分类方法类似，按多核处理机内的计算内核的地位对等与否划分，多核
处理机可以分为同构多核和异构多核两种类型。 
1)同构多核(homogenous multi-core)处理机 
同构多核处理机内的所有计算内核结构相同，地位对等。 
同构多核处理机大多由通用的处理机核心构成，每个处理机核心可以独立地执行任务，
其结构与通用单核处理机结构相近。同构多核处理机的各个核心之间可以通过共享存储器
互连，也可以通过 cache 或局部存储器互连。在英特尔公司的通用桌面计算机上的多核处理
机通常采用同构多核结构。 
2)异构多核(heterogeneous multi-core)处理机 
异构多核处理机内的各个计算内核结构不同，地位不对等。 
异构多核处理机根据不同的应用需求配置不同的处理机核心，一般多采用“主处理核+
协处理核”的主从架构。异构多核处理机的优势在于可以同时发挥不同类型处理机各自的
长处来满足不同种类的应用的性能和功耗需求。异构多核处理机将结构、功能、功耗、运
算性能各不相同的多个核心集成在芯片上，并通过任务分工和划分将不同的任务分配给不
同的核心，让每个核心处理自己擅长的任务。 
目前的异构多核处理机通常同时集成通用处理机、数字信号处理机(DSP)、媒体处理机、
网络处理机等多种类型的处理机核心，并针对不同需求配置应用其计算性能。其中，通用
处理机核常作为处理机控制主核，并用于通用计算；而其他处理机核则作为从核用于加速
特定的应用。例如，多核异构网络处理机配有负责管理调度的主核和负责网络处理功能的
从核，经常用于科学计算的异构多核处理机在主核之外可以配置用于定点运算和浮点运算
等计算功能的专用核心。 
研究表明，异构组织方式比同构的多核处理机执行任务更有效率，实现了资源的最优
化配置，而且降低了系统的整体功耗。 
2. 多核处理机的对称性 
同构多核和异构多核是对处理机内核硬件结构和地位一致性的划分。如果再考虑各个
核之上的操作系统，从用户的角度看，可以把多核处理机的运行模式划分为对称(symmetric 
multiprocessing，SMP)多核和非对称(asymmetric multiprocessing，AMP)多核两种类型。 
多核处理机中的对称(SMP)多核结构是指处理机片内包含相同结构的核，多个核紧密
耦合，并运行一个统一的操作系统。每个核的地位是对等的，共同处理操作系统的所有任
务。SMP 由多个同构的处理机核和共享存储器构成，由一个操作系统的实例同时管理所有
处理机核，并将应用程序分配至各个核上运行。只要有一个内核空闲可用，操作系统就在
线程等待队列中分配下一个线程给这个空闲内核来运行。应用程序本身可以不关心有多少
个核在运行，由操作系统自动协调运行，并管理共享资源。 
同构多核处理机也可以构成非对称(AMP)多核结构。若处理机芯片内部是同构多核，
但每个核运行一个独立的操作系统或同一操作系统的独立实例，那就变成非对称多核。AMP
多核系统也可以由异构多核和共享存储器构成。 
3. 多核处理机的 cache 组织 
在设计多核处理机时，除了处理机的结构和数量，cache 的级数和大小也是需要考虑的
重要问题。根据多核处理机内的 cache 配置，可以把多核处理机的组织结构分成以下四种。 
1)片内私有 L1 cache 结构 
图 9.10(a)显示的多核结构是简单的多核计算机片内 cache 结构。系统 cache 由 L1 和
L2 两级组成。处理机片内的多个核各自有自己私有的 L1 cache，一般被划分为指令 L1 
cache(L1-I)和数据 L1 cache(L1-D)。而多核共享的 L2 cache 则存在于处理机芯片之外。 
ARM 公司 ARM11 微体系结构的 MPCore 多核嵌入式处理机就采用这种结构。 

2)片内私有 L2 cache 结构 
在图 9.10(b)显示的多核结构中，处理机片内的多个核仍然保留自己私有的指令 L1 
cache(L1-I)和数据 L1 cache(L1-D)，但 L2 cache 被移至处理机片内，且 L2 cache 为各个核
私有。多核共享处理机芯片之外的主存。 
AMD 公司专门为服务器和工作站设计的皓龙(Opteron)处理机就采用这种结构。 
3)片内共享 L2 cache 结构 
在图 9.10(c)显示的多核结构与图 9.10(b)显示的多核结构相似，都是片上两级 cache
结构。不同之处在于处理机片内的私有 L2 cache 变为多核共享 L2 cache。多核仍然共享处
理机芯片之外的主存。 
对处理机的每个核而言，片内私有 L2 cache 的访问速度更高。但在处理机片内使用共
享的 L2 cache 取代各个核私有的 L2 cache 能够获得系统整体性能的提升，这是因为： 

(1)共享 cache 有助于提高整体 cache 命中率。如果处理机内的多个核先后访问主存同
一个页面，首次访问该地址的操作会将该页面调入共享 cache，其他核在此后访问同样的主
存页面时可以直接在共享 cache 中快速存取，从而减少访问主存的次数。并且，在私有 cache
结构中，不同核访问主存相同页面会在各自私有 cache 中都保存该主存页面的副本，而共享
cache 则不会重复复制数据。 
(2)共享 cache 的存储空间可以在不同核之间动态按需分配，实现“统计时分复用”。而
私有 cache 的大小是固定不变的。 
(3)共享 cache 还可以作为处理机间交互信息的通道。 
(4)多核处理机必须解决多级 cache 的一致性问题，而只设计 L1 一级私有 cache 可以降
低解决 cache 一致性问题的难度，从而提供额外的性能优势。 
英特尔公司的第一代酷睿双核(Core Duo)低功耗处理机就采用这种结构。 
4)片内共享 L3 cache 结构 
随着处理机芯片上的可用存储器资源的增长，高性能的处理机甚至把 L3 cache 也从处
理机片外移至片内。图 9.10(d)显示的多核结构在图 9.10(b)显示的片内私有 L2 cache 结构
的基础上增加了片内多核共享 L3 cache，使存储系统的性能有了较大提高。 
由于处理机片内核心数和片内存储空间容量都在增长，在共享 L2 cache 结构或私有 L2 
cache 结构上增加共享的 L3 cache 显然有助于提高处理机的整体性能。 
英特尔公司于 2008 年推出的 64 位酷睿 i7(Core i7)四核处理机就采用这种结构。 

尽管多核技术与单核技术相比存在性能高、集成度高、并行度高、结构简单和设计验
证方便等诸多优势，但从单核到多核的转变并不是直接把多个芯片上的多个处理机集成到
单一芯片之中这么简单。多核处理机必须解决诸多技术难题。 
1. 多核处理机架构 
多核处理机的体系结构直接影响着多核的性能。而不同的应用的特性又差别很大，这
些特性又对多核应该采用什么样的结构有着非常大的影响。为此，必须针对不同的应用设
计多核的实现架构。 
首先是每个核自身的结构，这关系到整个芯片的面积、功耗和性能。就每个核而言，
如何继承并扩展传统单处理机设计的成果，直接影响多核处理机的性能和实现周期。多核
系统中的每个核是否应该采用超标量技术或超线程技术，是性能和成本平衡的问题。随着
对处理机的性能要求的不断提高，在多核处理机的每个核上采用超线程技术的架构应用越
来越广。而软件的并行化设计思想的推广也让超线程技术越来越有吸引力。 
其次就是多核之间的对等性，以及芯片上的核的数目。采用同构多核还是异构多核，
一般要根据具体的应用场景、设计目标等因素综合决定。 
最初的多核处理机都采用同构处理机架构，每个核的功能较强，但集成的处理机核的
数量较少，一般以总线或交叉开关互连。这种设计实际上是利用半导体技术的进步把原来
放在不同芯片上的多处理机集成到一个芯片上，通过简单增加片内处理机核心的数量来提
升处理机的性能，体系结构上的改进并不明显。这种设计方法简单、有效，可以重用复杂
的处理机设计，并且借用板级总线协议，是多核发展的初级阶段。 

同构多核结构原理简单，硬件实现复杂度低，在通用桌面系统中被普遍采用。但在现
实世界的应用场景中，并不总是能够把计算任务均匀分配到同构的多个核心上，多核必须
面对如何平衡若干处理机的负载并进行任务协调等难题。即使能够不断增加同类型的处理
机核心的数量以加强并行处理能力，整个系统的处理性能仍然会受到软件中必须串行执行
的那部分的制约。达到极限值之后，性能就无法再随着内核数量的增加而提升了。这就是
著名的阿姆达尔定律(Amdahl’s law)。 
异构多核则通过配置不同特点的核心来优化处理机内部结构，实现处理机性能的最佳
化，并能有效地降低系统功耗。 
异构多核架构的一个典型实例就是在通用个人计算机上将图形处理单元(Graphic 
Processing Unit，GPU)与通用 CPU 集成在一颗芯片上构成的异构多核处理机。在这样的架
构下，系统中必须串行执行的部分能在一个强大的 CPU 核上加速，而可以并行的部分则通
过很多很小的 GPU 核来提速。 
GPU 是在通用计算机系统上支持图形处理的专用处理单元。GPU 的计算能力随着图形
运算的复杂度的上升而逐渐提高，尤其是浮点运算能力已经远远超过通用 CPU 数倍。与 CPU
相比，GPU 更适合重复计算，因为 GPU 是专门为图形运算而设计的，在设计时就考虑到了
图形运算的特征。例如，对图形的色彩处理往往需要对所有待处理的像素执行相同或类似
的重复运算。这恰恰让 GPU 非常适合进行 SIMD 运算。因此，人们很自然地试图利用 GPU
的这种优化设计来进行图形之外的通用计算，将 GPU 通用化，于是出现了通用图形处理机
(General Purpose GPU，GPGPU)。GPGPU 兼有通用计算和图形处理两大功能，能完成 CPU
的运算工作，更适合高性能计算，并能使用高级程序设计语言，在性能和通用性上更加强
大。GPGPU 向着集成化方向发展，即将 GPU 核集成到 CPU 片内，就构成异构多核处理机。
面向并行处理的应用软件所要求的浮点运算及定点运算将由 GPU 执行；而 CPU 内核则把
重点放在执行传统处理机的主要任务，即运行操作系统、执行商务软件中的整数运算等。 
异构多核结构也存在着一些难点，如选择哪几种不同的核相互搭配、核间任务如何分
工、如何实现良好的可扩展性等，必须在性能、成本、功耗等方面仔细平衡，并通过软硬
件相互配合使任务的并行性最大化。 
2. 多核系统存储结构设计 
为了使处理机的处理能力得到充分发挥，存储系统必须能够提供与处理机性能相匹配
的存储器带宽。因此，处理机与主存储器之间的速度差距一直是处理机结构设计中必须考
虑的问题。由于处理机内的核心数目增多，并且各核心采用共享存储器结构进行信息交互，
对主存的访问需求进一步增加，在单处理机时代面临的存储墙问题依然存在，而且问题更
加严重。故必须针对多核处理机进行相应的存储结构设计，并解决好存储系统的效率问题。 
目前的存储系统设计仍然采用存储器分级的方式解决存储速度问题，高性能的处理机
采用二级甚至三级 cache 提高存储系统的等效访问速度，并且处理机片内的 cache 容量尽可
能增大。但多核系统中的存储系统设计必须平衡系统整体性能、功耗、成本、运行效率等
诸多因素。因此，在多核处理机设计时，必须评估共享 cache 和私有 cache 孰优孰劣、需要
在芯内设置几级 cache 等因素。 
此外，在多核系统中，还面临多级 cache 的一致性(cache coherency)问题。 

3. 多核处理机的 cache 一致性 
cache 一致性问题产生的原因是：在一个处理机系统中，不同的 cache 和主存空间中可
能存放着同一个数据的多个副本，在写操作时，这些副本存在着潜在的不一致。在单处理
机系统中，cache 一致性问题主要表现为在内存写操作过程中如何保持 cache 中的数据副本
和主存内容的一致，即使有 I/O 通道共享 cache，也可以通过全写法较好地解决一致性问题。 
而在多核系统中，多个核都能够对内存进行写操作，而 cache 级数更多，同一数据的多
个副本可能同时存放在多个 cache 存储器中，某个核的私有 cache 又只能被该核自身访问。
即使采用全写法，也只能维持一个 cache 和主存之间的一致性，不能自动更新其他处理机核
的私有 cache 中的相同副本。这些因素无疑加大了 cache 一致性问题的复杂度，同时又影响
着多核系统的存储系统整体设计。 
维护 cache 一致性的关键在于跟踪每一个 cache 块的状态，并根据处理机的读写操作及
总线上的相应事件更新 cache 块的状态。 
一般来说，导致多核处理机系统中 cache 内容不一致的原因如下。 
可写数据的共享  一台处理机采用全写法或回写法修改某一个数据块时，会引起其他
处理机的 cache 中同一副本的不一致。 
I/O 活动  如果 I/O 处理机直接接在系统总线上，也会导致 cache 不一致。 
核间线程迁移  核间线程迁移就是把一个尚未执行完的线程调度到另一个空闲的处理
机核中去执行。为提高整个系统的效率，有的系统允许线程核间迁移，使系统负载平衡。
但这有可能引起 cache 的不一致。 
对于 I/O 活动和核间线程迁移而导致的 cache 不一致，可以分别通过禁止 I/O 通道与处
理机共享 cache 以及禁止核间线程迁移来解决。因而多处理机中 cache 一致性问题主要是针
对可写数据的共享。 
在多核系统中，cache 一致性可以使用软件或者硬件维护。 
软件方法采取的手段是“预防”。在使用软件方式维护 cache 一致性时，处理机需要提
供专门的显式 cache 操作指令，如 cache 块拷贝、回收和无效等指令，让程序员或编译器分
析源程序的逻辑结构和数据相关性，判断可能出现的 cache 一致性问题，利用这些指令维
护 cache 一致性。软件维护 cache 一致性的优点是硬件开销小，缺点是在多数情况下对性能
有较大影响， 而且需要程序员的介入。 
多数情况下，cache 一致性由硬件维护。硬件方法采取的手段是“通过硬件发现和解决
所发生的 cache 一致性问题”。不同的处理机系统使用不同的 cache 一致性协议维护 cache
一致性。cache 一致性协议维护一个有限状态机，并根据存储器读写指令或者总线上的操作
进行状态转移并完成相应 cache 块的操作，以维护 cache 一致性。 
目前，大多数多核处理机采用总线侦听(bus snooping)协议，也有的系统采用目录
(directory)协议解决多级 cache 的一致性问题。目录协议在全局的角度统一监管不同 cache
的状态；而在总线侦听方式中，每个 cache 分别管理自身 cache 块的状态，并通过广播进行
不同 cache 间的状态同步。 
1)目录协议 
目录协议收集并维护有关数据块副本驻存在何处的信息。典型地，系统有一中央控制
器，它是主存控制器的一部分，目录就存于主存中。目录会有关于各个局部 cache 内容的

全局性状态信息。当某个特定的 cache 控制器产生一个请求时，中央控制器检查此请求并
发出必要的命令，以在存储器和 cache 之间或不同 cache 之间传送数据。中央控制器亦负责
保持状态信息的更新。于是，任何一个能影响 cache 行的全局状态的局部动作必须报告给中
央控制器。 
中央控制器维护着关于哪个处理机核具有哪个数据行副本的信息。在处理机核向局部
cache 行副本写入信息之前，必须向中央控制器请求排他性访问权限。在同意这次排他性访
问之前，控制器发送一个消息给所有 cache 中保持有这一行副本的处理机核，以强迫每个处
理机核使它的副本无效。接收到这些处理机核返回的确认信息后，控制器才将排他性访问
权授予提出请求的处理机核。当一 cache 行已授权给某处理机核专有，而另外的处理机核企
图读此行时，它将送出一个未命中指示给控制器。控制器则向持有此行的处理机核发布命
令，要求它将此行写回到主存。于是，现在此行可被原先的处理机核和提出请求的处理机
核读共享了。 
目录协议的缺点是存在中央瓶颈，且各 cache 控制器和中央控制器之间的通信开销也
较大。然而，在采用了多条总线或某种另外的复杂互连机构的大型系统中，它们是很有效的。 
2)监听协议 
监听协议将维护 cache 一致性的责任分布到多核处理机中每个 cache 控制器上。一个
cache 必须知晓它保存的某个 cache 行何时会与其他 cache 共享。当对共享的 cache 行进行修
改时，必须通过一种广播机制通知到所有其他 cache。各 cache 控制器应能监听网络，以得
到这些广播通知，并做出相应的反应。 
监听协议非常适合于基于总线的多核处理机，因为共享的总线能为广播和监听提供简
洁的方式。然而，使用局部 cache 的目标之一就是希望避免或减少总线访问，因此必须小
心设计以避免由于广播和监听而增加的总线传输抵消了使用局部 cache 的好处。 
监听协议已开发出两种基本方法：写-作废(write-invalidate)和写-更新(write-update)。 
使用写-作废协议，系统任一时刻可有多个读者，但只能有一个写者。最初，一个数据
可能在几个 cache 中处于读共享状态。当某个 cache 要对此行进行写操作时，它要先发出
一个通知，以使其他 cache 中此行作废，使此行变为 cache 写独占状态。一旦行变为独占状
态，拥有该行的处理机核就可进行本地写操作，直到某些其他处理机核请求该数据行。 
在写-作废协议中，cache 行的状态被分别标识为修改(Modified)、独占(Exclusive)、共
享(Shared)和无效(Invilid)。故写-作废协议也称为 MESI 协议。 
写-更新协议又称为写-广播(write-broadcast)协议。采用该协议，系统中可有多个写者
以及多个读者。当一个处理机核打算修改一个共享 cache 行时，将被写入的字数据也被同时
广播到所有其他 cache，于是拥有该数据行副本的 cache 能同时进行写修改。 
监听协议实现比较简单，但只适用于总线结构的多处理机系统，而且不管是写作废还
是写更新，都要占用总线不少时间，所以只能用于处理机核数量不多的系统中。通常总线
上能连接的处理机核不能超过 10～16 个。 
监听协议是应用广泛的 cache 一致性协议。 
4. 多核处理机的核间通信与同步技术  
多核处理机片内的多个核心虽然各自执行自己的代码，但是不同核心间需要进行数据
的共享和同步，因此多核处理机硬件结构必须支持高效的核间通信，片上通信结构的性能

也将直接影响处理机的性能。 
当前主流的片上通信方式有三种：总线共享 cache 结构、交叉开关互连结构和片上网络
结构。 
1)总线共享 cache 结构 
总线共享 cache 结构是指多核处理机核共享 L2 cache 或 L3 cache，处理机片上核心、输
入输出端口以及主存储器通过连接核心的总线进行通信。 
这种方式的优点是结构简单、易于设计实现、通信速度高，但缺点是总线结构的可扩
展性较差，只适用于核心数较少的情况。 
采用总线共享结构的处理机有斯坦福大学研制的 Hydra 处理机、英特尔公司开发的酷
睿(Core)处理机、IBM 公司开发的 Power4 处理机和 Power5 处理机等。 
2)交叉开关互连结构 
总线采用分时复用的工作模式，因而在同一总线上同时只能有一个相互通信的过程。
交叉开关(crossbar switch)结构则能够有效提高数据交换的带宽。 
交叉开关是在传统电话交换机中沿用数十年的经典技术，它可以按照任意的次序把输
入线路和输出线路连接起来。图 9.11 所示为连接 8 个处理机核和 8 个内存模块的交叉开关
结构。 

图中左侧的每条水平线和每条垂直线的交点都是可控的交叉节点，可以根据控制信号
的状态打开或闭合。闭合状态的交叉节点使其连接的垂直线和水平线处于连通状态。图中
黑色实心节点处于闭合状态，空心节点处于打开状态，图中右侧显示了放大的节点示意图。
图中显示有三个开关处于闭合状态，这意味着同时可以有三个处理机核分别与不同的存储
器模块进行信息交互。 
交叉开关网络是一种无阻塞的网络，这就意味着不会因为网络本身的限制导致处理机
第 9 章  并行组织与结构       301 
核无法与内存模块建立连接。只要不存在存储器模块本身的冲突，图 9.11 所示的 8×8 交叉
开关结构最多可以同时支持八个连接。 
与总线结构相比，交叉开关的优势是数据通道多、访问带宽更大，但缺点是交叉开关
结构占用的片上面积也较大，因为 n×n 的交叉开关需要 n2 个交叉节点。而且随着核心数的
增加，交叉开关结构的性能也会下降。因此这种方式也只适用中等规模的系统。 
AMD 公司的速龙(Athlon)X2 双核处理机就是采用交叉开关来控制核心与外部通信的
典型实例。 
3)片上网络结构 
片上网络(network on a chip，NoC)技术借鉴了并行计算机的互联网络，在单芯片上集
成大量的计算资源以及连接这些资源的片上通信网络。每个处理机核心具有独立的处理单
元及其私有的 cache，并通过片上通信网络连接在一起，处理机核之间采用消息通信机制，
用路由和分组交换技术替代传统的片上总线来完成通信任务，从而克服由总线互连所带来
的各种瓶颈问题。 
片上网络与传统分布式计算机网络有很多相似
之处，但限于片上资源有限，设计时要考虑更多的开
销限制，针对延时、功耗、面积等性能标准进行优化
设计，为实现高性能片上系统提供高效的通信支持。 
片上网络可以采用多种拓扑结构，如环形拓扑、
网状拓扑、树状拓扑等。图 9.12 显示了一种常用的
二维网状网络(2D Mesh)片上网络结构。片上网络包
括计算子系统和通信子系统两部分。计算子系统由处
理单元(processing element，PE)构成，完成计算任务，
PE 可以是处理机核心，也可以是各种专用功能的硬
件部件或存储器阵列等。通信子系统由交换(switch)
节点(图中缩写为“S”)及节点间的互连线路组成，
负责连接 PE，实现计算资源之间的高速通信。通信节点及其间的互连线所构成的网络就是
片上通信网络。在二维网状网络结构中，每个 PE 与一个交换节点相连，而每个交换节点则
与四个相邻的交换节点和一个 PE 相连，交换节点实现路由功能，并作为每个相邻的 PE 的
网络接口。 
与总线结构和交叉开关互连结构相比，片上网络可以连接更多的计算节点，可靠性高，
可扩展性强，功耗也更低。因此片上网络被认为是更加理想的大规模多核处理机核间互连
技术。这种结构的缺点是硬件结构复杂，且软件改动较大。 
这三种互连结构还可以相互融合，例如，在整体结构上采用片上网络方式，而在局部
选择总线或交叉开关结构，以实现性能与复杂度的平衡。 
由于多核处理机内的各个处理机核之间需要通过中断方式进行通信，所以多核处理机
的中断处理方式也和单核有很大不同。多个处理机核内部的本地中断控制器和负责仲裁各
核之间中断分配的全局中断控制器也需要封装在芯片内部。  
多核系统还需要解决的一个问题就是核之间的同步和互斥。多核处理机上运行的多个
任务会竞争共享资源，因此需要操作系统和硬件配合提供核间同步机制与共享资源互斥访
 

问机制。例如，多核系统硬件应提供“读-修改-写回”的原子操作或其他同步互斥机制，
保证对共享资源的互斥访问。 
5. 低功耗设计 
随着环保理念的普及和移动计算应用的推广，对处理机和整个计算机系统的功耗的关
注度越来越高。低功耗设计是一个多层次的问题，需要同时在操作系统级、算法级、结构
级、电路级等多个层次上综合考虑。 
在单处理机时代，低功耗技术主要在电路层次上进行低功耗设计，注重降低半导体电
路的动态电能消耗和静态电能消耗。 
由于多核处理机在结构和实现上的特点，在多核处理机上可以采用异构结构设计、动
态线程分派与转移技术等降低功耗。异构结构设计就是利用异构多核结构对片上资源进行
优化配置，使处理机在提高性能的同时降低功耗。动态线程分派与转移技术则是在程序运
行时动态地将某个核心上较高的负载转移到负载较小的核心上，从而使处理机在不降低处
理性能的情况下，降低处理机功耗。当整体负载任务较少时，关闭某些核心或降低其处理
机频率也可以使整个系统功耗降低。 
6. 多核软件设计 
虽然多核技术与多处理机有许多相似之处，但二者之间的差别导致在许多情况下多处
理机系统中的软件并不能直接拿到多核系统中运行。在多处理机系统中，各个处理机之间
的界线是非常清晰的，每个处理机基本上都是独立运行的。而在多核系统中，资源的共享
更加普遍。 
由于多核处理机内部有多个核心，因而如何在多个处理机核之间分配任务是必须要解
决的关键问题。因此，支持多核的操作系统必须解决任务分配、任务调度、仲裁以及负载
平衡等问题，必要时还需要支持多核之间的动态任务迁移。 
对于多核处理机，优化操作系统任务调度算法是保证效率的关键。当前关于多核的任
务调度算法主要有全局队列调度和局部队列调度等算法。 
全局队列调度策略由操作系统维护一个全局的任务等待队列，当系统中有某个处理机
核心空闲时，操作系统便从全局任务等待队列中选取就绪任务并开始在此核心上执行。这
种调度策略的优点是处理机核心的利用率较高。 
局部队列调度策略是操作系统为每个处理机核心维护一个局部的任务等待队列，当系
统中有某个处理机核心空闲时，便从该核心的任务等待队列中选取恰当的任务执行。局部
队列调度策略的优点是任务基本上无需在多个处理机核心之间迁移，有利于提高处理机核
心私有 cache 的命中率，缺点是处理机核心的利用率较低。 
目前，大多数支持多核的操作系统采用基于全局队列的任务调度算法。 
从某种程度上说，应用软件的设计是多核系统设计的难点。这是因为，人的自然思维
模式是单任务串行化的，正所谓“一心不能二用”。而多核系统中运行的程序只有按照并行
化的思想设计才可能最大限度地发挥多核处理机的潜能。并行编程困难的问题从并行计算
机产生以来就存在，只是随着多核的主流化，问题更加突出了。虽然多处理机技术和多计
算机技术已经应用多年，但当前的多核计算机系统与以往的并行计算机系统有很大的不同，
以往的并行计算机系统都是应用在服务器或者超级计算中心等适合进行大型并行计算的领
域，这些领域很容易发挥并行计算的优势。而现在的多核计算机系统则是应用到普通用户

的各个层面，甚至是嵌入式系统中，在这些应用场景中实现软件并行编程，难度可能比服
务器和超级计算中心更高。 
多核系统下的并行编程，必须充分发挥多核的线程级并行性，但是已有的编程语言不
能完全适合多核环境，不能将多核的多线程并行潜力充分挖掘出来。为此，需要针对多核
环境下对并行编程应用的要求，对现有的并行编程模式和编程语言(如 OpenMP、MPI、并
行 C 等)进行改进和优化，希望利用编程工具尽可能地帮助程序设计者发掘并行性。 
除了并行编程工具之外，另一个重要问题是并行设计思想。原来运行在单处理机上的
众多应用程序并没有利用多核的性能潜力，其中很多应用程序的线程级加速潜力有限。改
造这些依据串行化思想设计的程序不能单纯依赖并行编程工具，必须将其从单线程的编程
模式改造为并行程序执行模式。所以对于这些应用程序，或者要重新编写并行代码，或者
研发更加先进的面向多核结构的自动并行化工具，使得这些应用程序能在多核处理机系统
中高效运行。 
7. 平衡设计原则 
除了上面讨论的一些多核处理机的关键技术，多核系统设计还必须遵循一个重要的设
计原则，就是平衡设计。 
与单处理机系统相比，多核计算机系统的设计复杂度大幅度提高。因为在解决某个方
面问题的同时往往会带来其他方面的问题，所以多核处理机结构设计的重点不在于其中某
一个细节采用什么复杂或性能表现较好的设计，而是在于整体设计目标。 
因此，在多核系统设计过程中必须仔细权衡对某些问题的解决方法，尽量采用简单、
易于实现、成本低廉而且对整体性能影响不大的设计方案。平衡设计原则是指在芯片的复
杂度、内部结构、性能、功耗、扩展性、部件成本等各个方面做一定的权衡，不能为了单
纯地获得某一方面的性能提升而导致其他方面的问题。在设计过程中要坚持从整体结构的
角度去权衡具体的结构问题。要得到在一个通常情况下，逻辑结构简单并且对大多数应用
程序而言性能优良的处理机结构，为了整体目标往往要牺牲某些局部的最佳设计方案。 

Cortex-A15 MPCore 处理机是 ARM 公司 2010 年 9 月推出的 ARMv7-A 体系结构的多核
产品。借助先进的多核处理机架构，Cortex-A15 MPCore 处理机在高性能产品应用中的运行
主频最高可达 2.5GHz，在提供强大的计算性能的同时，又保持着 ARM 特有的低功耗特性。
该处理机有非常强的可扩展性(scalability)，支持单片 1 至 4 个处理机内核，可广泛应用在
移动计算、高端数字家电、无线基站和企业级基础设施产品等领域。 
1. ARM Cortex-A15 处理机的整体结构 
图 9.13 显示了 ARM Cortex-A15 MPCore 四核处理机的整体结构。 
每个核内部包含支持 ARMv7-A 体系结构的 32 位 CPU，采用超标量、可变长、乱序执行
流水线结构。指令流水线为 15 至 24 级，其中 12 级为按序执行，另外 3 至 12 级为乱序执行。 
Cortex-A15 处理机另外配备支持 IEEE 754 标准的向量浮点运算单元(FPU)，对半精度、
单精度和双精度浮点算法中的浮点操作提供硬件支持。 

ARM 处理机独有的 NEON 媒体处理引擎则为消费类多媒体应用提供灵活强大的加速
功能。NEON 是 ARM Cortex-A 系列处理机上的 128 位单指令流多数据流(SIMD)体系结构
扩展技术，媒体处理引擎扩展了 Cortex-A15 处理机的浮点运算单元 ，支持整数和浮点向
量的 SIMD 运算。NEON 通用 SIMD 引擎旨在加速视频编解码、2D/3D 图形、游戏、音频
和语音处理、图像处理、电话和声音合成等多媒体和信号处理算法，从而明显改善用户
体验。 
Cortex-A15 还为每个处理机核配备了程序跟踪宏单元接口(program trace macrocell 
interface，PTM I/F)，连接至多核调试和跟踪部件。 
Cortex-A15 的每个处理机核内包含 32KB 的 L1 指令 cache 和 32KB 的 L1 数据 cache，
L1 cache 专门针对性能和功耗进行了优化。在高性能应用中，可以通过可配置的 512KB～
4MB 的共享 L2 cache 实现对内存的低延迟、高带宽访问。L1 指令 cache 支持奇偶校验功能，
L1 数据 cache 和 L2 cache 则支持可选的纠错编码(error correction code，ECC)功能，可纠正
单比特错误、检测双比特错误。处理机内还集成了三个独立的 32 表项全相联 L1 转换后援
缓冲器(TLB)，分别用于取指令、读数据和写数据。每个处理机内还包含 512 表项的 4 路
组相联 L2 TLB。 
Cortex-A15 为主存提供了超大寻址空间，40 位物理地址可支持 1TB 的主存空间。 
2. ARM Cortex-A15 的多核支持功能 
Cortex-A15 处理机利用被广泛认可的 ARM MPCore 多核技术，支持性能可扩展性和动
态功耗控制功能。 
1)动态功耗控制 
当配备该处理机的设备需要高性能时，片内的所有处理机核可以全速运行，满足运算

需求，但核间任务分担机制可以平衡各个核的工作负载，以保持尽可能低的功耗。当设备
不需要满负荷运行时，四个处理机核中的任何一个都可以被动态关闭，以降低功耗。 
2)监听控制单元 
监听控制单元(snoop control unit，SCU)提供系统一致性管理功能，负责管理 cache 之
间以及 cache 与系统主存之间的互连和通信，并解决 cache 一致性问题、实现数据传输优先
级仲裁以及其他相关的功能。除了处理机核，Cortex-A15 MPCore 处理机内的其他系统加
速器(如 FPU 和 NEON)和支持非缓冲 DMA 访问的外设也能够利用监听控制单元提供的支
持，以便提高系统级的性能并降低功耗。监听控制单元提供的系统一致性管理功能还可降
低在各个操作系统驱动程序中维持软件一致性的软件复杂度。 
3)加速器一致性端口 
加速器一致性端口是监听控制单元上提供的支持 AMBA 4 AXI(高级可扩展接口，ARM 
推出的第四代 AMBA 接口规范)规范的从设备接口，能够让主设备直接连接到 Cortex-A15
处理机。该接口支持所有标准读操作和写操作，而不需要特别考虑一致性问题。不过，针
对主存一致区域的任何读操作都必须首先与监听控制单元交互，以确认被访问的信息是否
已存储在 L1 cache 中。任何写操作也将首先由监听控制单元进行一致性处理，然后才提交
给存储系统并可在 L2 cache 中分配空间，从而消除直接写入片外主存空间对功耗和性能的
影响。 
4)通用中断控制器 
标准化和结构化的通用中断控制器可以灵活地支持处理机核间通信功能，实现系统中
断的优先级仲裁及其在处理机核之间的分配。中断控制器最多支持 224 个独立中断源。在
软件控制下，每个中断均可在处理机核之间调配，进行硬件优先级排队。 

2012 年 4 月，英特尔在北京发布了多款基于 Ivy Bridge(简称 IVB)微架构(micro- 
architecture)的第三代智能酷睿(Core i)系列处理机，是当时业界制造工艺最为先进的处理
机。2011 年推出的采用 32nm 半导体工艺的第二代智能酷睿处理机微架构 Sandy Bridge 处
理机实现了处理机核、图形核心、视频引擎的单芯片封装。与 Sandy Bridge(简称 SNB)相
比，Ivy Bridge 对处理机架构没有做太大调整，但采用更加先进的 22nm 制造工艺，并结合
3D 晶体管技术，在大幅度提高晶体管密度的同时，处理机片上的图形核心的执行单元的数
量翻一番，核芯显卡等部分性能有了一倍以上的提升。制造工艺的改进带来更小的核心面
积、更低的功耗以及更加容易控制的发热量。 
1. 酷睿多核处理机的整体结构 
Ivy Bridge 微架构处理机由处理核心、三级 cache、图形核心、内存控制器、系统助手
(system agent)、显示控制器、显示接口、PCI-E I/O 控制器、DMI 总线控制器等众多模块整
合而成。Ivy Bridge 微架构处理机采用模块化设计，有很强的可扩展性，支持多种不同主处
理机核心数、不同性能的图形核心和 cache 容量的组合配置。 
从 Sandy Bridge 微架构开始，每个处理机内部处理除了中央处理机核之外，还集成了
图形处理单元(GPU)核。这种与中央处理机封装在同一芯片上的图形处理单元又称为核芯
显卡。Sandy Bridge 和 Ivy Bridge 处理机上的处理机核和图形处理核采用完全融合的方式，

在同一块晶圆中分别划分出 CPU 区域和 GPU 区域，CPU 和 GPU 各自承担数据处理与图形
处理任务。这种整合设计大大降低了处理机核、图形处理核、内存及内存控制器间的数据
周转时间，可有效提升处理效能并大幅降低芯片组的整体功耗。在 Ivy Bridge 系列处理机中
包含了两种集成 GPU 核：GT1 和 GT2。GT1 有 6 个执行单元(execution unit，EU)和 24 个
算术逻辑单元(ALU)及一个纹理单元。GT2 有 16 个执行单元、64 个 ALU 和 2 个纹理单元。 
处理机内的各个 CPU 核之外还集成了最后一级 cache(last-level Cache，LLC)，即与主
存储器直接相连的 L3 cache。 
目前发布的 Ivy Bridge 微架构有 4 种设计版本：4 个中央处理机核心+8MB 缓存+ GT2
图形核心；2 个中央处理机核心+4MB 缓存+ GT1 图形核心；4 个中央处理机核心+6MB 缓
存+GT1 图形核心；2 个中央处理机核心+4MB 缓存+GT1 图形核心。图 9.14(a)～(d)分别显
示了 Ivy Bridge 微架构支持的四种配置。 

2. 酷睿多核处理机的环形总线 
图 9.15 显示了 Ivy Bridge 四核处理机的完整体系结构。图中可以看出，Ivy Bridge 微架
构使用全新的环形总线(ring bus)结构连接各个 CPU 核、最后一级 cache、图形处理单元
(GPU)以及系统助手等模块。 
系统助手从功能上类似以前的北桥芯片，但包含了更为丰富的功能，包括集成内存控
制器、支持 16 条 PCI-E 2.0 通道的 PCI-E 控制器、显示控制器、电源控制单元(PCU)以及
DMI 总线(英特尔开发用于连接主板南北桥的总线)的 IO 接口等。 
环形总线由四条独立的环组成，分别是数据环(data ring)、请求环(request ring)、响应
环(acknowledge ring)和监听环(snoop ring)。借助于环形总线，CPU 与 GPU 可以共享 LLC 
cache，从而大幅度提升 GPU 的性能。在环形总线上分布着多个环节点(ring stop)。环节点
在每个 CPU 核、GPU 核或最后一级 cache 上有两个连接点。 

在以往的产品中，多个核心共享一个最后一级 cache，核心需要访问 cache 时必须先经
过流水线发送请求，再进行优先级排队后才能进行。环形总线则可以大大减少核心访问最
后一级 cache 的时间延迟。环形总线将最后一级 cache 分割成了若干部分，环形总线上的每
个节点与其相邻的另两个节点采用点到点的连接方式，故环形总线是由多个子环组成的。
借助于每个环节点，核心可以快速访问最后一级 cache。又由于每个核心与最后一级 cache
之间可以实现并行访问，使得整体带宽可以显著提升。 

为了满足人类社会对计算性能的无止境需求，处理机内部的核心数量不断增加。当处
理机内的核心的数量超过 32 个时，称为众核(many-core)处理机。 
2012 年，英特尔公司发布了基于英特尔集成众核(many integrated core，MIC)架构的至
强融核(XEON phi)产品。 
英特尔集成众核处理机可作为中央处理机的协处理机工作，可通过 PCI-E 总线连接到
配置英特尔至强(XEON)处理机的主机上，是高度优化、高度并行的协处理机，其运算性能
超过每秒一百万亿次浮点双精度持续计算。至强融核使用开源 Linux 操作系统和通用源代
码，可运行完整的应用程序，用于高度并行的计算密集型负荷，采用和至强处理机一致的
通用编程模型与软件工具。 
至强处理机与一颗或多颗至强融核处理机构成异构多处理机架构，而至强融核本身则
在单芯片内集成了 57～61 个处理核心(向量 IA 内核)。 
图 9.16 显示了至强融核众核处理机的微架构。处理机片上环形互连总线连接众多的计
算核心、8 个支持 GDDR5 的存储器控制器(MC)和 1 个 PCI-E 终端逻辑单元(PCIe client 
logic)。每个计算核心支持四个硬件线程，支持 U、V 两条七级指令流水线，双指令发射、
按序执行，故每个时钟周期可以执行两条指令。计算核心通过环形总线接口(CRI)与互连总
线相连。环形总线接口由 L2 cache 和分布式标签目录(tag directory, TD)组成，后者为每个核
心的 L2 cache 建立标识目录副本，从而全局监视所有核心的 L2 cache，确保 cache 一致性。 
虽然至强融核处理机的每个计算核心的主频只有 1～1.25GHz，处理能力不算强大，但
在运行高性能计算应用时，可以将高度并行的计算任务分解成更小的子任务，采用 SIMD

方式分布到众多核心中并行运行，而高速的至强处理机主机上则可运行最低限度的串行代
码。依靠众核架构，系统能够获得额外的性能提升。 
 

龙芯(Loongson)3 号是中国科学院计算技术研究所研发的国产多核处理机系列产品，集
高性能、低成本和低功耗于一身，主要面向服务器和高性能计算应用。龙芯 3 号单芯片内
集成多个高性能 64 位超标量通用处理机核以及大容量 L2 cache，并通过高速 I/O 接口实现
多芯片互连，以组成更大规模的系统。龙芯 3 号尤其可以满足国家安全需求。首台采用龙
芯 3A 处理机的万亿次高性能计算机 KD-60 于 2010 年 4 月通过鉴定，实现了我国高性能计
算机国产化的重大突破。 
1. 龙芯 3A 处理机的整体结构 
龙芯 3A 是龙芯 3 号多核处理机系列的第一款产品，每个处理机芯片集成 4 颗 64 位的
四发射超标量 GS464 高性能处理机核，最高工作主频为 1GHz。片内集成 4 MB 的分体共享
L2 cache(由 4 个体模块组成，每个体模块容量为 1MB)。处理机内部通过目录协议维护多核
及 I/O DMA 访问的 cache 一致性。处理机芯片内还集成了 DDR2/DDR3 存储器控制器、
Hyper-Transport(HT)控制器、PCI-X/PCI 总线控制器、LPC、UART、SPI 等外围接口部件。
图 9.17 显示了龙芯 3A 四核处理机的整体结构。 
每个处理机有两级 AXI 交叉开关。第一级互连采用 6×6 的 AXI 交叉开关(X1 Switch)，
连接 P0、P1、P2 和 P3 四个处理机核心(作为主设备)，统一编址的 S0、S1、S2 和 S3 四个 L2 cache
模块(作为从设备)，以及两个 I/O 端口(每个端口使用一个主端口和一个从端口)。每个 I/O
端口通过一个 DMA 控制器连接一个 16 位的 HT 控制器(每个 16 位的 HT 端口可以拆分成
两个 8 位的 HT 端口使用)。第二级互连采用 5×4 的交叉开关(X2 Switch)，连接四个 L2 cache
第 9 章  并行组织与结构       309 
模块、两个 DDR2 存储器控制器(MC)和 I/O 接口(包括 PCI、LPC、SPI 等)以及芯片内部
的控制寄存器模块。两级互连开关都采用读写分离的数据通道，数据通道宽度为 128 位，
工作频率与处理机核相同，用于提供高速的片上数据传输。 
2. 龙芯 3 号的 GS464 处理机核心 
GS464 是一款实现 64 位 MIPS64 指令系统及龙芯扩展指令系统的通用 RISC 处理机 IP
核。GS464 有两个定点运算部件、两个浮点运算部件和一个访存部件。每个浮点部件都可
以全流水地执行 64 位双精度浮点乘加操作。GS464 的指令流水线在每个时钟周期取四条指
令进行译码，并且动态地发射到五个全流水的功能部件中。指令按序发射，乱序执行。GS464
的基本结构如图 9.18 所示。 
GS464 的基本流水线包括取指、预译码、译码、寄存器重命名、调度、发射、读寄存
器、执行、提交等 9 级，各个流水级的功能如下。 
(1)取指流水级：根据程序计数器 PC 的值访问指令 cache 和指令 TLB，如果指令 cache
和指令 TLB 都命中，则把四条新的指令取到指令寄存器 IR 中。 
(2)预译码流水级：主要对转移指令进行译码并预测跳转的方向。 
(3)译码流水级：把 IR 中的四条指令转换成 GS464 内部指令格式送往寄存器重命名    
模块。 
(4)寄存器重命名流水级：为逻辑目标寄存器分配新的物理寄存器，并将逻辑源寄存器
映射到最近分配给该逻辑寄存器的物理寄存器。 
(5)调度流水级：将重命名的指令分配到定点或浮点保留站中等待执行，同时送到重排
序队列中用于执行后的顺序提交；此外，转移指令和访存指令还分别被送往转移队列和访
存队列。 
(6)发射流水级：从定点或浮点保留站中为每个功能部件选出一条所有操作数都准备好
的指令；在重命名时操作数没准备好的指令将等待其操作数准备好。 

(7)读寄存器流水级：为发射的指令从物理寄存器堆中读取相应的源操作数送到相应的
功能部件。 
(8)执行流水级：根据指令的类型执行指令并把计算结果写回寄存器堆。 
(9)提交流水级：按照重排序队列记录的指令顺序提交已经执行完的指令，GS464 最多
每拍可以提交四条指令。 
GS464 的 L1 cache 由 64KB 的指令 cache 和 64KB 的数据 cache 组成，均采用四路组相
联结构。GS464 的 TLB 有 64 项，采用全相联结构。GS464 支持 128 位的访存操作，其虚
地址和物理地址均为 48 位。 
3. 龙芯 3A 处理机的互连结构 
龙芯 3A 采用可扩展的互连结构，片内二维 Mesh 网络利用 AXI 交叉开关进行片内核间
互连，片间通过 HT 接口进行可伸缩互连，构建多处理机系统。 
图 9.19 显示了四颗龙芯 3A 处理机构成的 2×2 Mesh 网络结构。系统由 16 个处理机核
心构成。全系统统一编址，硬件自动维护各处理机间的数据一致性。互连系统的物理实现
对软件透明，不同配置的系统可以运行相同的操作系统。 
本 章 小 结 
并行性是指计算机系统具有同时进行运算或操作的特性，它包括同时性(两个以上事件
在同一时刻发生)与并发性(两个以上事件在同一时间间隔内发生)两种含义。并行性的 4 种
技术是：①时间并行(时间重叠)；②空间并行(资源重复)；③时间并行+空间并行；④资源
共享(软件方法)。 
Flynn 将计算机体系结构分为 SISD、SIMD、MISD 和 MIMD 四种类型。虽然 MISD 没
有实际机器，但是四种类型的分类方法确实纲目清晰，有利于认识计算机系统的总体结构。 
传统单处理机依靠超标量技术和超长指令字技术提高指令级并行性，而多线程技术和超线
程技术则把重点放在线程级并行性上，在处理机内部增加少量部件，将一个物理处理机模拟成
多个逻辑处理机，从而减少访存延迟造成的执行部件浪费，提高处理机内部资源的使用率。 
多处理机属于 MIMD 结构，是传统上为提高作业级或任务级并行性所采用的并行体系
结构。多处理机系统由多台独立的处理机组成，通过通信网络或共享存储器进行通信，共
同完成处理任务。SMP 是多处理机的常见形式，组成 SMP 的每台处理机的能力都完全相同。 
多核处理机在一个处理机芯片内集成多个完整的计算引擎(内核)，通过开发程序内的
线程级或进程级并行性提高性能。多核处理机具有高并行性、高通信效率、高资源利用率、
低功耗、低设计复杂度、低成本等优势。可以根据多个核心的物理特征把多核系统分为同
构多核和异构多核，也可以在逻辑上把多核系统分为 SMP 结构和 AMP 结构。SMP 向上提
供了一个完整的运行平台，上层应用程序不需要意识到多核的存在，而 AMP 必须由应用程
序来对各个核心分配任务。多核系统必须解决核间通信、cache 一致性等诸多问题。 
